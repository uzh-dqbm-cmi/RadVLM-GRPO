+ cat /var/spool/slurmd/job527619/slurm_script
#!/bin/bash
#SBATCH -A a135
#SBATCH --job-name=radvlm-sft  # Job name
#SBATCH --nodes=32    # Number of nodes
#SBATCH --ntasks-per-node=1                  # Number of tasks per node (1 process per node)
#SBATCH --gpus-per-task=4                  # Number of GPUs per ta
#SBATCH --time=06:00:00                      # Time limit
#SBATCH --output=job_outputs/%x.txt    # Standard output and error log
#SBATCH --mem=460000
#SBATCH --partition=normal  

# Initialization
set -x
cat $0
export MASTER_PORT=29500
#export MASTER_ADDR=$(hostname)
export MASTER_ADDR=$(scontrol show hostname | head -n 1)
export HF_HOME=$SCRATCH/huggingface_home

PROMPT_VERSION="qwen_1_5"

RUN_NAME=${SLURM_JOB_NAME}
echo "RUN_NAME: ${RUN_NAME}"
CKPT_PATH="lmms-lab/llava-onevision-qwen2-7b-si" # this could also be the previous stage checkpoint

NUM_EPOCHS=1
LR=1e-5
SAVE_STEPS=200

WORKDIR="$SCRATCH/code/RadVLM"

# Run main script
srun -ul  --environment=llava_env_clariden bash -c "
  pip install accelerate==0.28.0
  cd $WORKDIR  # Change cwd and run the main training script.
  export PYTHONPATH=$WORKDIR/finetuning
  export WANDB_API_KEY=81291d9e2d99efeb2a4e3f4d507abe879e646a22
  TORCHRUN_ARGS=\"
   --node-rank=\${SLURM_PROCID} \
   --master-addr=\${MASTER_ADDR} \
   --master-port=\${MASTER_PORT} \
   --nnodes=\${SLURM_NNODES} \
   --nproc-per-node=4 \
  \"

 ACCELERATE_CPU_AFFINITY=1  torchrun \${TORCHRUN_ARGS} finetuning/llava/train/train_mem.py \
    --deepspeed finetuning/scripts/zero3.json \
    --model_name_or_path $CKPT_PATH \
    --version ${PROMPT_VERSION} \
    --data_path radvlm/data/llava_datasets/combined.json \
    --image_folder . \
    --mm_tunable_parts="mm_vision_tower,mm_mlp_adapter,mm_language_model" \
    --mm_vision_tower_lr=2e-6 \
    --vision_tower google/siglip-so400m-patch14-384 \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --group_by_modality_length True \
   --image_aspect_ratio anyres_max_9 \
    --image_grid_pinpoints \"(1x1),...,(6x6)\" \
    --mm_patch_merge_type spatial_unpad \
    --bf16 True \
    --run_name $RUN_NAME \
    --output_dir="$SCRATCH/checkpoints/${RUN_NAME}" \
    --num_train_epochs $NUM_EPOCHS \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 2 \
    --evaluation_strategy \"no\" \
    --save_strategy \"steps\" \
    --save_steps $SAVE_STEPS \
    --save_total_limit 1 \
    --learning_rate $LR \
    --warmup_ratio 0.03 \
    --lr_scheduler_type \"cosine\" \
    --weight_decay 0. \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 32768 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb \
    --torch_compile True \
    --torch_compile_backend \"inductor\" \
    --dataloader_drop_last True \
    --frames_upbound 32
"


+ export MASTER_PORT=29500
+ MASTER_PORT=29500
++ scontrol show hostname
++ head -n 1
+ export MASTER_ADDR=nid006495
+ MASTER_ADDR=nid006495
+ export HF_HOME=/capstor/scratch/cscs/ndeperr/huggingface_home
+ HF_HOME=/capstor/scratch/cscs/ndeperr/huggingface_home
+ PROMPT_VERSION=qwen_1_5
+ RUN_NAME=radvlm-sft
+ echo 'RUN_NAME: radvlm-sft'
RUN_NAME: radvlm-sft
+ CKPT_PATH=lmms-lab/llava-onevision-qwen2-7b-si
+ NUM_EPOCHS=1
+ LR=1e-5
+ SAVE_STEPS=200
+ WORKDIR=/capstor/scratch/cscs/ndeperr/code/RadVLM
+ srun -ul --environment=llava_env_clariden bash -c '
  pip install accelerate==0.28.0
  cd /capstor/scratch/cscs/ndeperr/code/RadVLM  # Change cwd and run the main training script.
  export PYTHONPATH=/capstor/scratch/cscs/ndeperr/code/RadVLM/finetuning
  export WANDB_API_KEY=81291d9e2d99efeb2a4e3f4d507abe879e646a22
  TORCHRUN_ARGS="
   --node-rank=${SLURM_PROCID}    --master-addr=${MASTER_ADDR}    --master-port=${MASTER_PORT}    --nnodes=${SLURM_NNODES}    --nproc-per-node=4   "

 ACCELERATE_CPU_AFFINITY=1  torchrun ${TORCHRUN_ARGS} finetuning/llava/train/train_mem.py     --deepspeed finetuning/scripts/zero3.json     --model_name_or_path lmms-lab/llava-onevision-qwen2-7b-si     --version qwen_1_5     --data_path radvlm/data/llava_datasets/combined.json     --image_folder .     --mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model     --mm_vision_tower_lr=2e-6     --vision_tower google/siglip-so400m-patch14-384     --mm_projector_type mlp2x_gelu     --mm_vision_select_layer -2     --mm_use_im_start_end False     --mm_use_im_patch_token False     --group_by_modality_length True    --image_aspect_ratio anyres_max_9     --image_grid_pinpoints "(1x1),...,(6x6)"     --mm_patch_merge_type spatial_unpad     --bf16 True     --run_name radvlm-sft     --output_dir=/capstor/scratch/cscs/ndeperr/checkpoints/radvlm-sft     --num_train_epochs 1     --per_device_train_batch_size 1     --per_device_eval_batch_size 2     --gradient_accumulation_steps 2     --evaluation_strategy "no"     --save_strategy "steps"     --save_steps 200     --save_total_limit 1     --learning_rate 1e-5     --warmup_ratio 0.03     --lr_scheduler_type "cosine"     --weight_decay 0.     --logging_steps 1     --tf32 True     --model_max_length 32768     --gradient_checkpointing True     --dataloader_num_workers 4     --lazy_preprocess True     --report_to wandb     --torch_compile True     --torch_compile_backend "inductor"     --dataloader_drop_last True     --frames_upbound 32
'
 4: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
28: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 6: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
23: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
25: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 2: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 8: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
16: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
31: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 9: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 0: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
30: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
26: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
13: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
17: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
20: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
22: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
11: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
18: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
15: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 7: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
21: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
19: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
12: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
14: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
24: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
10: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 3: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 5: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
29: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
27: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
 1: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
23: Collecting accelerate==0.28.0
 6: Collecting accelerate==0.28.0
11: Collecting accelerate==0.28.0
23:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
23: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
23: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
23: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
23: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
23: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
23: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
23: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
21: Collecting accelerate==0.28.0
 3: Collecting accelerate==0.28.0
 5: Collecting accelerate==0.28.0
 4: Collecting accelerate==0.28.0
10: Collecting accelerate==0.28.0
19: Collecting accelerate==0.28.0
26: Collecting accelerate==0.28.0
20: Collecting accelerate==0.28.0
 6:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
23: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
23: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
23: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
23: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
23: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
23: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 6: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 6: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 6: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 6: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 6: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 6: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
11:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 6: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
28: Collecting accelerate==0.28.0
11: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
11: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
11: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
11: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
11: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
11: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
11: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 6: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 6: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 6: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 6: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 6: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 6: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
23: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
23: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
11: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
11: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
11: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
11: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
11: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
11: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 6: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 6: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 2: Collecting accelerate==0.28.0
17: Collecting accelerate==0.28.0
11: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
11: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 7: Collecting accelerate==0.28.0
16: Collecting accelerate==0.28.0
21:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
12: Collecting accelerate==0.28.0
21: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
21: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
21: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
21: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
21: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
21: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 5:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
21: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 4:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 3:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
24: Collecting accelerate==0.28.0
31: Collecting accelerate==0.28.0
27: Collecting accelerate==0.28.0
 4: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 4: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 4: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 5: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 4: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 5: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 4: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 5: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 4: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 5: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 4: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 5: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
15: Collecting accelerate==0.28.0
 5: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 5: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
21: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
21: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 3: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
21: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 3: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
21: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
21: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 3: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
21: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 3: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 3: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 3: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 3: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
22: Collecting accelerate==0.28.0
 4: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 4: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 4: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
19:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 4: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
10:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 0: Collecting accelerate==0.28.0
 4: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
26:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 4: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 5: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 5: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 8: Collecting accelerate==0.28.0
 5: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 5: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 5: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 5: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
20:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 3: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 3: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 3: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 3: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 3: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 3: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
21: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
21: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
19: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
26: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
19: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
10: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
19: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
26: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
10: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
19: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
26: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
10: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
26: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
19: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
10: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
19: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
26: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
10: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
19: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
26: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
10: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
26: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
10: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
20: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
20: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
20: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
20: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
20: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
20: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
20: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 4: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 4: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 5: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 5: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
28:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 3: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 3: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
26: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
19: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
30: Collecting accelerate==0.28.0
26: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
19: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
10: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
26: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
19: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
26: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
10: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
19: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
26: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
10: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
19: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
26: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
10: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
19: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
10: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
10: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
20: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
20: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
20: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
20: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
20: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
20: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
28: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
28: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
28: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
28: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
28: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
28: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
28: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
13: Collecting accelerate==0.28.0
 6: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
11: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
23: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 4: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
21: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 5: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
29: Collecting accelerate==0.28.0
14: Collecting accelerate==0.28.0
26: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
26: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
19: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 3: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
19: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
10: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
10: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
17:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
28: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
28: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
28: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
20: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
28: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
20: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
28: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
28: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
17: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
17: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
17: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
17: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
17: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
17: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 2:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
17: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 6: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 4: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
11: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 6: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
21: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
23: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 5: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 4: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 3: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 6: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
11: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
21: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
23: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 4: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 5: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 6: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
11: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 3: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
21: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 4: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
23: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 5: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
11: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 3: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
21: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
23: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 5: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 3: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
28: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
28: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
26: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
19: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
10: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 2: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 2: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 2: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
20: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 2: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 2: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 2: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 2: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
26: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
19: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
26: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
17: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
26: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
19: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
17: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
26: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
19: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
17: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
19: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
17: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
10: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
17: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
17: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
10: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
18: Collecting accelerate==0.28.0
10: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
10: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
16:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
20: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
20: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
12:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
20: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
20: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 7:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 6: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 4: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
11: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
23: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
21: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 5: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 3: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
26: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
19: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
20: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
10: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 2: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 2: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 2: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 2: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 2: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 2: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
16: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
16: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
16: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
16: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
12: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
16: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
16: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
12: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
16: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
12: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
28: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 7: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
12: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 7: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
12: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 7: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
12: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 7: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
12: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 7: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 7: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 7: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
17: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
17: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
28: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
28: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
28: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
28: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
27:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
15:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
28: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
31:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
24:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
16: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
16: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
16: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
16: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
16: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 2: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
12: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
16: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 7: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 2: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
12: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
12: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 7: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
12: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 7: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
12: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 7: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
12: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 7: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 7: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
27: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
27: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
27: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
27: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
15: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
27: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
15: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
27: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
15: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
27: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
15: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
15: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
31: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
15: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
15: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
31: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 8:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
31: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
31: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
31: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
24: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
31: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
24: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
31: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
24: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
24: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
24: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
24: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
24: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
17: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 0:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
22:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
17: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
17: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
16: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
17: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
16: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
17: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 8: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 8: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 8: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 7: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 8: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
12: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 7: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 8: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
12: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
27: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 8: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 8: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
27: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
15: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
17: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
27: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
15: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
27: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
15: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
27: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
15: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
27: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 2: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
15: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
15: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
31: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
31: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
31: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
31: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
24: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
31: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
31: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
24: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
24: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
24: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
24: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
24: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 2: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 0: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
30:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 2: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 0: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 0: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 2: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 0: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 2: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 0: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
22: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 0: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
22: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 0: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
22: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
22: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
22: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
22: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 2: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
22: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 8: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 8: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 8: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 8: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 1: Collecting accelerate==0.28.0
 8: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 8: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
30: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
30: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
30: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
30: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
27: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
30: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
27: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
30: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
15: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
30: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
16: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
15: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 7: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 0: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
31: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
12: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 0: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
31: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 0: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 0: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
22: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 0: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
24: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 0: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
22: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
24: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
22: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
22: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
22: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
22: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
16: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
16: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
16: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
13:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
16: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 7: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
12: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 7: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
25: Collecting accelerate==0.28.0
12: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 7: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 7: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
12: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
12: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
16: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 7: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
12: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
30: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
30: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 8: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
30: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
30: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 8: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
14:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
30: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
30: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
29:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
13: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
13: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
13: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
13: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
13: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
13: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
13: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 0: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 0: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
22: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
27: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
22: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
15: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
31: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
14: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
14: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
24: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
14: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
14: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
14: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
14: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
14: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
29: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
29: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
29: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
29: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
27: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
15: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
29: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
27: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
15: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
29: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
27: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
15: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
29: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
27: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
15: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
31: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
24: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
31: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
24: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
27: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
31: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
15: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
24: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
30: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
31: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
24: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
30: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
13: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
31: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
13: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
24: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
13: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
13: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
13: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
13: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 8: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
14: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
14: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
14: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
14: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
14: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 9: Collecting accelerate==0.28.0
 8: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
14: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
29: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 8: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
29: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 8: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
29: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 8: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
29: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
29: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
29: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
22: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 8: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 0: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 5: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 5: [?25l
26: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 6: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
26: [?25l
 6: [?25l
11: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 3: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
21: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
23: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
11: [?25l
 3: [?25l
21: [?25l
23: [?25l
 5:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
26:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 4: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 6:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 4: [?25l
10: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
11:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
16: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 3:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
21:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
10: [?25l
23:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
28: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
16: [?25l
19: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
12: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
28: [?25l
17: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
12: [?25l
19: [?25l
 4:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
17: [?25l
 7: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
10:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
16:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 7: [?25l
28:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
19:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
12:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
22: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
17:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 0: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
18:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
22: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 0: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
22: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 7:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 0: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
22: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
20: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 0: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 2: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
20: [?25l
 2: [?25l
20:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 2:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
13: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
22: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 0: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
13: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
30: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
30: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
30: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
14: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
30: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
14: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
30: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
29: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
29: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
18: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
30: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
18: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
18: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
27: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
18: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
27: [?25l
18: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
18: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
18: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
31: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
31: [?25l
27:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
15: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
15: [?25l
24: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
24: [?25l
31:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
15:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
24:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 8: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 8: [?25l
13: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 8:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
18: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
18: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
18: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
18: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
18: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
18: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
14: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
13: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
13: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
13: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
13: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
29: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
22: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
22: [?25l
13: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
22:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 0: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
14: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 0: [?25l
 5: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
 5: [?25h
14: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
14: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
26: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
26: [?25h
14: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
21: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.3 MB/s[0m eta [36m0:00:00[0m
21: [?25h
 6: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.2 MB/s[0m eta [36m0:00:00[0m
 6: [?25h
 0:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 3: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
 3: [?25h
23: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
23: [?25h
11: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
11: [?25h
29: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 4: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
 4: [?25h
29: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
29: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
10: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
10: [?25h
28: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
28: [?25h
14: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
29: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
16: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
16: [?25h
17: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.7 MB/s[0m eta [36m0:00:00[0m
17: [?25h
19: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.8 MB/s[0m eta [36m0:00:00[0m
19: [?25h
29: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 7: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
 7: [?25h
12: [2K   [38;5;197m[0m[38;5;197m[0m[38;5;237m[0m [32m122.9/290.1 kB[0m [31m5.9 MB/s[0m eta [36m0:00:01[0m
20: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
20: [?25h
 2: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
 2: [?25h
18: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
18: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
12: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
12: [?25h
27: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
27: [?25h
31: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
31: [?25h
15: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
15: [?25h
24: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.8 MB/s[0m eta [36m0:00:00[0m
24: [?25h
 1:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
30: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
30: [?25l
30:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 8: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
 8: [?25h
13: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
13: [?25l
13:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
29: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
29: [?25l
29:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
18: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 1: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 1: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 1: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
25:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
 1: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
22: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
22: [?25h
 1: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
14: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
14: [?25l
 1: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 1: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
14:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
18: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
18: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
18: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
18: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 0: [2K   [38;5;197m[0m[38;5;237m[0m[38;5;237m[0m [32m276.5/290.1 kB[0m [31m8.3 MB/s[0m eta [36m0:00:01[0m
 0: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.4 MB/s[0m eta [36m0:00:00[0m
 0: [?25h
18: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
25: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
25: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
25: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
25: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
25: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
25: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
25: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
 1: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 1: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 1: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 1: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 1: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 1: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 9:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
25: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
25: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
25: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
25: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
25: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
25: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
30: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.1 MB/s[0m eta [36m0:00:00[0m
30: [?25h
13: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.2 MB/s[0m eta [36m0:00:00[0m
13: [?25h
 9: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
 9: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
 9: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
 9: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
 9: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
 1: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
 9: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
 1: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 9: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
29: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
29: [?25h
18: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
18: [?25l
18:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
14: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
14: [?25h
25: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
25: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
 9: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
 9: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
 9: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
 9: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
 9: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
 9: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
 1: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
25: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 1: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 1: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 1: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 1: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 1: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 9: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
25: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 9: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
25: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
25: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
25: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
25: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
18: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.6 MB/s[0m eta [36m0:00:00[0m
18: [?25h
 9: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
 9: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
 9: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
 9: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
 9: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
 9: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
 1: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 1: [?25l
 1:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
25: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
25: [?25l
25:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 9: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
 9: [?25l
 9:    [38;5;237m[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
 1: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
 1: [?25h
25: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
25: [?25h
 9: [2K   [38;5;70m[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
 9: [?25h
30: Installing collected packages: accelerate
30:   Attempting uninstall: accelerate
 4: Installing collected packages: accelerate
 4:   Attempting uninstall: accelerate
14: Installing collected packages: accelerate
23: Installing collected packages: accelerate
23:   Attempting uninstall: accelerate
14:   Attempting uninstall: accelerate
25: Installing collected packages: accelerate
18: Installing collected packages: accelerate
25:   Attempting uninstall: accelerate
18:   Attempting uninstall: accelerate
21: Installing collected packages: accelerate
20: Installing collected packages: accelerate
28: Installing collected packages: accelerate
21:   Attempting uninstall: accelerate
20:   Attempting uninstall: accelerate
28:   Attempting uninstall: accelerate
30:     Found existing installation: accelerate 1.7.0
 4:     Found existing installation: accelerate 1.7.0
22: Installing collected packages: accelerate
14:     Found existing installation: accelerate 1.7.0
23:     Found existing installation: accelerate 1.7.0
22:   Attempting uninstall: accelerate
25:     Found existing installation: accelerate 1.7.0
18:     Found existing installation: accelerate 1.7.0
21:     Found existing installation: accelerate 1.7.0
20:     Found existing installation: accelerate 1.7.0
28:     Found existing installation: accelerate 1.7.0
22:     Found existing installation: accelerate 1.7.0
15: Installing collected packages: accelerate
15:   Attempting uninstall: accelerate
15:     Found existing installation: accelerate 1.7.0
25:     Uninstalling accelerate-1.7.0:
23:     Uninstalling accelerate-1.7.0:
 4:     Uninstalling accelerate-1.7.0:
28:     Uninstalling accelerate-1.7.0:
20:     Uninstalling accelerate-1.7.0:
18:     Uninstalling accelerate-1.7.0:
30:     Uninstalling accelerate-1.7.0:
21:     Uninstalling accelerate-1.7.0:
26: Installing collected packages: accelerate
14:     Uninstalling accelerate-1.7.0:
29: Installing collected packages: accelerate
26:   Attempting uninstall: accelerate
29:   Attempting uninstall: accelerate
22:     Uninstalling accelerate-1.7.0:
26:     Found existing installation: accelerate 1.7.0
29:     Found existing installation: accelerate 1.7.0
15:     Uninstalling accelerate-1.7.0:
 6: Installing collected packages: accelerate
 6:   Attempting uninstall: accelerate
 6:     Found existing installation: accelerate 1.7.0
17: Installing collected packages: accelerate
17:   Attempting uninstall: accelerate
17:     Found existing installation: accelerate 1.7.0
 8: Installing collected packages: accelerate
 8:   Attempting uninstall: accelerate
 8:     Found existing installation: accelerate 1.7.0
24: Installing collected packages: accelerate
24:   Attempting uninstall: accelerate
12: Installing collected packages: accelerate
12:   Attempting uninstall: accelerate
 1: Installing collected packages: accelerate
 1:   Attempting uninstall: accelerate
24:     Found existing installation: accelerate 1.7.0
12:     Found existing installation: accelerate 1.7.0
 1:     Found existing installation: accelerate 1.7.0
 7: Installing collected packages: accelerate
 7:   Attempting uninstall: accelerate
10: Installing collected packages: accelerate
10:   Attempting uninstall: accelerate
 7:     Found existing installation: accelerate 1.7.0
10:     Found existing installation: accelerate 1.7.0
11: Installing collected packages: accelerate
11:   Attempting uninstall: accelerate
11:     Found existing installation: accelerate 1.7.0
26:     Uninstalling accelerate-1.7.0:
 2: Installing collected packages: accelerate
 2:   Attempting uninstall: accelerate
16: Installing collected packages: accelerate
 2:     Found existing installation: accelerate 1.7.0
16:   Attempting uninstall: accelerate
16:     Found existing installation: accelerate 1.7.0
 5: Installing collected packages: accelerate
 5:   Attempting uninstall: accelerate
19: Installing collected packages: accelerate
 3: Installing collected packages: accelerate
 3:   Attempting uninstall: accelerate
19:   Attempting uninstall: accelerate
 6:     Uninstalling accelerate-1.7.0:
 5:     Found existing installation: accelerate 1.7.0
19:     Found existing installation: accelerate 1.7.0
27: Installing collected packages: accelerate
27:   Attempting uninstall: accelerate
31: Installing collected packages: accelerate
31:   Attempting uninstall: accelerate
 3:     Found existing installation: accelerate 1.7.0
29:     Uninstalling accelerate-1.7.0:
 0: Installing collected packages: accelerate
 0:   Attempting uninstall: accelerate
27:     Found existing installation: accelerate 1.7.0
17:     Uninstalling accelerate-1.7.0:
31:     Found existing installation: accelerate 1.7.0
 9: Installing collected packages: accelerate
 9:   Attempting uninstall: accelerate
 0:     Found existing installation: accelerate 1.7.0
13: Installing collected packages: accelerate
 9:     Found existing installation: accelerate 1.7.0
13:   Attempting uninstall: accelerate
13:     Found existing installation: accelerate 1.7.0
 8:     Uninstalling accelerate-1.7.0:
24:     Uninstalling accelerate-1.7.0:
12:     Uninstalling accelerate-1.7.0:
 7:     Uninstalling accelerate-1.7.0:
10:     Uninstalling accelerate-1.7.0:
11:     Uninstalling accelerate-1.7.0:
 2:     Uninstalling accelerate-1.7.0:
16:     Uninstalling accelerate-1.7.0:
19:     Uninstalling accelerate-1.7.0:
 3:     Uninstalling accelerate-1.7.0:
31:     Uninstalling accelerate-1.7.0:
 1:     Uninstalling accelerate-1.7.0:
 5:     Uninstalling accelerate-1.7.0:
 9:     Uninstalling accelerate-1.7.0:
 0:     Uninstalling accelerate-1.7.0:
13:     Uninstalling accelerate-1.7.0:
27:     Uninstalling accelerate-1.7.0:
28:       Successfully uninstalled accelerate-1.7.0
25:       Successfully uninstalled accelerate-1.7.0
21:       Successfully uninstalled accelerate-1.7.0
20:       Successfully uninstalled accelerate-1.7.0
14:       Successfully uninstalled accelerate-1.7.0
18:       Successfully uninstalled accelerate-1.7.0
 8:       Successfully uninstalled accelerate-1.7.0
 7:       Successfully uninstalled accelerate-1.7.0
 2:       Successfully uninstalled accelerate-1.7.0
 4:       Successfully uninstalled accelerate-1.7.0
23:       Successfully uninstalled accelerate-1.7.0
30:       Successfully uninstalled accelerate-1.7.0
10:       Successfully uninstalled accelerate-1.7.0
28: Successfully installed accelerate-0.28.0
25: Successfully installed accelerate-0.28.0
11:       Successfully uninstalled accelerate-1.7.0
 6:       Successfully uninstalled accelerate-1.7.0
20: Successfully installed accelerate-0.28.0
21: Successfully installed accelerate-0.28.0
14: Successfully installed accelerate-0.28.0
29:       Successfully uninstalled accelerate-1.7.0
26:       Successfully uninstalled accelerate-1.7.0
16:       Successfully uninstalled accelerate-1.7.0
15:       Successfully uninstalled accelerate-1.7.0
22:       Successfully uninstalled accelerate-1.7.0
18: Successfully installed accelerate-0.28.0
 8: Successfully installed accelerate-0.28.0
24:       Successfully uninstalled accelerate-1.7.0
17:       Successfully uninstalled accelerate-1.7.0
 3:       Successfully uninstalled accelerate-1.7.0
31:       Successfully uninstalled accelerate-1.7.0
13:       Successfully uninstalled accelerate-1.7.0
 2: Successfully installed accelerate-0.28.0
 9:       Successfully uninstalled accelerate-1.7.0
12:       Successfully uninstalled accelerate-1.7.0
 7: Successfully installed accelerate-0.28.0
 0:       Successfully uninstalled accelerate-1.7.0
19:       Successfully uninstalled accelerate-1.7.0
 5:       Successfully uninstalled accelerate-1.7.0
27:       Successfully uninstalled accelerate-1.7.0
 4: Successfully installed accelerate-0.28.0
23: Successfully installed accelerate-0.28.0
30: Successfully installed accelerate-0.28.0
 1:       Successfully uninstalled accelerate-1.7.0
10: Successfully installed accelerate-0.28.0
11: Successfully installed accelerate-0.28.0
 6: Successfully installed accelerate-0.28.0
26: Successfully installed accelerate-0.28.0
16: Successfully installed accelerate-0.28.0
29: Successfully installed accelerate-0.28.0
15: Successfully installed accelerate-0.28.0
22: Successfully installed accelerate-0.28.0
24: Successfully installed accelerate-0.28.0
17: Successfully installed accelerate-0.28.0
31: Successfully installed accelerate-0.28.0
 3: Successfully installed accelerate-0.28.0
13: Successfully installed accelerate-0.28.0
12: Successfully installed accelerate-0.28.0
 9: Successfully installed accelerate-0.28.0
19: Successfully installed accelerate-0.28.0
 0: Successfully installed accelerate-0.28.0
 5: Successfully installed accelerate-0.28.0
27: Successfully installed accelerate-0.28.0
 1: Successfully installed accelerate-0.28.0
28: 
28: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
28: [notice] To update, run: python -m pip install --upgrade pip
21: 
21: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
21: [notice] To update, run: python -m pip install --upgrade pip
10: 
10: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
10: [notice] To update, run: python -m pip install --upgrade pip
26: 
26: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
26: [notice] To update, run: python -m pip install --upgrade pip
27: 
27: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
27: [notice] To update, run: python -m pip install --upgrade pip
 0: 
 0: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 0: [notice] To update, run: python -m pip install --upgrade pip
14: 
14: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
14: [notice] To update, run: python -m pip install --upgrade pip
19: 
19: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
19: [notice] To update, run: python -m pip install --upgrade pip
 1: 
 1: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 1: [notice] To update, run: python -m pip install --upgrade pip
25: 
25: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
25: [notice] To update, run: python -m pip install --upgrade pip
 7: 
 7: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 7: [notice] To update, run: python -m pip install --upgrade pip
 8: 
 8: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 8: [notice] To update, run: python -m pip install --upgrade pip
17: 
17: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
17: [notice] To update, run: python -m pip install --upgrade pip
 4: 
 4: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 4: [notice] To update, run: python -m pip install --upgrade pip
20: 
20: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
20: [notice] To update, run: python -m pip install --upgrade pip
31: 
31: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
31: [notice] To update, run: python -m pip install --upgrade pip
23: 
23: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
23: [notice] To update, run: python -m pip install --upgrade pip
16: 
16: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
16: [notice] To update, run: python -m pip install --upgrade pip
30: 
30: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
30: [notice] To update, run: python -m pip install --upgrade pip
29: 
29: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
29: [notice] To update, run: python -m pip install --upgrade pip
 2: 
 2: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 2: [notice] To update, run: python -m pip install --upgrade pip
 5: 
 5: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 5: [notice] To update, run: python -m pip install --upgrade pip
13: 
13: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
13: [notice] To update, run: python -m pip install --upgrade pip
15: 
15: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
15: [notice] To update, run: python -m pip install --upgrade pip
18: 
18: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
18: [notice] To update, run: python -m pip install --upgrade pip
22: 
22: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
22: [notice] To update, run: python -m pip install --upgrade pip
 9: 
 9: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 9: [notice] To update, run: python -m pip install --upgrade pip
 3: 
 3: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 3: [notice] To update, run: python -m pip install --upgrade pip
 6: 
 6: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
 6: [notice] To update, run: python -m pip install --upgrade pip
24: 
24: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
24: [notice] To update, run: python -m pip install --upgrade pip
11: 
11: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
11: [notice] To update, run: python -m pip install --upgrade pip
12: 
12: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
12: [notice] To update, run: python -m pip install --upgrade pip
14: W0624 19:22:32.944000 70369010976864 torch/distributed/run.py:778] 
14: W0624 19:22:32.944000 70369010976864 torch/distributed/run.py:778] *****************************************
14: W0624 19:22:32.944000 70369010976864 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
14: W0624 19:22:32.944000 70369010976864 torch/distributed/run.py:778] *****************************************
26: W0624 19:22:32.944000 70369266894944 torch/distributed/run.py:778] 
26: W0624 19:22:32.944000 70369266894944 torch/distributed/run.py:778] *****************************************
26: W0624 19:22:32.944000 70369266894944 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
26: W0624 19:22:32.944000 70369266894944 torch/distributed/run.py:778] *****************************************
28: W0624 19:22:32.945000 70369600145504 torch/distributed/run.py:778] 
28: W0624 19:22:32.945000 70369600145504 torch/distributed/run.py:778] *****************************************
28: W0624 19:22:32.945000 70369600145504 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
28: W0624 19:22:32.945000 70369600145504 torch/distributed/run.py:778] *****************************************
 0: W0624 19:22:32.946000 70369530480736 torch/distributed/run.py:778] 
 0: W0624 19:22:32.946000 70369530480736 torch/distributed/run.py:778] *****************************************
 0: W0624 19:22:32.946000 70369530480736 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 0: W0624 19:22:32.946000 70369530480736 torch/distributed/run.py:778] *****************************************
21: W0624 19:22:32.946000 70369484212320 torch/distributed/run.py:778] 
21: W0624 19:22:32.946000 70369484212320 torch/distributed/run.py:778] *****************************************
21: W0624 19:22:32.946000 70369484212320 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
21: W0624 19:22:32.946000 70369484212320 torch/distributed/run.py:778] *****************************************
10: W0624 19:22:33.063000 70369084442720 torch/distributed/run.py:778] 
10: W0624 19:22:33.063000 70369084442720 torch/distributed/run.py:778] *****************************************
10: W0624 19:22:33.063000 70369084442720 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10: W0624 19:22:33.063000 70369084442720 torch/distributed/run.py:778] *****************************************
25: W0624 19:22:33.184000 70369712867424 torch/distributed/run.py:778] 
25: W0624 19:22:33.184000 70369712867424 torch/distributed/run.py:778] *****************************************
25: W0624 19:22:33.184000 70369712867424 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
25: W0624 19:22:33.184000 70369712867424 torch/distributed/run.py:778] *****************************************
 8: W0624 19:22:33.220000 70369619740768 torch/distributed/run.py:778] 
 8: W0624 19:22:33.220000 70369619740768 torch/distributed/run.py:778] *****************************************
 8: W0624 19:22:33.220000 70369619740768 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 8: W0624 19:22:33.220000 70369619740768 torch/distributed/run.py:778] *****************************************
27: W0624 19:22:33.262000 70369750419552 torch/distributed/run.py:778] 
27: W0624 19:22:33.262000 70369750419552 torch/distributed/run.py:778] *****************************************
27: W0624 19:22:33.262000 70369750419552 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
27: W0624 19:22:33.262000 70369750419552 torch/distributed/run.py:778] *****************************************
 7: W0624 19:22:33.288000 70368793528416 torch/distributed/run.py:778] 
 7: W0624 19:22:33.288000 70368793528416 torch/distributed/run.py:778] *****************************************
 7: W0624 19:22:33.288000 70368793528416 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 7: W0624 19:22:33.288000 70368793528416 torch/distributed/run.py:778] *****************************************
 5: W0624 19:22:33.293000 70369589528672 torch/distributed/run.py:778] 
 5: W0624 19:22:33.293000 70369589528672 torch/distributed/run.py:778] *****************************************
 5: W0624 19:22:33.293000 70369589528672 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 5: W0624 19:22:33.293000 70369589528672 torch/distributed/run.py:778] *****************************************
 2: W0624 19:22:33.322000 70369007634528 torch/distributed/run.py:778] 
 2: W0624 19:22:33.322000 70369007634528 torch/distributed/run.py:778] *****************************************
 2: W0624 19:22:33.322000 70369007634528 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 2: W0624 19:22:33.322000 70369007634528 torch/distributed/run.py:778] *****************************************
30: W0624 19:22:33.327000 70369099581536 torch/distributed/run.py:778] 
30: W0624 19:22:33.327000 70369099581536 torch/distributed/run.py:778] *****************************************
30: W0624 19:22:33.327000 70369099581536 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
30: W0624 19:22:33.327000 70369099581536 torch/distributed/run.py:778] *****************************************
16: W0624 19:22:33.344000 70369760249952 torch/distributed/run.py:778] 
16: W0624 19:22:33.344000 70369760249952 torch/distributed/run.py:778] *****************************************
16: W0624 19:22:33.344000 70369760249952 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
16: W0624 19:22:33.344000 70369760249952 torch/distributed/run.py:778] *****************************************
 3: W0624 19:22:33.400000 70368802244704 torch/distributed/run.py:778] 
 3: W0624 19:22:33.400000 70368802244704 torch/distributed/run.py:778] *****************************************
 3: W0624 19:22:33.400000 70368802244704 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 3: W0624 19:22:33.400000 70368802244704 torch/distributed/run.py:778] *****************************************
31: W0624 19:22:33.407000 70369816938592 torch/distributed/run.py:778] 
31: W0624 19:22:33.407000 70369816938592 torch/distributed/run.py:778] *****************************************
31: W0624 19:22:33.407000 70369816938592 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
31: W0624 19:22:33.407000 70369816938592 torch/distributed/run.py:778] *****************************************
11: W0624 19:22:33.415000 70369062750304 torch/distributed/run.py:778] 
11: W0624 19:22:33.415000 70369062750304 torch/distributed/run.py:778] *****************************************
11: W0624 19:22:33.415000 70369062750304 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
11: W0624 19:22:33.415000 70369062750304 torch/distributed/run.py:778] *****************************************
19: W0624 19:22:33.422000 70369010190432 torch/distributed/run.py:778] 
19: W0624 19:22:33.422000 70369010190432 torch/distributed/run.py:778] *****************************************
19: W0624 19:22:33.422000 70369010190432 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
19: W0624 19:22:33.422000 70369010190432 torch/distributed/run.py:778] *****************************************
17: W0624 19:22:33.446000 70368908937312 torch/distributed/run.py:778] 
17: W0624 19:22:33.446000 70368908937312 torch/distributed/run.py:778] *****************************************
17: W0624 19:22:33.446000 70368908937312 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
17: W0624 19:22:33.446000 70368908937312 torch/distributed/run.py:778] *****************************************
20: W0624 19:22:33.473000 70368941901920 torch/distributed/run.py:778] 
20: W0624 19:22:33.473000 70368941901920 torch/distributed/run.py:778] *****************************************
20: W0624 19:22:33.473000 70368941901920 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
20: W0624 19:22:33.473000 70368941901920 torch/distributed/run.py:778] *****************************************
24: W0624 19:22:33.477000 70369267746912 torch/distributed/run.py:778] 
24: W0624 19:22:33.477000 70369267746912 torch/distributed/run.py:778] *****************************************
24: W0624 19:22:33.477000 70369267746912 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
24: W0624 19:22:33.477000 70369267746912 torch/distributed/run.py:778] *****************************************
12: W0624 19:22:33.480000 70369046169696 torch/distributed/run.py:778] 
12: W0624 19:22:33.480000 70369046169696 torch/distributed/run.py:778] *****************************************
12: W0624 19:22:33.480000 70369046169696 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
12: W0624 19:22:33.480000 70369046169696 torch/distributed/run.py:778] *****************************************
23: W0624 19:22:33.481000 70369654081632 torch/distributed/run.py:778] 
23: W0624 19:22:33.481000 70369654081632 torch/distributed/run.py:778] *****************************************
23: W0624 19:22:33.481000 70369654081632 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
23: W0624 19:22:33.481000 70369654081632 torch/distributed/run.py:778] *****************************************
22: W0624 19:22:33.500000 70369747077216 torch/distributed/run.py:778] 
22: W0624 19:22:33.500000 70369747077216 torch/distributed/run.py:778] *****************************************
22: W0624 19:22:33.500000 70369747077216 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
22: W0624 19:22:33.500000 70369747077216 torch/distributed/run.py:778] *****************************************
18: W0624 19:22:33.531000 70369650083936 torch/distributed/run.py:778] 
18: W0624 19:22:33.531000 70369650083936 torch/distributed/run.py:778] *****************************************
18: W0624 19:22:33.531000 70369650083936 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
18: W0624 19:22:33.531000 70369650083936 torch/distributed/run.py:778] *****************************************
15: W0624 19:22:33.532000 70369183860832 torch/distributed/run.py:778] 
15: W0624 19:22:33.532000 70369183860832 torch/distributed/run.py:778] *****************************************
15: W0624 19:22:33.532000 70369183860832 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
15: W0624 19:22:33.532000 70369183860832 torch/distributed/run.py:778] *****************************************
13: W0624 19:22:33.540000 70369528514656 torch/distributed/run.py:778] 
13: W0624 19:22:33.540000 70369528514656 torch/distributed/run.py:778] *****************************************
13: W0624 19:22:33.540000 70369528514656 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
13: W0624 19:22:33.540000 70369528514656 torch/distributed/run.py:778] *****************************************
 9: W0624 19:22:33.576000 70369707886688 torch/distributed/run.py:778] 
 9: W0624 19:22:33.576000 70369707886688 torch/distributed/run.py:778] *****************************************
 9: W0624 19:22:33.576000 70369707886688 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 9: W0624 19:22:33.576000 70369707886688 torch/distributed/run.py:778] *****************************************
 1: W0624 19:22:33.606000 70369655851104 torch/distributed/run.py:778] 
 1: W0624 19:22:33.606000 70369655851104 torch/distributed/run.py:778] *****************************************
 1: W0624 19:22:33.606000 70369655851104 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 1: W0624 19:22:33.606000 70369655851104 torch/distributed/run.py:778] *****************************************
 4: W0624 19:22:33.651000 70369220560992 torch/distributed/run.py:778] 
 4: W0624 19:22:33.651000 70369220560992 torch/distributed/run.py:778] *****************************************
 4: W0624 19:22:33.651000 70369220560992 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 4: W0624 19:22:33.651000 70369220560992 torch/distributed/run.py:778] *****************************************
 6: W0624 19:22:33.656000 70369624918112 torch/distributed/run.py:778] 
 6: W0624 19:22:33.656000 70369624918112 torch/distributed/run.py:778] *****************************************
 6: W0624 19:22:33.656000 70369624918112 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
 6: W0624 19:22:33.656000 70369624918112 torch/distributed/run.py:778] *****************************************
29: W0624 19:22:34.010000 70369530153056 torch/distributed/run.py:778] 
29: W0624 19:22:34.010000 70369530153056 torch/distributed/run.py:778] *****************************************
29: W0624 19:22:34.010000 70369530153056 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
29: W0624 19:22:34.010000 70369530153056 torch/distributed/run.py:778] *****************************************
 4: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 4: 
 4: 
 4: 
 7: Please install pyav to use video processing functions.
20: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
20: 
20: 
20: 
31: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
31: 
31: 
31: 
 7: Please install pyav to use video processing functions.
 7: Please install pyav to use video processing functions.
 7: Please install pyav to use video processing functions.
25: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
25: 
29: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
29: 
29: 
29: 
14: Please install pyav to use video processing functions.
25: Please install pyav to use video processing functions.
21: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
21: 
14: Please install pyav to use video processing functions.
28: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
28: 
28: 
 8: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 8: 
21: Please install pyav to use video processing functions.
 8: Please install pyav to use video processing functions.
21: Please install pyav to use video processing functions.
25: Please install pyav to use video processing functions.
28: Please install pyav to use video processing functions.
18: Please install pyav to use video processing functions.
18: Please install pyav to use video processing functions.
14: Please install pyav to use video processing functions.
 8: Please install pyav to use video processing functions.
14: Please install pyav to use video processing functions.
18: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
18: 
 2: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 2: 
 2: 
 2: Please install pyav to use video processing functions.
26: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
26: 
26: 
26: 
27: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
27: 
27: 
27: 
22: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
22: 
22: 
22: 
 1: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 1: 
 1: 
 1: Please install pyav to use video processing functions.
24: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
24: 
24: 
24: 
11: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
11: 
11: 
11: 
23: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
23: 
23: 
23: 
 3: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 3: 
 3: 
 3: Please install pyav to use video processing functions.
15: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
15: 
15: 
15: 
16: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
16: 
16: 
16: 
 5: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 5: 
 5: 
 5: Please install pyav to use video processing functions.
13: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
13: 
13: 
13: Please install pyav to use video processing functions.
 0: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 0: 
 0: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 0: 
 6: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 6: 
 6: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 6: 
 6: 
12: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
12: 
12: 
12: 
30: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
30: 
30: 
30: Please install pyav to use video processing functions.
 9: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
 9: 
 9: 
 9: Please install pyav to use video processing functions.
17: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
17: 
17: Please install pyav to use video processing functions.
17: 
10: Please install pyav to use video processing functions.
10: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
10: 
10: Please install pyav to use video processing functions.
19: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
19: 
19: 
19: Please install pyav to use video processing functions.
18: [2025-06-24 19:22:48,548] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [2025-06-24 19:22:48,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [2025-06-24 19:22:48,583] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: df: /users/ndeperr/.triton/autotune
18: : No such file or directory
14: df: /users/ndeperr/.triton/autotune: No such file or directory
14: [2025-06-24 19:22:48,760] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [2025-06-24 19:22:48,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: [2025-06-24 19:22:48,867] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
31: [2025-06-24 19:22:48,870] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
20: [2025-06-24 19:22:48,877] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
18: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
18: 
 4: [2025-06-24 19:22:48,883] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [2025-06-24 19:22:48,885] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: [2025-06-24 19:22:48,903] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: [2025-06-24 19:22:48,904] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: [2025-06-24 19:22:48,906] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
31: [2025-06-24 19:22:48,906] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
20: [2025-06-24 19:22:48,911] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [2025-06-24 19:22:48,915] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [2025-06-24 19:22:48,918] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
21: [2025-06-24 19:22:48,920] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [2025-06-24 19:22:48,921] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: df: /users/ndeperr/.triton/autotune
 7: : No such file or directory
31: df: /users/ndeperr/.triton/autotune
31: : No such file or directory
18: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
18: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
18: 
18: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 7: [2025-06-24 19:22:48,939] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
18: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
18: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
20: df: /users/ndeperr/.triton/autotune
20: : No such file or directory
14: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
14: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
14: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 8: [2025-06-24 19:22:48,940] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: [2025-06-24 19:22:48,941] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
31: [2025-06-24 19:22:48,943] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: df: /users/ndeperr/.triton/autotune
 4: : No such file or directory
20: [2025-06-24 19:22:48,946] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [2025-06-24 19:22:48,952] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [2025-06-24 19:22:48,952] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [2025-06-24 19:22:48,955] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
21: [2025-06-24 19:22:48,955] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: df: /users/ndeperr/.triton/autotune
29: : No such file or directory
 6: [2025-06-24 19:22:48,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: [2025-06-24 19:22:48,976] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: [2025-06-24 19:22:48,976] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: [2025-06-24 19:22:48,977] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
31: [2025-06-24 19:22:48,978] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
20: [2025-06-24 19:22:48,981] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [2025-06-24 19:22:48,983] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [2025-06-24 19:22:48,987] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [2025-06-24 19:22:48,988] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [2025-06-24 19:22:48,989] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
21: [2025-06-24 19:22:48,991] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: df: /users/ndeperr/.triton/autotune
 8: : No such file or directory
25: df: /users/ndeperr/.triton/autotune
25: : No such file or directory
28: df: /users/ndeperr/.triton/autotune
28: : No such file or directory
28: [2025-06-24 19:22:49,011] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: [2025-06-24 19:22:49,012] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [2025-06-24 19:22:49,018] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [2025-06-24 19:22:49,019] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [2025-06-24 19:22:49,023] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
21: df: /users/ndeperr/.triton/autotune
21: : No such file or directory
21: [2025-06-24 19:22:49,026] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 3: [2025-06-24 19:22:49,043] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 6: df: /users/ndeperr/.triton/autotune
 6: : No such file or directory
 6: 
14: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 6: [2025-06-24 19:22:49,081] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 7: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
20: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 7: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: 
 4: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
20: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
13: [2025-06-24 19:22:49,112] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 3: [2025-06-24 19:22:49,115] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 5: [2025-06-24 19:22:49,117] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
26: [2025-06-24 19:22:49,123] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 3: df: /users/ndeperr/.triton/autotune
 3: : No such file or directory
14: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
20: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
20: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
20: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 3: [2025-06-24 19:22:49,150] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 5: [2025-06-24 19:22:49,152] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
14: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
14: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
14: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
14: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
14: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
26: [2025-06-24 19:22:49,162] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
20: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
20: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
20: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
28: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: 
 8: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 8: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 8: 
 8: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 8: 
21: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
21: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
21: 
21: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
31: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
31: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 7: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 7: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 7: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 4: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 4: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 4: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
20: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 7: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 7: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 7: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
31: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
31: 
31: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
31: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 0: [2025-06-24 19:22:49,181] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
29: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
29: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 4: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 4: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 4: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 5: [2025-06-24 19:22:49,188] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 9: [2025-06-24 19:22:49,190] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
20: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
20: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
20: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
31: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: 
28: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
26: [2025-06-24 19:22:49,197] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
13: df: /users/ndeperr/.triton/autotune
13: : No such file or directory
14: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
14: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 7: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
26: df: /users/ndeperr/.triton/autotune
26: : No such file or directory
 4: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 5: df: /users/ndeperr/.triton/autotune
 5: : No such file or directory
 7: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 7: 
31: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
31: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
31: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
24: [2025-06-24 19:22:49,218] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
18: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
18: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
18: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 6: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
13: [2025-06-24 19:22:49,223] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
25: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
25: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
25: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
29: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
29: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
29: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 0: [2025-06-24 19:22:49,229] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
26: [2025-06-24 19:22:49,231] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
29: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
29: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
14:   def forward(ctx, input, weight, bias=None):
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
14:   def backward(ctx, grad_output):
20: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
25: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 8: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 8: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 8: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 8: 
 8: 
 8: 
 8: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 8: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 8: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 8: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
21: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
21: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
21: 
21: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
21: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: 
21: 
21: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
13: [2025-06-24 19:22:49,257] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
24: [2025-06-24 19:22:49,257] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: 
31: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 9: [2025-06-24 19:22:49,261] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
18:   def forward(ctx, input, weight, bias=None):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
18:   def forward(ctx, input, weight, bias=None):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
18:   def backward(ctx, grad_output):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
18:   def backward(ctx, grad_output):
 0: [2025-06-24 19:22:49,264] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 7: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 7: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 7: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: [2025-06-24 19:22:49,268] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
25: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
25: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
28: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
28: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
28: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 6: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 6: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 6: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
21: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
21: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: df: /users/ndeperr/.triton/autotune
 9: : No such file or directory
 8: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 8: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 8: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 0: df: /users/ndeperr/.triton/autotune
 0: : No such file or directory
 3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
24: df: /users/ndeperr/.triton/autotune
24: : No such file or directory
24: [2025-06-24 19:22:49,292] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 6: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
28: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
28: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
28: 
15: [2025-06-24 19:22:49,304] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 6: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 6: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 6: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
28: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
28: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
28: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
10: [2025-06-24 19:22:49,315] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 7: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 7: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
20: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
20: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
20: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
14: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
14: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
18: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
18: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
14: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
14: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
31: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
31: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
31: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
28: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
28: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
28: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
28: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
28: 
28: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
28: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
24: [2025-06-24 19:22:49,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 4: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 4: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
29: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
29: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: [2025-06-24 19:22:49,332] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
26: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
26: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: [2025-06-24 19:22:49,339] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 4: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 4: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
14: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: df: /users/ndeperr/.triton/autotune
15: : No such file or directory
 5: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 5: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
14:   def forward(ctx, input, weight, bias=None):
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
14:   def backward(ctx, grad_output):
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
14:   def forward(ctx, input, weight, bias=None):
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
14:   def backward(ctx, grad_output):
23: [2025-06-24 19:22:49,356] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 5: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
18: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
18: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
18: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
18: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
18: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
18: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
26: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 3: [2025-06-24 19:22:49,365] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
13: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
13: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
15: [2025-06-24 19:22:49,374] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
14: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
14: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
20: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
20: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
31: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
31: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
20: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
20: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
29: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
29: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 7: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 7: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
23: [2025-06-24 19:22:49,392] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 4: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
26: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
26: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
26: 
26: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
26: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
26: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
26: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 5: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 5: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 5: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 5: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 5: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 5: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
31: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
31: 
31: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 6: [2025-06-24 19:22:49,402] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 4: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 7: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 7: 
 7: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 5: [2025-06-24 19:22:49,407] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
24: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 5: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 5: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 5: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
17: [2025-06-24 19:22:49,412] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
24: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
20:   def forward(ctx, input, weight, bias=None):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
20:   def backward(ctx, grad_output):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
20:   def forward(ctx, input, weight, bias=None):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
20:   def backward(ctx, grad_output):
20: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
20: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
31:   def forward(ctx, input, weight, bias=None):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
31:   def backward(ctx, grad_output):
26: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
26: 
26: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
26: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
25: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
25: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
31: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
31: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 9: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
23: [2025-06-24 19:22:49,427] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
29:   def forward(ctx, input, weight, bias=None):
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
29:   def backward(ctx, grad_output):
 0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 0: 
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 7:   def forward(ctx, input, weight, bias=None):
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 7:   def backward(ctx, grad_output):
 0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 4:   def forward(ctx, input, weight, bias=None):
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 4:   def forward(ctx, input, weight, bias=None):
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 4:   def backward(ctx, grad_output):
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 4:   def backward(ctx, grad_output):
26: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
31:   def forward(ctx, input, weight, bias=None):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
31:   def backward(ctx, grad_output):
29: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 9: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
21: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
21: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 7: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 7: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
28: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
28: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 8: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 8: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
23: df: /users/ndeperr/.triton/autotune
23: : No such file or directory
10: df: /users/ndeperr/.triton/autotune: No such file or directory
20: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
20: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
25: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
25: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 7:   def forward(ctx, input, weight, bias=None):
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 7:   def backward(ctx, grad_output):
25: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
21: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
25: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
21: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
24: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
24: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
24: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 7: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
31: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 7: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
31: 
31: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 4: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 4: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
24: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
24: 
24: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
24: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 8: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 8: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
17: [2025-06-24 19:22:49,450] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 8: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 8: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
21: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
21: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
21: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
21: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 4: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 4: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
20:   def forward(ctx, input, weight, bias=None):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
20:   def backward(ctx, grad_output):
 8: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 8: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
25:   def forward(ctx, input, weight, bias=None):
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
25:   def backward(ctx, grad_output):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
31:   def forward(ctx, input, weight, bias=None):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
31:   def backward(ctx, grad_output):
23: [2025-06-24 19:22:49,463] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
25: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
24: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: 
26: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
26: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
26: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
29: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
29: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: [2025-06-24 19:22:49,470] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
28:   def forward(ctx, input, weight, bias=None):
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
28:   def backward(ctx, grad_output):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
20:   def forward(ctx, input, weight, bias=None):
20: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
20:   def backward(ctx, grad_output):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
21:   def forward(ctx, input, weight, bias=None):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
21:   def forward(ctx, input, weight, bias=None):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
21:   def backward(ctx, grad_output):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
21:   def backward(ctx, grad_output):
 6: [2025-06-24 19:22:49,475] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 7:   def forward(ctx, input, weight, bias=None):
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 7:   def backward(ctx, grad_output):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 8:   def forward(ctx, input, weight, bias=None):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 8:   def backward(ctx, grad_output):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 8:   def forward(ctx, input, weight, bias=None):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 8:   def backward(ctx, grad_output):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
31:   def forward(ctx, input, weight, bias=None):
31: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
31:   def backward(ctx, grad_output):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 8:   def forward(ctx, input, weight, bias=None):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 8:   def backward(ctx, grad_output):
 0: [2025-06-24 19:22:49,479] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
25:   def forward(ctx, input, weight, bias=None):
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
25:   def forward(ctx, input, weight, bias=None):
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
25:   def backward(ctx, grad_output):
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
25:   def backward(ctx, grad_output):
 9: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 9: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 9: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
21:   def forward(ctx, input, weight, bias=None):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
21:   def backward(ctx, grad_output):
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 7:   def forward(ctx, input, weight, bias=None):
 7: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 7:   def backward(ctx, grad_output):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 8:   def forward(ctx, input, weight, bias=None):
 8: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 8:   def backward(ctx, grad_output):
24: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
24: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
24: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 9: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 9: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: [2025-06-24 19:22:49,484] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 0: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 4:   def forward(ctx, input, weight, bias=None):
 0: 
 0: 
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 4:   def backward(ctx, grad_output):
 0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 0: 
17: [2025-06-24 19:22:49,485] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 4:   def forward(ctx, input, weight, bias=None):
 4: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 4:   def backward(ctx, grad_output):
 0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 0: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
21:   def forward(ctx, input, weight, bias=None):
21: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
21:   def backward(ctx, grad_output):
13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
28: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
28: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 3:   def forward(ctx, input, weight, bias=None):
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 3:   def forward(ctx, input, weight, bias=None):
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 3:   def backward(ctx, grad_output):
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 3:   def backward(ctx, grad_output):
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
29:   def forward(ctx, input, weight, bias=None):
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
29:   def backward(ctx, grad_output):
13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
29: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
29: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
24: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
15: 
15: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
15: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
15: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
15: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: [2025-06-24 19:22:49,515] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
13: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
13: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
15: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: 
 6: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
25:   def forward(ctx, input, weight, bias=None):
25: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
25:   def backward(ctx, grad_output):
17: [2025-06-24 19:22:49,520] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 1: [2025-06-24 19:22:49,521] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
17: df: /users/ndeperr/.triton/autotune
17: : No such file or directory
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
29:   def forward(ctx, input, weight, bias=None):
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
29:   def backward(ctx, grad_output):
13: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
13: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
29:   def forward(ctx, input, weight, bias=None):
29: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
29:   def backward(ctx, grad_output):
24: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
24: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
24: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
15: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
15: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
15: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
28: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
28: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
13: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
15: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
15: 
28: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
28: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 3:   def forward(ctx, input, weight, bias=None):
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 3:   def backward(ctx, grad_output):
 9: [2025-06-24 19:22:49,546] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
28:   def forward(ctx, input, weight, bias=None):
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
28:   def backward(ctx, grad_output):
 6: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 6: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 6: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: [2025-06-24 19:22:49,552] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
26: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
26: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 1: [2025-06-24 19:22:49,556] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 6: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 6: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 6: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 6: 
 6: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: [2025-06-24 19:22:49,568] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
28:   def forward(ctx, input, weight, bias=None):
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
28:   def backward(ctx, grad_output):
 5: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 5: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 9: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 5: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 5: 
 5: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
26: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
26: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
28:   def forward(ctx, input, weight, bias=None):
28: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
28:   def backward(ctx, grad_output):
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
13:   def forward(ctx, input, weight, bias=None):
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
13:   def backward(ctx, grad_output):
 5: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 5: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
16: [2025-06-24 19:22:49,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
26: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
26: 
26: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 2: [2025-06-24 19:22:49,586] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
23: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
23: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 9: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 9: 
 9: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 9: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
26:   def forward(ctx, input, weight, bias=None):
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
26:   def backward(ctx, grad_output):
 5: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
24: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
24: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
24: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
24: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
26: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
26: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: [2025-06-24 19:22:49,603] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 6:   def forward(ctx, input, weight, bias=None):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 6:   def backward(ctx, grad_output):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 6:   def forward(ctx, input, weight, bias=None):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 6:   def backward(ctx, grad_output):
 1: df: /users/ndeperr/.triton/autotune
 1: : No such file or directory
 1: 
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
26:   def forward(ctx, input, weight, bias=None):
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
26:   def backward(ctx, grad_output):
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
26:   def forward(ctx, input, weight, bias=None):
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
26:   def backward(ctx, grad_output):
16: [2025-06-24 19:22:49,614] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
15: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
15: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
15: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 5:   def forward(ctx, input, weight, bias=None):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 5:   def forward(ctx, input, weight, bias=None):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 5:   def forward(ctx, input, weight, bias=None):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 5:   def backward(ctx, grad_output):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 5:   def backward(ctx, grad_output):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 5:   def backward(ctx, grad_output):
10: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 2: [2025-06-24 19:22:49,621] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
23: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 2: df: /users/ndeperr/.triton/autotune
 2: : No such file or directory
24: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
24: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 2: df: /users/ndeperr/.triton/autotune
 2: : No such file or directory
30: [2025-06-24 19:22:49,638] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
24:   def forward(ctx, input, weight, bias=None):
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
24:   def forward(ctx, input, weight, bias=None):
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
24:   def backward(ctx, grad_output):
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
24:   def backward(ctx, grad_output):
 5: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 5: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 5: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
26:   def forward(ctx, input, weight, bias=None):
26: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
26:   def backward(ctx, grad_output):
 6: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
23: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
23: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
23: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
23: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
23: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
23: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
16: [2025-06-24 19:22:49,650] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
24: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
24: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 0: 
 0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
13: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
10: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
10: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 9: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 9: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 9: 
 9: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
24:   def forward(ctx, input, weight, bias=None):
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
24:   def backward(ctx, grad_output):
 0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: [2025-06-24 19:22:49,673] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
23: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
23: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
23: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
22: [2025-06-24 19:22:49,681] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
27: [2025-06-24 19:22:49,682] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
13: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
13: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: df: /users/ndeperr/.triton/autotune: No such file or directory
16: [2025-06-24 19:22:49,685] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
23: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: df: /users/ndeperr/.triton/autotune
30: : No such file or directory
17: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
17: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
17: 
17: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
17: 
15: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
15: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 6: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 6: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 6: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
24:   def forward(ctx, input, weight, bias=None):
24: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
24:   def backward(ctx, grad_output):
 6: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 6: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
15: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
15: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
13:   def forward(ctx, input, weight, bias=None):
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
13:   def backward(ctx, grad_output):
16: df: /users/ndeperr/.triton/autotune
16: : No such file or directory
16: df: /users/ndeperr/.triton/autotune
16: : No such file or directory
 0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 0: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 0: 
13: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
13: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
13: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 0:   def forward(ctx, input, weight, bias=None):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 0:   def forward(ctx, input, weight, bias=None):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 0:   def backward(ctx, grad_output):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 0:   def backward(ctx, grad_output):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 0:   def forward(ctx, input, weight, bias=None):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 0:   def backward(ctx, grad_output):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 9:   def forward(ctx, input, weight, bias=None):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 9:   def forward(ctx, input, weight, bias=None):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 9:   def backward(ctx, grad_output):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 9:   def backward(ctx, grad_output):
15: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
15: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [2025-06-24 19:22:49,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
27: [2025-06-24 19:22:49,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
13:   def forward(ctx, input, weight, bias=None):
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
13:   def backward(ctx, grad_output):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 6:   def forward(ctx, input, weight, bias=None):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 6:   def backward(ctx, grad_output):
23: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
23: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
23: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
15:   def forward(ctx, input, weight, bias=None):
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
15:   def forward(ctx, input, weight, bias=None):
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
15:   def backward(ctx, grad_output):
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
15:   def backward(ctx, grad_output):
15: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
15: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 9: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 9: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
17: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
17: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
17: 
17: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
17: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
17: 
17: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
17: 
17: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
17: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
17: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
18: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
18: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
18: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
18: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
17: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
14: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
14: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [2025-06-24 19:22:49,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
27: [2025-06-24 19:22:49,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
15:   def forward(ctx, input, weight, bias=None):
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
15:   def backward(ctx, grad_output):
 1: [2025-06-24 19:22:49,773] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
15:   def forward(ctx, input, weight, bias=None):
15: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
15:   def backward(ctx, grad_output):
17: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
17: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
17: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
27: df: /users/ndeperr/.triton/autotune
27: : No such file or directory
27: [2025-06-24 19:22:49,791] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 9:   def forward(ctx, input, weight, bias=None):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 9:   def backward(ctx, grad_output):
22: [2025-06-24 19:22:49,793] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
18:   def forward(ctx, input, weight, bias=None):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
18:   def forward(ctx, input, weight, bias=None):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
18:   def backward(ctx, grad_output):
18: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
18:   def backward(ctx, grad_output):
22: df: /users/ndeperr/.triton/autotune
22: : No such file or directory
23: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
23: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 5: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 5: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
23: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
23: 
23: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: [2025-06-24 19:22:49,815] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
14:   def forward(ctx, input, weight, bias=None):
14: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
14:   def backward(ctx, grad_output):
23: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
23: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 9: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 9: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 9: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: [2025-06-24 19:22:49,852] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
12: [2025-06-24 19:22:49,856] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
23:   def forward(ctx, input, weight, bias=None):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
23:   def forward(ctx, input, weight, bias=None):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
23:   def backward(ctx, grad_output):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
23:   def backward(ctx, grad_output):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
23:   def forward(ctx, input, weight, bias=None):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
23:   def backward(ctx, grad_output):
23: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
23: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 3:   def forward(ctx, input, weight, bias=None):
 3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 3:   def backward(ctx, grad_output):
10: [2025-06-24 19:22:49,883] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 2: 
 2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
11: [2025-06-24 19:22:49,888] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
19: [2025-06-24 19:22:49,895] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
12: [2025-06-24 19:22:49,895] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 0:   def forward(ctx, input, weight, bias=None):
 0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 0:   def backward(ctx, grad_output):
30: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
23:   def forward(ctx, input, weight, bias=None):
23: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
23:   def backward(ctx, grad_output):
13: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
13: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: [2025-06-24 19:22:49,921] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11: [2025-06-24 19:22:49,924] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
19: [2025-06-24 19:22:49,930] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11: df: /users/ndeperr/.triton/autotune
11: : No such file or directory
 1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 1: 
 1: 
 1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 1: 
 1: 
 1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: 
 1: 
12: [2025-06-24 19:22:49,931] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 5:   def forward(ctx, input, weight, bias=None):
 5: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 5:   def backward(ctx, grad_output):
 6: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 6: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
16: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
16: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
10: [2025-06-24 19:22:49,957] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
16: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
19: [2025-06-24 19:22:49,966] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
17: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
17: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [2025-06-24 19:22:49,968] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
17: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
17: 
17: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
17: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
17: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
17: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
17: 
17: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
13:   def forward(ctx, input, weight, bias=None):
13: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
13:   def backward(ctx, grad_output):
16: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
30: 
30: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
30: 
30: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
30: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
30: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
30: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
30: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 2: 
 2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 2: 
 2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 2: 
 2: 
 2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: 
19: df: /users/ndeperr/.triton/autotune
19: : No such file or directory
19: [2025-06-24 19:22:50,003] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 6:   def forward(ctx, input, weight, bias=None):
 6: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 6:   def backward(ctx, grad_output):
12: df: /users/ndeperr/.triton/autotune
12: : No such file or directory
30: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
30: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
30: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
17:   def forward(ctx, input, weight, bias=None):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
17:   def forward(ctx, input, weight, bias=None):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
17:   def backward(ctx, grad_output):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
17:   def backward(ctx, grad_output):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
17:   def forward(ctx, input, weight, bias=None):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
17:   def backward(ctx, grad_output):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
17:   def forward(ctx, input, weight, bias=None):
17: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
17:   def backward(ctx, grad_output):
10: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
16: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
16: 
16: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
16: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
16: 
16: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
16: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
16: 
16: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
27: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
27: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
22: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
22: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
22: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
10: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
10: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
16: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
16: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
16: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
10: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
10: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
10: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
27: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 9: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 9: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
10: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
 1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 9:   def forward(ctx, input, weight, bias=None):
 9: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 9:   def backward(ctx, grad_output):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
10:   def forward(ctx, input, weight, bias=None):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
10:   def backward(ctx, grad_output):
22: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
22: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
22: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
22: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
22: 
22: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
22: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
22: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
22: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
22: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
27: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
27: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
27: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
27: 
27: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
27: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
27: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
27: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
10: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
10: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
10: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
22: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
22: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
22: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
27: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
27: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
27: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
30: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
11: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
11: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
27: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
27: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
27: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
30: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
30: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 2: 
 2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 1:   def forward(ctx, input, weight, bias=None):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 1:   def forward(ctx, input, weight, bias=None):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 1:   def forward(ctx, input, weight, bias=None):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 1:   def backward(ctx, grad_output):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 1:   def backward(ctx, grad_output):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 1:   def backward(ctx, grad_output):
 2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
30:   def forward(ctx, input, weight, bias=None):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
30:   def backward(ctx, grad_output):
11: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
30: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
30: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
30: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
30: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
16: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
16: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
16: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
16: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
19: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
12: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
11: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
11: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
11: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
11: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
11: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
11: 
11: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
11: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
11: 
11: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
30:   def forward(ctx, input, weight, bias=None):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
30:   def backward(ctx, grad_output):
16: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
16: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
12: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
16: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
16: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
12: 
10: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
10: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
10: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
10: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 2:   def forward(ctx, input, weight, bias=None):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 2:   def forward(ctx, input, weight, bias=None):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 2:   def forward(ctx, input, weight, bias=None):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 2:   def backward(ctx, grad_output):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 2:   def backward(ctx, grad_output):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 2:   def backward(ctx, grad_output):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 2:   def forward(ctx, input, weight, bias=None):
 2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 2:   def backward(ctx, grad_output):
11: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
11: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
11: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
10:   def forward(ctx, input, weight, bias=None):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
10:   def backward(ctx, grad_output):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
30:   def forward(ctx, input, weight, bias=None):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
30:   def backward(ctx, grad_output):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
30:   def forward(ctx, input, weight, bias=None):
30: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
30:   def backward(ctx, grad_output):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
16:   def forward(ctx, input, weight, bias=None):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
16:   def forward(ctx, input, weight, bias=None):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
16:   def forward(ctx, input, weight, bias=None):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
16:   def forward(ctx, inp
16: ut, weight, bias=None):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
16:   def backward(ctx, grad_output):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
16:   def backward(ctx, grad_output):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
16:   def backward(ctx, grad_output):
16: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
16:   def backward(ctx, grad_output):
12: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
12: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
12: 
12: 
12: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
12: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
12: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
12: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
12: 
12: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
12: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
12: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
12: 
12: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
12: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
10: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
10: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
19: 
19: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
19: 
19: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
19: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
19: 
19: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
19: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
19: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
19: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
22: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
22: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
22: 
22: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
22: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
22: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
 1:   def forward(ctx, input, weight, bias=None):
 1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
 1:   def backward(ctx, grad_output):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
10:   def forward(ctx, input, weight, bias=None):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
10:   def backward(ctx, grad_output):
27: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
27: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
19: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
19: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
27: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
27: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
27: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
27: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
27: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
27: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
22:   def forward(ctx, input, weight, bias=None):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
22:   def forward(ctx, input, weight, bias=None):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
22:   def forward(ctx, input, weight, bias=None):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
22:   def backward(ctx, grad_output):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
22:   def backward(ctx, grad_output):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
22:   def backward(ctx, grad_output):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
22:   def forward(ctx, input, weight, bias=None):
22: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
22:   def backward(ctx, grad_output):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
27:   def forward(ctx, input, weight, bias=None):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
27:   def backward(ctx, grad_output):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
27:   def forward(ctx, input, weight, bias=None):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
27:   def backward(ctx, grad_output):
11: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
11: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
27:   def forward(ctx, input, weight, bias=None):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
27:   def backward(ctx, grad_output):
11: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
11: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
27:   def forward(ctx, input, weight, bias=None):
27: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
27:   def backward(ctx, grad_output):
11: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
11: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
11: 
11: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
11:   def forward(ctx, input, weight, bias=None):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
11:   def forward(ctx, input, weight, bias=None):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
11:   def backward(ctx, grad_output):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
11:   def backward(ctx, grad_output):
19: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
19: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
11:   def forward(ctx, input, weight, bias=None):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
11:   def backward(ctx, grad_output):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
11:   def forward(ctx, input, weight, bias=None):
11: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
11:   def backward(ctx, grad_output):
10: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
10: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
12: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
12: 
12: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
12: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
12: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
12: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
19: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
19: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
10:   def forward(ctx, input, weight, bias=None):
10: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
10:   def backward(ctx, grad_output):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
12:   def forward(ctx, input, weight, bias=None):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
12:   def forward(ctx, input, weight, bias=None):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
12:   def forward(ctx, input, weight, bias=None):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
12:   def backward(ctx, grad_output):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
12:   def backward(ctx, grad_output):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
12:   def backward(ctx, grad_output):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
12:   def forward(ctx, input, weight, bias=None):
12: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
12:   def backward(ctx, grad_output):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
19:   def forward(ctx, input, weight, bias=None):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
19:   def backward(ctx, grad_output):
19: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
19: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
19:   def forward(ctx, input, weight, bias=None):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
19:   def backward(ctx, grad_output):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
19:   def forward(ctx, input, weight, bias=None):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
19:   def backward(ctx, grad_output):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
19:   def forward(ctx, input, weight, bias=None):
19: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
19:   def backward(ctx, grad_output):
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads al
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
 8: ways resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads al
31: ways resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads al
23: ways resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
28: [2025-06-24 19:22:51,892] [INFO] [comm.py:637:init_distributed] cdb=None
15: [2025-06-24 19:22:51,892] [INFO] [comm.py:637:init_distributed] cdb=None
26: [2025-06-24 19:22:51,892] [INFO] [comm.py:637:init_distributed] cdb=None
15: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
29: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
 7: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
 7: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
29: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
25: [2025-06-24 19:22:51,893] [INFO] [comm.py:637:init_distributed] cdb=None
20: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
 4: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
 4: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
28: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
 8: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
 8: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
15: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
 8: [2025-06-24 19:22:51,894] [INFO] [comm.py:637:init_distributed] cdb=None
25: [2025-06-24 19:22:51,895] [INFO] [comm.py:637:init_distributed] cdb=None
13: [2025-06-24 19:22:51,895] [INFO] [comm.py:637:init_distributed] cdb=None
29: [2025-06-24 19:22:51,895] [INFO] [comm.py:637:init_distributed] cdb=None
14: [2025-06-24 19:22:51,895] [INFO] [comm.py:637:init_distributed] cdb=None
 4: [2025-06-24 19:22:51,896] [INFO] [comm.py:637:init_distributed] cdb=None
 7: [2025-06-24 19:22:51,896] [INFO] [comm.py:637:init_distributed] cdb=None
28: [2025-06-24 19:22:51,896] [INFO] [comm.py:637:init_distributed] cdb=None
15: [2025-06-24 19:22:51,897] [INFO] [comm.py:637:init_distributed] cdb=None
26: [2025-06-24 19:22:51,897] [INFO] [comm.py:637:init_distributed] cdb=None
 8: [2025-06-24 19:22:51,897] [INFO] [comm.py:637:init_distributed] cdb=None
26: [2025-06-24 19:22:51,898] [INFO] [comm.py:637:init_distributed] cdb=None
14: [2025-06-24 19:22:51,899] [INFO] [comm.py:637:init_distributed] cdb=None
25: [2025-06-24 19:22:51,899] [INFO] [comm.py:637:init_distributed] cdb=None
 4: [2025-06-24 19:22:51,900] [INFO] [comm.py:637:init_distributed] cdb=None
20: [2025-06-24 19:22:51,900] [INFO] [comm.py:637:init_distributed] cdb=None
20: [2025-06-24 19:22:51,900] [INFO] [comm.py:637:init_distributed] cdb=None
29: [2025-06-24 19:22:51,901] [INFO] [comm.py:637:init_distributed] cdb=None
 3: [2025-06-24 19:22:51,902] [INFO] [comm.py:637:init_distributed] cdb=None
21: [2025-06-24 19:22:51,903] [INFO] [comm.py:637:init_distributed] cdb=None
24: [2025-06-24 19:22:51,904] [INFO] [comm.py:637:init_distributed] cdb=None
24: [2025-06-24 19:22:51,904] [INFO] [comm.py:637:init_distributed] cdb=None
21: [2025-06-24 19:22:51,905] [INFO] [comm.py:637:init_distributed] cdb=None
21: [2025-06-24 19:22:51,905] [INFO] [comm.py:637:init_distributed] cdb=None
 3: [2025-06-24 19:22:51,908] [INFO] [comm.py:637:init_distributed] cdb=None
31: [2025-06-24 19:22:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
31: [2025-06-24 19:22:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
 7: [2025-06-24 19:22:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
 5: [2025-06-24 19:22:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
21: [2025-06-24 19:22:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
24: [2025-06-24 19:22:51,913] [INFO] [comm.py:637:init_distributed] cdb=None
24: [2025-06-24 19:22:51,913] [INFO] [comm.py:637:init_distributed] cdb=None
 0: [2025-06-24 19:22:51,914] [INFO] [comm.py:637:init_distributed] cdb=None
 0: [2025-06-24 19:22:51,914] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
31: [2025-06-24 19:22:51,915] [INFO] [comm.py:637:init_distributed] cdb=None
 5: [2025-06-24 19:22:51,915] [INFO] [comm.py:637:init_distributed] cdb=None
 0: [2025-06-24 19:22:51,917] [INFO] [comm.py:637:init_distributed] cdb=None
 5: [2025-06-24 19:22:51,917] [INFO] [comm.py:637:init_distributed] cdb=None
13: [2025-06-24 19:22:51,918] [INFO] [comm.py:637:init_distributed] cdb=None
31: [2025-06-24 19:22:51,918] [INFO] [comm.py:637:init_distributed] cdb=None
 3: [2025-06-24 19:22:51,924] [INFO] [comm.py:637:init_distributed] cdb=None
26: [2025-06-24 19:22:51,925] [INFO] [comm.py:637:init_distributed] cdb=None
13: [2025-06-24 19:22:51,930] [INFO] [comm.py:637:init_distributed] cdb=None
20: [2025-06-24 19:22:51,937] [INFO] [comm.py:637:init_distributed] cdb=None
 6: [2025-06-24 19:22:51,938] [INFO] [comm.py:637:init_distributed] cdb=None
 6: [2025-06-24 19:22:51,939] [INFO] [comm.py:637:init_distributed] cdb=None
 0: [2025-06-24 19:22:51,941] [INFO] [comm.py:637:init_distributed] cdb=None
 5: [2025-06-24 19:22:51,961] [INFO] [comm.py:637:init_distributed] cdb=None
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
 6: [2025-06-24 19:22:51,972] [INFO] [comm.py:637:init_distributed] cdb=None
 0: [2025-06-24 19:22:51,974] [INFO] [comm.py:637:init_distributed] cdb=None
 3: [2025-06-24 19:22:51,985] [INFO] [comm.py:637:init_distributed] cdb=None
28: [2025-06-24 19:22:51,993] [INFO] [comm.py:637:init_distributed] cdb=None
 6: [2025-06-24 19:22:52,023] [INFO] [comm.py:637:init_distributed] cdb=None
13: [2025-06-24 19:22:52,044] [INFO] [comm.py:637:init_distributed] cdb=None
23: [2025-06-24 19:22:52,044] [INFO] [comm.py:637:init_distributed] cdb=None
14: [2025-06-24 19:22:52,048] [INFO] [comm.py:637:init_distributed] cdb=None
23: [2025-06-24 19:22:52,048] [INFO] [comm.py:637:init_distributed] cdb=None
23: [2025-06-24 19:22:52,049] [INFO] [comm.py:637:init_distributed] cdb=None
 9: [2025-06-24 19:22:52,052] [INFO] [comm.py:637:init_distributed] cdb=None
23: [2025-06-24 19:22:52,052] [INFO] [comm.py:637:init_distributed] cdb=None
 9: [2025-06-24 19:22:52,053] [INFO] [comm.py:637:init_distributed] cdb=None
 9: [2025-06-24 19:22:52,055] [INFO] [comm.py:637:init_distributed] cdb=None
14: [2025-06-24 19:22:52,074] [INFO] [comm.py:637:init_distributed] cdb=None
25: [2025-06-24 19:22:52,075] [INFO] [comm.py:637:init_distributed] cdb=None
 9: [2025-06-24 19:22:52,110] [INFO] [comm.py:637:init_distributed] cdb=None
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
17: [2025-06-24 19:22:52,217] [INFO] [comm.py:637:init_distributed] cdb=None
17: [2025-06-24 19:22:52,217] [INFO] [comm.py:637:init_distributed] cdb=None
17: [2025-06-24 19:22:52,217] [INFO] [comm.py:637:init_distributed] cdb=None
17: [2025-06-24 19:22:52,217] [INFO] [comm.py:637:init_distributed] cdb=None
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
 1: [2025-06-24 19:22:52,416] [INFO] [comm.py:637:init_distributed] cdb=None
 1: [2025-06-24 19:22:52,417] [INFO] [comm.py:637:init_distributed] cdb=None
 1: [2025-06-24 19:22:52,419] [INFO] [comm.py:637:init_distributed] cdb=None
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
 1: [2025-06-24 19:22:52,499] [INFO] [comm.py:637:init_distributed] cdb=None
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
30: [2025-06-24 19:22:52,715] [INFO] [comm.py:637:init_distributed] cdb=None
30: [2025-06-24 19:22:52,716] [INFO] [comm.py:637:init_distributed] cdb=None
30: [2025-06-24 19:22:52,740] [INFO] [comm.py:637:init_distributed] cdb=None
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
30: [2025-06-24 19:22:52,819] [INFO] [comm.py:637:init_distributed] cdb=None
10: [2025-06-24 19:22:52,842] [INFO] [comm.py:637:init_distributed] cdb=None
10: [2025-06-24 19:22:52,846] [INFO] [comm.py:637:init_distributed] cdb=None
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
10: [2025-06-24 19:22:52,854] [INFO] [comm.py:637:init_distributed] cdb=None
10: [2025-06-24 19:22:52,857] [INFO] [comm.py:637:init_distributed] cdb=None
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
 2: [2025-06-24 19:22:53,048] [INFO] [comm.py:637:init_distributed] cdb=None
 2: [2025-06-24 19:22:53,050] [INFO] [comm.py:637:init_distributed] cdb=None
 2: [2025-06-24 19:22:53,051] [INFO] [comm.py:637:init_distributed] cdb=None
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
 2: [2025-06-24 19:22:53,229] [INFO] [comm.py:637:init_distributed] cdb=None
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
18: [2025-06-24 19:22:53,144] [INFO] [comm.py:637:init_distributed] cdb=None
18: [2025-06-24 19:22:53,144] [INFO] [comm.py:637:init_distributed] cdb=None
16: [2025-06-24 19:22:53,097] [INFO] [comm.py:637:init_distributed] cdb=None
27: [2025-06-24 19:22:53,292] [INFO] [comm.py:637:init_distributed] cdb=None
22: [2025-06-24 19:22:53,234] [INFO] [comm.py:637:init_distributed] cdb=None
22: [2025-06-24 19:22:53,234] [INFO] [comm.py:637:init_distributed] cdb=None
18: [2025-06-24 19:22:53,144] [INFO] [comm.py:637:init_distributed] cdb=None
16: [2025-06-24 19:22:53,098] [INFO] [comm.py:637:init_distributed] cdb=None
22: [2025-06-24 19:22:53,236] [INFO] [comm.py:637:init_distributed] cdb=None
18: [2025-06-24 19:22:53,144] [INFO] [comm.py:637:init_distributed] cdb=None
16: [2025-06-24 19:22:53,102] [INFO] [comm.py:637:init_distributed] cdb=None
22: [2025-06-24 19:22:53,244] [INFO] [comm.py:637:init_distributed] cdb=None
16: [2025-06-24 19:22:53,103] [INFO] [comm.py:637:init_distributed] cdb=None
27: [2025-06-24 19:22:53,295] [INFO] [comm.py:637:init_distributed] cdb=None
27: [2025-06-24 19:22:53,296] [INFO] [comm.py:637:init_distributed] cdb=None
27: [2025-06-24 19:22:53,313] [INFO] [comm.py:637:init_distributed] cdb=None
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
11: [2025-06-24 19:22:53,337] [INFO] [comm.py:637:init_distributed] cdb=None
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
11: [2025-06-24 19:22:53,344] [INFO] [comm.py:637:init_distributed] cdb=None
11: [2025-06-24 19:22:53,346] [INFO] [comm.py:637:init_distributed] cdb=None
11: [2025-06-24 19:22:53,363] [INFO] [comm.py:637:init_distributed] cdb=None
12: [2025-06-24 19:22:53,516] [INFO] [comm.py:637:init_distributed] cdb=None
12: [2025-06-24 19:22:53,516] [INFO] [comm.py:637:init_distributed] cdb=None
12: [2025-06-24 19:22:53,517] [INFO] [comm.py:637:init_distributed] cdb=None
19: [2025-06-24 19:22:53,575] [INFO] [comm.py:637:init_distributed] cdb=None
19: [2025-06-24 19:22:53,576] [INFO] [comm.py:637:init_distributed] cdb=None
12: [2025-06-24 19:22:53,587] [INFO] [comm.py:637:init_distributed] cdb=None
19: [2025-06-24 19:22:53,591] [INFO] [comm.py:637:init_distributed] cdb=None
19: [2025-06-24 19:22:53,596] [INFO] [comm.py:637:init_distributed] cdb=None
 6: Combined 1137190 records into all_train.json
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 6:   warnings.warn(
 6: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
 6: Combined 1137190 records into all_train.json
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 6:   warnings.warn(
 6: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
10: Combined 1137190 records into all_train.json
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
10:   warnings.warn(
10: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
28: Combined 1137190 records into all_train.json
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
28:   warnings.warn(
28: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
28: Combined 1137190 records into all_train.json
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
21: Combined 1137190 records into all_train.json
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
 5: Combined 1137190 records into all_train.json
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
28:   warnings.warn(
28: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
21:   warnings.warn(
21: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 5:   warnings.warn(
 5: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
21: Combined 1137190 records into all_train.json
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
 5: Combined 1137190 records into all_train.json
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
21:   warnings.warn(
21: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 5:   warnings.warn(
 5: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
28: Combined 1137190 records into all_train.json
28: 
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
28:   warnings.warn(
28: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
 5: Combined 1137190 records into all_train.json
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
15: Combined 1137190 records into all_train.json
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
31: Combined 1137190 records into all_train.json
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 5:   warnings.warn(
 5: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
15:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
15: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
24: Combined 1137190 records into all_train.json
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
23: Combined 1137190 records into all_train.json
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
31:   warnings.warn(
31: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
28: Combined 1137190 records into all_train.json
28: 
28: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
28:   warnings.warn(
 8: Combined 1137190 records into all_train.json
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
24:   warnings.warn(
24: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
25: Combined 1137190 records into all_train.json
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
15: Combined 1137190 records into all_train.json
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
23:   warnings.warn(
23: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 4: Combined 1137190 records into all_train.json
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
28:   warnings.warn(
28: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
 7: Combined 1137190 records into all_train.json
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
31: Combined 1137190 records into all_train.json
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 8:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
25:   warnings.warn(
 8: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
15:   warnings.warn(
25: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
24: Combined 1137190 records into all_train.json
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 4:   warnings.warn(
 4: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
29: Combined 1137190 records into all_train.json
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 7:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
31:   warnings.warn(
31: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
23: Combined 1137190 records into all_train.json
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
 8: Combined 1137190 records into all_train.json
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
29:   warnings.warn(
29: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
23:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
25: Combined 1137190 records into all_train.json
25: 
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
23: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 8:   warnings.warn(
 8: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
 4: Combined 1137190 records into all_train.json
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
25:   warnings.warn(
 5: Combined 1137190 records into all_train.json
 5: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 5:   warnings.warn(
25: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
24:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
 7: Combined 1137190 records into all_train.json
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
24: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
14: Combined 1137190 records into all_train.json
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 4:   warnings.warn(
 4: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 5:   warnings.warn(
 5: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 7:   warnings.warn(
 7: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
29: Combined 1137190 records into all_train.json
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
14:   warnings.warn(
14: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
29:   warnings.warn(
29: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
 1: Combined 1137190 records into all_train.json
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
20: Combined 1137190 records into all_train.json
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
 9: Combined 1137190 records into all_train.json
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
14: Combined 1137190 records into all_train.json
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 1:   warnings.warn(
 1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
20:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
14:   warnings.warn(
26: Combined 1137190 records into all_train.json
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
20: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
14: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
 1: Combined 1137190 records into all_train.json
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
20: Combined 1137190 records into all_train.json
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
26:   warnings.warn(
26: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 9:   warnings.warn(
 9: Combined 1137190 records into all_train.json
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 9: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 1:   warnings.warn(
 1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
20:   warnings.warn(
20: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 9:   warnings.warn(
 9: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
29: Combined 1137190 records into all_train.json
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
26: Combined 1137190 records into all_train.json
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
29:   warnings.warn(
29: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
26:   warnings.warn(
26: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
14: Combined 1137190 records into all_train.json
14: 
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
14:   warnings.warn(
14: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
 1: Combined 1137190 records into all_train.json
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
 9: Combined 1137190 records into all_train.json
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 1:   warnings.warn(
 1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
18: Combined 1137190 records into all_train.json
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
29: Combined 1137190 records into all_train.json
29: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
29:   warnings.warn(
13: Combined 1137190 records into all_train.json
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 9:   warnings.warn(
 9: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
 2: Combined 1137190 records into all_train.json
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
29:   warnings.warn(
29: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
13:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
13: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 2:   warnings.warn(
 2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
22: Combined 1137190 records into all_train.json
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
27: Combined 1137190 records into all_train.json
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
11: Combined 1137190 records into all_train.json
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
14: Combined 1137190 records into all_train.json
14: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
14:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
22:   warnings.warn(
18: Combined 1137190 records into all_train.json
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
22: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
13: Combined 1137190 records into all_train.json
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
 2: Combined 1137190 records into all_train.json
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
11:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
14:   warnings.warn(
27: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
11: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
14: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
13:   warnings.warn(
13: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 2:   warnings.warn(
 2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
 1: Combined 1137190 records into all_train.json
 1: 
 1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 1:   warnings.warn(
22: Combined 1137190 records into all_train.json
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
27: Combined 1137190 records into all_train.json
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
11: Combined 1137190 records into all_train.json
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 1:   warnings.warn(
 9: Combined 1137190 records into all_train.json
 9: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 9:   warnings.warn(
 1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
22:   warnings.warn(
22: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
27:   warnings.warn(
27: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
11:   warnings.warn(
11: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 9:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
 9: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
12: Combined 1137190 records into all_train.json
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
18:   warnings.warn(
18: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
 6: Combined 1137190 records into all_train.json
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 6:   warnings.warn(
 6: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
12: Combined 1137190 records into all_train.json
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
12:   warnings.warn(
12: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: Combined 1137190 records into all_train.json
 6: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 6:   warnings.warn(
18: Combined 1137190 records into all_train.json
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
21: Combined 1137190 records into all_train.json
21: 
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
12:   warnings.warn(
13: Combined 1137190 records into all_train.json
13: 
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
12: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
31: Combined 1137190 records into all_train.json
31: 
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
18:   warnings.warn(
18: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 6:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
 6: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
21:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
13:   warnings.warn(
21: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
13: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
24: Combined 1137190 records into all_train.json
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
31:   warnings.warn(
31: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
23: Combined 1137190 records into all_train.json
23: 
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
25: Combined 1137190 records into all_train.json
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
18: Combined 1137190 records into all_train.json
18: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
18:   warnings.warn(
 4: Combined 1137190 records into all_train.json
 4: 
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
 7: Combined 1137190 records into all_train.json
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
21: Combined 1137190 records into all_train.json
21: 
21: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
21:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
24:   warnings.warn(
24: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
13: Combined 1137190 records into all_train.json
13: 
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
23:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
13:   warnings.warn(
23: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: Combined 1137190 records into all_train.json
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
 8: Combined 1137190 records into all_train.json
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
25:   warnings.warn(
25: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
31: Combined 1137190 records into all_train.json
31: 
31: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
31:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 4:   warnings.warn(
 4: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 7:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
21:   warnings.warn(
 7: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
21: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
13:   warnings.warn(
26: Combined 1137190 records into all_train.json
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
13: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
15:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
15: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 8:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
 8: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
31:   warnings.warn(
24: Combined 1137190 records into all_train.json
24: 
24: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
24:   warnings.warn(
31: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
26:   warnings.warn(
26: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
23: Combined 1137190 records into all_train.json
23: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
23:   warnings.warn(
25: Combined 1137190 records into all_train.json
25: 
25: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
25:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
24:   warnings.warn(
24: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 4: Combined 1137190 records into all_train.json
 4: 
 4: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 4:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
 7: Combined 1137190 records into all_train.json
 7: 
 7: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 7:   warnings.warn(
15: Combined 1137190 records into all_train.json
15: 
15: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
15:   warnings.warn(
 8: Combined 1137190 records into all_train.json
 8: 
 8: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 8:   warnings.warn(
20: Combined 1137190 records into all_train.json
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
23:   warnings.warn(
23: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
26: Combined 1137190 records into all_train.json
26: 
26: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
26:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
25:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
25: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 4:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
 4: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 7:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 7: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
15:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 8:   warnings.warn(
15: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
 8: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
12: Combined 1137190 records into all_train.json
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
20:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
20: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
27: Combined 1137190 records into all_train.json
27: 
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
26:   warnings.warn(
26: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
11: Combined 1137190 records into all_train.json
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
20: Combined 1137190 records into all_train.json
20: 
20: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
20:   warnings.warn(
 2: Combined 1137190 records into all_train.json
 2: 
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
12:   warnings.warn(
22: Combined 1137190 records into all_train.json
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
12: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
12: Combined 1137190 records into all_train.json
12: 
12: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
12:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
27:   warnings.warn(
27: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
27: Combined 1137190 records into all_train.json
27: 
27: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
11:   warnings.warn(
11: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
20:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 2:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
20: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
11: Combined 1137190 records into all_train.json
11: 
11: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
11:   warnings.warn(
 2: Combined 1137190 records into all_train.json
 2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 2:   warnings.warn(
22: Combined 1137190 records into all_train.json
22: 
22: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
22:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
22:   warnings.warn(
22: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
18:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
18: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
12:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
27:   warnings.warn(
12: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
27: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
11:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 2:   warnings.warn(
11: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
22:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
22: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
18:   warnings.warn(
18: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
30: Combined 1137190 records into all_train.json
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
 0: Combined 1137190 records into all_train.json
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
30:   warnings.warn(
30: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 0: Combined 1137190 records into all_train.json
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 0:   warnings.warn(
30: Combined 1137190 records into all_train.json
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
 0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: Combined 1137190 records into all_train.json
 0: 
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
30: Combined 1137190 records into all_train.json
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 0:   warnings.warn(
 0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
16: Combined 1137190 records into all_train.json
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 0:   warnings.warn(
 0: Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
 0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
30:   warnings.warn(
30: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
17: Combined 1137190 records into all_train.json
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
16: Combined 1137190 records into all_train.json
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
 0: Combined 1137190 records into all_train.json
 0: 
 0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 0:   warnings.warn(
17: Combined 1137190 records into all_train.json
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
30: Combined 1137190 records into all_train.json
30: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
30:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
16:   warnings.warn(
16: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
17:   warnings.warn(
16: Combined 1137190 records into all_train.json
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
17: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
16:   warnings.warn(
16: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
17: Combined 1137190 records into all_train.json
17: 
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 0:   warnings.warn(
 0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
17:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
17: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
30:   warnings.warn(
10: Combined 1137190 records into all_train.json
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
30: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
 0: nid006495:241019:241019 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 0: nid006495:241019:241019 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.28<0>
 0: nid006495:241019:241019 [0] NCCL INFO cudaDriverVersion 12050
 0: nid006495:241019:241019 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
10: nid006506:263727:263727 [0] NCCL INFO cudaDriverVersion 12050
10: nid006506:263727:263727 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 0: nid006495:241020:241020 [1] NCCL INFO cudaDriverVersion 12050
 0: nid006495:241021:241021 [2] NCCL INFO cudaDriverVersion 12050
10: nid006506:263727:263727 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.64<0>
10: nid006506:263727:263727 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
16: nid006554:221584:221584 [3] NCCL INFO cudaDriverVersion 12050
16: nid006554:221584:221584 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
30:   warnings.warn(
16: nid006554:221584:221584 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.224<0>
16: nid006554:221584:221584 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
30: nid007318:20583:20583 [3] NCCL INFO cudaDriverVersion 12050
30: nid007318:20583:20583 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
30: nid007318:20581:20581 [1] NCCL INFO cudaDriverVersion 12050
30: nid007318:20581:20581 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
30: nid007318:20583:20583 [3] NCCL INFO Bootstrap : Using hsn0:172.28.42.32<0>
30: nid007318:20581:20581 [1] NCCL INFO Bootstrap : Using hsn0:172.28.42.32<0>
30: nid007318:20583:20583 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
30: nid007318:20581:20581 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
17: nid006555:206594:206594 [1] NCCL INFO cudaDriverVersion 12050
17: nid006555:206594:206594 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: nid006555:206596:206596 [3] NCCL INFO cudaDriverVersion 12050
17: nid006555:206596:206596 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: nid006555:206596:206596 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.228<0>
17: nid006555:206594:206594 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.228<0>
17: nid006555:206596:206596 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
17: nid006555:206594:206594 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
20: nid006558:215467:215467 [0] NCCL INFO cudaDriverVersion 12050
20: nid006558:215467:215467 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 9: nid006505:249081:249081 [1] NCCL INFO cudaDriverVersion 12050
 9: nid006505:249081:249081 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
15: nid006553:223925:223925 [2] NCCL INFO cudaDriverVersion 12050
27: nid006566:216749:216749 [3] NCCL INFO cudaDriverVersion 12050
15: nid006553:223925:223925 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
27: nid006566:216749:216749 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
31: nid007342:59767:59767 [0] NCCL INFO cudaDriverVersion 12050
31: nid007342:59767:59767 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
12: nid006508:205768:205768 [2] NCCL INFO cudaDriverVersion 12050
12: nid006508:205768:205768 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
20: nid006558:215467:215467 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.240<0>
20: nid006558:215467:215467 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 9: nid006505:249081:249081 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.60<0>
 9: nid006505:249081:249081 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216749:216749 [3] NCCL INFO Bootstrap : Using hsn0:172.28.53.225<0>
27: nid006566:216749:216749 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
15: nid006553:223925:223925 [2] NCCL INFO Bootstrap : Using hsn0:172.28.52.207<0>
15: nid006553:223925:223925 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
31: nid007342:59767:59767 [0] NCCL INFO Bootstrap : Using hsn0:172.28.42.112<0>
31: nid007342:59767:59767 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
12: nid006508:205768:205768 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.72<0>
12: nid006508:205768:205768 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
25: nid006564:223021:223021 [0] NCCL INFO cudaDriverVersion 12050
25: nid006564:223021:223021 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
15: nid006553:223924:223924 [1] NCCL INFO cudaDriverVersion 12050
15: nid006553:223924:223924 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222468:222468 [3] NCCL INFO cudaDriverVersion 12050
12: nid006508:205767:205767 [1] NCCL INFO cudaDriverVersion 12050
26: nid006565:222468:222468 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
12: nid006508:205767:205767 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
20: nid006558:215469:215469 [2] NCCL INFO cudaDriverVersion 12050
25: nid006564:223021:223021 [0] NCCL INFO Bootstrap : Using hsn0:172.28.32.8<0>
25: nid006564:223021:223021 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
15: nid006553:223924:223924 [1] NCCL INFO Bootstrap : Using hsn0:172.28.52.207<0>
15: nid006553:223924:223924 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 4: nid006499:254559:254559 [3] NCCL INFO cudaDriverVersion 12050
 4: nid006499:254559:254559 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222468:222468 [3] NCCL INFO Bootstrap : Using hsn0:172.28.32.12<0>
26: nid006565:222468:222468 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
12: nid006508:205767:205767 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.72<0>
12: nid006508:205767:205767 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 1: nid006496:242586:242586 [2] NCCL INFO cudaDriverVersion 12050
 1: nid006496:242586:242586 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
27: nid006566:216747:216747 [1] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218411:218411 [1] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218411:218411 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 4: nid006499:254559:254559 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.36<0>
 4: nid006499:254559:254559 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
13: nid006509:201788:201788 [0] NCCL INFO cudaDriverVersion 12050
13: nid006509:201788:201788 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 2: nid006497:227576:227576 [1] NCCL INFO cudaDriverVersion 12050
 2: nid006497:227576:227576 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211338:211338 [3] NCCL INFO cudaDriverVersion 12050
 1: nid006496:242586:242586 [2] NCCL INFO Bootstrap : Using hsn0:172.28.50.55<0>
 1: nid006496:242586:242586 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
11: nid006507:211338:211338 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
31: nid007342:59769:59769 [2] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218411:218411 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.52<0>
 8: nid006503:218411:218411 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 9: nid006505:249082:249082 [2] NCCL INFO cudaDriverVersion 12050
13: nid006509:201788:201788 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.76<0>
13: nid006509:201788:201788 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 2: nid006497:227576:227576 [1] NCCL INFO Bootstrap : Using hsn0:172.28.49.169<0>
 2: nid006497:227576:227576 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222274:222274 [3] NCCL INFO cudaDriverVersion 12050
22: nid006560:222274:222274 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211338:211338 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.68<0>
 7: nid006502:252587:252587 [3] NCCL INFO cudaDriverVersion 12050
11: nid006507:211338:211338 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252587:252587 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 2: nid006497:227577:227577 [2] NCCL INFO cudaDriverVersion 12050
24: nid006563:221140:221140 [0] NCCL INFO cudaDriverVersion 12050
24: nid006563:221140:221140 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 2: nid006497:227577:227577 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 7: nid006502:252587:252587 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.48<0>
 7: nid006502:252587:252587 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222274:222274 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.248<0>
22: nid006560:222274:222274 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 5: nid006500:260083:260083 [0] NCCL INFO cudaDriverVersion 12050
 5: nid006500:260083:260083 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222467:222467 [2] NCCL INFO cudaDriverVersion 12050
 2: nid006497:227577:227577 [2] NCCL INFO Bootstrap : Using hsn0:172.28.49.169<0>
 2: nid006497:227577:227577 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
24: nid006563:221140:221140 [0] NCCL INFO Bootstrap : Using hsn0:172.28.32.4<0>
24: nid006563:221140:221140 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 5: nid006500:260083:260083 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.40<0>
 5: nid006500:260083:260083 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
29: nid007305:27222:27222 [0] NCCL INFO cudaDriverVersion 12050
15: nid006553:223926:223926 [3] NCCL INFO cudaDriverVersion 12050
29: nid007305:27222:27222 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
13: nid006509:201789:201789 [1] NCCL INFO cudaDriverVersion 12050
25: nid006564:223023:223023 [2] NCCL INFO cudaDriverVersion 12050
21: nid006559:211123:211123 [0] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218410:218410 [0] NCCL INFO cudaDriverVersion 12050
21: nid006559:211123:211123 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
12: nid006508:205769:205769 [3] NCCL INFO cudaDriverVersion 12050
29: nid007305:27222:27222 [0] NCCL INFO Bootstrap : Using hsn0:172.28.41.244<0>
29: nid007305:27222:27222 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 1: nid006496:242584:242584 [0] NCCL INFO cudaDriverVersion 12050
14: nid006510:229443:229443 [1] NCCL INFO cudaDriverVersion 12050
14: nid006510:229443:229443 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221942:221942 [1] NCCL INFO cudaDriverVersion 12050
15: nid006553:223923:223923 [0] NCCL INFO cudaDriverVersion 12050
 6: nid006501:221942:221942 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
21: nid006559:211123:211123 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.244<0>
21: nid006559:211123:211123 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
12: nid006508:205766:205766 [0] NCCL INFO cudaDriverVersion 12050
20: nid006558:215470:215470 [3] NCCL INFO cudaDriverVersion 12050
14: nid006510:229443:229443 [1] NCCL INFO Bootstrap : Using hsn0:172.28.51.75<0>
14: nid006510:229443:229443 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 6: nid006501:221942:221942 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.44<0>
 6: nid006501:221942:221942 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 4: nid006499:254557:254557 [1] NCCL INFO cudaDriverVersion 12050
31: nid007342:59768:59768 [1] NCCL INFO cudaDriverVersion 12050
 9: nid006505:249080:249080 [0] NCCL INFO cudaDriverVersion 12050
20: nid006558:215468:215468 [1] NCCL INFO cudaDriverVersion 12050
31: nid007342:59770:59770 [3] NCCL INFO cudaDriverVersion 12050
21: nid006559:211125:211125 [2] NCCL INFO cudaDriverVersion 12050
24: nid006563:221141:221141 [1] NCCL INFO cudaDriverVersion 12050
21: nid006559:211125:211125 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
29: nid007305:27225:27225 [3] NCCL INFO cudaDriverVersion 12050
29: nid007305:27225:27225 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
10: Combined 1137190 records into all_train.json
10: 
 9: nid006505:249083:249083 [3] NCCL INFO cudaDriverVersion 12050
21: nid006559:211125:211125 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.244<0>
21: nid006559:211125:211125 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216746:216746 [0] NCCL INFO cudaDriverVersion 12050
 2: nid006497:227575:227575 [0] NCCL INFO cudaDriverVersion 12050
11: nid006507:211337:211337 [2] NCCL INFO cudaDriverVersion 12050
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
29: nid007305:27225:27225 [3] NCCL INFO Bootstrap : Using hsn0:172.28.41.244<0>
29: nid007305:27225:27225 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222271:222271 [0] NCCL INFO cudaDriverVersion 12050
26: nid006565:222465:222465 [0] NCCL INFO cudaDriverVersion 12050
27: nid006566:216748:216748 [2] NCCL INFO cudaDriverVersion 12050
 2: nid006497:227578:227578 [3] NCCL INFO cudaDriverVersion 12050
14: nid006510:229444:229444 [2] NCCL INFO cudaDriverVersion 12050
26: nid006565:222466:222466 [1] NCCL INFO cudaDriverVersion 12050
29: nid007305:27223:27223 [1] NCCL INFO cudaDriverVersion 12050
25: nid006564:223022:223022 [1] NCCL INFO cudaDriverVersion 12050
 4: nid006499:254556:254556 [0] NCCL INFO cudaDriverVersion 12050
 7: nid006502:252584:252584 [0] NCCL INFO cudaDriverVersion 12050
25: nid006564:223024:223024 [3] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218412:218412 [2] NCCL INFO cudaDriverVersion 12050
13: nid006509:201791:201791 [3] NCCL INFO cudaDriverVersion 12050
 1: nid006496:242585:242585 [1] NCCL INFO cudaDriverVersion 12050
 4: nid006499:254558:254558 [2] NCCL INFO cudaDriverVersion 12050
 8: nid006503:218413:218413 [3] NCCL INFO cudaDriverVersion 12050
 1: nid006496:242587:242587 [3] NCCL INFO cudaDriverVersion 12050
28: nid007251:72170:72170 [1] NCCL INFO cudaDriverVersion 12050
28: nid007251:72170:72170 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
13: nid006509:201790:201790 [2] NCCL INFO cudaDriverVersion 12050
23: nid006561:220713:220713 [2] NCCL INFO cudaDriverVersion 12050
23: nid006561:220713:220713 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 5: nid006500:260085:260085 [2] NCCL INFO cudaDriverVersion 12050
29: nid007305:27224:27224 [2] NCCL INFO cudaDriverVersion 12050
21: nid006559:211124:211124 [1] NCCL INFO cudaDriverVersion 12050
28: nid007251:72170:72170 [1] NCCL INFO Bootstrap : Using hsn0:172.28.41.60<0>
28: nid007251:72170:72170 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
23: nid006561:220713:220713 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.252<0>
23: nid006561:220713:220713 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72169:72169 [0] NCCL INFO cudaDriverVersion 12050
21: nid006559:211126:211126 [3] NCCL INFO cudaDriverVersion 12050
28: nid007251:72169:72169 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
22: nid006560:222272:222272 [1] NCCL INFO cudaDriverVersion 12050
11: nid006507:211336:211336 [1] NCCL INFO cudaDriverVersion 12050
28: nid007251:72171:72171 [2] NCCL INFO cudaDriverVersion 12050
28: nid007251:72171:72171 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 5: nid006500:260084:260084 [1] NCCL INFO cudaDriverVersion 12050
24: nid006563:221143:221143 [3] NCCL INFO cudaDriverVersion 12050
 7: nid006502:252586:252586 [2] NCCL INFO cudaDriverVersion 12050
22: nid006560:222273:222273 [2] NCCL INFO cudaDriverVersion 12050
11: nid006507:211335:211335 [0] NCCL INFO cudaDriverVersion 12050
 5: nid006500:260086:260086 [3] NCCL INFO cudaDriverVersion 12050
 6: nid006501:221944:221944 [3] NCCL INFO cudaDriverVersion 12050
28: nid007251:72172:72172 [3] NCCL INFO cudaDriverVersion 12050
18: nid006556:210775:210775 [1] NCCL INFO cudaDriverVersion 12050
18: nid006556:210775:210775 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
24: nid006563:221142:221142 [2] NCCL INFO cudaDriverVersion 12050
28: nid007251:72172:72172 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 7: nid006502:252585:252585 [1] NCCL INFO cudaDriverVersion 12050
14: nid006510:229445:229445 [3] NCCL INFO cudaDriverVersion 12050
18: nid006556:210775:210775 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.232<0>
18: nid006556:210775:210775 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72169:72169 [0] NCCL INFO Bootstrap : Using hsn0:172.28.41.60<0>
28: nid007251:72169:72169 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72171:72171 [2] NCCL INFO Bootstrap : Using hsn0:172.28.41.60<0>
28: nid007251:72172:72172 [3] NCCL INFO Bootstrap : Using hsn0:172.28.41.60<0>
28: nid007251:72171:72171 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72172:72172 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
14: nid006510:229442:229442 [0] NCCL INFO cudaDriverVersion 12050
23: nid006561:220714:220714 [3] NCCL INFO cudaDriverVersion 12050
 6: nid006501:221943:221943 [2] NCCL INFO cudaDriverVersion 12050
18: nid006556:210774:210774 [0] NCCL INFO cudaDriverVersion 12050
18: nid006556:210774:210774 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221941:221941 [0] NCCL INFO cudaDriverVersion 12050
18: nid006556:210774:210774 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.232<0>
18: nid006556:210774:210774 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
18: nid006556:210776:210776 [2] NCCL INFO cudaDriverVersion 12050
18: nid006556:210777:210777 [3] NCCL INFO cudaDriverVersion 12050
23: nid006561:220711:220711 [0] NCCL INFO cudaDriverVersion 12050
23: nid006561:220712:220712 [1] NCCL INFO cudaDriverVersion 12050
 0: nid006495:241020:241020 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 0: nid006495:241020:241020 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.28<0>
 0: nid006495:241020:241020 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
16: Combined 1137190 records into all_train.json
16: 
16: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
16:   warnings.warn(
20: nid006558:215469:215469 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
20: nid006558:215469:215469 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.240<0>
20: nid006558:215469:215469 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216747:216747 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
10: Combined 1137190 records into all_train.json
27: nid006566:216747:216747 [1] NCCL INFO Bootstrap : Using hsn0:172.28.53.225<0>
10: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
10:   warnings.warn(
27: nid006566:216747:216747 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
15: nid006553:223926:223926 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 9: nid006505:249082:249082 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 9: nid006505:249082:249082 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.60<0>
 9: nid006505:249082:249082 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
15: nid006553:223926:223926 [3] NCCL INFO Bootstrap : Using hsn0:172.28.52.207<0>
15: nid006553:223926:223926 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
25: nid006564:223023:223023 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
25: nid006564:223023:223023 [2] NCCL INFO Bootstrap : Using hsn0:172.28.32.8<0>
25: nid006564:223023:223023 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
31: nid007342:59769:59769 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
13: nid006509:201789:201789 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
31: nid007342:59769:59769 [2] NCCL INFO Bootstrap : Using hsn0:172.28.42.112<0>
31: nid007342:59769:59769 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
13: nid006509:201789:201789 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.76<0>
13: nid006509:201789:201789 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
11: nid006507:211337:211337 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
22: nid006560:222271:222271 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211337:211337 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.68<0>
11: nid006507:211337:211337 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222271:222271 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.248<0>
22: nid006560:222271:222271 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252584:252584 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
24: nid006563:221141:221141 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 7: nid006502:252584:252584 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.48<0>
 7: nid006502:252584:252584 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 5: nid006500:260085:260085 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
24: nid006563:221141:221141 [1] NCCL INFO Bootstrap : Using hsn0:172.28.32.4<0>
24: nid006563:221141:221141 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 5: nid006500:260085:260085 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.40<0>
 5: nid006500:260085:260085 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
 6: nid006501:221944:221944 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221941:221941 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221941:221941 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.44<0>
 6: nid006501:221941:221941 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 6: nid006501:221944:221944 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.44<0>
 6: nid006501:221944:221944 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
29: nid007305:27223:27223 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
29: nid007305:27223:27223 [1] NCCL INFO Bootstrap : Using hsn0:172.28.41.244<0>
29: nid007305:27223:27223 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
14: nid006510:229444:229444 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
14: nid006510:229444:229444 [2] NCCL INFO Bootstrap : Using hsn0:172.28.51.75<0>
14: nid006510:229444:229444 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
23: nid006561:220714:220714 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
23: nid006561:220714:220714 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.252<0>
23: nid006561:220714:220714 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
18: nid006556:210776:210776 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 8: nid006503:218410:218410 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
18: nid006556:210776:210776 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.232<0>
18: nid006556:210776:210776 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
 1: nid006496:242584:242584 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 8: nid006503:218410:218410 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.52<0>
 8: nid006503:218410:218410 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
21: nid006559:211124:211124 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
12: nid006508:205769:205769 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
12: nid006508:205766:205766 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 1: nid006496:242584:242584 [0] NCCL INFO Bootstrap : Using hsn0:172.28.50.55<0>
 1: nid006496:242584:242584 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 4: nid006499:254557:254557 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222467:222467 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
21: nid006559:211124:211124 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.244<0>
21: nid006559:211124:211124 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 4: nid006499:254557:254557 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.36<0>
 4: nid006499:254557:254557 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
26: nid006565:222467:222467 [2] NCCL INFO Bootstrap : Using hsn0:172.28.32.12<0>
26: nid006565:222467:222467 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
12: nid006508:205766:205766 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.72<0>
12: nid006508:205769:205769 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.72<0>
12: nid006508:205766:205766 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
12: nid006508:205769:205769 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 0: nid006495:241021:241021 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 0: nid006495:241021:241021 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.28<0>
 0: nid006495:241021:241021 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
20: nid006558:215470:215470 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
20: nid006558:215470:215470 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.240<0>
20: nid006558:215470:215470 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 9: nid006505:249080:249080 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 9: nid006505:249080:249080 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.60<0>
 9: nid006505:249080:249080 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
15: nid006553:223923:223923 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: Combined 1137190 records into all_train.json
17: 
15: nid006553:223923:223923 [0] NCCL INFO Bootstrap : Using hsn0:172.28.52.207<0>
15: nid006553:223923:223923 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
17: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
17:   warnings.warn(
25: nid006564:223022:223022 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
25: nid006564:223022:223022 [1] NCCL INFO Bootstrap : Using hsn0:172.28.32.8<0>
25: nid006564:223022:223022 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
24: nid006563:221143:221143 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211336:211336 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
24: nid006563:221143:221143 [3] NCCL INFO Bootstrap : Using hsn0:172.28.32.4<0>
24: nid006563:221143:221143 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252586:252586 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211336:211336 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.68<0>
11: nid006507:211336:211336 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252586:252586 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.48<0>
 7: nid006502:252586:252586 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
30: nid007318:20580:20580 [0] NCCL INFO cudaDriverVersion 12050
29: nid007305:27224:27224 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
31: nid007342:59768:59768 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
29: nid007305:27224:27224 [2] NCCL INFO Bootstrap : Using hsn0:172.28.41.244<0>
29: nid007305:27224:27224 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
31: nid007342:59768:59768 [1] NCCL INFO Bootstrap : Using hsn0:172.28.42.112<0>
31: nid007342:59768:59768 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 6: nid006501:221943:221943 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221943:221943 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.44<0>
 6: nid006501:221943:221943 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
14: nid006510:229445:229445 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
14: nid006510:229445:229445 [3] NCCL INFO Bootstrap : Using hsn0:172.28.51.75<0>
14: nid006510:229445:229445 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 8: nid006503:218412:218412 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 8: nid006503:218412:218412 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.52<0>
 8: nid006503:218412:218412 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
 5: nid006500:260084:260084 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
18: nid006556:210777:210777 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 5: nid006500:260084:260084 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.40<0>
 5: nid006500:260084:260084 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
23: nid006561:220711:220711 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
18: nid006556:210777:210777 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.232<0>
18: nid006556:210777:210777 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
23: nid006561:220711:220711 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.252<0>
23: nid006561:220711:220711 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 2: nid006497:227575:227575 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222465:222465 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 4: nid006499:254556:254556 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222465:222465 [0] NCCL INFO Bootstrap : Using hsn0:172.28.32.12<0>
26: nid006565:222465:222465 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 2: nid006497:227575:227575 [0] NCCL INFO Bootstrap : Using hsn0:172.28.49.169<0>
 2: nid006497:227575:227575 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 4: nid006499:254556:254556 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.36<0>
 4: nid006499:254556:254556 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 0: nid006495:241022:241022 [3] NCCL INFO cudaDriverVersion 12050
20: nid006558:215468:215468 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
20: nid006558:215468:215468 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.240<0>
20: nid006558:215468:215468 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216746:216746 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
27: nid006566:216746:216746 [0] NCCL INFO Bootstrap : Using hsn0:172.28.53.225<0>
27: nid006566:216746:216746 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216748:216748 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
27: nid006566:216748:216748 [2] NCCL INFO Bootstrap : Using hsn0:172.28.53.225<0>
27: nid006566:216748:216748 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
14: nid006510:229442:229442 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
14: nid006510:229442:229442 [0] NCCL INFO Bootstrap : Using hsn0:172.28.51.75<0>
14: nid006510:229442:229442 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 9: nid006505:249083:249083 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 9: nid006505:249083:249083 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.60<0>
 9: nid006505:249083:249083 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252585:252585 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 7: nid006502:252585:252585 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.48<0>
 7: nid006502:252585:252585 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
31: nid007342:59770:59770 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
31: nid007342:59770:59770 [3] NCCL INFO Bootstrap : Using hsn0:172.28.42.112<0>
31: nid007342:59770:59770 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
25: nid006564:223024:223024 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
25: nid006564:223024:223024 [3] NCCL INFO Bootstrap : Using hsn0:172.28.32.8<0>
25: nid006564:223024:223024 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
13: nid006509:201791:201791 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
13: nid006509:201790:201790 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 8: nid006503:218413:218413 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
13: nid006509:201791:201791 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.76<0>
13: nid006509:201790:201790 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.76<0>
 8: nid006503:218413:218413 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.52<0>
 8: nid006503:218413:218413 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
13: nid006509:201791:201791 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
13: nid006509:201790:201790 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222272:222272 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
22: nid006560:222272:222272 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.248<0>
22: nid006560:222272:222272 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
22: nid006560:222273:222273 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
22: nid006560:222273:222273 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.248<0>
22: nid006560:222273:222273 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
24: nid006563:221142:221142 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
24: nid006563:221142:221142 [2] NCCL INFO Bootstrap : Using hsn0:172.28.32.4<0>
24: nid006563:221142:221142 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
11: nid006507:211335:211335 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
11: nid006507:211335:211335 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.68<0>
11: nid006507:211335:211335 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72169:73329 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72169:73329 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
28: nid007251:72169:73329 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 1: nid006496:242587:242587 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 1: nid006496:242585:242585 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 1: nid006496:242587:242587 [3] NCCL INFO Bootstrap : Using hsn0:172.28.50.55<0>
 1: nid006496:242585:242585 [1] NCCL INFO Bootstrap : Using hsn0:172.28.50.55<0>
 1: nid006496:242587:242587 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 1: nid006496:242585:242585 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
17:   warnings.warn(
21: nid006559:211126:211126 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
28: nid007251:72170:73327 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72170:73327 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
28: nid007251:72170:73327 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211126:211126 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.244<0>
21: nid006559:211126:211126 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
26: nid006565:222466:222466 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
26: nid006565:222466:222466 [1] NCCL INFO Bootstrap : Using hsn0:172.28.32.12<0>
26: nid006565:222466:222466 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
28: 
28: nid007251:72169:73329 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
28: 
28: nid007251:72170:73327 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
23: nid006561:220712:220712 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
17:   warnings.warn(
23: nid006561:220712:220712 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.252<0>
23: nid006561:220712:220712 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72171:73330 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72171:73330 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
28: nid007251:72171:73330 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Creating one domain per process
17: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Creating one domain per process
28: nid007251:72172:73331 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72172:73331 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
28: nid007251:72172:73331 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
28: 
28: nid007251:72171:73330 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Support for global registrations: false
28: nid007251:72169:73329 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Support for global registrations: false
28: nid007251:72170:73327 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254558:254558 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
28: 
28: nid007251:72172:73331 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 4: nid006499:254558:254558 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.36<0>
 4: nid006499:254558:254558 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Creating one domain per process
28: nid007251:72169:73329 [0] NCCL INFO Using network AWS Libfabric
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
28: nid007251:72170:73327 [1] NCCL INFO Using network AWS Libfabric
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Creating one domain per process
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Support for global registrations: false
28: nid007251:72171:73330 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Support for global registrations: false
28: nid007251:72172:73331 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
28: nid007251:72171:73330 [2] NCCL INFO Using network AWS Libfabric
28: nid007251:72172:73331 [3] NCCL INFO Using network AWS Libfabric
17: nid006555:206595:206595 [2] NCCL INFO cudaDriverVersion 12050
17: nid006555:206595:206595 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
17: nid006555:206595:206595 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.228<0>
17: nid006555:206595:206595 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
 2: nid006497:227578:227578 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 2: nid006497:227578:227578 [3] NCCL INFO Bootstrap : Using hsn0:172.28.49.169<0>
 2: nid006497:227578:227578 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 0: nid006495:241022:241022 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 0: nid006495:241022:241022 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.28<0>
 0: nid006495:241022:241022 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
16:   warnings.warn(
16: nid006554:221581:221581 [0] NCCL INFO cudaDriverVersion 12050
16: nid006554:221581:221581 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
10:   warnings.warn(
16: nid006554:221581:221581 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.224<0>
16: nid006554:221581:221581 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
16: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
16: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
10: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
10: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
10: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
27: nid006566:216749:217883 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216747:217884 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216747:217884 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
27: nid006566:216749:217883 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
27: nid006566:216747:217884 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
27: nid006566:216749:217883 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
27: nid006566:216746:217885 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216746:217885 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
27: nid006566:216748:217887 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216746:217885 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
27: nid006566:216748:217887 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
27: nid006566:216748:217887 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
27: 
27: nid006566:216748:217887 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: 
27: nid006566:216746:217885 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: 
27: nid006566:216749:217883 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: 
27: nid006566:216747:217884 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
29: nid007305:27222:28341 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27225:28342 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27222:28341 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
29: nid007305:27222:28341 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
29: nid007305:27225:28342 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
29: nid007305:27225:28342 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27223:28343 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27223:28343 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
29: nid007305:27223:28343 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27224:28344 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27224:28344 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
29: nid007305:27224:28344 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: nid006506:263730:263730 [3] NCCL INFO cudaDriverVersion 12050
10: nid006506:263730:263730 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
10: nid006506:263730:263730 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.64<0>
10: nid006506:263730:263730 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
16: nid006554:221582:221582 [1] NCCL INFO cudaDriverVersion 12050
16: nid006554:221582:221582 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
16: nid006554:221583:221583 [2] NCCL INFO cudaDriverVersion 12050
16: nid006554:221583:221583 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
16: nid006554:221583:221583 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.224<0>
16: nid006554:221582:221582 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.224<0>
16: nid006554:221582:221582 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
16: nid006554:221583:221583 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
10: nid006506:263729:263729 [2] NCCL INFO cudaDriverVersion 12050
21: nid006559:211123:212275 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211125:212276 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211125:212276 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
21: nid006559:211123:212275 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
21: nid006559:211125:212276 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
21: nid006559:211123:212275 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: nid006506:263729:263729 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
29: 
29: nid007305:27223:28343 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263729:263729 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.64<0>
10: nid006506:263729:263729 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Creating one domain per process
21: nid006559:211126:212278 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211126:212278 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
21: nid006559:211126:212278 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: nid006506:263728:263728 [1] NCCL INFO cudaDriverVersion 12050
10: nid006506:263728:263728 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
29: 
29: nid007305:27225:28342 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263728:263728 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.64<0>
10: nid006506:263728:263728 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: 
29: nid007305:27222:28341 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
21: nid006559:211124:212277 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211124:212277 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
21: nid006559:211124:212277 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: 
21: nid006559:211123:212275 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
21: 
21: nid006559:211125:212276 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
21: 
21: nid006559:211126:212278 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Support for global registrations: false
27: nid006566:216748:217887 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
29: 
29: nid007305:27224:28344 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: nid006566:216748:217887 [2] NCCL INFO Using network AWS Libfabric
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Creating one domain per process
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Creating one domain per process
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Creating one domain per process
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
21: 
21: nid006559:211124:212277 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Support for global registrations: false
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Support for global registrations: false
27: nid006566:216749:217883 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
27: nid006566:216747:217884 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Support for global registrations: false
27: nid006566:216746:217885 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 6: nid006501:221941:223089 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221941:223089 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 6: nid006501:221941:223089 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
27: nid006566:216749:217883 [3] NCCL INFO Using network AWS Libfabric
27: nid006566:216747:217884 [1] NCCL INFO Using network AWS Libfabric
27: nid006566:216746:217885 [0] NCCL INFO Using network AWS Libfabric
 6: 
 6: nid006501:221941:223089 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: nid006502:252584:253741 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252587:253740 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252587:253740 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 7: nid006502:252584:253741 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 7: nid006502:252584:253741 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 7: nid006502:252587:253740 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241019:242169 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241022:242172 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241019:242169 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 0: nid006495:241022:242172 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 0: nid006495:241019:242169 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 0: nid006495:241022:242172 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Creating one domain per process
17: nid006555:206594:207777 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206596:207778 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206596:207778 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
17: nid006555:206594:207777 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
17: nid006555:206594:207777 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
17: nid006555:206596:207778 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241021:242171 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241020:242170 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241021:242171 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 0: nid006495:241020:242170 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 0: nid006495:241021:242171 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 0: nid006495:241020:242170 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Creating one domain per process
16: nid006554:221584:222757 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221584:222757 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
16: nid006554:221584:222757 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 7: nid006502:252586:253742 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252586:253742 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 7: nid006502:252586:253742 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 7: nid006502:252585:253743 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252585:253743 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 7: nid006502:252585:253743 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Support for global registrations: false
 6: nid006501:221941:223089 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 6: nid006501:221943:223091 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221942:223088 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221942:223088 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 6: nid006501:221943:223091 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 6: nid006501:221942:223088 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 6: nid006501:221943:223091 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Creating one domain per process
 7: 
 7: nid006502:252587:253740 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: 
 7: nid006502:252584:253741 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Creating one domain per process
 6: nid006501:221944:223090 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221944:223090 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 6: nid006501:221944:223090 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
16: nid006554:221581:222760 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221581:222760 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
16: nid006554:221581:222760 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Creating one domain per process
31: nid007342:59767:61194 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59767:61194 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
31: nid007342:59767:61194 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
17: 
17: nid006555:206596:207778 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Creating one domain per process
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Creating one domain per process
17: 
17: nid006555:206594:207777 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 6: nid006501:221941:223089 [0] NCCL INFO Using network AWS Libfabric
16: 
16: nid006554:221584:222757 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Support for global registrations: false
29: nid007305:27223:28343 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
16: 
16: nid006554:221581:222760 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Creating one domain per process
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Support for global registrations: false
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Support for global registrations: false
21: nid006559:211123:212275 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
21: nid006559:211125:212276 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Creating one domain per process
10: nid006506:263727:264880 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263727:264880 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
10: nid006506:263727:264880 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Support for global registrations: false
29: nid007305:27222:28341 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
14: nid006510:229443:230607 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229444:230608 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229442:230610 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229444:230608 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
14: nid006510:229443:230607 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
14: nid006510:229444:230608 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
14: nid006510:229442:230610 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
14: nid006510:229443:230607 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
14: nid006510:229442:230610 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Support for global registrations: false
21: nid006559:211126:212278 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Support for global registrations: false
21: nid006559:211124:212277 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 9: nid006505:249082:250266 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249081:250246 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249081:250246 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 9: nid006505:249082:250266 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 9: nid006505:249082:250266 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 9: nid006505:249081:250246 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 9: nid006505:249083:250268 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249080:250267 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249083:250268 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 9: nid006505:249080:250267 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 9: nid006505:249083:250268 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 9: nid006505:249080:250267 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Support for global registrations: false
29: nid007305:27225:28342 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
29: nid007305:27223:28343 [1] NCCL INFO Using network AWS Libfabric
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Creating one domain per process
31: 
31: nid007342:59767:61194 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Creating one domain per process
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
31: nid007342:59769:61195 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59769:61195 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
31: nid007342:59769:61195 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Support for global registrations: false
29: nid007305:27224:28344 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
21: nid006559:211123:212275 [0] NCCL INFO Using network AWS Libfabric
21: nid006559:211125:212276 [2] NCCL INFO Using network AWS Libfabric
 6: 
 6: nid006501:221942:223088 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 6: 
 6: nid006501:221943:223091 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 6: 
 6: nid006501:221944:223090 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
29: nid007305:27222:28341 [0] NCCL INFO Using network AWS Libfabric
14: nid006510:229445:230609 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229445:230609 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
14: nid006510:229445:230609 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Creating one domain per process
21: nid006559:211126:212278 [3] NCCL INFO Using network AWS Libfabric
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
21: nid006559:211124:212277 [1] NCCL INFO Using network AWS Libfabric
 0: 
 0: nid006495:241019:242169 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 0: 
 0: nid006495:241022:242172 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
31: nid007342:59768:61196 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59768:61196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 0: 
 0: nid006495:241020:242170 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 0: 
 0: nid006495:241021:242171 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
31: nid007342:59768:61196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
29: nid007305:27225:28342 [3] NCCL INFO Using network AWS Libfabric
31: nid007342:59770:61197 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59770:61197 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
31: nid007342:59770:61197 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
31: 
31: nid007342:59769:61195 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
29: nid007305:27224:28344 [2] NCCL INFO Using network AWS Libfabric
10: 
10: nid006506:263727:264880 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Support for global registrations: false
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Support for global registrations: false
17: nid006555:206594:207777 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
17: nid006555:206596:207778 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Creating one domain per process
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Creating one domain per process
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Support for global registrations: false
16: nid006554:221584:222757 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Support for global registrations: false
16: nid006554:221581:222760 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Support for global registrations: false
31: nid007342:59767:61194 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Creating one domain per process
 7: 
 7: nid006502:252586:253742 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
17: nid006555:206594:207777 [1] NCCL INFO Using network AWS Libfabric
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Creating one domain per process
17: nid006555:206596:207778 [3] NCCL INFO Using network AWS Libfabric
 7: 
 7: nid006502:252585:253743 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
31: 
31: nid007342:59768:61196 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
31: 
31: nid007342:59770:61197 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
16: nid006554:221584:222757 [3] NCCL INFO Using network AWS Libfabric
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
16: nid006554:221581:222760 [0] NCCL INFO Using network AWS Libfabric
31: nid007342:59767:61194 [0] NCCL INFO Using network AWS Libfabric
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Support for global registrations: false
10: nid006506:263727:264880 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Support for global registrations: false
31: nid007342:59769:61195 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
14: 
14: nid006510:229444:230608 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263727:264880 [0] NCCL INFO Using network AWS Libfabric
14: 
14: nid006510:229442:230610 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
31: nid007342:59769:61195 [2] NCCL INFO Using network AWS Libfabric
14: 
14: nid006510:229443:230607 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
14: 
14: nid006510:229445:230609 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Support for global registrations: false
 7: nid006502:252584:253741 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Creating one domain per process
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Creating one domain per process
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Creating one domain per process
 9: 
 9: nid006505:249081:250246 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Support for global registrations: false
 7: nid006502:252587:253740 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: 
 9: nid006505:249082:250266 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Creating one domain per process
 9: 
 9: nid006505:249083:250268 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 9: 
 9: nid006505:249080:250267 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 7: nid006502:252584:253741 [0] NCCL INFO Using network AWS Libfabric
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Creating one domain per process
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20580:20580 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20580:20580 [0] NCCL INFO Bootstrap : Using hsn0:172.28.42.32<0>
30: nid007318:20580:20580 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
 7: nid006502:252587:253740 [3] NCCL INFO Using network AWS Libfabric
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Creating one domain per process
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Creating one domain per process
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Support for global registrations: false
31: nid007342:59770:61197 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Support for global registrations: false
31: nid007342:59768:61196 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Support for global registrations: false
 6: nid006501:221942:223088 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Support for global registrations: false
 6: nid006501:221944:223090 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Support for global registrations: false
 6: nid006501:221943:223091 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
31: nid007342:59770:61197 [3] NCCL INFO Using network AWS Libfabric
31: nid007342:59768:61196 [1] NCCL INFO Using network AWS Libfabric
13: nid006509:201791:202940 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201791:202940 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
13: nid006509:201791:202940 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: nid006509:201790:202941 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201789:202939 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201790:202941 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
13: nid006509:201790:202941 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
13: nid006509:201789:202939 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
13: nid006509:201789:202939 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 6: nid006501:221942:223088 [1] NCCL INFO Using network AWS Libfabric
13: nid006509:201788:202938 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201788:202938 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
13: nid006509:201788:202938 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 6: nid006501:221944:223090 [3] NCCL INFO Using network AWS Libfabric
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Support for global registrations: false
 7: nid006502:252586:253742 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Creating one domain per process
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Support for global registrations: false
 7: nid006502:252585:253743 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 6: nid006501:221943:223091 [2] NCCL INFO Using network AWS Libfabric
 7: nid006502:252586:253742 [2] NCCL INFO Using network AWS Libfabric
 7: nid006502:252585:253743 [1] NCCL INFO Using network AWS Libfabric
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Support for global registrations: false
 9: nid006505:249082:250266 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: nid006505:249082:250266 [2] NCCL INFO Using network AWS Libfabric
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Creating one domain per process
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Creating one domain per process
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Creating one domain per process
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Creating one domain per process
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Support for global registrations: false
 0: nid006495:241022:242172 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Support for global registrations: false
 9: nid006505:249081:250246 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
13: 
13: nid006509:201788:202938 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 0: nid006495:241022:242172 [3] NCCL INFO Using network AWS Libfabric
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Support for global registrations: false
 0: nid006495:241019:242169 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
24: nid006563:221141:222303 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221141:222303 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
24: nid006563:221141:222303 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
13: 
13: nid006509:201791:202940 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: 
13: nid006509:201789:202939 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
13: 
13: nid006509:201790:202941 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Support for global registrations: false
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Support for global registrations: false
 0: nid006495:241021:242171 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Creating one domain per process
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Support for global registrations: false
 9: nid006505:249080:250267 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 9: nid006505:249081:250246 [1] NCCL INFO Using network AWS Libfabric
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Support for global registrations: false
 9: nid006505:249083:250268 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Support for global registrations: false
14: nid006510:229444:230608 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Creating one domain per process
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Creating one domain per process
 0: nid006495:241020:242170 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 0: nid006495:241019:242169 [0] NCCL INFO Using network AWS Libfabric
 0: nid006495:241021:242171 [2] NCCL INFO Using network AWS Libfabric
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 0: nid006495:241020:242170 [1] NCCL INFO Using network AWS Libfabric
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 9: nid006505:249080:250267 [0] NCCL INFO Using network AWS Libfabric
30: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
24: 
24: nid006563:221141:222303 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 9: nid006505:249083:250268 [3] NCCL INFO Using network AWS Libfabric
14: nid006510:229444:230608 [2] NCCL INFO Using network AWS Libfabric
22: nid006560:222274:223424 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222274:223424 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
22: nid006560:222274:223424 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Creating one domain per process
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
24: nid006563:221140:222302 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221140:222302 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
24: nid006563:221140:222302 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Support for global registrations: false
14: nid006510:229445:230609 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Support for global registrations: false
14: nid006510:229443:230607 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
22: nid006560:222271:223425 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222271:223425 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
22: nid006560:222271:223425 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Support for global registrations: false
14: nid006510:229442:230610 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
24: nid006563:221143:222304 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221143:222304 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
24: nid006563:221143:222304 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: nid006563:221142:222307 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221142:222307 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
24: nid006563:221142:222307 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: 
24: nid006563:221140:222302 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
25: nid006564:223021:224160 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223023:224161 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223023:224161 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
25: nid006564:223021:224160 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
25: nid006564:223021:224160 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
25: nid006564:223023:224161 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229443:230607 [1] NCCL INFO Using network AWS Libfabric
14: nid006510:229445:230609 [3] NCCL INFO Using network AWS Libfabric
25: nid006564:223024:224163 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223024:224163 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
25: nid006564:223024:224163 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
25: nid006564:223022:224162 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223022:224162 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
25: nid006564:223022:224162 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
14: nid006510:229442:230610 [0] NCCL INFO Using network AWS Libfabric
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218410:219569 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218411:219568 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218410:219569 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 8: nid006503:218411:219568 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 8: nid006503:218411:219568 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 8: nid006503:218410:219569 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Support for global registrations: false
24: nid006563:221141:222303 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 8: nid006503:218413:219571 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218413:219571 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 8: nid006503:218413:219571 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 8: nid006503:218412:219570 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218412:219570 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 8: nid006503:218412:219570 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
22: 
22: nid006560:222271:223425 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: 
22: nid006560:222274:223424 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221141:222303 [1] NCCL INFO Using network AWS Libfabric
11: nid006507:211338:212472 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211337:212473 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211335:212475 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211338:212472 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
11: nid006507:211337:212473 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
11: nid006507:211338:212472 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
11: nid006507:211335:212475 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
11: nid006507:211337:212473 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
11: nid006507:211335:212475 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
11: nid006507:211336:212474 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
11: nid006507:211336:212474 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
11: nid006507:211336:212474 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
22: nid006560:222272:223426 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222272:223426 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
22: nid006560:222272:223426 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
23: nid006561:220713:221895 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220714:221896 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220714:221896 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
23: nid006561:220713:221895 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
23: nid006561:220713:221895 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
23: nid006561:220714:221896 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
24: 
24: nid006563:221143:222304 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: 
24: nid006563:221142:222307 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Support for global registrations: false
24: nid006563:221140:222302 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
22: nid006560:222273:223427 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222273:223427 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
22: nid006560:222273:223427 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
23: nid006561:220711:221897 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220711:221897 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
23: nid006561:220711:221897 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Creating one domain per process
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Creating one domain per process
26: nid006565:222467:223652 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222468:223651 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222467:223652 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
26: nid006565:222468:223651 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
26: nid006565:222468:223651 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
26: nid006565:222467:223652 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Creating one domain per process
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: nid006561:220712:221898 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220712:221898 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
23: nid006561:220712:221898 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222465:223653 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222465:223653 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
26: nid006565:222465:223653 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222466:223654 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222466:223654 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
26: nid006565:222466:223654 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
24: nid006563:221140:222302 [0] NCCL INFO Using network AWS Libfabric
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: 
23: nid006561:220713:221895 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
23: 
23: nid006561:220714:221896 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: 
22: nid006560:222272:223426 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: 
22: nid006560:222273:223427 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Support for global registrations: false
22: nid006560:222271:223425 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
25: 
25: nid006564:223021:224160 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
25: 
25: nid006564:223024:224163 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Support for global registrations: false
13: nid006509:201788:202938 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Support for global registrations: false
22: nid006560:222274:223424 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
23: 
23: nid006561:220711:221897 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
25: 
25: nid006564:223023:224161 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Creating one domain per process
25: 
25: nid006564:223022:224162 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 8: 
 8: nid006503:218411:219568 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Creating one domain per process
 8: 
 8: nid006503:218410:219569 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Creating one domain per process
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Creating one domain per process
 8: 
 8: nid006503:218412:219570 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 8: 
 8: nid006503:218413:219571 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
22: nid006560:222271:223425 [0] NCCL INFO Using network AWS Libfabric
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Creating one domain per process
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Creating one domain per process
18: nid006556:210775:211994 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210775:211994 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
18: nid006556:210775:211994 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Creating one domain per process
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: nid006509:201788:202938 [0] NCCL INFO Using network AWS Libfabric
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
22: nid006560:222274:223424 [3] NCCL INFO Using network AWS Libfabric
18: nid006556:210776:211996 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210776:211996 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
18: nid006556:210776:211996 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: 
26: nid006565:222467:223652 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
26: 
26: nid006565:222468:223651 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 5: nid006500:260086:260086 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
18: nid006556:210774:211995 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210774:211995 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
18: nid006556:210774:211995 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: 
26: nid006565:222465:223653 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 5: nid006500:260086:260086 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.40<0>
 5: nid006500:260086:260086 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Support for global registrations: false
24: nid006563:221143:222304 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
23: 
23: nid006561:220712:221898 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Support for global registrations: false
13: nid006509:201791:202940 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Support for global registrations: false
13: nid006509:201789:202939 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
18: nid006556:210777:211997 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210777:211997 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
18: nid006556:210777:211997 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Support for global registrations: false
13: nid006509:201790:202941 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
11: 
11: nid006507:211335:212475 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
26: 
26: nid006565:222466:223654 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Support for global registrations: false
24: nid006563:221142:222307 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Support for global registrations: false
22: nid006560:222272:223426 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
11: 
11: nid006507:211338:212472 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
11: 
11: nid006507:211337:212473 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
11: 
11: nid006507:211336:212474 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Support for global registrations: false
22: nid006560:222273:223427 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
24: nid006563:221143:222304 [3] NCCL INFO Using network AWS Libfabric
18: 
18: nid006556:210775:211994 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
13: nid006509:201789:202939 [1] NCCL INFO Using network AWS Libfabric
13: nid006509:201791:202940 [3] NCCL INFO Using network AWS Libfabric
13: nid006509:201790:202941 [2] NCCL INFO Using network AWS Libfabric
24: nid006563:221142:222307 [2] NCCL INFO Using network AWS Libfabric
22: nid006560:222272:223426 [1] NCCL INFO Using network AWS Libfabric
18: 
18: nid006556:210776:211996 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
18: 
18: nid006556:210774:211995 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
22: nid006560:222273:223427 [2] NCCL INFO Using network AWS Libfabric
18: 
18: nid006556:210777:211997 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223924:225067 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223925:225066 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223925:225066 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
15: nid006553:223924:225067 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
15: nid006553:223925:225066 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
15: nid006553:223924:225067 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
15: nid006553:223923:225069 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223923:225069 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
15: nid006553:223923:225069 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Support for global registrations: false
23: nid006561:220713:221895 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Creating one domain per process
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Support for global registrations: false
23: nid006561:220714:221896 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20582:20582 [2] NCCL INFO cudaDriverVersion 12050
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Creating one domain per process
23: nid006561:220713:221895 [2] NCCL INFO Using network AWS Libfabric
30: nid007318:20582:20582 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Creating one domain per process
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20582:20582 [2] NCCL INFO Bootstrap : Using hsn0:172.28.42.32<0>
30: nid007318:20582:20582 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Creating one domain per process
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Support for global registrations: false
 8: nid006503:218411:219568 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Support for global registrations: false
23: nid006561:220711:221897 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Support for global registrations: false
23: nid006561:220712:221898 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223926:225068 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223926:225068 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223926:225068 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
23: nid006561:220714:221896 [3] NCCL INFO Using network AWS Libfabric
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218411:219568 [1] NCCL INFO Using network AWS Libfabric
23: nid006561:220711:221897 [0] NCCL INFO Using network AWS Libfabric
23: nid006561:220712:221898 [1] NCCL INFO Using network AWS Libfabric
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Support for global registrations: false
11: nid006507:211335:212475 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Creating one domain per process
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Support for global registrations: false
 8: nid006503:218410:219569 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Support for global registrations: false
25: nid006564:223024:224163 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Support for global registrations: false
25: nid006564:223021:224160 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Support for global registrations: false
25: nid006564:223022:224162 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Creating one domain per process
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Creating one domain per process
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Creating one domain per process
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Creating one domain per process
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Creating one domain per process
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Support for global registrations: false
 8: nid006503:218412:219570 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Support for global registrations: false
25: nid006564:223023:224161 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
20: nid006558:215467:216641 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215467:216641 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
20: nid006558:215467:216641 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Support for global registrations: false
 8: nid006503:218413:219571 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211335:212475 [0] NCCL INFO Using network AWS Libfabric
 8: nid006503:218410:219569 [0] NCCL INFO Using network AWS Libfabric
15: 
15: nid006553:223923:225069 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
15: 
15: nid006553:223924:225067 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
15: 
15: nid006553:223925:225066 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
25: nid006564:223024:224163 [3] NCCL INFO Using network AWS Libfabric
25: nid006564:223022:224162 [1] NCCL INFO Using network AWS Libfabric
25: nid006564:223023:224161 [2] NCCL INFO Using network AWS Libfabric
 8: nid006503:218412:219570 [2] NCCL INFO Using network AWS Libfabric
20: nid006558:215469:216642 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215469:216642 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
20: nid006558:215469:216642 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
12: nid006508:205766:206941 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
12: nid006508:205766:206941 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
12: nid006508:205766:206941 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 8: nid006503:218413:219571 [3] NCCL INFO Using network AWS Libfabric
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Support for global registrations: false
26: nid006565:222465:223653 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Support for global registrations: false
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222467:223652 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Creating one domain per process
20: 
20: nid006558:215467:216641 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Creating one domain per process
15: 
15: nid006553:223926:225068 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Creating one domain per process
25: nid006564:223021:224160 [0] NCCL INFO Using network AWS Libfabric
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Support for global registrations: false
26: nid006565:222468:223651 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Support for global registrations: false
26: nid006565:222466:223654 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Support for global registrations: false
11: nid006507:211336:212474 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Creating one domain per process
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Support for global registrations: false
11: nid006507:211338:212472 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Creating one domain per process
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Support for global registrations: false
11: nid006507:211337:212473 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
26: nid006565:222467:223652 [2] NCCL INFO Using network AWS Libfabric
26: nid006565:222465:223653 [0] NCCL INFO Using network AWS Libfabric
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
20: 
20: nid006558:215469:216642 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: nid006508:205769:206942 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
12: nid006508:205769:206942 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
12: nid006508:205769:206942 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
12: nid006508:205767:206940 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
12: nid006508:205767:206940 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
12: nid006508:205767:206940 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
26: nid006565:222468:223651 [3] NCCL INFO Using network AWS Libfabric
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Creating one domain per process
26: nid006565:222466:223654 [1] NCCL INFO Using network AWS Libfabric
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
11: nid006507:211336:212474 [1] NCCL INFO Using network AWS Libfabric
11: nid006507:211338:212472 [3] NCCL INFO Using network AWS Libfabric
11: nid006507:211337:212473 [2] NCCL INFO Using network AWS Libfabric
20: nid006558:215468:216644 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215468:216644 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
20: nid006558:215468:216644 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Support for global registrations: false
18: nid006556:210774:211995 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
12: 
12: nid006508:205766:206941 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Support for global registrations: false
18: nid006556:210775:211994 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Support for global registrations: false
 4: nid006499:254557:255713 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254559:255712 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254559:255712 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 4: nid006499:254557:255713 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 4: nid006499:254557:255713 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 4: nid006499:254559:255712 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
18: nid006556:210776:211996 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Support for global registrations: false
18: nid006556:210777:211997 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Support for global registrations: false
20: nid006558:215467:216641 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254556:255714 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254556:255714 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 4: nid006499:254556:255714 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
12: nid006508:205768:206939 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
12: nid006508:205768:206939 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
12: nid006508:205768:206939 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
18: nid006556:210774:211995 [0] NCCL INFO Using network AWS Libfabric
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Support for global registrations: false
20: nid006558:215469:216642 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
20: nid006558:215470:216643 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215470:216643 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
20: nid006558:215470:216643 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
18: nid006556:210775:211994 [1] NCCL INFO Using network AWS Libfabric
20: nid006558:215467:216641 [0] NCCL INFO Using network AWS Libfabric
18: nid006556:210776:211996 [2] NCCL INFO Using network AWS Libfabric
18: nid006556:210777:211997 [3] NCCL INFO Using network AWS Libfabric
20: 
20: nid006558:215468:216644 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
20: nid006558:215469:216642 [2] NCCL INFO Using network AWS Libfabric
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Creating one domain per process
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 1: nid006496:242587:243774 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242587:243774 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 1: nid006496:242587:243774 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
20: 
20: nid006558:215470:216643 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 4: nid006499:254558:255715 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254558:255715 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 4: nid006499:254558:255715 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Creating one domain per process
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 4: 
 4: nid006499:254557:255713 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 4: 
 4: nid006499:254559:255712 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Creating one domain per process
 4: 
 4: nid006499:254556:255714 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 1: nid006496:242586:243772 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242584:243773 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242584:243773 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 1: nid006496:242586:243772 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 1: nid006496:242584:243773 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 1: nid006496:242586:243772 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Support for global registrations: false
20: nid006558:215468:216644 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 1: 
 1: nid006496:242587:243774 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: 
12: nid006508:205767:206940 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: 
12: nid006508:205769:206942 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: 
12: nid006508:205768:206939 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 1: nid006496:242585:243775 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242585:243775 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 1: nid006496:242585:243775 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
20: nid006558:215468:216644 [1] NCCL INFO Using network AWS Libfabric
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Support for global registrations: false
20: nid006558:215470:216643 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Support for global registrations: false
12: nid006508:205766:206941 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
20: nid006558:215470:216643 [3] NCCL INFO Using network AWS Libfabric
12: nid006508:205766:206941 [0] NCCL INFO Using network AWS Libfabric
 1: 
 1: nid006496:242584:243773 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 1: 
 1: nid006496:242586:243772 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 4: 
 4: nid006499:254558:255715 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 1: 
 1: nid006496:242585:243775 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Creating one domain per process
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Creating one domain per process
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Creating one domain per process
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Creating one domain per process
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Creating one domain per process
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Creating one domain per process
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Creating one domain per process
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Creating one domain per process
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Support for global registrations: false
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Support for global registrations: false
15: nid006553:223924:225067 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
15: nid006553:223925:225066 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Support for global registrations: false
12: nid006508:205769:206942 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Support for global registrations: false
12: nid006508:205767:206940 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Support for global registrations: false
12: nid006508:205768:206939 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Support for global registrations: false
15: nid006553:223923:225069 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Support for global registrations: false
15: nid006553:223926:225068 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Creating one domain per process
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
15: nid006553:223924:225067 [1] NCCL INFO Using network AWS Libfabric
15: nid006553:223925:225066 [2] NCCL INFO Using network AWS Libfabric
12: nid006508:205769:206942 [3] NCCL INFO Using network AWS Libfabric
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
12: nid006508:205767:206940 [1] NCCL INFO Using network AWS Libfabric
15: nid006553:223923:225069 [0] NCCL INFO Using network AWS Libfabric
12: nid006508:205768:206939 [2] NCCL INFO Using network AWS Libfabric
15: nid006553:223926:225068 [3] NCCL INFO Using network AWS Libfabric
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Creating one domain per process
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Creating one domain per process
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Creating one domain per process
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Support for global registrations: false
 1: nid006496:242587:243774 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Support for global registrations: false
 4: nid006499:254557:255713 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Creating one domain per process
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Creating one domain per process
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Creating one domain per process
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Support for global registrations: false
 4: nid006499:254559:255712 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 1: nid006496:242587:243774 [3] NCCL INFO Using network AWS Libfabric
 4: nid006499:254557:255713 [1] NCCL INFO Using network AWS Libfabric
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Support for global registrations: false
 4: nid006499:254556:255714 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Support for global registrations: false
 4: nid006499:254558:255715 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 4: nid006499:254559:255712 [3] NCCL INFO Using network AWS Libfabric
 4: nid006499:254556:255714 [0] NCCL INFO Using network AWS Libfabric
 4: nid006499:254558:255715 [2] NCCL INFO Using network AWS Libfabric
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Support for global registrations: false
 1: nid006496:242585:243775 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Support for global registrations: false
 1: nid006496:242584:243773 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Support for global registrations: false
 1: nid006496:242586:243772 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20583:21730 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20581:21731 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20581:21731 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
30: nid007318:20583:21730 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
30: nid007318:20581:21731 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
30: nid007318:20583:21730 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
30: nid007318:20580:21733 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20580:21733 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
30: nid007318:20580:21733 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 1: nid006496:242584:243773 [0] NCCL INFO Using network AWS Libfabric
 1: nid006496:242586:243772 [2] NCCL INFO Using network AWS Libfabric
 1: nid006496:242585:243775 [1] NCCL INFO Using network AWS Libfabric
30: 
30: nid007318:20583:21730 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: 
30: nid007318:20581:21731 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: 
30: nid007318:20580:21733 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: nid007318:20582:21736 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20582:21736 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
30: nid007318:20582:21736 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 2: nid006497:227576:228686 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227577:228687 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227576:228686 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 2: nid006497:227577:228687 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 2: nid006497:227578:228689 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227576:228686 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 2: nid006497:227577:228687 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 2: nid006497:227578:228689 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 2: nid006497:227578:228689 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 2: nid006497:227575:228688 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 2: nid006497:227575:228688 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 2: nid006497:227575:228688 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
30: 
30: nid007318:20582:21736 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Creating one domain per process
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Creating one domain per process
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Creating one domain per process
 2: 
 2: nid006497:227577:228687 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 2: 
 2: nid006497:227576:228686 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 2: 
 2: nid006497:227575:228688 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 2: 
 2: nid006497:227578:228689 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Creating one domain per process
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Support for global registrations: false
30: nid007318:20583:21730 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20583:21730 [3] NCCL INFO Using network AWS Libfabric
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Support for global registrations: false
30: nid007318:20581:21731 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Support for global registrations: false
30: nid007318:20580:21733 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Support for global registrations: false
30: nid007318:20582:21736 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
30: nid007318:20581:21731 [1] NCCL INFO Using network AWS Libfabric
30: nid007318:20580:21733 [0] NCCL INFO Using network AWS Libfabric
30: nid007318:20582:21736 [2] NCCL INFO Using network AWS Libfabric
 5: nid006500:260084:261238 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260084:261238 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 5: nid006500:260084:261238 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 5: nid006500:260086:261239 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260086:261239 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 5: nid006500:260086:261239 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 5: nid006500:260083:261236 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260085:261237 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260085:261237 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 5: nid006500:260083:261236 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 5: nid006500:260085:261237 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 5: nid006500:260083:261236 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 5: 
 5: nid006500:260084:261238 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Creating one domain per process
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Creating one domain per process
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Creating one domain per process
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 5: 
 5: nid006500:260086:261239 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Creating one domain per process
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Creating one domain per process
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 5: 
 5: nid006500:260085:261237 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 5: 
 5: nid006500:260083:261236 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Support for global registrations: false
 2: nid006497:227577:228687 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Support for global registrations: false
 5: nid006500:260084:261238 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 2: nid006497:227577:228687 [2] NCCL INFO Using network AWS Libfabric
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Support for global registrations: false
 2: nid006497:227576:228686 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 5: nid006500:260084:261238 [1] NCCL INFO Using network AWS Libfabric
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Support for global registrations: false
 2: nid006497:227575:228688 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Support for global registrations: false
 2: nid006497:227578:228689 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 2: nid006497:227576:228686 [1] NCCL INFO Using network AWS Libfabric
 2: nid006497:227575:228688 [0] NCCL INFO Using network AWS Libfabric
 2: nid006497:227578:228689 [3] NCCL INFO Using network AWS Libfabric
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Creating one domain per process
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Creating one domain per process
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Creating one domain per process
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Support for global registrations: false
 5: nid006500:260086:261239 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Support for global registrations: false
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Support for global registrations: false
 5: nid006500:260083:261236 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 5: nid006500:260085:261237 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 5: nid006500:260086:261239 [3] NCCL INFO Using network AWS Libfabric
 5: nid006500:260085:261237 [2] NCCL INFO Using network AWS Libfabric
 5: nid006500:260083:261236 [0] NCCL INFO Using network AWS Libfabric
10: nid006506:263727:264880 [0] NCCL INFO DMA-BUF is available on GPU device 0
10: nid006506:263730:264885 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263730:264885 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
10: nid006506:263730:264885 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: 
10: nid006506:263730:264885 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263729:264886 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263729:264886 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
10: nid006506:263729:264886 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Creating one domain per process
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
10: 
10: nid006506:263729:264886 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Creating one domain per process
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Support for global registrations: false
10: nid006506:263730:264885 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
10: nid006506:263730:264885 [3] NCCL INFO Using network AWS Libfabric
10: nid006506:263728:264887 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263728:264887 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
10: nid006506:263728:264887 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Support for global registrations: false
10: nid006506:263729:264886 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
10: nid006506:263729:264886 [2] NCCL INFO Using network AWS Libfabric
10: 
10: nid006506:263728:264887 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Creating one domain per process
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Support for global registrations: false
10: nid006506:263728:264887 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
10: nid006506:263728:264887 [1] NCCL INFO Using network AWS Libfabric
17: nid006555:206595:207800 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206595:207800 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
17: nid006555:206595:207800 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
17: 
17: nid006555:206595:207800 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Creating one domain per process
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Support for global registrations: false
17: nid006555:206595:207800 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
17: nid006555:206595:207800 [2] NCCL INFO Using network AWS Libfabric
16: nid006554:221584:222757 [3] NCCL INFO DMA-BUF is available on GPU device 3
16: nid006554:221581:222760 [0] NCCL INFO DMA-BUF is available on GPU device 0
16: nid006554:221582:222761 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221583:222762 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221583:222762 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
16: nid006554:221582:222761 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
16: nid006554:221582:222761 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
16: nid006554:221583:222762 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
16: 
16: nid006554:221582:222761 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
16: 
16: nid006554:221583:222762 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Creating one domain per process
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Creating one domain per process
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Support for global registrations: false
16: nid006554:221582:222761 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Support for global registrations: false
16: nid006554:221583:222762 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
16: nid006554:221582:222761 [1] NCCL INFO Using network AWS Libfabric
16: nid006554:221583:222762 [2] NCCL INFO Using network AWS Libfabric
17: nid006555:206594:207777 [1] NCCL INFO DMA-BUF is available on GPU device 1
17: nid006555:206596:207778 [3] NCCL INFO DMA-BUF is available on GPU device 3
17: nid006555:206595:207800 [2] NCCL INFO DMA-BUF is available on GPU device 2
17: nid006555:206593:206593 [0] NCCL INFO cudaDriverVersion 12050
17: nid006555:206593:206593 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
17: nid006555:206593:206593 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.228<0>
17: nid006555:206593:206593 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
17: nid006555:206593:207804 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206593:207804 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
17: nid006555:206593:207804 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
17: 
17: nid006555:206593:207804 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Creating one domain per process
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Support for global registrations: false
17: nid006555:206593:207804 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
17: nid006555:206593:207804 [0] NCCL INFO Using network AWS Libfabric
28: nid007251:72169:73329 [0] NCCL INFO DMA-BUF is available on GPU device 0
28: nid007251:72170:73327 [1] NCCL INFO DMA-BUF is available on GPU device 1
28: nid007251:72171:73330 [2] NCCL INFO DMA-BUF is available on GPU device 2
28: nid007251:72172:73331 [3] NCCL INFO DMA-BUF is available on GPU device 3
 6: nid006501:221941:223089 [0] NCCL INFO DMA-BUF is available on GPU device 0
27: nid006566:216748:217887 [2] NCCL INFO DMA-BUF is available on GPU device 2
21: nid006559:211123:212275 [0] NCCL INFO DMA-BUF is available on GPU device 0
31: nid007342:59767:61194 [0] NCCL INFO DMA-BUF is available on GPU device 0
29: nid007305:27223:28343 [1] NCCL INFO DMA-BUF is available on GPU device 1
27: nid006566:216749:217883 [3] NCCL INFO DMA-BUF is available on GPU device 3
21: nid006559:211125:212276 [2] NCCL INFO DMA-BUF is available on GPU device 2
29: nid007305:27222:28341 [0] NCCL INFO DMA-BUF is available on GPU device 0
27: nid006566:216747:217884 [1] NCCL INFO DMA-BUF is available on GPU device 1
 9: nid006505:249082:250266 [2] NCCL INFO DMA-BUF is available on GPU device 2
21: nid006559:211126:212278 [3] NCCL INFO DMA-BUF is available on GPU device 3
29: nid007305:27225:28342 [3] NCCL INFO DMA-BUF is available on GPU device 3
 7: nid006502:252584:253741 [0] NCCL INFO DMA-BUF is available on GPU device 0
27: nid006566:216746:217885 [0] NCCL INFO DMA-BUF is available on GPU device 0
21: nid006559:211124:212277 [1] NCCL INFO DMA-BUF is available on GPU device 1
29: nid007305:27224:28344 [2] NCCL INFO DMA-BUF is available on GPU device 2
31: nid007342:59769:61195 [2] NCCL INFO DMA-BUF is available on GPU device 2
 6: nid006501:221942:223088 [1] NCCL INFO DMA-BUF is available on GPU device 1
31: nid007342:59770:61197 [3] NCCL INFO DMA-BUF is available on GPU device 3
 9: nid006505:249081:250246 [1] NCCL INFO DMA-BUF is available on GPU device 1
 6: nid006501:221944:223090 [3] NCCL INFO DMA-BUF is available on GPU device 3
 7: nid006502:252587:253740 [3] NCCL INFO DMA-BUF is available on GPU device 3
31: nid007342:59768:61196 [1] NCCL INFO DMA-BUF is available on GPU device 1
 0: nid006495:241022:242172 [3] NCCL INFO DMA-BUF is available on GPU device 3
 9: nid006505:249080:250267 [0] NCCL INFO DMA-BUF is available on GPU device 0
 6: nid006501:221943:223091 [2] NCCL INFO DMA-BUF is available on GPU device 2
10: nid006506:263730:264885 [3] NCCL INFO DMA-BUF is available on GPU device 3
 7: nid006502:252586:253742 [2] NCCL INFO DMA-BUF is available on GPU device 2
 0: nid006495:241019:242169 [0] NCCL INFO DMA-BUF is available on GPU device 0
 9: nid006505:249083:250268 [3] NCCL INFO DMA-BUF is available on GPU device 3
13: nid006509:201788:202938 [0] NCCL INFO DMA-BUF is available on GPU device 0
 7: nid006502:252585:253743 [1] NCCL INFO DMA-BUF is available on GPU device 1
 0: nid006495:241021:242171 [2] NCCL INFO DMA-BUF is available on GPU device 2
22: nid006560:222271:223425 [0] NCCL INFO DMA-BUF is available on GPU device 0
24: nid006563:221141:222303 [1] NCCL INFO DMA-BUF is available on GPU device 1
10: nid006506:263729:264886 [2] NCCL INFO DMA-BUF is available on GPU device 2
12: nid006508:205766:206941 [0] NCCL INFO DMA-BUF is available on GPU device 0
13: nid006509:201789:202939 [1] NCCL INFO DMA-BUF is available on GPU device 1
14: nid006510:229444:230608 [2] NCCL INFO DMA-BUF is available on GPU device 2
 0: nid006495:241020:242170 [1] NCCL INFO DMA-BUF is available on GPU device 1
24: nid006563:221140:222302 [0] NCCL INFO DMA-BUF is available on GPU device 0
10: nid006506:263728:264887 [1] NCCL INFO DMA-BUF is available on GPU device 1
14: nid006510:229443:230607 [1] NCCL INFO DMA-BUF is available on GPU device 1
13: nid006509:201791:202940 [3] NCCL INFO DMA-BUF is available on GPU device 3
16: nid006554:221582:222761 [1] NCCL INFO DMA-BUF is available on GPU device 1
22: nid006560:222274:223424 [3] NCCL INFO DMA-BUF is available on GPU device 3
14: nid006510:229445:230609 [3] NCCL INFO DMA-BUF is available on GPU device 3
13: nid006509:201790:202941 [2] NCCL INFO DMA-BUF is available on GPU device 2
16: nid006554:221583:222762 [2] NCCL INFO DMA-BUF is available on GPU device 2
22: nid006560:222272:223426 [1] NCCL INFO DMA-BUF is available on GPU device 1
14: nid006510:229442:230610 [0] NCCL INFO DMA-BUF is available on GPU device 0
22: nid006560:222273:223427 [2] NCCL INFO DMA-BUF is available on GPU device 2
24: nid006563:221143:222304 [3] NCCL INFO DMA-BUF is available on GPU device 3
23: nid006561:220713:221895 [2] NCCL INFO DMA-BUF is available on GPU device 2
24: nid006563:221142:222307 [2] NCCL INFO DMA-BUF is available on GPU device 2
 8: nid006503:218411:219568 [1] NCCL INFO DMA-BUF is available on GPU device 1
23: nid006561:220714:221896 [3] NCCL INFO DMA-BUF is available on GPU device 3
26: nid006565:222467:223652 [2] NCCL INFO DMA-BUF is available on GPU device 2
20: nid006558:215467:216641 [0] NCCL INFO DMA-BUF is available on GPU device 0
18: nid006556:210774:211995 [0] NCCL INFO DMA-BUF is available on GPU device 0
 8: nid006503:218410:219569 [0] NCCL INFO DMA-BUF is available on GPU device 0
23: nid006561:220711:221897 [0] NCCL INFO DMA-BUF is available on GPU device 0
25: nid006564:223024:224163 [3] NCCL INFO DMA-BUF is available on GPU device 3
26: nid006565:222465:223653 [0] NCCL INFO DMA-BUF is available on GPU device 0
11: nid006507:211335:212475 [0] NCCL INFO DMA-BUF is available on GPU device 0
18: nid006556:210775:211994 [1] NCCL INFO DMA-BUF is available on GPU device 1
 8: nid006503:218412:219570 [2] NCCL INFO DMA-BUF is available on GPU device 2
23: nid006561:220712:221898 [1] NCCL INFO DMA-BUF is available on GPU device 1
25: nid006564:223022:224162 [1] NCCL INFO DMA-BUF is available on GPU device 1
26: nid006565:222468:223651 [3] NCCL INFO DMA-BUF is available on GPU device 3
11: nid006507:211336:212474 [1] NCCL INFO DMA-BUF is available on GPU device 1
18: nid006556:210776:211996 [2] NCCL INFO DMA-BUF is available on GPU device 2
 8: nid006503:218413:219571 [3] NCCL INFO DMA-BUF is available on GPU device 3
25: nid006564:223023:224161 [2] NCCL INFO DMA-BUF is available on GPU device 2
26: nid006565:222466:223654 [1] NCCL INFO DMA-BUF is available on GPU device 1
20: nid006558:215469:216642 [2] NCCL INFO DMA-BUF is available on GPU device 2
11: nid006507:211338:212472 [3] NCCL INFO DMA-BUF is available on GPU device 3
18: nid006556:210777:211997 [3] NCCL INFO DMA-BUF is available on GPU device 3
25: nid006564:223021:224160 [0] NCCL INFO DMA-BUF is available on GPU device 0
20: nid006558:215468:216644 [1] NCCL INFO DMA-BUF is available on GPU device 1
11: nid006507:211337:212473 [2] NCCL INFO DMA-BUF is available on GPU device 2
12: nid006508:205769:206942 [3] NCCL INFO DMA-BUF is available on GPU device 3
20: nid006558:215470:216643 [3] NCCL INFO DMA-BUF is available on GPU device 3
 4: nid006499:254557:255713 [1] NCCL INFO DMA-BUF is available on GPU device 1
12: nid006508:205767:206940 [1] NCCL INFO DMA-BUF is available on GPU device 1
15: nid006553:223924:225067 [1] NCCL INFO DMA-BUF is available on GPU device 1
 1: nid006496:242587:243774 [3] NCCL INFO DMA-BUF is available on GPU device 3
 4: nid006499:254559:255712 [3] NCCL INFO DMA-BUF is available on GPU device 3
12: nid006508:205768:206939 [2] NCCL INFO DMA-BUF is available on GPU device 2
15: nid006553:223925:225066 [2] NCCL INFO DMA-BUF is available on GPU device 2
 1: nid006496:242584:243773 [0] NCCL INFO DMA-BUF is available on GPU device 0
 4: nid006499:254556:255714 [0] NCCL INFO DMA-BUF is available on GPU device 0
15: nid006553:223923:225069 [0] NCCL INFO DMA-BUF is available on GPU device 0
 1: nid006496:242586:243772 [2] NCCL INFO DMA-BUF is available on GPU device 2
 4: nid006499:254558:255715 [2] NCCL INFO DMA-BUF is available on GPU device 2
15: nid006553:223926:225068 [3] NCCL INFO DMA-BUF is available on GPU device 3
 1: nid006496:242585:243775 [1] NCCL INFO DMA-BUF is available on GPU device 1
 5: nid006500:260084:261238 [1] NCCL INFO DMA-BUF is available on GPU device 1
17: nid006555:206593:207804 [0] NCCL INFO DMA-BUF is available on GPU device 0
30: nid007318:20583:21730 [3] NCCL INFO DMA-BUF is available on GPU device 3
30: nid007318:20581:21731 [1] NCCL INFO DMA-BUF is available on GPU device 1
30: nid007318:20580:21733 [0] NCCL INFO DMA-BUF is available on GPU device 0
 5: nid006500:260086:261239 [3] NCCL INFO DMA-BUF is available on GPU device 3
30: nid007318:20582:21736 [2] NCCL INFO DMA-BUF is available on GPU device 2
 2: nid006497:227577:228687 [2] NCCL INFO DMA-BUF is available on GPU device 2
 5: nid006500:260085:261237 [2] NCCL INFO DMA-BUF is available on GPU device 2
 2: nid006497:227576:228686 [1] NCCL INFO DMA-BUF is available on GPU device 1
 5: nid006500:260083:261236 [0] NCCL INFO DMA-BUF is available on GPU device 0
 2: nid006497:227575:228688 [0] NCCL INFO DMA-BUF is available on GPU device 0
 2: nid006497:227578:228689 [3] NCCL INFO DMA-BUF is available on GPU device 3
 3: Combined 1137190 records into all_train.json
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
19: Combined 1137190 records into all_train.json
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
19: Combined 1137190 records into all_train.json
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
 3: Combined 1137190 records into all_train.json
 3: 
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
 3: Combined 1137190 records into all_train.json
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 3:   warnings.warn(
19: Combined 1137190 records into all_train.json
19: 
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
 3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: Combined 1137190 records into all_train.json
 3: 
 3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
 3:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
19:   warnings.warn(
19: Combined 1137190 records into all_train.json
19: 
19: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
19:   warnings.warn(
 3: nid006498:226768:226768 [3] NCCL INFO cudaDriverVersion 12050
 3: nid006498:226768:226768 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 3: nid006498:226768:226768 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.32<0>
 3: nid006498:226768:226768 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
19: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
19:   warnings.warn(
19: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 3:   warnings.warn(
 3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: nid006557:208420:208420 [3] NCCL INFO cudaDriverVersion 12050
 3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: nid006557:208420:208420 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
19: nid006557:208420:208420 [3] NCCL INFO Bootstrap : Using hsn0:172.28.31.236<0>
19: nid006557:208420:208420 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: nid006498:226766:226766 [1] NCCL INFO cudaDriverVersion 12050
 3: nid006498:226766:226766 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 3: nid006498:226766:226766 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.32<0>
 3: nid006498:226766:226766 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
 3: nid006498:226767:226767 [2] NCCL INFO cudaDriverVersion 12050
 3: nid006498:226767:226767 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 3: nid006498:226767:226767 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.32<0>
 3: nid006498:226767:226767 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
 3:   warnings.warn(
 3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: nid006498:226768:227915 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226768:227915 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 3: nid006498:226768:227915 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 3: 
 3: nid006498:226768:227915 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Creating one domain per process
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Support for global registrations: false
 3: nid006498:226768:227915 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 3: nid006498:226768:227915 [3] NCCL INFO Using network AWS Libfabric
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
19:   warnings.warn(
19: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: nid006557:208419:208419 [2] NCCL INFO cudaDriverVersion 12050
19: nid006557:208419:208419 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
19: nid006557:208419:208419 [2] NCCL INFO Bootstrap : Using hsn0:172.28.31.236<0>
19: nid006557:208419:208419 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: nid006557:208418:208418 [1] NCCL INFO cudaDriverVersion 12050
19: nid006557:208418:208418 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
19: nid006557:208418:208418 [1] NCCL INFO Bootstrap : Using hsn0:172.28.31.236<0>
19: nid006557:208418:208418 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
19: nid006557:208420:209604 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208420:209604 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
19: nid006557:208420:209604 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
19: 
19: nid006557:208420:209604 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Creating one domain per process
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Support for global registrations: false
19: nid006557:208420:209604 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
19: nid006557:208420:209604 [3] NCCL INFO Using network AWS Libfabric
19: nid006557:208419:209607 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208419:209607 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
19: nid006557:208419:209607 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
19: 
19: nid006557:208419:209607 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Creating one domain per process
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Support for global registrations: false
19: nid006557:208419:209607 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
19: nid006557:208419:209607 [2] NCCL INFO Using network AWS Libfabric
 3: nid006498:226768:227915 [3] NCCL INFO DMA-BUF is available on GPU device 3
 3: nid006498:226767:227919 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226767:227919 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 3: nid006498:226767:227919 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 3: nid006498:226766:227918 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226766:227918 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 3: nid006498:226766:227918 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 3: 
 3: nid006498:226767:227919 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 3: 
 3: nid006498:226766:227918 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Creating one domain per process
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Creating one domain per process
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Support for global registrations: false
 3: nid006498:226767:227919 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Support for global registrations: false
 3: nid006498:226766:227918 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 3: nid006498:226767:227919 [2] NCCL INFO Using network AWS Libfabric
 3: nid006498:226766:227918 [1] NCCL INFO Using network AWS Libfabric
19: nid006557:208420:209604 [3] NCCL INFO DMA-BUF is available on GPU device 3
19: nid006557:208417:208417 [0] NCCL INFO cudaDriverVersion 12050
19: nid006557:208417:208417 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
19: nid006557:208417:208417 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.236<0>
19: nid006557:208417:208417 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
19: nid006557:208419:209607 [2] NCCL INFO DMA-BUF is available on GPU device 2
19: nid006557:208418:209608 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208418:209608 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
19: nid006557:208418:209608 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
19: 
19: nid006557:208418:209608 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Creating one domain per process
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Support for global registrations: false
19: nid006557:208418:209608 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
19: nid006557:208418:209608 [1] NCCL INFO Using network AWS Libfabric
 3: nid006498:226766:227918 [1] NCCL INFO DMA-BUF is available on GPU device 1
 3: nid006498:226767:227919 [2] NCCL INFO DMA-BUF is available on GPU device 2
19: nid006557:208418:209608 [1] NCCL INFO DMA-BUF is available on GPU device 1
 3: nid006498:226765:226765 [0] NCCL INFO cudaDriverVersion 12050
 3: nid006498:226765:226765 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
 3: nid006498:226765:226765 [0] NCCL INFO Bootstrap : Using hsn0:172.28.31.32<0>
 3: nid006498:226765:226765 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
19: nid006557:208417:209611 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208417:209611 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
19: nid006557:208417:209611 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
19: 
19: nid006557:208417:209611 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Creating one domain per process
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Support for global registrations: false
19: nid006557:208417:209611 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
19: nid006557:208417:209611 [0] NCCL INFO Using network AWS Libfabric
 3: nid006498:226765:227925 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226765:227925 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
 3: nid006498:226765:227925 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
 3: 
 3: nid006498:226765:227925 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Creating one domain per process
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Support for global registrations: false
 3: nid006498:226765:227925 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
 3: nid006498:226765:227925 [0] NCCL INFO Using network AWS Libfabric
19: nid006557:208417:209611 [0] NCCL INFO DMA-BUF is available on GPU device 0
 3: nid006498:226765:227925 [0] NCCL INFO DMA-BUF is available on GPU device 0
 4: nid006499:254559:255712 [3] NCCL INFO ncclCommInitRank comm 0xaaaaffcbe690 rank 19 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 4: nid006499:254558:255715 [2] NCCL INFO ncclCommInitRank comm 0xaaab14912c30 rank 18 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 4: nid006499:254557:255713 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf4721fc0 rank 17 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 5: nid006500:260083:261236 [0] NCCL INFO ncclCommInitRank comm 0xaaaaddd64a90 rank 20 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 5: nid006500:260085:261237 [2] NCCL INFO ncclCommInitRank comm 0xaaab1f1aeb40 rank 22 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 5: nid006500:260084:261238 [1] NCCL INFO ncclCommInitRank comm 0xaaaaebd115d0 rank 21 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 5: nid006500:260086:261239 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf70716c0 rank 23 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 6: nid006501:221941:223089 [0] NCCL INFO ncclCommInitRank comm 0xaaab18cffb10 rank 24 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 6: nid006501:221942:223088 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf2b5ecd0 rank 25 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 6: nid006501:221943:223091 [2] NCCL INFO ncclCommInitRank comm 0xaaaafac13400 rank 26 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 6: nid006501:221944:223090 [3] NCCL INFO ncclCommInitRank comm 0xaaab07c52630 rank 27 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 7: nid006502:252584:253741 [0] NCCL INFO ncclCommInitRank comm 0xaaaafb714860 rank 28 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 7: nid006502:252585:253743 [1] NCCL INFO ncclCommInitRank comm 0xaaab03122220 rank 29 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 7: nid006502:252586:253742 [2] NCCL INFO ncclCommInitRank comm 0xaaab16820860 rank 30 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 7: nid006502:252587:253740 [3] NCCL INFO ncclCommInitRank comm 0xaaab12a7f380 rank 31 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 8: nid006503:218410:219569 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8554fd0 rank 32 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 8: nid006503:218412:219570 [2] NCCL INFO ncclCommInitRank comm 0xaaaadb064120 rank 34 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 8: nid006503:218411:219568 [1] NCCL INFO ncclCommInitRank comm 0xaaab0c010470 rank 33 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 8: nid006503:218413:219571 [3] NCCL INFO ncclCommInitRank comm 0xaaaad50f1540 rank 35 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 9: nid006505:249083:250268 [3] NCCL INFO ncclCommInitRank comm 0xaaab25b11310 rank 39 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 9: nid006505:249080:250267 [0] NCCL INFO ncclCommInitRank comm 0xaaaac2b051b0 rank 36 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 9: nid006505:249081:250246 [1] NCCL INFO ncclCommInitRank comm 0xaaab0bfe2b70 rank 37 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 9: nid006505:249082:250266 [2] NCCL INFO ncclCommInitRank comm 0xaaaaddaa10c0 rank 38 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
10: nid006506:263727:264880 [0] NCCL INFO ncclCommInitRank comm 0xaaaacce51d30 rank 40 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
10: nid006506:263729:264886 [2] NCCL INFO ncclCommInitRank comm 0xaaab0c9e0740 rank 42 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
10: nid006506:263730:264885 [3] NCCL INFO ncclCommInitRank comm 0xaaaadbd63f10 rank 43 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
11: nid006507:211338:212472 [3] NCCL INFO ncclCommInitRank comm 0xaaab1f201250 rank 47 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
11: nid006507:211335:212475 [0] NCCL INFO ncclCommInitRank comm 0xaaaadbe34c90 rank 44 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
11: nid006507:211337:212473 [2] NCCL INFO ncclCommInitRank comm 0xaaab1716ee10 rank 46 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
10: nid006506:263728:264887 [1] NCCL INFO ncclCommInitRank comm 0xaaab066b0b90 rank 41 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
11: nid006507:211336:212474 [1] NCCL INFO ncclCommInitRank comm 0xaaab0a883290 rank 45 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
12: nid006508:205766:206941 [0] NCCL INFO ncclCommInitRank comm 0xaaab01b649c0 rank 48 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
12: nid006508:205767:206940 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf9abf4e0 rank 49 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
12: nid006508:205769:206942 [3] NCCL INFO ncclCommInitRank comm 0xaaaafffd57b0 rank 51 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
12: nid006508:205768:206939 [2] NCCL INFO ncclCommInitRank comm 0xaaaae60828b0 rank 50 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
13: nid006509:201788:202938 [0] NCCL INFO ncclCommInitRank comm 0xaaab17976000 rank 52 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
13: nid006509:201789:202939 [1] NCCL INFO ncclCommInitRank comm 0xaaab23f6f620 rank 53 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
13: nid006509:201790:202941 [2] NCCL INFO ncclCommInitRank comm 0xaaaac07c29c0 rank 54 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
13: nid006509:201791:202940 [3] NCCL INFO ncclCommInitRank comm 0xaaaafc8b2560 rank 55 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
14: nid006510:229442:230610 [0] NCCL INFO ncclCommInitRank comm 0xaaaadd8b3cb0 rank 56 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
14: nid006510:229444:230608 [2] NCCL INFO ncclCommInitRank comm 0xaaab050af830 rank 58 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
14: nid006510:229443:230607 [1] NCCL INFO ncclCommInitRank comm 0xaaab02930260 rank 57 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
14: nid006510:229445:230609 [3] NCCL INFO ncclCommInitRank comm 0xaaaad4ac1400 rank 59 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
15: nid006553:223923:225069 [0] NCCL INFO ncclCommInitRank comm 0xaaab008f4e30 rank 60 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
15: nid006553:223924:225067 [1] NCCL INFO ncclCommInitRank comm 0xaaaae1630990 rank 61 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
15: nid006553:223925:225066 [2] NCCL INFO ncclCommInitRank comm 0xaaab1b0b02b0 rank 62 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
15: nid006553:223926:225068 [3] NCCL INFO ncclCommInitRank comm 0xaaab186224e0 rank 63 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
16: nid006554:221581:222760 [0] NCCL INFO ncclCommInitRank comm 0xaaaae2836550 rank 64 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
16: nid006554:221582:222761 [1] NCCL INFO ncclCommInitRank comm 0xaaaad7c30460 rank 65 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
16: nid006554:221584:222757 [3] NCCL INFO ncclCommInitRank comm 0xaaaafdce2b10 rank 67 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
16: nid006554:221583:222762 [2] NCCL INFO ncclCommInitRank comm 0xaaaadd7bfd30 rank 66 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
17: nid006555:206596:207778 [3] NCCL INFO ncclCommInitRank comm 0xaaaafe0d1bf0 rank 71 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
17: nid006555:206595:207800 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf5a54290 rank 70 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
17: nid006555:206594:207777 [1] NCCL INFO ncclCommInitRank comm 0xaaab0a7e34b0 rank 69 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
17: nid006555:206593:207804 [0] NCCL INFO ncclCommInitRank comm 0xaaab06a54c60 rank 68 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
18: nid006556:210775:211994 [1] NCCL INFO ncclCommInitRank comm 0xaaab1c4215c0 rank 73 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
18: nid006556:210776:211996 [2] NCCL INFO ncclCommInitRank comm 0xaaab24c608a0 rank 74 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
18: nid006556:210774:211995 [0] NCCL INFO ncclCommInitRank comm 0xaaab00fe4b80 rank 72 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
18: nid006556:210777:211997 [3] NCCL INFO ncclCommInitRank comm 0xaaab001d2d70 rank 75 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
19: nid006557:208417:209611 [0] NCCL INFO ncclCommInitRank comm 0xaaaafeab5cd0 rank 76 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
19: nid006557:208418:209608 [1] NCCL INFO ncclCommInitRank comm 0xaaab2b67f850 rank 77 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
19: nid006557:208419:209607 [2] NCCL INFO ncclCommInitRank comm 0xaaab15322000 rank 78 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
19: nid006557:208420:209604 [3] NCCL INFO ncclCommInitRank comm 0xaaab123500f0 rank 79 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
20: nid006558:215467:216641 [0] NCCL INFO ncclCommInitRank comm 0xaaab024c49d0 rank 80 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
20: nid006558:215468:216644 [1] NCCL INFO ncclCommInitRank comm 0xaaaae4530560 rank 81 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
20: nid006558:215469:216642 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf4921880 rank 82 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
20: nid006558:215470:216643 [3] NCCL INFO ncclCommInitRank comm 0xaaab04942a10 rank 83 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
21: nid006559:211123:212275 [0] NCCL INFO ncclCommInitRank comm 0xaaaacd464770 rank 84 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
21: nid006559:211124:212277 [1] NCCL INFO ncclCommInitRank comm 0xaaaae87e0f50 rank 85 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
21: nid006559:211125:212276 [2] NCCL INFO ncclCommInitRank comm 0xaaaad6db0e40 rank 86 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
21: nid006559:211126:212278 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf4030670 rank 87 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
22: nid006560:222271:223425 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8db5160 rank 88 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
22: nid006560:222272:223426 [1] NCCL INFO ncclCommInitRank comm 0xaaaafbb630b0 rank 89 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
22: nid006560:222274:223424 [3] NCCL INFO ncclCommInitRank comm 0xaaaad06e0ae0 rank 91 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
22: nid006560:222273:223427 [2] NCCL INFO ncclCommInitRank comm 0xaaaae7423530 rank 90 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
23: nid006561:220711:221897 [0] NCCL INFO ncclCommInitRank comm 0xaaab1f303c10 rank 92 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
23: nid006561:220713:221895 [2] NCCL INFO ncclCommInitRank comm 0xaaab19e1fc40 rank 94 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
23: nid006561:220714:221896 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf7d8fec0 rank 95 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
23: nid006561:220712:221898 [1] NCCL INFO ncclCommInitRank comm 0xaaaafae71f80 rank 93 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
27: nid006566:216746:217885 [0] NCCL INFO ncclCommInitRank comm 0xaaab08db5770 rank 108 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
27: nid006566:216747:217884 [1] NCCL INFO ncclCommInitRank comm 0xaaab06140ab0 rank 109 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
27: nid006566:216749:217883 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf3be1d00 rank 111 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
27: nid006566:216748:217887 [2] NCCL INFO ncclCommInitRank comm 0xaaab0ae01a50 rank 110 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
28: nid007251:72169:73329 [0] NCCL INFO ncclCommInitRank comm 0xaaab09955090 rank 112 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
28: nid007251:72170:73327 [1] NCCL INFO ncclCommInitRank comm 0xaaaae3cff410 rank 113 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
28: nid007251:72171:73330 [2] NCCL INFO ncclCommInitRank comm 0xaaab01150380 rank 114 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
28: nid007251:72172:73331 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf6d30140 rank 115 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
29: nid007305:27222:28341 [0] NCCL INFO ncclCommInitRank comm 0xaaaae8e75560 rank 116 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
29: nid007305:27223:28343 [1] NCCL INFO ncclCommInitRank comm 0xaaaadd180740 rank 117 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
29: nid007305:27224:28344 [2] NCCL INFO ncclCommInitRank comm 0xaaab02ed3070 rank 118 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
29: nid007305:27225:28342 [3] NCCL INFO ncclCommInitRank comm 0xaaaafdcaec70 rank 119 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
30: nid007318:20580:21733 [0] NCCL INFO ncclCommInitRank comm 0xaaab05f55b40 rank 120 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
31: nid007342:59767:61194 [0] NCCL INFO ncclCommInitRank comm 0xaaab02e43cb0 rank 124 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
31: nid007342:59769:61195 [2] NCCL INFO ncclCommInitRank comm 0xaaaacd751c60 rank 126 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
30: nid007318:20582:21736 [2] NCCL INFO ncclCommInitRank comm 0xaaab18742680 rank 122 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
30: nid007318:20581:21731 [1] NCCL INFO ncclCommInitRank comm 0xaaab01d20530 rank 121 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
31: nid007342:59768:61196 [1] NCCL INFO ncclCommInitRank comm 0xaaaaebad4d50 rank 125 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
30: nid007318:20583:21730 [3] NCCL INFO ncclCommInitRank comm 0xaaaaee514e10 rank 123 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
31: nid007342:59770:61197 [3] NCCL INFO ncclCommInitRank comm 0xaaaae96a06d0 rank 127 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
26: nid006565:222467:223652 [2] NCCL INFO ncclCommInitRank comm 0xaaab1e0113c0 rank 106 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
26: nid006565:222468:223651 [3] NCCL INFO ncclCommInitRank comm 0xaaaacd640a00 rank 107 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
26: nid006565:222465:223653 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf61a4fc0 rank 104 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
26: nid006565:222466:223654 [1] NCCL INFO ncclCommInitRank comm 0xaaaadd720280 rank 105 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
25: nid006564:223024:224163 [3] NCCL INFO ncclCommInitRank comm 0xaaaafc951420 rank 103 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 0: nid006495:241021:242171 [2] NCCL INFO ncclCommInitRank comm 0xaaab287e25d0 rank 2 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 0: nid006495:241022:242172 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf2c322a0 rank 3 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 0: nid006495:241020:242170 [1] NCCL INFO ncclCommInitRank comm 0xaaaad1d00bf0 rank 1 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 0: nid006495:241019:242169 [0] NCCL INFO ncclCommInitRank comm 0xaaaaca364de0 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 3: nid006498:226765:227925 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf9754bd0 rank 12 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 2: nid006497:227577:228687 [2] NCCL INFO ncclCommInitRank comm 0xaaab266a1d50 rank 10 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 2: nid006497:227578:228689 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf3bd2ec0 rank 11 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 2: nid006497:227576:228686 [1] NCCL INFO ncclCommInitRank comm 0xaaaade5e19c0 rank 9 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 1: nid006496:242587:243774 [3] NCCL INFO ncclCommInitRank comm 0xaaaad33e0d00 rank 7 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 1: nid006496:242585:243775 [1] NCCL INFO ncclCommInitRank comm 0xaaab017914b0 rank 5 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 1: nid006496:242586:243772 [2] NCCL INFO ncclCommInitRank comm 0xaaaae9db1a00 rank 6 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 1: nid006496:242584:243773 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa2855c0 rank 4 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
 2: nid006497:227575:228688 [0] NCCL INFO ncclCommInitRank comm 0xaaab34a73550 rank 8 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
24: nid006563:221140:222302 [0] NCCL INFO ncclCommInitRank comm 0xaaaafb2f59b0 rank 96 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
24: nid006563:221141:222303 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf69b0d70 rank 97 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
25: nid006564:223023:224161 [2] NCCL INFO ncclCommInitRank comm 0xaaab04f72cc0 rank 102 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
25: nid006564:223021:224160 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf17f5630 rank 100 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
24: nid006563:221143:222304 [3] NCCL INFO ncclCommInitRank comm 0xaaab126b1510 rank 99 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
24: nid006563:221142:222307 [2] NCCL INFO ncclCommInitRank comm 0xaaab146907e0 rank 98 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
25: nid006564:223022:224162 [1] NCCL INFO ncclCommInitRank comm 0xaaaadab71590 rank 101 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 3: nid006498:226766:227918 [1] NCCL INFO ncclCommInitRank comm 0xaaaafc6c1d20 rank 13 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init START
 3: nid006498:226767:227919 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf026f8b0 rank 14 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init START
 3: nid006498:226768:227915 [3] NCCL INFO ncclCommInitRank comm 0xaaaae06b1570 rank 15 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init START
 4: nid006499:254556:255714 [0] NCCL INFO ncclCommInitRank comm 0xaaaafd9c3e70 rank 16 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init START
16: nid006554:221581:222760 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
10: nid006506:263730:264885 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
10: nid006506:263730:264885 [3] NCCL INFO NVLS multicast support is not available on dev 3
10: nid006506:263730:264885 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
16: nid006554:221581:222760 [0] NCCL INFO NVLS multicast support is not available on dev 0
16: nid006554:221581:222760 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
10: nid006506:263727:264880 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
10: nid006506:263727:264880 [0] NCCL INFO NVLS multicast support is not available on dev 0
10: nid006506:263727:264880 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
16: nid006554:221584:222757 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
16: nid006554:221584:222757 [3] NCCL INFO NVLS multicast support is not available on dev 3
16: nid006554:221584:222757 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
10: nid006506:263728:264887 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
10: nid006506:263729:264886 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
16: nid006554:221583:222762 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
10: nid006506:263728:264887 [1] NCCL INFO NVLS multicast support is not available on dev 1
10: nid006506:263728:264887 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
16: nid006554:221583:222762 [2] NCCL INFO NVLS multicast support is not available on dev 2
10: nid006506:263729:264886 [2] NCCL INFO NVLS multicast support is not available on dev 2
10: nid006506:263729:264886 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
16: nid006554:221583:222762 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
16: nid006554:221582:222761 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
16: nid006554:221582:222761 [1] NCCL INFO NVLS multicast support is not available on dev 1
16: nid006554:221582:222761 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 8: nid006503:218410:219569 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 8: nid006503:218410:219569 [0] NCCL INFO NVLS multicast support is not available on dev 0
 8: nid006503:218410:219569 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 8: nid006503:218413:219571 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 8: nid006503:218413:219571 [3] NCCL INFO NVLS multicast support is not available on dev 3
 8: nid006503:218413:219571 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 8: nid006503:218411:219568 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 8: nid006503:218412:219570 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 8: nid006503:218411:219568 [1] NCCL INFO NVLS multicast support is not available on dev 1
 8: nid006503:218412:219570 [2] NCCL INFO NVLS multicast support is not available on dev 2
 8: nid006503:218411:219568 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 8: nid006503:218412:219570 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
31: nid007342:59767:61194 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
31: nid007342:59767:61194 [0] NCCL INFO NVLS multicast support is not available on dev 0
31: nid007342:59767:61194 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
31: nid007342:59770:61197 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
31: nid007342:59770:61197 [3] NCCL INFO NVLS multicast support is not available on dev 3
31: nid007342:59770:61197 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
31: nid007342:59769:61195 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 9: nid006505:249082:250266 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
31: nid007342:59768:61196 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
31: nid007342:59768:61196 [1] NCCL INFO NVLS multicast support is not available on dev 1
31: nid007342:59768:61196 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
31: nid007342:59769:61195 [2] NCCL INFO NVLS multicast support is not available on dev 2
31: nid007342:59769:61195 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 9: nid006505:249082:250266 [2] NCCL INFO NVLS multicast support is not available on dev 2
 9: nid006505:249081:250246 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 9: nid006505:249082:250266 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 9: nid006505:249080:250267 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 9: nid006505:249081:250246 [1] NCCL INFO NVLS multicast support is not available on dev 1
 9: nid006505:249081:250246 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
20: nid006558:215468:216644 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
20: nid006558:215470:216643 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
20: nid006558:215468:216644 [1] NCCL INFO NVLS multicast support is not available on dev 1
 9: nid006505:249080:250267 [0] NCCL INFO NVLS multicast support is not available on dev 0
 9: nid006505:249080:250267 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
20: nid006558:215467:216641 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
20: nid006558:215469:216642 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
20: nid006558:215468:216644 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
20: nid006558:215467:216641 [0] NCCL INFO NVLS multicast support is not available on dev 0
20: nid006558:215467:216641 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 9: nid006505:249083:250268 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
20: nid006558:215470:216643 [3] NCCL INFO NVLS multicast support is not available on dev 3
20: nid006558:215469:216642 [2] NCCL INFO NVLS multicast support is not available on dev 2
20: nid006558:215470:216643 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
20: nid006558:215469:216642 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 9: nid006505:249083:250268 [3] NCCL INFO NVLS multicast support is not available on dev 3
 9: nid006505:249083:250268 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
22: nid006560:222271:223425 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
22: nid006560:222271:223425 [0] NCCL INFO NVLS multicast support is not available on dev 0
22: nid006560:222271:223425 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
22: nid006560:222274:223424 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
22: nid006560:222274:223424 [3] NCCL INFO NVLS multicast support is not available on dev 3
27: nid006566:216747:217884 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
27: nid006566:216747:217884 [1] NCCL INFO NVLS multicast support is not available on dev 1
27: nid006566:216747:217884 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
27: nid006566:216749:217883 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
27: nid006566:216749:217883 [3] NCCL INFO NVLS multicast support is not available on dev 3
27: nid006566:216749:217883 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
27: nid006566:216746:217885 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
27: nid006566:216748:217887 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
27: nid006566:216746:217885 [0] NCCL INFO NVLS multicast support is not available on dev 0
27: nid006566:216746:217885 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
27: nid006566:216748:217887 [2] NCCL INFO NVLS multicast support is not available on dev 2
27: nid006566:216748:217887 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 4: nid006499:254557:255713 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 4: nid006499:254559:255712 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 4: nid006499:254557:255713 [1] NCCL INFO NVLS multicast support is not available on dev 1
 4: nid006499:254557:255713 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 4: nid006499:254559:255712 [3] NCCL INFO NVLS multicast support is not available on dev 3
 4: nid006499:254559:255712 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 4: nid006499:254558:255715 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 4: nid006499:254556:255714 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 4: nid006499:254558:255715 [2] NCCL INFO NVLS multicast support is not available on dev 2
 4: nid006499:254556:255714 [0] NCCL INFO NVLS multicast support is not available on dev 0
 4: nid006499:254556:255714 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 4: nid006499:254558:255715 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
15: nid006553:223925:225066 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
14: nid006510:229444:230608 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
15: nid006553:223923:225069 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
15: nid006553:223925:225066 [2] NCCL INFO NVLS multicast support is not available on dev 2
15: nid006553:223925:225066 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 0: nid006495:241021:242171 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 5: nid006500:260084:261238 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 5: nid006500:260086:261239 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
14: nid006510:229444:230608 [2] NCCL INFO NVLS multicast support is not available on dev 2
14: nid006510:229444:230608 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
15: nid006553:223926:225068 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
15: nid006553:223923:225069 [0] NCCL INFO NVLS multicast support is not available on dev 0
 5: nid006500:260084:261238 [1] NCCL INFO NVLS multicast support is not available on dev 1
14: nid006510:229443:230607 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 5: nid006500:260084:261238 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
14: nid006510:229442:230610 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 5: nid006500:260086:261239 [3] NCCL INFO NVLS multicast support is not available on dev 3
 5: nid006500:260086:261239 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 5: nid006500:260083:261236 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
14: nid006510:229443:230607 [1] NCCL INFO NVLS multicast support is not available on dev 1
14: nid006510:229443:230607 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 0: nid006495:241021:242171 [2] NCCL INFO NVLS multicast support is not available on dev 2
 0: nid006495:241020:242170 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 0: nid006495:241019:242169 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 0: nid006495:241021:242171 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
14: nid006510:229442:230610 [0] NCCL INFO NVLS multicast support is not available on dev 0
14: nid006510:229442:230610 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 5: nid006500:260083:261236 [0] NCCL INFO NVLS multicast support is not available on dev 0
 5: nid006500:260083:261236 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 0: nid006495:241020:242170 [1] NCCL INFO NVLS multicast support is not available on dev 1
 0: nid006495:241019:242169 [0] NCCL INFO NVLS multicast support is not available on dev 0
 0: nid006495:241020:242170 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 0: nid006495:241019:242169 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 5: nid006500:260085:261237 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
14: nid006510:229445:230609 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 0: nid006495:241022:242172 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
14: nid006510:229445:230609 [3] NCCL INFO NVLS multicast support is not available on dev 3
 5: nid006500:260085:261237 [2] NCCL INFO NVLS multicast support is not available on dev 2
 5: nid006500:260085:261237 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
14: nid006510:229445:230609 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 0: nid006495:241022:242172 [3] NCCL INFO NVLS multicast support is not available on dev 3
 0: nid006495:241022:242172 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 2: nid006497:227577:228687 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 2: nid006497:227577:228687 [2] NCCL INFO NVLS multicast support is not available on dev 2
 2: nid006497:227577:228687 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 2: nid006497:227576:228686 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
13: nid006509:201789:202939 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
13: nid006509:201789:202939 [1] NCCL INFO NVLS multicast support is not available on dev 1
13: nid006509:201789:202939 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 2: nid006497:227576:228686 [1] NCCL INFO NVLS multicast support is not available on dev 1
 2: nid006497:227576:228686 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
13: nid006509:201788:202938 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 2: nid006497:227575:228688 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
13: nid006509:201791:202940 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
13: nid006509:201791:202940 [3] NCCL INFO NVLS multicast support is not available on dev 3
13: nid006509:201791:202940 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 2: nid006497:227575:228688 [0] NCCL INFO NVLS multicast support is not available on dev 0
 2: nid006497:227575:228688 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
13: nid006509:201790:202941 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
13: nid006509:201790:202941 [2] NCCL INFO NVLS multicast support is not available on dev 2
13: nid006509:201790:202941 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 2: nid006497:227578:228689 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 2: nid006497:227578:228689 [3] NCCL INFO NVLS multicast support is not available on dev 3
 2: nid006497:227578:228689 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
18: nid006556:210777:211997 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
18: nid006556:210777:211997 [3] NCCL INFO NVLS multicast support is not available on dev 3
18: nid006556:210777:211997 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
30: nid007318:20580:21733 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
30: nid007318:20582:21736 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
12: nid006508:205769:206942 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
30: nid007318:20583:21730 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
18: nid006556:210776:211996 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
30: nid007318:20582:21736 [2] NCCL INFO NVLS multicast support is not available on dev 2
30: nid007318:20583:21730 [3] NCCL INFO NVLS multicast support is not available on dev 3
30: nid007318:20582:21736 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
30: nid007318:20583:21730 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
12: nid006508:205769:206942 [3] NCCL INFO NVLS multicast support is not available on dev 3
12: nid006508:205769:206942 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
18: nid006556:210776:211996 [2] NCCL INFO NVLS multicast support is not available on dev 2
18: nid006556:210776:211996 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
12: nid006508:205766:206941 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
12: nid006508:205768:206939 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
18: nid006556:210775:211994 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
18: nid006556:210775:211994 [1] NCCL INFO NVLS multicast support is not available on dev 1
12: nid006508:205767:206940 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
12: nid006508:205768:206939 [2] NCCL INFO NVLS multicast support is not available on dev 2
12: nid006508:205768:206939 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
12: nid006508:205766:206941 [0] NCCL INFO NVLS multicast support is not available on dev 0
18: nid006556:210775:211994 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
12: nid006508:205766:206941 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
12: nid006508:205767:206940 [1] NCCL INFO NVLS multicast support is not available on dev 1
12: nid006508:205767:206940 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
11: nid006507:211338:212472 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
11: nid006507:211338:212472 [3] NCCL INFO NVLS multicast support is not available on dev 3
11: nid006507:211338:212472 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
25: nid006564:223022:224162 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
28: nid007251:72170:73327 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
11: nid006507:211337:212473 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
25: nid006564:223022:224162 [1] NCCL INFO NVLS multicast support is not available on dev 1
25: nid006564:223022:224162 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
28: nid007251:72170:73327 [1] NCCL INFO NVLS multicast support is not available on dev 1
28: nid007251:72169:73329 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
28: nid007251:72170:73327 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 7: nid006502:252584:253741 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
28: nid007251:72169:73329 [0] NCCL INFO NVLS multicast support is not available on dev 0
28: nid007251:72169:73329 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
11: nid006507:211335:212475 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
11: nid006507:211337:212473 [2] NCCL INFO NVLS multicast support is not available on dev 2
11: nid006507:211337:212473 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
28: nid007251:72172:73331 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
25: nid006564:223021:224160 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
18: nid006556:210774:211995 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 7: nid006502:252584:253741 [0] NCCL INFO NVLS multicast support is not available on dev 0
 7: nid006502:252584:253741 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
11: nid006507:211335:212475 [0] NCCL INFO NVLS multicast support is not available on dev 0
11: nid006507:211335:212475 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
28: nid007251:72172:73331 [3] NCCL INFO NVLS multicast support is not available on dev 3
28: nid007251:72172:73331 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
18: nid006556:210774:211995 [0] NCCL INFO NVLS multicast support is not available on dev 0
18: nid006556:210774:211995 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
28: nid007251:72171:73330 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 7: nid006502:252587:253740 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
25: nid006564:223021:224160 [0] NCCL INFO NVLS multicast support is not available on dev 0
25: nid006564:223023:224161 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
25: nid006564:223021:224160 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
25: nid006564:223023:224161 [2] NCCL INFO NVLS multicast support is not available on dev 2
25: nid006564:223024:224163 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
28: nid007251:72171:73330 [2] NCCL INFO NVLS multicast support is not available on dev 2
28: nid007251:72171:73330 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 7: nid006502:252587:253740 [3] NCCL INFO NVLS multicast support is not available on dev 3
 7: nid006502:252587:253740 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
11: nid006507:211336:212474 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
25: nid006564:223023:224161 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
25: nid006564:223024:224163 [3] NCCL INFO NVLS multicast support is not available on dev 3
25: nid006564:223024:224163 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 7: nid006502:252586:253742 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 7: nid006502:252585:253743 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 7: nid006502:252585:253743 [1] NCCL INFO NVLS multicast support is not available on dev 1
 7: nid006502:252586:253742 [2] NCCL INFO NVLS multicast support is not available on dev 2
19: nid006557:208417:209611 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
11: nid006507:211336:212474 [1] NCCL INFO NVLS multicast support is not available on dev 1
11: nid006507:211336:212474 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 7: nid006502:252585:253743 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 7: nid006502:252586:253742 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208417:209611 [0] NCCL INFO NVLS multicast support is not available on dev 0
19: nid006557:208417:209611 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208420:209604 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
26: nid006565:222468:223651 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
19: nid006557:208420:209604 [3] NCCL INFO NVLS multicast support is not available on dev 3
19: nid006557:208420:209604 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208418:209608 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
26: nid006565:222467:223652 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
26: nid006565:222468:223651 [3] NCCL INFO NVLS multicast support is not available on dev 3
26: nid006565:222468:223651 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208419:209607 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
19: nid006557:208418:209608 [1] NCCL INFO NVLS multicast support is not available on dev 1
26: nid006565:222467:223652 [2] NCCL INFO NVLS multicast support is not available on dev 2
26: nid006565:222466:223654 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
26: nid006565:222467:223652 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208418:209608 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
19: nid006557:208419:209607 [2] NCCL INFO NVLS multicast support is not available on dev 2
19: nid006557:208419:209607 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
26: nid006565:222465:223653 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
26: nid006565:222466:223654 [1] NCCL INFO NVLS multicast support is not available on dev 1
26: nid006565:222466:223654 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
26: nid006565:222465:223653 [0] NCCL INFO NVLS multicast support is not available on dev 0
26: nid006565:222465:223653 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
29: nid007305:27225:28342 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
29: nid007305:27223:28343 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
29: nid007305:27225:28342 [3] NCCL INFO NVLS multicast support is not available on dev 3
29: nid007305:27225:28342 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
29: nid007305:27223:28343 [1] NCCL INFO NVLS multicast support is not available on dev 1
29: nid007305:27223:28343 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
23: nid006561:220714:221896 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
29: nid007305:27222:28341 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
29: nid007305:27222:28341 [0] NCCL INFO NVLS multicast support is not available on dev 0
29: nid007305:27222:28341 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
23: nid006561:220714:221896 [3] NCCL INFO NVLS multicast support is not available on dev 3
23: nid006561:220714:221896 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
29: nid007305:27224:28344 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
23: nid006561:220713:221895 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
29: nid007305:27224:28344 [2] NCCL INFO NVLS multicast support is not available on dev 2
29: nid007305:27224:28344 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
23: nid006561:220712:221898 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
23: nid006561:220713:221895 [2] NCCL INFO NVLS multicast support is not available on dev 2
23: nid006561:220713:221895 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
23: nid006561:220712:221898 [1] NCCL INFO NVLS multicast support is not available on dev 1
23: nid006561:220712:221898 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
23: nid006561:220711:221897 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
23: nid006561:220711:221897 [0] NCCL INFO NVLS multicast support is not available on dev 0
23: nid006561:220711:221897 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 3: nid006498:226765:227925 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 3: nid006498:226765:227925 [0] NCCL INFO NVLS multicast support is not available on dev 0
 6: nid006501:221942:223088 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 6: nid006501:221942:223088 [1] NCCL INFO NVLS multicast support is not available on dev 1
 6: nid006501:221942:223088 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 3: nid006498:226765:227925 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 3: nid006498:226768:227915 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 3: nid006498:226767:227919 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
17: nid006555:206594:207777 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
17: nid006555:206594:207777 [1] NCCL INFO NVLS multicast support is not available on dev 1
17: nid006555:206594:207777 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 6: nid006501:221941:223089 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 3: nid006498:226766:227918 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 3: nid006498:226768:227915 [3] NCCL INFO NVLS multicast support is not available on dev 3
 3: nid006498:226767:227919 [2] NCCL INFO NVLS multicast support is not available on dev 2
 3: nid006498:226768:227915 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 3: nid006498:226767:227919 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 6: nid006501:221941:223089 [0] NCCL INFO NVLS multicast support is not available on dev 0
 6: nid006501:221941:223089 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 3: nid006498:226766:227918 [1] NCCL INFO NVLS multicast support is not available on dev 1
 3: nid006498:226766:227918 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 6: nid006501:221943:223091 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 6: nid006501:221943:223091 [2] NCCL INFO NVLS multicast support is not available on dev 2
 6: nid006501:221943:223091 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
17: nid006555:206595:207800 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
17: nid006555:206593:207804 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
17: nid006555:206595:207800 [2] NCCL INFO NVLS multicast support is not available on dev 2
17: nid006555:206595:207800 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
24: nid006563:221141:222303 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
24: nid006563:221140:222302 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
24: nid006563:221141:222303 [1] NCCL INFO NVLS multicast support is not available on dev 1
24: nid006563:221141:222303 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
17: nid006555:206593:207804 [0] NCCL INFO NVLS multicast support is not available on dev 0
17: nid006555:206593:207804 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
24: nid006563:221140:222302 [0] NCCL INFO NVLS multicast support is not available on dev 0
24: nid006563:221140:222302 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 6: nid006501:221944:223090 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 6: nid006501:221944:223090 [3] NCCL INFO NVLS multicast support is not available on dev 3
17: nid006555:206596:207778 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
17: nid006555:206596:207778 [3] NCCL INFO NVLS multicast support is not available on dev 3
17: nid006555:206596:207778 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
24: nid006563:221143:222304 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
24: nid006563:221143:222304 [3] NCCL INFO NVLS multicast support is not available on dev 3
24: nid006563:221143:222304 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 1: nid006496:242585:243775 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 1: nid006496:242587:243774 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 1: nid006496:242587:243774 [3] NCCL INFO NVLS multicast support is not available on dev 3
 1: nid006496:242587:243774 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 1: nid006496:242585:243775 [1] NCCL INFO NVLS multicast support is not available on dev 1
 1: nid006496:242585:243775 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 1: nid006496:242586:243772 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 1: nid006496:242584:243773 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 1: nid006496:242586:243772 [2] NCCL INFO NVLS multicast support is not available on dev 2
 1: nid006496:242586:243772 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 1: nid006496:242584:243773 [0] NCCL INFO NVLS multicast support is not available on dev 0
 1: nid006496:242584:243773 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
21: nid006559:211123:212275 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
21: nid006559:211125:212276 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
21: nid006559:211123:212275 [0] NCCL INFO NVLS multicast support is not available on dev 0
21: nid006559:211123:212275 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
21: nid006559:211124:212277 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
21: nid006559:211126:212278 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
21: nid006559:211125:212276 [2] NCCL INFO NVLS multicast support is not available on dev 2
21: nid006559:211125:212276 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
21: nid006559:211126:212278 [3] NCCL INFO NVLS multicast support is not available on dev 3
21: nid006559:211124:212277 [1] NCCL INFO NVLS multicast support is not available on dev 1
21: nid006559:211126:212278 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
21: nid006559:211124:212277 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
22: nid006560:222274:223424 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
22: nid006560:222272:223426 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
22: nid006560:222273:223427 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
22: nid006560:222272:223426 [1] NCCL INFO NVLS multicast support is not available on dev 1
22: nid006560:222273:223427 [2] NCCL INFO NVLS multicast support is not available on dev 2
22: nid006560:222272:223426 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
22: nid006560:222273:223427 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
30: nid007318:20581:21731 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
30: nid007318:20580:21733 [0] NCCL INFO NVLS multicast support is not available on dev 0
30: nid007318:20580:21733 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
30: nid007318:20581:21731 [1] NCCL INFO NVLS multicast support is not available on dev 1
30: nid007318:20581:21731 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
13: nid006509:201788:202938 [0] NCCL INFO NVLS multicast support is not available on dev 0
13: nid006509:201788:202938 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
15: nid006553:223923:225069 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
15: nid006553:223926:225068 [3] NCCL INFO NVLS multicast support is not available on dev 3
15: nid006553:223926:225068 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
15: nid006553:223924:225067 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
15: nid006553:223924:225067 [1] NCCL INFO NVLS multicast support is not available on dev 1
15: nid006553:223924:225067 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
 6: nid006501:221944:223090 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
24: nid006563:221142:222307 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
24: nid006563:221142:222307 [2] NCCL INFO NVLS multicast support is not available on dev 2
24: nid006563:221142:222307 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
21: nid006559:211126:212278 [3] NCCL INFO comm 0xaaaaf4030670 rank 87 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
21: nid006559:211126:212278 [3] NCCL INFO Trees [0] -1/-1/-1->87->86 [1] -1/-1/-1->87->86 [2] 84/-1/-1->87->86 [3] 84/-1/-1->87->86 [4] -1/-1/-1->87->86 [5] -1/-1/-1->87->86 [6] 84/-1/-1->87->86 [7] 84/-1/-1->87->86
21: nid006559:211125:212276 [2] NCCL INFO comm 0xaaaad6db0e40 rank 86 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
21: nid006559:211126:212278 [3] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211124:212277 [1] NCCL INFO comm 0xaaaae87e0f50 rank 85 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
21: nid006559:211123:212275 [0] NCCL INFO comm 0xaaaacd464770 rank 84 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
21: nid006559:211125:212276 [2] NCCL INFO Trees [0] 87/-1/-1->86->85 [1] 87/-1/-1->86->85 [2] 87/-1/-1->86->90 [3] 87/-1/-1->86->90 [4] 87/-1/-1->86->85 [5] 87/-1/-1->86->85 [6] 87/90/82->86->78 [7] 87/90/82->86->78
21: nid006559:211125:212276 [2] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215468:216644 [1] NCCL INFO comm 0xaaaae4530560 rank 81 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
20: nid006558:215469:216642 [2] NCCL INFO comm 0xaaaaf4921880 rank 82 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
20: nid006558:215470:216643 [3] NCCL INFO comm 0xaaab04942a10 rank 83 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
20: nid006558:215467:216641 [0] NCCL INFO comm 0xaaab024c49d0 rank 80 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
21: nid006559:211124:212277 [1] NCCL INFO Trees [0] 86/-1/-1->85->84 [1] 86/-1/-1->85->84 [2] -1/-1/-1->85->84 [3] -1/-1/-1->85->84 [4] 86/-1/-1->85->84 [5] 86/-1/-1->85->84 [6] -1/-1/-1->85->84 [7] -1/-1/-1->85->84
21: nid006559:211123:212275 [0] NCCL INFO Trees [0] 85/-1/-1->84->88 [1] 85/-1/-1->84->88 [2] 85/-1/-1->84->87 [3] 85/-1/-1->84->87 [4] 85/88/80->84->76 [5] 85/88/80->84->76 [6] 85/-1/-1->84->87 [7] 85/-1/-1->84->87
20: nid006558:215468:216644 [1] NCCL INFO Trees [0] 82/-1/-1->81->80 [1] 82/-1/-1->81->80 [2] -1/-1/-1->81->80 [3] -1/-1/-1->81->80 [4] 82/-1/-1->81->80 [5] 82/-1/-1->81->80 [6] -1/-1/-1->81->80 [7] -1/-1/-1->81->80
20: nid006558:215468:216644 [1] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211124:212277 [1] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211123:212275 [0] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215469:216642 [2] NCCL INFO Trees [0] 83/-1/-1->82->81 [1] 83/-1/-1->82->81 [2] 83/74/90->82->98 [3] 83/74/90->82->98 [4] 83/-1/-1->82->81 [5] 83/-1/-1->82->81 [6] 83/-1/-1->82->86 [7] 83/-1/-1->82->86
20: nid006558:215470:216643 [3] NCCL INFO Trees [0] -1/-1/-1->83->82 [1] -1/-1/-1->83->82 [2] 80/-1/-1->83->82 [3] 80/-1/-1->83->82 [4] -1/-1/-1->83->82 [5] -1/-1/-1->83->82 [6] 80/-1/-1->83->82 [7] 80/-1/-1->83->82
20: nid006558:215467:216641 [0] NCCL INFO Trees [0] 81/72/88->80->96 [1] 81/72/88->80->96 [2] 81/-1/-1->80->83 [3] 81/-1/-1->80->83 [4] 81/-1/-1->80->84 [5] 81/-1/-1->80->84 [6] 81/-1/-1->80->83 [7] 81/-1/-1->80->83
20: nid006558:215469:216642 [2] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208417:209611 [0] NCCL INFO comm 0xaaaafeab5cd0 rank 76 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
19: nid006557:208420:209604 [3] NCCL INFO comm 0xaaab123500f0 rank 79 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
20: nid006558:215470:216643 [3] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215467:216641 [0] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210777:211997 [3] NCCL INFO comm 0xaaab001d2d70 rank 75 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
18: nid006556:210775:211994 [1] NCCL INFO comm 0xaaab1c4215c0 rank 73 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
17: nid006555:206596:207778 [3] NCCL INFO comm 0xaaaafe0d1bf0 rank 71 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
17: nid006555:206595:207800 [2] NCCL INFO comm 0xaaaaf5a54290 rank 70 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
19: nid006557:208419:209607 [2] NCCL INFO comm 0xaaab15322000 rank 78 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
19: nid006557:208418:209608 [1] NCCL INFO comm 0xaaab2b67f850 rank 77 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
19: nid006557:208420:209604 [3] NCCL INFO Trees [0] -1/-1/-1->79->78 [1] -1/-1/-1->79->78 [2] 76/-1/-1->79->78 [3] 76/-1/-1->79->78 [4] -1/-1/-1->79->78 [5] -1/-1/-1->79->78 [6] 76/-1/-1->79->78 [7] 76/-1/-1->79->78
19: nid006557:208419:209607 [2] NCCL INFO Trees [0] 79/-1/-1->78->77 [1] 79/-1/-1->78->77 [2] 79/-1/-1->78->74 [3] 79/-1/-1->78->74 [4] 79/-1/-1->78->77 [5] 79/-1/-1->78->77 [6] 79/86/70->78->94 [7] 79/86/70->78->94
19: nid006557:208420:209604 [3] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210776:211996 [2] NCCL INFO comm 0xaaab24c608a0 rank 74 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
18: nid006556:210774:211995 [0] NCCL INFO comm 0xaaab00fe4b80 rank 72 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
18: nid006556:210777:211997 [3] NCCL INFO Trees [0] -1/-1/-1->75->74 [1] -1/-1/-1->75->74 [2] 72/-1/-1->75->74 [3] 72/-1/-1->75->74 [4] -1/-1/-1->75->74 [5] -1/-1/-1->75->74 [6] 72/-1/-1->75->74 [7] 72/-1/-1->75->74
18: nid006556:210775:211994 [1] NCCL INFO Trees [0] 74/-1/-1->73->72 [1] 74/-1/-1->73->72 [2] -1/-1/-1->73->72 [3] -1/-1/-1->73->72 [4] 74/-1/-1->73->72 [5] 74/-1/-1->73->72 [6] -1/-1/-1->73->72 [7] -1/-1/-1->73->72
18: nid006556:210777:211997 [3] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210776:211996 [2] NCCL INFO Trees [0] 75/-1/-1->74->73 [1] 75/-1/-1->74->73 [2] 75/70/78->74->82 [3] 75/70/78->74->82 [4] 75/-1/-1->74->73 [5] 75/-1/-1->74->73 [6] 75/-1/-1->74->70 [7] 75/-1/-1->74->70
18: nid006556:210775:211994 [1] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208418:209608 [1] NCCL INFO Trees [0] 78/-1/-1->77->76 [1] 78/-1/-1->77->76 [2] -1/-1/-1->77->76 [3] -1/-1/-1->77->76 [4] 78/-1/-1->77->76 [5] 78/-1/-1->77->76 [6] -1/-1/-1->77->76 [7] -1/-1/-1->77->76
19: nid006557:208419:209607 [2] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208418:209608 [1] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210776:211996 [2] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208417:209611 [0] NCCL INFO Trees [0] 77/-1/-1->76->72 [1] 77/-1/-1->76->72 [2] 77/-1/-1->76->79 [3] 77/-1/-1->76->79 [4] 77/84/68->76->92 [5] 77/84/68->76->92 [6] 77/-1/-1->76->79 [7] 77/-1/-1->76->79
19: nid006557:208417:209611 [0] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206596:207778 [3] NCCL INFO Trees [0] -1/-1/-1->71->70 [1] -1/-1/-1->71->70 [2] 68/-1/-1->71->70 [3] 68/-1/-1->71->70 [4] -1/-1/-1->71->70 [5] -1/-1/-1->71->70 [6] 68/-1/-1->71->70 [7] 68/-1/-1->71->70
17: nid006555:206595:207800 [2] NCCL INFO Trees [0] 71/-1/-1->70->69 [1] 71/-1/-1->70->69 [2] 71/-1/-1->70->74 [3] 71/-1/-1->70->74 [4] 71/-1/-1->70->69 [5] 71/-1/-1->70->69 [6] 71/74/66->70->78 [7] 71/74/66->70->78
17: nid006555:206596:207778 [3] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206595:207800 [2] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221584:222757 [3] NCCL INFO comm 0xaaaafdce2b10 rank 67 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
16: nid006554:221583:222762 [2] NCCL INFO comm 0xaaaadd7bfd30 rank 66 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
17: nid006555:206594:207777 [1] NCCL INFO comm 0xaaab0a7e34b0 rank 69 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
17: nid006555:206593:207804 [0] NCCL INFO comm 0xaaab06a54c60 rank 68 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
17: nid006555:206594:207777 [1] NCCL INFO Trees [0] 70/-1/-1->69->68 [1] 70/-1/-1->69->68 [2] -1/-1/-1->69->68 [3] -1/-1/-1->69->68 [4] 70/-1/-1->69->68 [5] 70/-1/-1->69->68 [6] -1/-1/-1->69->68 [7] -1/-1/-1->69->68
18: nid006556:210774:211995 [0] NCCL INFO Trees [0] 73/68/76->72->80 [1] 73/68/76->72->80 [2] 73/-1/-1->72->75 [3] 73/-1/-1->72->75 [4] 73/-1/-1->72->68 [5] 73/-1/-1->72->68 [6] 73/-1/-1->72->75 [7] 73/-1/-1->72->75
18: nid006556:210774:211995 [0] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221582:222761 [1] NCCL INFO comm 0xaaaad7c30460 rank 65 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
16: nid006554:221581:222760 [0] NCCL INFO comm 0xaaaae2836550 rank 64 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
16: nid006554:221584:222757 [3] NCCL INFO Trees [0] -1/-1/-1->67->66 [1] -1/-1/-1->67->66 [2] 64/-1/-1->67->66 [3] 64/-1/-1->67->66 [4] -1/-1/-1->67->66 [5] -1/-1/-1->67->66 [6] 64/-1/-1->67->66 [7] 64/-1/-1->67->66
15: nid006553:223926:225068 [3] NCCL INFO comm 0xaaab186224e0 rank 63 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
15: nid006553:223925:225066 [2] NCCL INFO comm 0xaaab1b0b02b0 rank 62 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
16: nid006554:221583:222762 [2] NCCL INFO Trees [0] 67/-1/-1->66->65 [1] 67/-1/-1->66->65 [2] 67/34/98->66->2 [3] 67/34/98->66->2 [4] 67/-1/-1->66->65 [5] 67/-1/-1->66->65 [6] 67/-1/-1->66->70 [7] 67/-1/-1->66->70
16: nid006554:221584:222757 [3] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206594:207777 [1] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221583:222762 [2] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206593:207804 [0] NCCL INFO Trees [0] 69/-1/-1->68->72 [1] 69/-1/-1->68->72 [2] 69/-1/-1->68->71 [3] 69/-1/-1->68->71 [4] 69/72/64->68->76 [5] 69/72/64->68->76 [6] 69/-1/-1->68->71 [7] 69/-1/-1->68->71
17: nid006555:206593:207804 [0] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221582:222761 [1] NCCL INFO Trees [0] 66/-1/-1->65->64 [1] 66/-1/-1->65->64 [2] -1/-1/-1->65->64 [3] -1/-1/-1->65->64 [4] 66/-1/-1->65->64 [5] 66/-1/-1->65->64 [6] -1/-1/-1->65->64 [7] -1/-1/-1->65->64
16: nid006554:221582:222761 [1] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223924:225067 [1] NCCL INFO comm 0xaaaae1630990 rank 61 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
14: nid006510:229445:230609 [3] NCCL INFO comm 0xaaaad4ac1400 rank 59 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
14: nid006510:229445:230609 [3] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] -1/-1/-1->59->58 [2] 56/-1/-1->59->58 [3] 56/-1/-1->59->58 [4] -1/-1/-1->59->58 [5] -1/-1/-1->59->58 [6] 56/-1/-1->59->58 [7] 56/-1/-1->59->58
15: nid006553:223926:225068 [3] NCCL INFO Trees [0] -1/-1/-1->63->62 [1] -1/-1/-1->63->62 [2] 60/-1/-1->63->62 [3] 60/-1/-1->63->62 [4] -1/-1/-1->63->62 [5] -1/-1/-1->63->62 [6] 60/-1/-1->63->62 [7] 60/-1/-1->63->62
15: nid006553:223923:225069 [0] NCCL INFO comm 0xaaab008f4e30 rank 60 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
15: nid006553:223925:225066 [2] NCCL INFO Trees [0] 63/-1/-1->62->61 [1] 63/-1/-1->62->61 [2] 63/-1/-1->62->58 [3] 63/-1/-1->62->58 [4] 63/-1/-1->62->61 [5] 63/-1/-1->62->61 [6] 63/94/30->62->126 [7] 63/94/30->62->126
14: nid006510:229445:230609 [3] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223926:225068 [3] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223925:225066 [2] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229444:230608 [2] NCCL INFO comm 0xaaab050af830 rank 58 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
15: nid006553:223924:225067 [1] NCCL INFO Trees [0] 62/-1/-1->61->60 [1] 62/-1/-1->61->60 [2] -1/-1/-1->61->60 [3] -1/-1/-1->61->60 [4] 62/-1/-1->61->60 [5] 62/-1/-1->61->60 [6] -1/-1/-1->61->60 [7] -1/-1/-1->61->60
15: nid006553:223924:225067 [1] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229443:230607 [1] NCCL INFO comm 0xaaab02930260 rank 57 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
14: nid006510:229442:230610 [0] NCCL INFO comm 0xaaaadd8b3cb0 rank 56 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
16: nid006554:221581:222760 [0] NCCL INFO Trees [0] 65/32/96->64->0 [1] 65/32/96->64->0 [2] 65/-1/-1->64->67 [3] 65/-1/-1->64->67 [4] 65/-1/-1->64->68 [5] 65/-1/-1->64->68 [6] 65/-1/-1->64->67 [7] 65/-1/-1->64->67
16: nid006554:221581:222760 [0] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201791:202940 [3] NCCL INFO comm 0xaaaafc8b2560 rank 55 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
13: nid006509:201790:202941 [2] NCCL INFO comm 0xaaaac07c29c0 rank 54 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
14: nid006510:229444:230608 [2] NCCL INFO Trees [0] 59/-1/-1->58->57 [1] 59/-1/-1->58->57 [2] 59/54/62->58->50 [3] 59/54/62->58->50 [4] 59/-1/-1->58->57 [5] 59/-1/-1->58->57 [6] 59/-1/-1->58->54 [7] 59/-1/-1->58->54
14: nid006510:229444:230608 [2] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223923:225069 [0] NCCL INFO Trees [0] 61/-1/-1->60->56 [1] 61/-1/-1->60->56 [2] 61/-1/-1->60->63 [3] 61/-1/-1->60->63 [4] 61/92/28->60->124 [5] 61/92/28->60->124 [6] 61/-1/-1->60->63 [7] 61/-1/-1->60->63
15: nid006553:223923:225069 [0] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229443:230607 [1] NCCL INFO Trees [0] 58/-1/-1->57->56 [1] 58/-1/-1->57->56 [2] -1/-1/-1->57->56 [3] -1/-1/-1->57->56 [4] 58/-1/-1->57->56 [5] 58/-1/-1->57->56 [6] -1/-1/-1->57->56 [7] -1/-1/-1->57->56
14: nid006510:229443:230607 [1] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229442:230610 [0] NCCL INFO Trees [0] 57/52/60->56->48 [1] 57/52/60->56->48 [2] 57/-1/-1->56->59 [3] 57/-1/-1->56->59 [4] 57/-1/-1->56->52 [5] 57/-1/-1->56->52 [6] 57/-1/-1->56->59 [7] 57/-1/-1->56->59
14: nid006510:229442:230610 [0] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201791:202940 [3] NCCL INFO Trees [0] -1/-1/-1->55->54 [1] -1/-1/-1->55->54 [2] 52/-1/-1->55->54 [3] 52/-1/-1->55->54 [4] -1/-1/-1->55->54 [5] -1/-1/-1->55->54 [6] 52/-1/-1->55->54 [7] 52/-1/-1->55->54
13: nid006509:201791:202940 [3] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201790:202941 [2] NCCL INFO Trees [0] 55/-1/-1->54->53 [1] 55/-1/-1->54->53 [2] 55/-1/-1->54->58 [3] 55/-1/-1->54->58 [4] 55/-1/-1->54->53 [5] 55/-1/-1->54->53 [6] 55/58/50->54->46 [7] 55/58/50->54->46
13: nid006509:201790:202941 [2] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201789:202939 [1] NCCL INFO comm 0xaaab23f6f620 rank 53 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
13: nid006509:201788:202938 [0] NCCL INFO comm 0xaaab17976000 rank 52 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
12: nid006508:205769:206942 [3] NCCL INFO comm 0xaaaafffd57b0 rank 51 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
12: nid006508:205768:206939 [2] NCCL INFO comm 0xaaaae60828b0 rank 50 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
13: nid006509:201789:202939 [1] NCCL INFO Trees [0] 54/-1/-1->53->52 [1] 54/-1/-1->53->52 [2] -1/-1/-1->53->52 [3] -1/-1/-1->53->52 [4] 54/-1/-1->53->52 [5] 54/-1/-1->53->52 [6] -1/-1/-1->53->52 [7] -1/-1/-1->53->52
13: nid006509:201789:202939 [1] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211338:212472 [3] NCCL INFO comm 0xaaab1f201250 rank 47 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
12: nid006508:205767:206940 [1] NCCL INFO comm 0xaaaaf9abf4e0 rank 49 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
12: nid006508:205768:206939 [2] NCCL INFO Trees [0] 51/-1/-1->50->49 [1] 51/-1/-1->50->49 [2] 51/42/58->50->34 [3] 51/42/58->50->34 [4] 51/-1/-1->50->49 [5] 51/-1/-1->50->49 [6] 51/-1/-1->50->54 [7] 51/-1/-1->50->54
12: nid006508:205769:206942 [3] NCCL INFO Trees [0] -1/-1/-1->51->50 [1] -1/-1/-1->51->50 [2] 48/-1/-1->51->50 [3] 48/-1/-1->51->50 [4] -1/-1/-1->51->50 [5] -1/-1/-1->51->50 [6] 48/-1/-1->51->50 [7] 48/-1/-1->51->50
12: nid006508:205766:206941 [0] NCCL INFO comm 0xaaab01b649c0 rank 48 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
12: nid006508:205768:206939 [2] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205769:206942 [3] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211338:212472 [3] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] 44/-1/-1->47->46 [3] 44/-1/-1->47->46 [4] -1/-1/-1->47->46 [5] -1/-1/-1->47->46 [6] 44/-1/-1->47->46 [7] 44/-1/-1->47->46
11: nid006507:211338:212472 [3] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205767:206940 [1] NCCL INFO Trees [0] 50/-1/-1->49->48 [1] 50/-1/-1->49->48 [2] -1/-1/-1->49->48 [3] -1/-1/-1->49->48 [4] 50/-1/-1->49->48 [5] 50/-1/-1->49->48 [6] -1/-1/-1->49->48 [7] -1/-1/-1->49->48
12: nid006508:205767:206940 [1] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201788:202938 [0] NCCL INFO Trees [0] 53/-1/-1->52->56 [1] 53/-1/-1->52->56 [2] 53/-1/-1->52->55 [3] 53/-1/-1->52->55 [4] 53/56/48->52->44 [5] 53/56/48->52->44 [6] 53/-1/-1->52->55 [7] 53/-1/-1->52->55
13: nid006509:201788:202938 [0] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211337:212473 [2] NCCL INFO comm 0xaaab1716ee10 rank 46 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
12: nid006508:205766:206941 [0] NCCL INFO Trees [0] 49/40/56->48->32 [1] 49/40/56->48->32 [2] 49/-1/-1->48->51 [3] 49/-1/-1->48->51 [4] 49/-1/-1->48->52 [5] 49/-1/-1->48->52 [6] 49/-1/-1->48->51 [7] 49/-1/-1->48->51
12: nid006508:205766:206941 [0] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211335:212475 [0] NCCL INFO comm 0xaaaadbe34c90 rank 44 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 2: nid006497:227578:228689 [3] NCCL INFO comm 0xaaaaf3bd2ec0 rank 11 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
10: nid006506:263730:264885 [3] NCCL INFO comm 0xaaaadbd63f10 rank 43 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
11: nid006507:211336:212474 [1] NCCL INFO comm 0xaaab0a883290 rank 45 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
11: nid006507:211337:212473 [2] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->42 [3] 47/-1/-1->46->42 [4] 47/-1/-1->46->45 [5] 47/-1/-1->46->45 [6] 47/54/38->46->30 [7] 47/54/38->46->30
11: nid006507:211337:212473 [2] NCCL INFO P2P Chunksize set to 131072
10: nid006506:263729:264886 [2] NCCL INFO comm 0xaaab0c9e0740 rank 42 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
10: nid006506:263730:264885 [3] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] 40/-1/-1->43->42 [3] 40/-1/-1->43->42 [4] -1/-1/-1->43->42 [5] -1/-1/-1->43->42 [6] 40/-1/-1->43->42 [7] 40/-1/-1->43->42
10: nid006506:263730:264885 [3] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227577:228687 [2] NCCL INFO comm 0xaaab266a1d50 rank 10 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 2: nid006497:227576:228686 [1] NCCL INFO comm 0xaaaade5e19c0 rank 9 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 9: nid006505:249083:250268 [3] NCCL INFO comm 0xaaab25b11310 rank 39 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
10: nid006506:263728:264887 [1] NCCL INFO comm 0xaaab066b0b90 rank 41 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 2: nid006497:227578:228689 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
 2: nid006497:227575:228688 [0] NCCL INFO comm 0xaaab34a73550 rank 8 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 2: nid006497:227578:228689 [3] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211335:212475 [0] NCCL INFO Trees [0] 45/-1/-1->44->40 [1] 45/-1/-1->44->40 [2] 45/-1/-1->44->47 [3] 45/-1/-1->44->47 [4] 45/52/36->44->28 [5] 45/52/36->44->28 [6] 45/-1/-1->44->47 [7] 45/-1/-1->44->47
 1: nid006496:242587:243774 [3] NCCL INFO comm 0xaaaad33e0d00 rank 7 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
10: nid006506:263729:264886 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/38/46->42->50 [3] 43/38/46->42->50 [4] 43/-1/-1->42->41 [5] 43/-1/-1->42->41 [6] 43/-1/-1->42->38 [7] 43/-1/-1->42->38
10: nid006506:263729:264886 [2] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249083:250268 [3] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] 36/-1/-1->39->38 [3] 36/-1/-1->39->38 [4] -1/-1/-1->39->38 [5] -1/-1/-1->39->38 [6] 36/-1/-1->39->38 [7] 36/-1/-1->39->38
 9: nid006505:249083:250268 [3] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211336:212474 [1] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] -1/-1/-1->45->44 [3] -1/-1/-1->45->44 [4] 46/-1/-1->45->44 [5] 46/-1/-1->45->44 [6] -1/-1/-1->45->44 [7] -1/-1/-1->45->44
11: nid006507:211335:212475 [0] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211336:212474 [1] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221142:222307 [2] NCCL INFO comm 0xaaab146907e0 rank 98 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
24: nid006563:221143:222304 [3] NCCL INFO comm 0xaaab126b1510 rank 99 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
22: nid006560:222274:223424 [3] NCCL INFO comm 0xaaaad06e0ae0 rank 91 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 2: nid006497:227577:228687 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->18 [3] 11/6/14->10->18 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
 2: nid006497:227576:228686 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
24: nid006563:221141:222303 [1] NCCL INFO comm 0xaaaaf69b0d70 rank 97 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
10: nid006506:263727:264880 [0] NCCL INFO comm 0xaaaacce51d30 rank 40 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 2: nid006497:227577:228687 [2] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227575:228688 [0] NCCL INFO Trees [0] 9/4/12->8->16 [1] 9/4/12->8->16 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
 2: nid006497:227576:228686 [1] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227575:228688 [0] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249082:250266 [2] NCCL INFO comm 0xaaaaddaa10c0 rank 38 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
23: nid006561:220714:221896 [3] NCCL INFO comm 0xaaaaf7d8fec0 rank 95 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
23: nid006561:220713:221895 [2] NCCL INFO comm 0xaaab19e1fc40 rank 94 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
10: nid006506:263728:264887 [1] NCCL INFO Trees [0] 42/-1/-1->41->40 [1] 42/-1/-1->41->40 [2] -1/-1/-1->41->40 [3] -1/-1/-1->41->40 [4] 42/-1/-1->41->40 [5] 42/-1/-1->41->40 [6] -1/-1/-1->41->40 [7] -1/-1/-1->41->40
10: nid006506:263728:264887 [1] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249081:250246 [1] NCCL INFO comm 0xaaab0bfe2b70 rank 37 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 0: nid006495:241022:242172 [3] NCCL INFO comm 0xaaaaf2c322a0 rank 3 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 0: nid006495:241021:242171 [2] NCCL INFO comm 0xaaab287e25d0 rank 2 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
23: nid006561:220711:221897 [0] NCCL INFO comm 0xaaab1f303c10 rank 92 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
23: nid006561:220712:221898 [1] NCCL INFO comm 0xaaaafae71f80 rank 93 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
10: nid006506:263727:264880 [0] NCCL INFO Trees [0] 41/36/44->40->48 [1] 41/36/44->40->48 [2] 41/-1/-1->40->43 [3] 41/-1/-1->40->43 [4] 41/-1/-1->40->36 [5] 41/-1/-1->40->36 [6] 41/-1/-1->40->43 [7] 41/-1/-1->40->43
10: nid006506:263727:264880 [0] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242587:243774 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
 1: nid006496:242587:243774 [3] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220714:221896 [3] NCCL INFO Trees [0] -1/-1/-1->95->94 [1] -1/-1/-1->95->94 [2] 92/-1/-1->95->94 [3] 92/-1/-1->95->94 [4] -1/-1/-1->95->94 [5] -1/-1/-1->95->94 [6] 92/-1/-1->95->94 [7] 92/-1/-1->95->94
23: nid006561:220713:221895 [2] NCCL INFO Trees [0] 95/-1/-1->94->93 [1] 95/-1/-1->94->93 [2] 95/-1/-1->94->90 [3] 95/-1/-1->94->90 [4] 95/-1/-1->94->93 [5] 95/-1/-1->94->93 [6] 95/110/78->94->62 [7] 95/110/78->94->62
 1: nid006496:242586:243772 [2] NCCL INFO comm 0xaaaae9db1a00 rank 6 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
24: nid006563:221142:222307 [2] NCCL INFO Trees [0] 99/-1/-1->98->97 [1] 99/-1/-1->98->97 [2] 99/82/114->98->66 [3] 99/82/114->98->66 [4] 99/-1/-1->98->97 [5] 99/-1/-1->98->97 [6] 99/-1/-1->98->102 [7] 99/-1/-1->98->102
24: nid006563:221143:222304 [3] NCCL INFO Trees [0] -1/-1/-1->99->98 [1] -1/-1/-1->99->98 [2] 96/-1/-1->99->98 [3] 96/-1/-1->99->98 [4] -1/-1/-1->99->98 [5] -1/-1/-1->99->98 [6] 96/-1/-1->99->98 [7] 96/-1/-1->99->98
23: nid006561:220714:221896 [3] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249082:250266 [2] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] 39/-1/-1->38->37 [2] 39/-1/-1->38->42 [3] 39/-1/-1->38->42 [4] 39/-1/-1->38->37 [5] 39/-1/-1->38->37 [6] 39/42/34->38->46 [7] 39/42/34->38->46
 9: nid006505:249082:250266 [2] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221142:222307 [2] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222273:223427 [2] NCCL INFO comm 0xaaaae7423530 rank 90 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 9: nid006505:249081:250246 [1] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] -1/-1/-1->37->36 [3] -1/-1/-1->37->36 [4] 38/-1/-1->37->36 [5] 38/-1/-1->37->36 [6] -1/-1/-1->37->36 [7] -1/-1/-1->37->36
 9: nid006505:249081:250246 [1] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221140:222302 [0] NCCL INFO comm 0xaaaafb2f59b0 rank 96 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
24: nid006563:221143:222304 [3] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220713:221895 [2] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222272:223426 [1] NCCL INFO comm 0xaaaafbb630b0 rank 89 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
22: nid006560:222274:223424 [3] NCCL INFO Trees [0] -1/-1/-1->91->90 [1] -1/-1/-1->91->90 [2] 88/-1/-1->91->90 [3] 88/-1/-1->91->90 [4] -1/-1/-1->91->90 [5] -1/-1/-1->91->90 [6] 88/-1/-1->91->90 [7] 88/-1/-1->91->90
22: nid006560:222271:223425 [0] NCCL INFO comm 0xaaaaf8db5160 rank 88 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 9: nid006505:249080:250267 [0] NCCL INFO comm 0xaaaac2b051b0 rank 36 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 1: nid006496:242584:243773 [0] NCCL INFO comm 0xaaaafa2855c0 rank 4 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 1: nid006496:242585:243775 [1] NCCL INFO comm 0xaaab017914b0 rank 5 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 0: nid006495:241020:242170 [1] NCCL INFO comm 0xaaaad1d00bf0 rank 1 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 0: nid006495:241021:242171 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/66/-1->2->-1 [3] 3/66/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
24: nid006563:221141:222303 [1] NCCL INFO Trees [0] 98/-1/-1->97->96 [1] 98/-1/-1->97->96 [2] -1/-1/-1->97->96 [3] -1/-1/-1->97->96 [4] 98/-1/-1->97->96 [5] 98/-1/-1->97->96 [6] -1/-1/-1->97->96 [7] -1/-1/-1->97->96
24: nid006563:221141:222303 [1] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220711:221897 [0] NCCL INFO Trees [0] 93/-1/-1->92->88 [1] 93/-1/-1->92->88 [2] 93/-1/-1->92->95 [3] 93/-1/-1->92->95 [4] 93/108/76->92->60 [5] 93/108/76->92->60 [6] 93/-1/-1->92->95 [7] 93/-1/-1->92->95
23: nid006561:220712:221898 [1] NCCL INFO Trees [0] 94/-1/-1->93->92 [1] 94/-1/-1->93->92 [2] -1/-1/-1->93->92 [3] -1/-1/-1->93->92 [4] 94/-1/-1->93->92 [5] 94/-1/-1->93->92 [6] -1/-1/-1->93->92 [7] -1/-1/-1->93->92
23: nid006561:220711:221897 [0] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220712:221898 [1] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222274:223424 [3] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249080:250267 [0] NCCL INFO Trees [0] 37/-1/-1->36->40 [1] 37/-1/-1->36->40 [2] 37/-1/-1->36->39 [3] 37/-1/-1->36->39 [4] 37/40/32->36->44 [5] 37/40/32->36->44 [6] 37/-1/-1->36->39 [7] 37/-1/-1->36->39
 9: nid006505:249080:250267 [0] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242586:243772 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
 1: nid006496:242586:243772 [2] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241021:242171 [2] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218413:219571 [3] NCCL INFO comm 0xaaaad50f1540 rank 35 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 8: nid006503:218412:219570 [2] NCCL INFO comm 0xaaaadb064120 rank 34 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
31: nid007342:59770:61197 [3] NCCL INFO comm 0xaaaae96a06d0 rank 127 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
31: nid007342:59769:61195 [2] NCCL INFO comm 0xaaaacd751c60 rank 126 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
22: nid006560:222273:223427 [2] NCCL INFO Trees [0] 91/-1/-1->90->89 [1] 91/-1/-1->90->89 [2] 91/86/94->90->82 [3] 91/86/94->90->82 [4] 91/-1/-1->90->89 [5] 91/-1/-1->90->89 [6] 91/-1/-1->90->86 [7] 91/-1/-1->90->86
22: nid006560:222273:223427 [2] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222272:223426 [1] NCCL INFO Trees [0] 90/-1/-1->89->88 [1] 90/-1/-1->89->88 [2] -1/-1/-1->89->88 [3] -1/-1/-1->89->88 [4] 90/-1/-1->89->88 [5] 90/-1/-1->89->88 [6] -1/-1/-1->89->88 [7] -1/-1/-1->89->88
 1: nid006496:242584:243773 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
 1: nid006496:242585:243775 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
 1: nid006496:242584:243773 [0] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242585:243775 [1] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241022:242172 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
 0: nid006495:241022:242172 [3] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222272:223426 [1] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241020:242170 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
 0: nid006495:241019:242169 [0] NCCL INFO comm 0xaaaaca364de0 rank 0 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
24: nid006563:221140:222302 [0] NCCL INFO Trees [0] 97/80/112->96->64 [1] 97/80/112->96->64 [2] 97/-1/-1->96->99 [3] 97/-1/-1->96->99 [4] 97/-1/-1->96->100 [5] 97/-1/-1->96->100 [6] 97/-1/-1->96->99 [7] 97/-1/-1->96->99
24: nid006563:221140:222302 [0] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20583:21730 [3] NCCL INFO comm 0xaaaaee514e10 rank 123 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
22: nid006560:222271:223425 [0] NCCL INFO Trees [0] 89/84/92->88->80 [1] 89/84/92->88->80 [2] 89/-1/-1->88->91 [3] 89/-1/-1->88->91 [4] 89/-1/-1->88->84 [5] 89/-1/-1->88->84 [6] 89/-1/-1->88->91 [7] 89/-1/-1->88->91
22: nid006560:222271:223425 [0] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241020:242170 [1] NCCL INFO P2P Chunksize set to 131072
31: nid007342:59768:61196 [1] NCCL INFO comm 0xaaaaebad4d50 rank 125 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 0: nid006495:241019:242169 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
 7: nid006502:252587:253740 [3] NCCL INFO comm 0xaaab12a7f380 rank 31 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
31: nid007342:59767:61194 [0] NCCL INFO comm 0xaaab02e43cb0 rank 124 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
31: nid007342:59770:61197 [3] NCCL INFO Trees [0] -1/-1/-1->127->126 [1] -1/-1/-1->127->126 [2] 124/-1/-1->127->126 [3] 124/-1/-1->127->126 [4] -1/-1/-1->127->126 [5] -1/-1/-1->127->126 [6] 124/-1/-1->127->126 [7] 124/-1/-1->127->126
31: nid007342:59770:61197 [3] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241019:242169 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
 7: nid006502:252586:253742 [2] NCCL INFO comm 0xaaab16820860 rank 30 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 8: nid006503:218410:219569 [0] NCCL INFO comm 0xaaaaf8554fd0 rank 32 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 8: nid006503:218411:219568 [1] NCCL INFO comm 0xaaab0c010470 rank 33 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 3: nid006498:226765:227925 [0] NCCL INFO comm 0xaaaaf9754bd0 rank 12 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
31: nid007342:59769:61195 [2] NCCL INFO Trees [0] 127/-1/-1->126->125 [1] 127/-1/-1->126->125 [2] 127/-1/-1->126->122 [3] 127/-1/-1->126->122 [4] 127/-1/-1->126->125 [5] 127/-1/-1->126->125 [6] 127/62/-1->126->-1 [7] 127/62/-1->126->-1
31: nid007342:59769:61195 [2] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241019:242169 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
 7: nid006502:252585:253743 [1] NCCL INFO comm 0xaaab03122220 rank 29 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 8: nid006503:218413:219571 [3] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] 32/-1/-1->35->34 [3] 32/-1/-1->35->34 [4] -1/-1/-1->35->34 [5] -1/-1/-1->35->34 [6] 32/-1/-1->35->34 [7] 32/-1/-1->35->34
 8: nid006503:218412:219570 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/18/50->34->66 [3] 35/18/50->34->66 [4] 35/-1/-1->34->33 [5] 35/-1/-1->34->33 [6] 35/-1/-1->34->38 [7] 35/-1/-1->34->38
 8: nid006503:218413:219571 [3] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218412:219570 [2] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20581:21731 [1] NCCL INFO comm 0xaaab01d20530 rank 121 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
30: nid007318:20583:21730 [3] NCCL INFO Trees [0] -1/-1/-1->123->122 [1] -1/-1/-1->123->122 [2] 120/-1/-1->123->122 [3] 120/-1/-1->123->122 [4] -1/-1/-1->123->122 [5] -1/-1/-1->123->122 [6] 120/-1/-1->123->122 [7] 120/-1/-1->123->122
 0: nid006495:241019:242169 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
31: nid007342:59768:61196 [1] NCCL INFO Trees [0] 126/-1/-1->125->124 [1] 126/-1/-1->125->124 [2] -1/-1/-1->125->124 [3] -1/-1/-1->125->124 [4] 126/-1/-1->125->124 [5] 126/-1/-1->125->124 [6] -1/-1/-1->125->124 [7] -1/-1/-1->125->124
31: nid007342:59768:61196 [1] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20582:21736 [2] NCCL INFO comm 0xaaab18742680 rank 122 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
30: nid007318:20583:21730 [3] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241019:242169 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
 7: nid006502:252587:253740 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 28/-1/-1->31->30 [3] 28/-1/-1->31->30 [4] -1/-1/-1->31->30 [5] -1/-1/-1->31->30 [6] 28/-1/-1->31->30 [7] 28/-1/-1->31->30
 7: nid006502:252587:253740 [3] NCCL INFO P2P Chunksize set to 131072
31: nid007342:59767:61194 [0] NCCL INFO Trees [0] 125/-1/-1->124->120 [1] 125/-1/-1->124->120 [2] 125/-1/-1->124->127 [3] 125/-1/-1->124->127 [4] 125/60/-1->124->-1 [5] 125/60/-1->124->-1 [6] 125/-1/-1->124->127 [7] 125/-1/-1->124->127
31: nid007342:59767:61194 [0] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20580:21733 [0] NCCL INFO comm 0xaaab05f55b40 rank 120 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 4: nid006499:254559:255712 [3] NCCL INFO comm 0xaaaaffcbe690 rank 19 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 0: nid006495:241019:242169 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
 0: nid006495:241019:242169 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
 0: nid006495:241019:242169 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
 7: nid006502:252586:253742 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->26 [3] 31/-1/-1->30->26 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/46/14->30->62 [7] 31/46/14->30->62
 7: nid006502:252586:253742 [2] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260086:261239 [3] NCCL INFO comm 0xaaaaf70716c0 rank 23 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 4: nid006499:254558:255715 [2] NCCL INFO comm 0xaaab14912c30 rank 18 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 4: nid006499:254557:255713 [1] NCCL INFO comm 0xaaaaf4721fc0 rank 17 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 4: nid006499:254556:255714 [0] NCCL INFO comm 0xaaaafd9c3e70 rank 16 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 0: nid006495:241019:242169 [0] NCCL INFO Trees [0] 1/64/-1->0->-1 [1] 1/64/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
 0: nid006495:241019:242169 [0] NCCL INFO P2P Chunksize set to 131072
 7: nid006502:252585:253743 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] -1/-1/-1->29->28 [3] -1/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->28 [6] -1/-1/-1->29->28 [7] -1/-1/-1->29->28
 8: nid006503:218411:219568 [1] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 34/-1/-1->33->32 [2] -1/-1/-1->33->32 [3] -1/-1/-1->33->32 [4] 34/-1/-1->33->32 [5] 34/-1/-1->33->32 [6] -1/-1/-1->33->32 [7] -1/-1/-1->33->32
 8: nid006503:218411:219568 [1] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226768:227915 [3] NCCL INFO comm 0xaaaae06b1570 rank 15 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 3: nid006498:226767:227919 [2] NCCL INFO comm 0xaaaaf026f8b0 rank 14 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 4: nid006499:254559:255712 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 16/-1/-1->19->18 [3] 16/-1/-1->19->18 [4] -1/-1/-1->19->18 [5] -1/-1/-1->19->18 [6] 16/-1/-1->19->18 [7] 16/-1/-1->19->18
 4: nid006499:254559:255712 [3] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221943:223091 [2] NCCL INFO comm 0xaaaafac13400 rank 26 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 6: nid006501:221944:223090 [3] NCCL INFO comm 0xaaab07c52630 rank 27 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 7: nid006502:252584:253741 [0] NCCL INFO comm 0xaaaafb714860 rank 28 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 7: nid006502:252585:253743 [1] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218410:219569 [0] NCCL INFO Trees [0] 33/16/48->32->64 [1] 33/16/48->32->64 [2] 33/-1/-1->32->35 [3] 33/-1/-1->32->35 [4] 33/-1/-1->32->36 [5] 33/-1/-1->32->36 [6] 33/-1/-1->32->35 [7] 33/-1/-1->32->35
 8: nid006503:218410:219569 [0] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226766:227918 [1] NCCL INFO comm 0xaaaafc6c1d20 rank 13 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
29: nid007305:27225:28342 [3] NCCL INFO comm 0xaaaafdcaec70 rank 119 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
29: nid007305:27224:28344 [2] NCCL INFO comm 0xaaab02ed3070 rank 118 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
30: nid007318:20581:21731 [1] NCCL INFO Trees [0] 122/-1/-1->121->120 [1] 122/-1/-1->121->120 [2] -1/-1/-1->121->120 [3] -1/-1/-1->121->120 [4] 122/-1/-1->121->120 [5] 122/-1/-1->121->120 [6] -1/-1/-1->121->120 [7] -1/-1/-1->121->120
30: nid007318:20581:21731 [1] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221942:223088 [1] NCCL INFO comm 0xaaaaf2b5ecd0 rank 25 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 7: nid006502:252584:253741 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/-1/-1->28->31 [3] 29/-1/-1->28->31 [4] 29/44/12->28->60 [5] 29/44/12->28->60 [6] 29/-1/-1->28->31 [7] 29/-1/-1->28->31
 7: nid006502:252584:253741 [0] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226768:227915 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
 3: nid006498:226767:227919 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/22/6->14->30 [7] 15/22/6->14->30
28: nid007251:72172:73331 [3] NCCL INFO comm 0xaaaaf6d30140 rank 115 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
29: nid007305:27223:28343 [1] NCCL INFO comm 0xaaaadd180740 rank 117 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
30: nid007318:20582:21736 [2] NCCL INFO Trees [0] 123/-1/-1->122->121 [1] 123/-1/-1->122->121 [2] 123/118/126->122->114 [3] 123/118/126->122->114 [4] 123/-1/-1->122->121 [5] 123/-1/-1->122->121 [6] 123/-1/-1->122->118 [7] 123/-1/-1->122->118
30: nid007318:20582:21736 [2] NCCL INFO P2P Chunksize set to 131072
 4: nid006499:254558:255715 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->34 [3] 19/10/26->18->34 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->22 [7] 19/-1/-1->18->22
 4: nid006499:254557:255713 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16 [2] -1/-1/-1->17->16 [3] -1/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] -1/-1/-1->17->16 [7] -1/-1/-1->17->16
 4: nid006499:254556:255714 [0] NCCL INFO Trees [0] 17/8/24->16->32 [1] 17/8/24->16->32 [2] 17/-1/-1->16->19 [3] 17/-1/-1->16->19 [4] 17/-1/-1->16->20 [5] 17/-1/-1->16->20 [6] 17/-1/-1->16->19 [7] 17/-1/-1->16->19
 5: nid006500:260085:261237 [2] NCCL INFO comm 0xaaab1f1aeb40 rank 22 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 5: nid006500:260084:261238 [1] NCCL INFO comm 0xaaaaebd115d0 rank 21 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
28: nid007251:72172:73331 [3] NCCL INFO Trees [0] -1/-1/-1->115->114 [1] -1/-1/-1->115->114 [2] 112/-1/-1->115->114 [3] 112/-1/-1->115->114 [4] -1/-1/-1->115->114 [5] -1/-1/-1->115->114 [6] 112/-1/-1->115->114 [7] 112/-1/-1->115->114
29: nid007305:27222:28341 [0] NCCL INFO comm 0xaaaae8e75560 rank 116 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
29: nid007305:27225:28342 [3] NCCL INFO Trees [0] -1/-1/-1->119->118 [1] -1/-1/-1->119->118 [2] 116/-1/-1->119->118 [3] 116/-1/-1->119->118 [4] -1/-1/-1->119->118 [5] -1/-1/-1->119->118 [6] 116/-1/-1->119->118 [7] 116/-1/-1->119->118
 4: nid006499:254558:255715 [2] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221941:223089 [0] NCCL INFO comm 0xaaab18cffb10 rank 24 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 6: nid006501:221943:223091 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/22/30->26->18 [3] 27/22/30->26->18 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->22 [7] 27/-1/-1->26->22
 5: nid006500:260083:261236 [0] NCCL INFO comm 0xaaaaddd64a90 rank 20 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 5: nid006500:260086:261239 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 20/-1/-1->23->22 [3] 20/-1/-1->23->22 [4] -1/-1/-1->23->22 [5] -1/-1/-1->23->22 [6] 20/-1/-1->23->22 [7] 20/-1/-1->23->22
 5: nid006500:260086:261239 [3] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226768:227915 [3] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226767:227919 [2] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72171:73330 [2] NCCL INFO comm 0xaaab01150380 rank 114 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
28: nid007251:72172:73331 [3] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72169:73329 [0] NCCL INFO comm 0xaaab09955090 rank 112 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
27: nid006566:216749:217883 [3] NCCL INFO comm 0xaaaaf3be1d00 rank 111 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
27: nid006566:216746:217885 [0] NCCL INFO comm 0xaaab08db5770 rank 108 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
29: nid007305:27223:28343 [1] NCCL INFO Trees [0] 118/-1/-1->117->116 [1] 118/-1/-1->117->116 [2] -1/-1/-1->117->116 [3] -1/-1/-1->117->116 [4] 118/-1/-1->117->116 [5] 118/-1/-1->117->116 [6] -1/-1/-1->117->116 [7] -1/-1/-1->117->116
29: nid007305:27224:28344 [2] NCCL INFO Trees [0] 119/-1/-1->118->117 [1] 119/-1/-1->118->117 [2] 119/-1/-1->118->122 [3] 119/-1/-1->118->122 [4] 119/-1/-1->118->117 [5] 119/-1/-1->118->117 [6] 119/122/114->118->110 [7] 119/122/114->118->110
29: nid007305:27225:28342 [3] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27223:28343 [1] NCCL INFO P2P Chunksize set to 131072
 4: nid006499:254557:255713 [1] NCCL INFO P2P Chunksize set to 131072
 4: nid006499:254556:255714 [0] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221944:223090 [3] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 24/-1/-1->27->26 [3] 24/-1/-1->27->26 [4] -1/-1/-1->27->26 [5] -1/-1/-1->27->26 [6] 24/-1/-1->27->26 [7] 24/-1/-1->27->26
 6: nid006501:221943:223091 [2] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221942:223088 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->24 [2] -1/-1/-1->25->24 [3] -1/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] -1/-1/-1->25->24 [7] -1/-1/-1->25->24
 6: nid006501:221944:223090 [3] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260085:261237 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->26 [3] 23/-1/-1->22->26 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/26/18->22->14 [7] 23/26/18->22->14
 5: nid006500:260084:261238 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] -1/-1/-1->21->20 [3] -1/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/-1/-1->21->20 [6] -1/-1/-1->21->20 [7] -1/-1/-1->21->20
 3: nid006498:226766:227918 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
28: nid007251:72170:73327 [1] NCCL INFO comm 0xaaaae3cff410 rank 113 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
27: nid006566:216748:217887 [2] NCCL INFO comm 0xaaab0ae01a50 rank 110 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
27: nid006566:216747:217884 [1] NCCL INFO comm 0xaaab06140ab0 rank 109 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
29: nid007305:27224:28344 [2] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27222:28341 [0] NCCL INFO Trees [0] 117/-1/-1->116->120 [1] 117/-1/-1->116->120 [2] 117/-1/-1->116->119 [3] 117/-1/-1->116->119 [4] 117/120/112->116->108 [5] 117/120/112->116->108 [6] 117/-1/-1->116->119 [7] 117/-1/-1->116->119
29: nid007305:27222:28341 [0] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20580:21733 [0] NCCL INFO Trees [0] 121/116/124->120->112 [1] 121/116/124->120->112 [2] 121/-1/-1->120->123 [3] 121/-1/-1->120->123 [4] 121/-1/-1->120->116 [5] 121/-1/-1->120->116 [6] 121/-1/-1->120->123 [7] 121/-1/-1->120->123
30: nid007318:20580:21733 [0] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221942:223088 [1] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260085:261237 [2] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260084:261238 [1] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260083:261236 [0] NCCL INFO Trees [0] 21/-1/-1->20->24 [1] 21/-1/-1->20->24 [2] 21/-1/-1->20->23 [3] 21/-1/-1->20->23 [4] 21/24/16->20->12 [5] 21/24/16->20->12 [6] 21/-1/-1->20->23 [7] 21/-1/-1->20->23
 5: nid006500:260083:261236 [0] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226766:227918 [1] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72169:73329 [0] NCCL INFO Trees [0] 113/104/120->112->96 [1] 113/104/120->112->96 [2] 113/-1/-1->112->115 [3] 113/-1/-1->112->115 [4] 113/-1/-1->112->116 [5] 113/-1/-1->112->116 [6] 113/-1/-1->112->115 [7] 113/-1/-1->112->115
28: nid007251:72169:73329 [0] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216746:217885 [0] NCCL INFO Trees [0] 109/-1/-1->108->104 [1] 109/-1/-1->108->104 [2] 109/-1/-1->108->111 [3] 109/-1/-1->108->111 [4] 109/116/100->108->92 [5] 109/116/100->108->92 [6] 109/-1/-1->108->111 [7] 109/-1/-1->108->111
 6: nid006501:221941:223089 [0] NCCL INFO Trees [0] 25/20/28->24->16 [1] 25/20/28->24->16 [2] 25/-1/-1->24->27 [3] 25/-1/-1->24->27 [4] 25/-1/-1->24->20 [5] 25/-1/-1->24->20 [6] 25/-1/-1->24->27 [7] 25/-1/-1->24->27
 6: nid006501:221941:223089 [0] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226765:227925 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/20/4->12->28 [5] 13/20/4->12->28 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
 3: nid006498:226765:227925 [0] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72171:73330 [2] NCCL INFO Trees [0] 115/-1/-1->114->113 [1] 115/-1/-1->114->113 [2] 115/106/122->114->98 [3] 115/106/122->114->98 [4] 115/-1/-1->114->113 [5] 115/-1/-1->114->113 [6] 115/-1/-1->114->118 [7] 115/-1/-1->114->118
28: nid007251:72170:73327 [1] NCCL INFO Trees [0] 114/-1/-1->113->112 [1] 114/-1/-1->113->112 [2] -1/-1/-1->113->112 [3] -1/-1/-1->113->112 [4] 114/-1/-1->113->112 [5] 114/-1/-1->113->112 [6] -1/-1/-1->113->112 [7] -1/-1/-1->113->112
28: nid007251:72171:73330 [2] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222465:223653 [0] NCCL INFO comm 0xaaaaf61a4fc0 rank 104 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
27: nid006566:216746:217885 [0] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216748:217887 [2] NCCL INFO Trees [0] 111/-1/-1->110->109 [1] 111/-1/-1->110->109 [2] 111/-1/-1->110->106 [3] 111/-1/-1->110->106 [4] 111/-1/-1->110->109 [5] 111/-1/-1->110->109 [6] 111/118/102->110->94 [7] 111/118/102->110->94
25: nid006564:223024:224163 [3] NCCL INFO comm 0xaaaafc951420 rank 103 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
25: nid006564:223021:224160 [0] NCCL INFO comm 0xaaaaf17f5630 rank 100 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
28: nid007251:72170:73327 [1] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222468:223651 [3] NCCL INFO comm 0xaaaacd640a00 rank 107 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
26: nid006565:222466:223654 [1] NCCL INFO comm 0xaaaadd720280 rank 105 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
26: nid006565:222467:223652 [2] NCCL INFO comm 0xaaab1e0113c0 rank 106 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
27: nid006566:216749:217883 [3] NCCL INFO Trees [0] -1/-1/-1->111->110 [1] -1/-1/-1->111->110 [2] 108/-1/-1->111->110 [3] 108/-1/-1->111->110 [4] -1/-1/-1->111->110 [5] -1/-1/-1->111->110 [6] 108/-1/-1->111->110 [7] 108/-1/-1->111->110
25: nid006564:223023:224161 [2] NCCL INFO comm 0xaaab04f72cc0 rank 102 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
26: nid006565:222465:223653 [0] NCCL INFO Trees [0] 105/100/108->104->112 [1] 105/100/108->104->112 [2] 105/-1/-1->104->107 [3] 105/-1/-1->104->107 [4] 105/-1/-1->104->100 [5] 105/-1/-1->104->100 [6] 105/-1/-1->104->107 [7] 105/-1/-1->104->107
27: nid006566:216747:217884 [1] NCCL INFO Trees [0] 110/-1/-1->109->108 [1] 110/-1/-1->109->108 [2] -1/-1/-1->109->108 [3] -1/-1/-1->109->108 [4] 110/-1/-1->109->108 [5] 110/-1/-1->109->108 [6] -1/-1/-1->109->108 [7] -1/-1/-1->109->108
27: nid006566:216748:217887 [2] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216747:217884 [1] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223022:224162 [1] NCCL INFO comm 0xaaaadab71590 rank 101 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
26: nid006565:222468:223651 [3] NCCL INFO Trees [0] -1/-1/-1->107->106 [1] -1/-1/-1->107->106 [2] 104/-1/-1->107->106 [3] 104/-1/-1->107->106 [4] -1/-1/-1->107->106 [5] -1/-1/-1->107->106 [6] 104/-1/-1->107->106 [7] 104/-1/-1->107->106
26: nid006565:222465:223653 [0] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222468:223651 [3] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216749:217883 [3] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223024:224163 [3] NCCL INFO Trees [0] -1/-1/-1->103->102 [1] -1/-1/-1->103->102 [2] 100/-1/-1->103->102 [3] 100/-1/-1->103->102 [4] -1/-1/-1->103->102 [5] -1/-1/-1->103->102 [6] 100/-1/-1->103->102 [7] 100/-1/-1->103->102
25: nid006564:223024:224163 [3] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222466:223654 [1] NCCL INFO Trees [0] 106/-1/-1->105->104 [1] 106/-1/-1->105->104 [2] -1/-1/-1->105->104 [3] -1/-1/-1->105->104 [4] 106/-1/-1->105->104 [5] 106/-1/-1->105->104 [6] -1/-1/-1->105->104 [7] -1/-1/-1->105->104
26: nid006565:222466:223654 [1] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222467:223652 [2] NCCL INFO Trees [0] 107/-1/-1->106->105 [1] 107/-1/-1->106->105 [2] 107/102/110->106->114 [3] 107/102/110->106->114 [4] 107/-1/-1->106->105 [5] 107/-1/-1->106->105 [6] 107/-1/-1->106->102 [7] 107/-1/-1->106->102
25: nid006564:223023:224161 [2] NCCL INFO Trees [0] 103/-1/-1->102->101 [1] 103/-1/-1->102->101 [2] 103/-1/-1->102->106 [3] 103/-1/-1->102->106 [4] 103/-1/-1->102->101 [5] 103/-1/-1->102->101 [6] 103/106/98->102->110 [7] 103/106/98->102->110
25: nid006564:223022:224162 [1] NCCL INFO Trees [0] 102/-1/-1->101->100 [1] 102/-1/-1->101->100 [2] -1/-1/-1->101->100 [3] -1/-1/-1->101->100 [4] 102/-1/-1->101->100 [5] 102/-1/-1->101->100 [6] -1/-1/-1->101->100 [7] -1/-1/-1->101->100
25: nid006564:223023:224161 [2] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222467:223652 [2] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223022:224162 [1] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223021:224160 [0] NCCL INFO Trees [0] 101/-1/-1->100->104 [1] 101/-1/-1->100->104 [2] 101/-1/-1->100->103 [3] 101/-1/-1->100->103 [4] 101/104/96->100->108 [5] 101/104/96->100->108 [6] 101/-1/-1->100->103 [7] 101/-1/-1->100->103
25: nid006564:223021:224160 [0] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211126:212278 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211126:212278 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215467:216641 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215467:216641 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229445:230609 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229445:230609 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 2: nid006497:227575:228688 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227575:228688 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208420:209604 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208420:209604 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211123:212275 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211123:212275 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221584:222757 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221584:222757 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215468:216644 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215468:216644 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72169:73329 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72169:73329 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210775:211994 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210775:211994 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223926:225068 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223926:225068 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205766:206941 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205766:206941 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229444:230608 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229444:230608 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218412:219570 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218412:219570 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20583:21730 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20583:21730 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223925:225066 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223925:225066 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211125:212276 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211125:212276 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220712:221898 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220712:221898 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206594:207777 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206594:207777 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215469:216642 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215469:216642 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201791:202940 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201791:202940 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206596:207778 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206596:207778 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222274:223424 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222274:223424 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242587:243774 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242587:243774 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210777:211997 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210777:211997 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201790:202941 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201790:202941 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
10: nid006506:263730:264885 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263730:264885 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211124:212277 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211124:212277 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208418:209608 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208418:209608 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211338:212472 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211338:212472 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221582:222761 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221582:222761 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215470:216643 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215470:216643 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249083:250268 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249083:250268 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210774:211995 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210774:211995 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
10: nid006506:263727:264880 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263727:264880 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205769:206942 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205769:206942 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206595:207800 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206595:207800 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221141:222303 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221141:222303 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208419:209607 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208419:209607 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252585:253743 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252585:253743 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229443:230607 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229443:230607 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201789:202939 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201789:202939 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254557:255713 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254557:255713 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229442:230610 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229442:230610 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260084:261238 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260084:261238 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211337:212473 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211337:212473 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206593:207804 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206593:207804 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205768:206939 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205768:206939 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254556:255714 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254556:255714 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210776:211996 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210776:211996 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249080:250267 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249080:250267 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221583:222762 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221583:222762 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221142:222307 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221142:222307 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226767:227919 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226767:227919 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221942:223088 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221942:223088 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 0: nid006495:241019:242169 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241019:242169 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20582:21736 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20582:21736 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223022:224162 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223022:224162 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59767:61194 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59767:61194 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223924:225067 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223924:225067 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242586:243772 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242586:243772 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221581:222760 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221581:222760 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252584:253741 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252584:253741 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216746:217885 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216746:217885 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220714:221896 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220714:221896 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208417:209611 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208417:209611 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205767:206940 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205767:206940 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27222:28341 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27222:28341 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222465:223653 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222465:223653 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27224:28344 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27224:28344 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72172:73331 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72172:73331 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223923:225069 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223923:225069 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242585:243775 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242585:243775 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 2: nid006497:227576:228686 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227576:228686 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223024:224163 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223024:224163 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221143:222304 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221143:222304 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59770:61197 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59770:61197 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 2: nid006497:227577:228687 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227577:228687 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
10: nid006506:263729:264886 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263729:264886 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211336:212474 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211336:212474 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20580:21733 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20580:21733 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216747:217884 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216747:217884 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221943:223091 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221943:223091 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242584:243773 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242584:243773 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201788:202938 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201788:202938 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20581:21731 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20581:21731 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220711:221897 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220711:221897 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222272:223426 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222272:223426 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220713:221895 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220713:221895 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72171:73330 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72171:73330 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249082:250266 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249082:250266 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 0: nid006495:241021:242171 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241021:242171 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 0: nid006495:241019:242169 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
10: nid006506:263728:264887 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263728:264887 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221140:222302 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221140:222302 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216748:217887 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216748:217887 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59768:61196 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59768:61196 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218413:219571 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218413:219571 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254558:255715 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254558:255715 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222273:223427 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222273:223427 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211335:212475 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211335:212475 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226768:227915 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226768:227915 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221941:223089 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221941:223089 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218411:219568 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218411:219568 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260085:261237 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260085:261237 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249081:250246 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249081:250246 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218410:219569 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218410:219569 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222466:223654 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222466:223654 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27223:28343 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27223:28343 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226766:227918 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226766:227918 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222271:223425 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222271:223425 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 2: nid006497:227578:228689 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227578:228689 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 0: nid006495:241020:242170 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241020:242170 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254559:255712 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254559:255712 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223023:224161 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223023:224161 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260083:261236 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260083:261236 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72170:73327 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72170:73327 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221944:223090 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221944:223090 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226765:227925 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226765:227925 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222468:223651 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222468:223651 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252586:253742 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252586:253742 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59769:61195 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59769:61195 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252587:253740 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252587:253740 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216749:217883 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216749:217883 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27225:28342 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27225:28342 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 0: nid006495:241022:242172 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241022:242172 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223021:224160 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223021:224160 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260086:261239 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260086:261239 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222467:223652 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222467:223652 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211126:212278 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211126:212278 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
21: nid006559:211126:212278 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
21: nid006559:211126:212278 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf4030670 rank 87 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
21: nid006559:211126:212278 [3] NCCL INFO Init timings: rank 87 nranks 128 total 11.86 (kernels 0.11, bootstrap 11.37, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215467:216641 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215467:216641 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
20: nid006558:215467:216641 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
21: nid006559:211123:212275 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211123:212275 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
21: nid006559:211123:212275 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
20: nid006558:215467:216641 [0] NCCL INFO ncclCommInitRank comm 0xaaab024c49d0 rank 80 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
20: nid006558:215467:216641 [0] NCCL INFO Init timings: rank 80 nranks 128 total 12.12 (kernels 0.50, bootstrap 11.23, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211123:212275 [0] NCCL INFO ncclCommInitRank comm 0xaaaacd464770 rank 84 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
21: nid006559:211123:212275 [0] NCCL INFO Init timings: rank 84 nranks 128 total 12.11 (kernels 0.35, bootstrap 11.37, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211125:212276 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211125:212276 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
21: nid006559:211125:212276 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
21: nid006559:211124:212277 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
21: nid006559:211125:212276 [2] NCCL INFO ncclCommInitRank comm 0xaaaad6db0e40 rank 86 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
21: nid006559:211124:212277 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
21: nid006559:211124:212277 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
21: nid006559:211125:212276 [2] NCCL INFO Init timings: rank 86 nranks 128 total 12.11 (kernels 0.35, bootstrap 11.37, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211124:212277 [1] NCCL INFO ncclCommInitRank comm 0xaaaae87e0f50 rank 85 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
21: nid006559:211124:212277 [1] NCCL INFO Init timings: rank 85 nranks 128 total 12.06 (kernels 0.31, bootstrap 11.37, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215469:216642 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215469:216642 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
20: nid006558:215469:216642 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
20: nid006558:215469:216642 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf4921880 rank 82 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
20: nid006558:215468:216644 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215469:216642 [2] NCCL INFO Init timings: rank 82 nranks 128 total 12.08 (kernels 0.46, bootstrap 11.23, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215468:216644 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
20: nid006558:215468:216644 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
20: nid006558:215468:216644 [1] NCCL INFO ncclCommInitRank comm 0xaaaae4530560 rank 81 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
20: nid006558:215470:216643 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
20: nid006558:215468:216644 [1] NCCL INFO Init timings: rank 81 nranks 128 total 11.99 (kernels 0.38, bootstrap 11.22, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215470:216643 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
20: nid006558:215470:216643 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
20: nid006558:215470:216643 [3] NCCL INFO ncclCommInitRank comm 0xaaab04942a10 rank 83 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
20: nid006558:215470:216643 [3] NCCL INFO Init timings: rank 83 nranks 128 total 12.03 (kernels 0.42, bootstrap 11.22, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229445:230609 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229445:230609 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
14: nid006510:229445:230609 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
14: nid006510:229445:230609 [3] NCCL INFO ncclCommInitRank comm 0xaaaad4ac1400 rank 59 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
14: nid006510:229445:230609 [3] NCCL INFO Init timings: rank 59 nranks 128 total 12.01 (kernels 0.29, bootstrap 11.33, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210775:211994 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210775:211994 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
18: nid006556:210775:211994 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
18: nid006556:210775:211994 [1] NCCL INFO ncclCommInitRank comm 0xaaab1c4215c0 rank 73 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
18: nid006556:210775:211994 [1] NCCL INFO Init timings: rank 73 nranks 128 total 12.10 (kernels 0.45, bootstrap 11.27, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229443:230607 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229443:230607 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
14: nid006510:229443:230607 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
14: nid006510:229443:230607 [1] NCCL INFO ncclCommInitRank comm 0xaaab02930260 rank 57 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
14: nid006510:229443:230607 [1] NCCL INFO Init timings: rank 57 nranks 128 total 12.11 (kernels 0.39, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210777:211997 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210777:211997 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
18: nid006556:210777:211997 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
18: nid006556:210777:211997 [3] NCCL INFO ncclCommInitRank comm 0xaaab001d2d70 rank 75 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
18: nid006556:210777:211997 [3] NCCL INFO Init timings: rank 75 nranks 128 total 12.00 (kernels 0.35, bootstrap 11.26, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210774:211995 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229444:230608 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210774:211995 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
18: nid006556:210774:211995 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
18: nid006556:210774:211995 [0] NCCL INFO ncclCommInitRank comm 0xaaab00fe4b80 rank 72 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
14: nid006510:229444:230608 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
14: nid006510:229444:230608 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
18: nid006556:210774:211995 [0] NCCL INFO Init timings: rank 72 nranks 128 total 12.10 (kernels 0.45, bootstrap 11.27, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229444:230608 [2] NCCL INFO ncclCommInitRank comm 0xaaab050af830 rank 58 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
14: nid006510:229444:230608 [2] NCCL INFO Init timings: rank 58 nranks 128 total 12.06 (kernels 0.34, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210776:211996 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
18: nid006556:210776:211996 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
18: nid006556:210776:211996 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
14: nid006510:229442:230610 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
14: nid006510:229442:230610 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
14: nid006510:229442:230610 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
14: nid006510:229442:230610 [0] NCCL INFO ncclCommInitRank comm 0xaaaadd8b3cb0 rank 56 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
18: nid006556:210776:211996 [2] NCCL INFO ncclCommInitRank comm 0xaaab24c608a0 rank 74 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
14: nid006510:229442:230610 [0] NCCL INFO Init timings: rank 56 nranks 128 total 11.97 (kernels 0.24, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210776:211996 [2] NCCL INFO Init timings: rank 74 nranks 128 total 12.06 (kernels 0.41, bootstrap 11.27, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206594:207777 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206594:207777 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
17: nid006555:206594:207777 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
17: nid006555:206594:207777 [1] NCCL INFO ncclCommInitRank comm 0xaaab0a7e34b0 rank 69 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
17: nid006555:206594:207777 [1] NCCL INFO Init timings: rank 69 nranks 128 total 12.12 (kernels 0.39, bootstrap 11.34, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206595:207800 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206595:207800 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
17: nid006555:206595:207800 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
17: nid006555:206595:207800 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf5a54290 rank 70 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
17: nid006555:206595:207800 [2] NCCL INFO Init timings: rank 70 nranks 128 total 11.83 (kernels 0.85, bootstrap 10.60, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223923:225069 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206593:207804 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206596:207778 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
17: nid006555:206593:207804 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
15: nid006553:223923:225069 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
15: nid006553:223923:225069 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
15: nid006553:223923:225069 [0] NCCL INFO ncclCommInitRank comm 0xaaab008f4e30 rank 60 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
17: nid006555:206596:207778 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
17: nid006555:206593:207804 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
17: nid006555:206596:207778 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
17: nid006555:206593:207804 [0] NCCL INFO ncclCommInitRank comm 0xaaab06a54c60 rank 68 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
15: nid006553:223926:225068 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223923:225069 [0] NCCL INFO Init timings: rank 60 nranks 128 total 12.02 (kernels 0.39, bootstrap 11.24, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206596:207778 [3] NCCL INFO ncclCommInitRank comm 0xaaaafe0d1bf0 rank 71 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
17: nid006555:206593:207804 [0] NCCL INFO Init timings: rank 68 nranks 128 total 10.12 (kernels 0.06, bootstrap 9.67, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223926:225068 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
15: nid006553:223926:225068 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
15: nid006553:223926:225068 [3] NCCL INFO ncclCommInitRank comm 0xaaab186224e0 rank 63 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
15: nid006553:223926:225068 [3] NCCL INFO Init timings: rank 63 nranks 128 total 12.08 (kernels 0.45, bootstrap 11.24, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206596:207778 [3] NCCL INFO Init timings: rank 71 nranks 128 total 12.12 (kernels 0.39, bootstrap 11.34, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 1: nid006496:242585:243775 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223924:225067 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
15: nid006553:223925:225066 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242585:243775 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 1: nid006496:242585:243775 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 1: nid006496:242585:243775 [1] NCCL INFO ncclCommInitRank comm 0xaaab017914b0 rank 5 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
15: nid006553:223924:225067 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 1: nid006496:242585:243775 [1] NCCL INFO Init timings: rank 5 nranks 128 total 11.87 (kernels 0.27, bootstrap 11.21, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223925:225066 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
15: nid006553:223924:225067 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
15: nid006553:223925:225066 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
15: nid006553:223924:225067 [1] NCCL INFO ncclCommInitRank comm 0xaaaae1630990 rank 61 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
15: nid006553:223925:225066 [2] NCCL INFO ncclCommInitRank comm 0xaaab1b0b02b0 rank 62 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
15: nid006553:223924:225067 [1] NCCL INFO Init timings: rank 61 nranks 128 total 12.12 (kernels 0.48, bootstrap 11.24, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223925:225066 [2] NCCL INFO Init timings: rank 62 nranks 128 total 12.12 (kernels 0.48, bootstrap 11.24, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 1: nid006496:242587:243774 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242587:243774 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 1: nid006496:242587:243774 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 1: nid006496:242586:243772 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242587:243774 [3] NCCL INFO ncclCommInitRank comm 0xaaaad33e0d00 rank 7 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 1: nid006496:242586:243772 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 1: nid006496:242586:243772 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
13: nid006509:201789:202939 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201789:202939 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
13: nid006509:201789:202939 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 1: nid006496:242587:243774 [3] NCCL INFO Init timings: rank 7 nranks 128 total 11.87 (kernels 0.26, bootstrap 11.22, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 1: nid006496:242586:243772 [2] NCCL INFO ncclCommInitRank comm 0xaaaae9db1a00 rank 6 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
13: nid006509:201789:202939 [1] NCCL INFO ncclCommInitRank comm 0xaaab23f6f620 rank 53 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
13: nid006509:201789:202939 [1] NCCL INFO Init timings: rank 53 nranks 128 total 12.07 (kernels 0.37, bootstrap 11.31, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 1: nid006496:242586:243772 [2] NCCL INFO Init timings: rank 6 nranks 128 total 12.11 (kernels 0.51, bootstrap 11.22, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 1: nid006496:242584:243773 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 1: nid006496:242584:243773 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 1: nid006496:242584:243773 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 1: nid006496:242584:243773 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa2855c0 rank 4 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 1: nid006496:242584:243773 [0] NCCL INFO Init timings: rank 4 nranks 128 total 12.06 (kernels 0.46, bootstrap 11.22, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221141:222303 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72169:73329 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221141:222303 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
24: nid006563:221141:222303 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
24: nid006563:221141:222303 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf69b0d70 rank 97 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221141:222303 [1] NCCL INFO Init timings: rank 97 nranks 128 total 12.07 (kernels 0.39, bootstrap 11.30, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72169:73329 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
28: nid007251:72169:73329 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 4: nid006499:254557:255713 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201790:202941 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201790:202941 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
13: nid006509:201790:202941 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
28: nid007251:72169:73329 [0] NCCL INFO ncclCommInitRank comm 0xaaab09955090 rank 112 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 4: nid006499:254557:255713 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 4: nid006499:254557:255713 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
13: nid006509:201790:202941 [2] NCCL INFO ncclCommInitRank comm 0xaaaac07c29c0 rank 54 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
28: nid007251:72169:73329 [0] NCCL INFO Init timings: rank 112 nranks 128 total 12.11 (kernels 0.23, bootstrap 11.49, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254557:255713 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf4721fc0 rank 17 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 4: nid006499:254557:255713 [1] NCCL INFO Init timings: rank 17 nranks 128 total 12.06 (kernels 0.45, bootstrap 11.22, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201791:202940 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201790:202941 [2] NCCL INFO Init timings: rank 54 nranks 128 total 11.91 (kernels 0.21, bootstrap 11.31, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201791:202940 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
13: nid006509:201791:202940 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
13: nid006509:201791:202940 [3] NCCL INFO ncclCommInitRank comm 0xaaaafc8b2560 rank 55 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
13: nid006509:201791:202940 [3] NCCL INFO Init timings: rank 55 nranks 128 total 11.91 (kernels 0.21, bootstrap 11.31, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221143:222304 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221143:222304 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
24: nid006563:221143:222304 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
13: nid006509:201788:202938 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
13: nid006509:201788:202938 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
13: nid006509:201788:202938 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
13: nid006509:201788:202938 [0] NCCL INFO ncclCommInitRank comm 0xaaab17976000 rank 52 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221143:222304 [3] NCCL INFO ncclCommInitRank comm 0xaaab126b1510 rank 99 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221143:222304 [3] NCCL INFO Init timings: rank 99 nranks 128 total 12.02 (kernels 0.34, bootstrap 11.29, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201788:202938 [0] NCCL INFO Init timings: rank 52 nranks 128 total 12.11 (kernels 0.42, bootstrap 11.31, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254556:255714 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254556:255714 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 4: nid006499:254556:255714 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 4: nid006499:254556:255714 [0] NCCL INFO ncclCommInitRank comm 0xaaaafd9c3e70 rank 16 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221140:222302 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254556:255714 [0] NCCL INFO Init timings: rank 16 nranks 128 total 12.00 (kernels 0.39, bootstrap 11.24, allgathers 0.03, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254558:255715 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221142:222307 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221140:222302 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
24: nid006563:221142:222307 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221584:222757 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221140:222302 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
24: nid006563:221142:222307 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
16: nid006554:221584:222757 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221584:222757 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 3: nid006498:226767:227919 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254558:255715 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 4: nid006499:254559:255712 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254558:255715 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
24: nid006563:221140:222302 [0] NCCL INFO ncclCommInitRank comm 0xaaaafb2f59b0 rank 96 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221142:222307 [2] NCCL INFO ncclCommInitRank comm 0xaaab146907e0 rank 98 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
23: nid006561:220712:221898 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 4: nid006499:254559:255712 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221584:222757 [3] NCCL INFO ncclCommInitRank comm 0xaaaafdce2b10 rank 67 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
24: nid006563:221140:222302 [0] NCCL INFO Init timings: rank 96 nranks 128 total 12.11 (kernels 0.44, bootstrap 11.29, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220712:221898 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
23: nid006561:220712:221898 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 3: nid006498:226767:227919 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221584:222757 [3] NCCL INFO Init timings: rank 67 nranks 128 total 12.13 (kernels 0.39, bootstrap 11.35, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221581:222760 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
24: nid006563:221142:222307 [2] NCCL INFO Init timings: rank 98 nranks 128 total 11.89 (kernels 0.21, bootstrap 11.29, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220712:221898 [1] NCCL INFO ncclCommInitRank comm 0xaaaafae71f80 rank 93 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226767:227919 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 4: nid006499:254558:255715 [2] NCCL INFO ncclCommInitRank comm 0xaaab14912c30 rank 18 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 4: nid006499:254559:255712 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
16: nid006554:221581:222760 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221581:222760 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
16: nid006554:221581:222760 [0] NCCL INFO ncclCommInitRank comm 0xaaaae2836550 rank 64 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
16: nid006554:221581:222760 [0] NCCL INFO Init timings: rank 64 nranks 128 total 11.80 (kernels 0.07, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220712:221898 [1] NCCL INFO Init timings: rank 93 nranks 128 total 11.86 (kernels 0.19, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226767:227919 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf026f8b0 rank 14 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 4: nid006499:254558:255715 [2] NCCL INFO Init timings: rank 18 nranks 128 total 11.85 (kernels 0.25, bootstrap 11.21, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254559:255712 [3] NCCL INFO ncclCommInitRank comm 0xaaaaffcbe690 rank 19 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
23: nid006561:220713:221895 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220713:221895 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
23: nid006561:220713:221895 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 3: nid006498:226767:227919 [2] NCCL INFO Init timings: rank 14 nranks 128 total 2.78 (kernels 0.62, bootstrap 1.79, allgathers 0.02, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254559:255712 [3] NCCL INFO Init timings: rank 19 nranks 128 total 12.11 (kernels 0.50, bootstrap 11.22, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220713:221895 [2] NCCL INFO ncclCommInitRank comm 0xaaab19e1fc40 rank 94 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
23: nid006561:220713:221895 [2] NCCL INFO Init timings: rank 94 nranks 128 total 12.11 (kernels 0.44, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72170:73327 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218412:219570 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221583:222762 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221583:222762 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221583:222762 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 8: nid006503:218412:219570 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 8: nid006503:218412:219570 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
16: nid006554:221583:222762 [2] NCCL INFO ncclCommInitRank comm 0xaaaadd7bfd30 rank 66 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
31: nid007342:59767:61194 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221583:222762 [2] NCCL INFO Init timings: rank 66 nranks 128 total 11.76 (kernels 1.07, bootstrap 10.30, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223022:224162 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220714:221896 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72171:73330 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
28: nid007251:72170:73327 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
28: nid007251:72172:73331 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218412:219570 [2] NCCL INFO ncclCommInitRank comm 0xaaaadb064120 rank 34 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
16: nid006554:221582:222761 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223022:224162 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
23: nid006561:220714:221896 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
23: nid006561:220714:221896 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
28: nid007251:72171:73330 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
28: nid007251:72170:73327 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
31: nid007342:59767:61194 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
31: nid007342:59767:61194 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 8: nid006503:218412:219570 [2] NCCL INFO Init timings: rank 34 nranks 128 total 12.01 (kernels 0.33, bootstrap 11.28, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221582:222761 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
16: nid006554:221582:222761 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
16: nid006554:221582:222761 [1] NCCL INFO ncclCommInitRank comm 0xaaaad7c30460 rank 65 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
25: nid006564:223022:224162 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
25: nid006564:223022:224162 [1] NCCL INFO ncclCommInitRank comm 0xaaaadab71590 rank 101 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
23: nid006561:220714:221896 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf7d8fec0 rank 95 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
28: nid007251:72172:73331 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
28: nid007251:72171:73330 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
31: nid007342:59767:61194 [0] NCCL INFO ncclCommInitRank comm 0xaaab02e43cb0 rank 124 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
31: nid007342:59767:61194 [0] NCCL INFO Init timings: rank 124 nranks 128 total 12.12 (kernels 0.39, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205766:206941 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
16: nid006554:221582:222761 [1] NCCL INFO Init timings: rank 65 nranks 128 total 11.76 (kernels 1.07, bootstrap 10.30, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223022:224162 [1] NCCL INFO Init timings: rank 101 nranks 128 total 12.02 (kernels 0.35, bootstrap 11.29, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220711:221897 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220714:221896 [3] NCCL INFO Init timings: rank 95 nranks 128 total 12.06 (kernels 0.40, bootstrap 11.28, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220711:221897 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
23: nid006561:220711:221897 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
28: nid007251:72170:73327 [1] NCCL INFO ncclCommInitRank comm 0xaaaae3cff410 rank 113 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
28: nid007251:72171:73330 [2] NCCL INFO ncclCommInitRank comm 0xaaab01150380 rank 114 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
28: nid007251:72172:73331 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205766:206941 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
12: nid006508:205766:206941 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 8: nid006503:218410:219569 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
23: nid006561:220711:221897 [0] NCCL INFO ncclCommInitRank comm 0xaaab1f303c10 rank 92 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226768:227915 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226768:227915 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
28: nid007251:72171:73330 [2] NCCL INFO Init timings: rank 114 nranks 128 total 12.11 (kernels 0.24, bootstrap 11.48, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249080:250267 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
12: nid006508:205766:206941 [0] NCCL INFO ncclCommInitRank comm 0xaaab01b649c0 rank 48 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 6: nid006501:221942:223088 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218410:219569 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 8: nid006503:218410:219569 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
25: nid006564:223024:224163 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223024:224163 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
25: nid006564:223024:224163 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
23: nid006561:220711:221897 [0] NCCL INFO Init timings: rank 92 nranks 128 total 12.00 (kernels 0.33, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226768:227915 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
28: nid007251:72172:73331 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf6d30140 rank 115 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
28: nid007251:72170:73327 [1] NCCL INFO Init timings: rank 113 nranks 128 total 12.11 (kernels 0.24, bootstrap 11.48, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249081:250246 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249080:250267 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 9: nid006505:249080:250267 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 9: nid006505:249081:250246 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
12: nid006508:205766:206941 [0] NCCL INFO Init timings: rank 48 nranks 128 total 12.06 (kernels 0.44, bootstrap 11.23, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221942:223088 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 6: nid006501:221942:223088 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 8: nid006503:218410:219569 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8554fd0 rank 32 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 8: nid006503:218410:219569 [0] NCCL INFO Init timings: rank 32 nranks 128 total 12.06 (kernels 0.39, bootstrap 11.28, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223024:224163 [3] NCCL INFO ncclCommInitRank comm 0xaaaafc951420 rank 103 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226768:227915 [3] NCCL INFO ncclCommInitRank comm 0xaaaae06b1570 rank 15 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226768:227915 [3] NCCL INFO Init timings: rank 15 nranks 128 total 2.90 (kernels 0.14, bootstrap 2.38, allgathers 0.02, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72172:73331 [3] NCCL INFO Init timings: rank 115 nranks 128 total 12.11 (kernels 0.24, bootstrap 11.48, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249080:250267 [0] NCCL INFO ncclCommInitRank comm 0xaaaac2b051b0 rank 36 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
12: nid006508:205769:206942 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221942:223088 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf2b5ecd0 rank 25 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 8: nid006503:218411:219568 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223024:224163 [3] NCCL INFO Init timings: rank 103 nranks 128 total 11.92 (kernels 0.25, bootstrap 11.29, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226766:227918 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226766:227918 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 3: nid006498:226766:227918 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
31: nid007342:59768:61196 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20582:21736 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249081:250246 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205769:206942 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
12: nid006508:205769:206942 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 6: nid006501:221942:223088 [1] NCCL INFO Init timings: rank 25 nranks 128 total 12.11 (kernels 0.38, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 8: nid006503:218413:219571 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218411:219568 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
25: nid006564:223023:224161 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226766:227918 [1] NCCL INFO ncclCommInitRank comm 0xaaaafc6c1d20 rank 13 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226766:227918 [1] NCCL INFO Init timings: rank 13 nranks 128 total 2.78 (kernels 0.62, bootstrap 1.78, allgathers 0.02, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59768:61196 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
31: nid007342:59768:61196 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20582:21736 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
30: nid007318:20582:21736 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 9: nid006505:249080:250267 [0] NCCL INFO Init timings: rank 36 nranks 128 total 12.03 (kernels 0.30, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249081:250246 [1] NCCL INFO ncclCommInitRank comm 0xaaab0bfe2b70 rank 37 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
12: nid006508:205769:206942 [3] NCCL INFO ncclCommInitRank comm 0xaaaafffd57b0 rank 51 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 8: nid006503:218413:219571 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 8: nid006503:218411:219568 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 8: nid006503:218413:219571 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 3: nid006498:226765:227925 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 3: nid006498:226765:227925 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 3: nid006498:226765:227925 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
19: nid006557:208418:209608 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208418:209608 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
19: nid006557:208418:209608 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
31: nid007342:59768:61196 [1] NCCL INFO ncclCommInitRank comm 0xaaaaebad4d50 rank 125 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
30: nid007318:20582:21736 [2] NCCL INFO ncclCommInitRank comm 0xaaab18742680 rank 122 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 9: nid006505:249081:250246 [1] NCCL INFO Init timings: rank 37 nranks 128 total 12.12 (kernels 0.39, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205767:206940 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218411:219568 [1] NCCL INFO ncclCommInitRank comm 0xaaab0c010470 rank 33 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 8: nid006503:218413:219571 [3] NCCL INFO ncclCommInitRank comm 0xaaaad50f1540 rank 35 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
25: nid006564:223021:224160 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
25: nid006564:223023:224161 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
25: nid006564:223021:224160 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
25: nid006564:223023:224161 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 3: nid006498:226765:227925 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf9754bd0 rank 12 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 3: nid006498:226765:227925 [0] NCCL INFO Init timings: rank 12 nranks 128 total 1.05 (kernels 0.06, bootstrap 0.60, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208417:209611 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222465:223653 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59768:61196 [1] NCCL INFO Init timings: rank 125 nranks 128 total 12.01 (kernels 0.29, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
30: nid007318:20582:21736 [2] NCCL INFO Init timings: rank 122 nranks 128 total 11.62 (kernels 0.07, bootstrap 11.17, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249083:250268 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249083:250268 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 9: nid006505:249083:250268 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205769:206942 [3] NCCL INFO Init timings: rank 51 nranks 128 total 12.06 (kernels 0.44, bootstrap 11.23, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205767:206940 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
12: nid006508:205767:206940 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205768:206939 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252585:253743 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 8: nid006503:218411:219568 [1] NCCL INFO Init timings: rank 33 nranks 128 total 12.12 (kernels 0.44, bootstrap 11.28, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 8: nid006503:218413:219571 [3] NCCL INFO Init timings: rank 35 nranks 128 total 11.91 (kernels 0.24, bootstrap 11.28, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223021:224160 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
19: nid006557:208418:209608 [1] NCCL INFO ncclCommInitRank comm 0xaaab2b67f850 rank 77 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
26: nid006565:222465:223653 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
26: nid006565:222465:223653 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
31: nid007342:59769:61195 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59769:61195 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
31: nid007342:59769:61195 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 9: nid006505:249082:250266 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 9: nid006505:249083:250268 [3] NCCL INFO ncclCommInitRank comm 0xaaab25b11310 rank 39 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
12: nid006508:205767:206940 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf9abf4e0 rank 49 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
12: nid006508:205768:206939 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
12: nid006508:205767:206940 [1] NCCL INFO Init timings: rank 49 nranks 128 total 12.12 (kernels 0.50, bootstrap 11.23, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221941:223089 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221941:223089 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 6: nid006501:221941:223089 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252585:253743 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 7: nid006502:252585:253743 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
25: nid006564:223023:224161 [2] NCCL INFO ncclCommInitRank comm 0xaaab04f72cc0 rank 102 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
25: nid006564:223021:224160 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf17f5630 rank 100 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
19: nid006557:208417:209611 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
26: nid006565:222465:223653 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf61a4fc0 rank 104 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
31: nid007342:59770:61197 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
31: nid007342:59769:61195 [2] NCCL INFO ncclCommInitRank comm 0xaaaacd751c60 rank 126 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 9: nid006505:249082:250266 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 9: nid006505:249083:250268 [3] NCCL INFO Init timings: rank 39 nranks 128 total 11.95 (kernels 0.22, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205768:206939 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205768:206939 [2] NCCL INFO ncclCommInitRank comm 0xaaaae60828b0 rank 50 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 6: nid006501:221941:223089 [0] NCCL INFO ncclCommInitRank comm 0xaaab18cffb10 rank 24 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 6: nid006501:221941:223089 [0] NCCL INFO Init timings: rank 24 nranks 128 total 12.07 (kernels 0.33, bootstrap 11.35, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252585:253743 [1] NCCL INFO ncclCommInitRank comm 0xaaab03122220 rank 29 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
25: nid006564:223023:224161 [2] NCCL INFO Init timings: rank 102 nranks 128 total 12.08 (kernels 0.40, bootstrap 11.29, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223021:224160 [0] NCCL INFO Init timings: rank 100 nranks 128 total 12.12 (kernels 0.44, bootstrap 11.29, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208418:209608 [1] NCCL INFO Init timings: rank 77 nranks 128 total 2.67 (kernels 1.08, bootstrap 1.20, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208417:209611 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
26: nid006565:222465:223653 [0] NCCL INFO Init timings: rank 104 nranks 128 total 12.00 (kernels 0.34, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59770:61197 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 9: nid006505:249082:250266 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
12: nid006508:205768:206939 [2] NCCL INFO Init timings: rank 50 nranks 128 total 12.12 (kernels 0.50, bootstrap 11.22, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221943:223091 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252585:253743 [1] NCCL INFO Init timings: rank 29 nranks 128 total 11.95 (kernels 0.21, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208417:209611 [0] NCCL INFO ncclCommInitRank comm 0xaaaafeab5cd0 rank 76 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
31: nid007342:59769:61195 [2] NCCL INFO Init timings: rank 126 nranks 128 total 12.07 (kernels 0.35, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59770:61197 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20580:21733 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20580:21733 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 6: nid006501:221944:223090 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221943:223091 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 6: nid006501:221944:223090 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
19: nid006557:208417:209611 [0] NCCL INFO Init timings: rank 76 nranks 128 total 1.60 (kernels 0.57, bootstrap 0.65, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59770:61197 [3] NCCL INFO ncclCommInitRank comm 0xaaaae96a06d0 rank 127 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
31: nid007342:59770:61197 [3] NCCL INFO Init timings: rank 127 nranks 128 total 11.93 (kernels 0.21, bootstrap 11.34, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27224:28344 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20583:21730 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
30: nid007318:20580:21733 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20583:21730 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 9: nid006505:249082:250266 [2] NCCL INFO ncclCommInitRank comm 0xaaaaddaa10c0 rank 38 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 9: nid006505:249082:250266 [2] NCCL INFO Init timings: rank 38 nranks 128 total 12.08 (kernels 0.35, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221943:223091 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252586:253742 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208420:209604 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208420:209604 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27224:28344 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27224:28344 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20580:21733 [0] NCCL INFO ncclCommInitRank comm 0xaaab05f55b40 rank 120 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
30: nid007318:20583:21730 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20581:21731 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 6: nid006501:221944:223090 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 6: nid006501:221943:223091 [2] NCCL INFO ncclCommInitRank comm 0xaaaafac13400 rank 26 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 6: nid006501:221944:223090 [3] NCCL INFO ncclCommInitRank comm 0xaaab07c52630 rank 27 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 7: nid006502:252586:253742 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 7: nid006502:252586:253742 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252586:253742 [2] NCCL INFO ncclCommInitRank comm 0xaaab16820860 rank 30 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
19: nid006557:208420:209604 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
29: nid007305:27224:28344 [2] NCCL INFO ncclCommInitRank comm 0xaaab02ed3070 rank 118 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
30: nid007318:20580:21733 [0] NCCL INFO Init timings: rank 120 nranks 128 total 11.70 (kernels 0.14, bootstrap 11.18, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
30: nid007318:20583:21730 [3] NCCL INFO ncclCommInitRank comm 0xaaaaee514e10 rank 123 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
30: nid007318:20581:21731 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
30: nid007318:20583:21730 [3] NCCL INFO Init timings: rank 123 nranks 128 total 12.13 (kernels 0.56, bootstrap 11.18, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221943:223091 [2] NCCL INFO Init timings: rank 26 nranks 128 total 12.01 (kernels 0.28, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221944:223090 [3] NCCL INFO Init timings: rank 27 nranks 128 total 12.07 (kernels 0.34, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252584:253741 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252586:253742 [2] NCCL INFO Init timings: rank 30 nranks 128 total 12.02 (kernels 0.28, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.02, rest 0.00)
19: nid006557:208420:209604 [3] NCCL INFO ncclCommInitRank comm 0xaaab123500f0 rank 79 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
19: nid006557:208420:209604 [3] NCCL INFO Init timings: rank 79 nranks 128 total 2.81 (kernels 0.17, bootstrap 2.25, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.01)
29: nid007305:27224:28344 [2] NCCL INFO Init timings: rank 118 nranks 128 total 12.01 (kernels 0.25, bootstrap 11.38, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
30: nid007318:20581:21731 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252584:253741 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 7: nid006502:252584:253741 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252584:253741 [0] NCCL INFO ncclCommInitRank comm 0xaaaafb714860 rank 28 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260086:261239 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
19: nid006557:208419:209607 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27222:28341 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27222:28341 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27222:28341 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
30: nid007318:20581:21731 [1] NCCL INFO ncclCommInitRank comm 0xaaab01d20530 rank 121 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
30: nid007318:20581:21731 [1] NCCL INFO Init timings: rank 121 nranks 128 total 12.13 (kernels 0.56, bootstrap 11.18, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252584:253741 [0] NCCL INFO Init timings: rank 28 nranks 128 total 12.07 (kernels 0.34, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260086:261239 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 5: nid006500:260086:261239 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
26: nid006565:222466:223654 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222466:223654 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27222:28341 [0] NCCL INFO ncclCommInitRank comm 0xaaaae8e75560 rank 116 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
11: nid006507:211335:212475 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 7: nid006502:252587:253740 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260086:261239 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf70716c0 rank 23 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
26: nid006565:222466:223654 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
26: nid006565:222466:223654 [1] NCCL INFO ncclCommInitRank comm 0xaaaadd720280 rank 105 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
29: nid007305:27222:28341 [0] NCCL INFO Init timings: rank 116 nranks 128 total 12.11 (kernels 0.34, bootstrap 11.39, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252587:253740 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 5: nid006500:260086:261239 [3] NCCL INFO Init timings: rank 23 nranks 128 total 11.65 (kernels 0.14, bootstrap 11.11, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222466:223654 [1] NCCL INFO Init timings: rank 105 nranks 128 total 11.87 (kernels 0.20, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222467:223652 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27225:28342 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27225:28342 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27225:28342 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252587:253740 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 7: nid006502:252587:253740 [3] NCCL INFO ncclCommInitRank comm 0xaaab12a7f380 rank 31 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
19: nid006557:208419:209607 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
19: nid006557:208419:209607 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
26: nid006565:222467:223652 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
26: nid006565:222467:223652 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
26: nid006565:222468:223651 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27225:28342 [3] NCCL INFO ncclCommInitRank comm 0xaaaafdcaec70 rank 119 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
29: nid007305:27225:28342 [3] NCCL INFO Init timings: rank 119 nranks 128 total 12.11 (kernels 0.34, bootstrap 11.39, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211335:212475 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 7: nid006502:252587:253740 [3] NCCL INFO Init timings: rank 31 nranks 128 total 12.11 (kernels 0.38, bootstrap 11.34, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.02, rest 0.00)
19: nid006557:208419:209607 [2] NCCL INFO ncclCommInitRank comm 0xaaab15322000 rank 78 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
26: nid006565:222467:223652 [2] NCCL INFO ncclCommInitRank comm 0xaaab1e0113c0 rank 106 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
11: nid006507:211338:212472 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211335:212475 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
19: nid006557:208419:209607 [2] NCCL INFO Init timings: rank 78 nranks 128 total 2.68 (kernels 0.38, bootstrap 1.91, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222468:223651 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
26: nid006565:222467:223652 [2] NCCL INFO Init timings: rank 106 nranks 128 total 12.06 (kernels 0.40, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27223:28343 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
29: nid007305:27223:28343 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
29: nid007305:27223:28343 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
29: nid007305:27223:28343 [1] NCCL INFO ncclCommInitRank comm 0xaaaadd180740 rank 117 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
11: nid006507:211338:212472 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 5: nid006500:260084:261238 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222468:223651 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
29: nid007305:27223:28343 [1] NCCL INFO Init timings: rank 117 nranks 128 total 12.07 (kernels 0.30, bootstrap 11.38, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222274:223424 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211338:212472 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
11: nid006507:211338:212472 [3] NCCL INFO ncclCommInitRank comm 0xaaab1f201250 rank 47 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260084:261238 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 5: nid006500:260084:261238 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 5: nid006500:260084:261238 [1] NCCL INFO ncclCommInitRank comm 0xaaaaebd115d0 rank 21 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260084:261238 [1] NCCL INFO Init timings: rank 21 nranks 128 total 12.01 (kernels 0.49, bootstrap 11.12, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222468:223651 [3] NCCL INFO ncclCommInitRank comm 0xaaaacd640a00 rank 107 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
22: nid006560:222274:223424 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
11: nid006507:211338:212472 [3] NCCL INFO Init timings: rank 47 nranks 128 total 12.12 (kernels 0.44, bootstrap 11.28, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211335:212475 [0] NCCL INFO ncclCommInitRank comm 0xaaaadbe34c90 rank 44 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260085:261237 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
26: nid006565:222468:223651 [3] NCCL INFO Init timings: rank 107 nranks 128 total 12.12 (kernels 0.45, bootstrap 11.28, allgathers 0.03, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222274:223424 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
22: nid006560:222274:223424 [3] NCCL INFO ncclCommInitRank comm 0xaaaad06e0ae0 rank 91 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
11: nid006507:211335:212475 [0] NCCL INFO Init timings: rank 44 nranks 128 total 11.88 (kernels 0.20, bootstrap 11.28, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260085:261237 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 5: nid006500:260085:261237 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 5: nid006500:260085:261237 [2] NCCL INFO ncclCommInitRank comm 0xaaab1f1aeb40 rank 22 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260083:261236 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222274:223424 [3] NCCL INFO Init timings: rank 91 nranks 128 total 12.11 (kernels 0.43, bootstrap 11.29, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:212474 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
11: nid006507:211337:212473 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260085:261237 [2] NCCL INFO Init timings: rank 22 nranks 128 total 12.07 (kernels 0.57, bootstrap 11.11, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222273:223427 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222273:223427 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
22: nid006560:222273:223427 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
11: nid006507:211336:212474 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
11: nid006507:211337:212473 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
22: nid006560:222273:223427 [2] NCCL INFO ncclCommInitRank comm 0xaaaae7423530 rank 90 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
22: nid006560:222273:223427 [2] NCCL INFO Init timings: rank 90 nranks 128 total 11.90 (kernels 0.23, bootstrap 11.28, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:212474 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
11: nid006507:211337:212473 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
10: nid006506:263727:264880 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263727:264880 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
22: nid006560:222272:223426 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260083:261236 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
10: nid006506:263728:264887 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222272:223426 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
22: nid006560:222272:223426 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
11: nid006507:211336:212474 [1] NCCL INFO ncclCommInitRank comm 0xaaab0a883290 rank 45 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
11: nid006507:211337:212473 [2] NCCL INFO ncclCommInitRank comm 0xaaab1716ee10 rank 46 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260083:261236 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
10: nid006506:263727:264880 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
22: nid006560:222271:223425 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
22: nid006560:222272:223426 [1] NCCL INFO ncclCommInitRank comm 0xaaaafbb630b0 rank 89 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
22: nid006560:222271:223425 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
11: nid006507:211337:212473 [2] NCCL INFO Init timings: rank 46 nranks 128 total 12.07 (kernels 0.40, bootstrap 11.28, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:212474 [1] NCCL INFO Init timings: rank 45 nranks 128 total 12.02 (kernels 0.34, bootstrap 11.28, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241021:242171 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 5: nid006500:260083:261236 [0] NCCL INFO ncclCommInitRank comm 0xaaaaddd64a90 rank 20 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 5: nid006500:260083:261236 [0] NCCL INFO Init timings: rank 20 nranks 128 total 12.11 (kernels 0.61, bootstrap 11.11, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263728:264887 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
22: nid006560:222272:223426 [1] NCCL INFO Init timings: rank 89 nranks 128 total 11.90 (kernels 0.23, bootstrap 11.29, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222271:223425 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 2: nid006497:227575:228688 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241021:242171 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 0: nid006495:241019:242169 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241021:242171 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
10: nid006506:263727:264880 [0] NCCL INFO ncclCommInitRank comm 0xaaaacce51d30 rank 40 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263728:264887 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
22: nid006560:222271:223425 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8db5160 rank 88 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 2: nid006497:227575:228688 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 2: nid006497:227575:228688 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 0: nid006495:241019:242169 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 0: nid006495:241021:242171 [2] NCCL INFO ncclCommInitRank comm 0xaaab287e25d0 rank 2 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263727:264880 [0] NCCL INFO Init timings: rank 40 nranks 128 total 12.13 (kernels 0.40, bootstrap 11.34, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222271:223425 [0] NCCL INFO Init timings: rank 88 nranks 128 total 12.07 (kernels 0.40, bootstrap 11.29, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 2: nid006497:227575:228688 [0] NCCL INFO ncclCommInitRank comm 0xaaab34a73550 rank 8 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 2: nid006497:227575:228688 [0] NCCL INFO Init timings: rank 8 nranks 128 total 12.00 (kernels 0.45, bootstrap 11.17, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.01)
 0: nid006495:241019:242169 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
10: nid006506:263728:264887 [1] NCCL INFO ncclCommInitRank comm 0xaaab066b0b90 rank 41 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263728:264887 [1] NCCL INFO Init timings: rank 41 nranks 128 total 11.75 (kernels 0.57, bootstrap 10.79, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241019:242169 [0] NCCL INFO ncclCommInitRank comm 0xaaaaca364de0 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263730:264885 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263730:264885 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
10: nid006506:263730:264885 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
10: nid006506:263730:264885 [3] NCCL INFO ncclCommInitRank comm 0xaaaadbd63f10 rank 43 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263730:264885 [3] NCCL INFO Init timings: rank 43 nranks 128 total 11.76 (kernels 0.57, bootstrap 10.80, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.01)
 0: nid006495:241020:242170 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241021:242171 [2] NCCL INFO Init timings: rank 2 nranks 128 total 12.06 (kernels 0.33, bootstrap 11.35, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263729:264886 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
10: nid006506:263729:264886 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
10: nid006506:263729:264886 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 0: nid006495:241019:242169 [0] NCCL INFO Init timings: rank 0 nranks 128 total 12.14 (kernels 0.40, bootstrap 11.35, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241020:242170 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
10: nid006506:263729:264886 [2] NCCL INFO ncclCommInitRank comm 0xaaab0c9e0740 rank 42 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
10: nid006506:263729:264886 [2] NCCL INFO Init timings: rank 42 nranks 128 total 11.75 (kernels 0.57, bootstrap 10.79, allgathers 0.05, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241020:242170 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 0: nid006495:241020:242170 [1] NCCL INFO ncclCommInitRank comm 0xaaaad1d00bf0 rank 1 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 0: nid006495:241020:242170 [1] NCCL INFO Init timings: rank 1 nranks 128 total 12.09 (kernels 0.36, bootstrap 11.35, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241022:242172 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 0: nid006495:241022:242172 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 0: nid006495:241022:242172 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 0: nid006495:241022:242172 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf2c322a0 rank 3 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 0: nid006495:241022:242172 [3] NCCL INFO Init timings: rank 3 nranks 128 total 11.82 (kernels 0.08, bootstrap 11.35, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.02, rest 0.00)
 2: nid006497:227577:228687 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227577:228687 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 2: nid006497:227577:228687 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 2: nid006497:227577:228687 [2] NCCL INFO ncclCommInitRank comm 0xaaab266a1d50 rank 10 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 2: nid006497:227577:228687 [2] NCCL INFO Init timings: rank 10 nranks 128 total 12.11 (kernels 0.56, bootstrap 11.17, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 2: nid006497:227576:228686 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227576:228686 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 2: nid006497:227576:228686 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 2: nid006497:227578:228689 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
 2: nid006497:227576:228686 [1] NCCL INFO ncclCommInitRank comm 0xaaaade5e19c0 rank 9 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 2: nid006497:227578:228689 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
 2: nid006497:227576:228686 [1] NCCL INFO Init timings: rank 9 nranks 128 total 12.12 (kernels 0.56, bootstrap 11.17, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 2: nid006497:227578:228689 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
 2: nid006497:227578:228689 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf3bd2ec0 rank 11 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
 2: nid006497:227578:228689 [3] NCCL INFO Init timings: rank 11 nranks 128 total 11.82 (kernels 0.27, bootstrap 11.17, allgathers 0.04, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216747:217884 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216747:217884 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
27: nid006566:216747:217884 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
27: nid006566:216747:217884 [1] NCCL INFO ncclCommInitRank comm 0xaaab06140ab0 rank 109 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf5335db8db3d84dc - Init COMPLETE
27: nid006566:216747:217884 [1] NCCL INFO Init timings: rank 109 nranks 128 total 12.08 (kernels 0.28, bootstrap 11.41, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216749:217883 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216749:217883 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
27: nid006566:216749:217883 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
27: nid006566:216749:217883 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf3be1d00 rank 111 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf5335db8db3d84dc - Init COMPLETE
27: nid006566:216749:217883 [3] NCCL INFO Init timings: rank 111 nranks 128 total 12.12 (kernels 0.32, bootstrap 11.41, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216746:217885 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216748:217887 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
27: nid006566:216746:217885 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
27: nid006566:216748:217887 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
27: nid006566:216746:217885 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
27: nid006566:216748:217887 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
27: nid006566:216746:217885 [0] NCCL INFO ncclCommInitRank comm 0xaaab08db5770 rank 108 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf5335db8db3d84dc - Init COMPLETE
27: nid006566:216748:217887 [2] NCCL INFO ncclCommInitRank comm 0xaaab0ae01a50 rank 110 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf5335db8db3d84dc - Init COMPLETE
27: nid006566:216746:217885 [0] NCCL INFO Init timings: rank 108 nranks 128 total 11.98 (kernels 0.18, bootstrap 11.41, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216748:217887 [2] NCCL INFO Init timings: rank 110 nranks 128 total 11.98 (kernels 0.18, bootstrap 11.41, allgathers 0.04, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221581:222797 [0] NCCL INFO Channel 00/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 04/0 : 64[0] -> 65[1] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 00/0 : 112[0] -> 113[1] via P2P/CUMEM
16: nid006554:221582:222799 [1] NCCL INFO Channel 00/0 : 65[1] -> 66[2] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 04/0 : 112[0] -> 113[1] via P2P/CUMEM
16: nid006554:221582:222799 [1] NCCL INFO Channel 04/0 : 65[1] -> 66[2] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 02/0 : 64[0] -> 67[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 03/0 : 64[0] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 00/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 06/0 : 64[0] -> 67[3] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 00/0 : 104[0] -> 105[1] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 04/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 07/0 : 64[0] -> 67[3] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 04/0 : 104[0] -> 105[1] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 01/0 : 84[0] -> 85[1] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 01/0 : 124[0] -> 125[1] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[1] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 00/0 : 72[0] -> 73[1] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 01/0 : 100[0] -> 101[1] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 05/0 : 84[0] -> 85[1] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 05/0 : 124[0] -> 125[1] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 05/0 : 60[0] -> 61[1] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 01/0 : 68[0] -> 69[1] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 04/0 : 72[0] -> 73[1] via P2P/CUMEM
17: nid006555:206594:207840 [1] NCCL INFO Channel 01/0 : 69[1] -> 70[2] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 05/0 : 100[0] -> 101[1] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[1] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 00/0 : 73[1] -> 74[2] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 01/0 : 116[0] -> 117[1] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 00/0 : 80[0] -> 81[1] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 04/0 : 73[1] -> 74[2] via P2P/CUMEM
 3: nid006498:226767:227937 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
17: nid006555:206595:207841 [2] NCCL INFO Channel 01/0 : 70[2] -> 71[3] via P2P/CUMEM
 3: nid006498:226767:227937 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[1] via P2P/CUMEM
17: nid006555:206594:207840 [1] NCCL INFO Channel 05/0 : 69[1] -> 70[2] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/CUMEM
28: nid007251:72170:73388 [1] NCCL INFO Channel 00/0 : 113[1] -> 114[2] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 04/0 : 48[0] -> 49[1] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
28: nid007251:72170:73388 [1] NCCL INFO Channel 04/0 : 113[1] -> 114[2] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 00/0 : 106[2] -> 107[3] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 05/0 : 68[0] -> 69[1] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 05/0 : 116[0] -> 117[1] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 04/0 : 80[0] -> 81[1] via P2P/CUMEM
21: nid006559:211125:212335 [2] NCCL INFO Channel 01/0 : 86[2] -> 87[3] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 02/0 : 72[0] -> 75[3] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 02/0 : 112[0] -> 115[3] via P2P/CUMEM
17: nid006555:206595:207841 [2] NCCL INFO Channel 05/0 : 70[2] -> 71[3] via P2P/CUMEM
 2: nid006497:227575:228744 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 00/0 : 105[1] -> 106[2] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 04/0 : 106[2] -> 107[3] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 05/0 : 36[0] -> 37[1] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[1] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 01/0 : 61[1] -> 62[2] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 00/0 : 88[0] -> 89[1] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 01/0 : 62[2] -> 63[3] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 00/0 : 74[2] -> 75[3] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[1] via P2P/CUMEM
21: nid006559:211125:212335 [2] NCCL INFO Channel 05/0 : 86[2] -> 87[3] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 04/0 : 105[1] -> 106[2] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 02/0 : 68[0] -> 71[3] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 03/0 : 72[0] -> 75[3] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[1] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 03/0 : 112[0] -> 115[3] via P2P/CUMEM
 2: nid006497:227575:228744 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 01/0 : 125[1] -> 126[2] via P2P/CUMEM
21: nid006559:211124:212337 [1] NCCL INFO Channel 01/0 : 85[1] -> 86[2] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 04/0 : 74[2] -> 75[3] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 03/0 : 68[0] -> 71[3] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[1] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 06/0 : 72[0] -> 75[3] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 05/0 : 61[1] -> 62[2] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 02/0 : 104[0] -> 107[3] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 05/0 : 62[2] -> 63[3] via P2P/CUMEM
31: nid007342:59769:61252 [2] NCCL INFO Channel 01/0 : 126[2] -> 127[3] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[1] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 05/0 : 52[0] -> 53[1] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/CUMEM
28: nid007251:72171:73389 [2] NCCL INFO Channel 00/0 : 114[2] -> 115[3] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 03/0 : 104[0] -> 107[3] via P2P/CUMEM
 3: nid006498:226766:227939 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
21: nid006559:211124:212337 [1] NCCL INFO Channel 05/0 : 85[1] -> 86[2] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 06/0 : 68[0] -> 71[3] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 06/0 : 112[0] -> 115[3] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 05/0 : 125[1] -> 126[2] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 00/0 : 49[1] -> 50[2] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 07/0 : 72[0] -> 75[3] via P2P/CUMEM
24: nid006563:221140:222361 [0] NCCL INFO Channel 00/0 : 96[0] -> 97[1] via P2P/CUMEM
31: nid007342:59769:61252 [2] NCCL INFO Channel 05/0 : 126[2] -> 127[3] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 04/0 : 88[0] -> 89[1] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 00/0 : 81[1] -> 82[2] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 03/0 : 16[0] -> 19[3] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 04/0 : 56[0] -> 57[1] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 04/0 : 49[1] -> 50[2] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 04/0 : 40[0] -> 41[1] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 02/0 : 60[0] -> 63[3] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 06/0 : 104[0] -> 107[3] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 06/0 : 16[0] -> 19[3] via P2P/CUMEM
 3: nid006498:226766:227939 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
17: nid006555:206593:207843 [0] NCCL INFO Channel 07/0 : 68[0] -> 71[3] via P2P/CUMEM
28: nid007251:72169:73387 [0] NCCL INFO Channel 07/0 : 112[0] -> 115[3] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 01/0 : 76[0] -> 77[1] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 05/0 : 20[0] -> 21[1] via P2P/CUMEM
28: nid007251:72171:73389 [2] NCCL INFO Channel 04/0 : 114[2] -> 115[3] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 02/0 : 124[0] -> 127[3] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 01/0 : 108[0] -> 109[1] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 04/0 : 32[0] -> 33[1] via P2P/CUMEM
 4: nid006499:254556:255750 [0] NCCL INFO Channel 07/0 : 16[0] -> 19[3] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 01/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 01/0 : 101[1] -> 102[2] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 02/0 : 84[0] -> 87[3] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 02/0 : 48[0] -> 51[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 01/0 : 92[0] -> 93[1] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 04/0 : 81[1] -> 82[2] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 07/0 : 104[0] -> 107[3] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 03/0 : 60[0] -> 63[3] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 03/0 : 124[0] -> 127[3] via P2P/CUMEM
24: nid006563:221140:222361 [0] NCCL INFO Channel 04/0 : 96[0] -> 97[1] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 01/0 : 37[1] -> 38[2] via P2P/CUMEM
13: nid006509:201790:202978 [2] NCCL INFO Channel 01/0 : 54[2] -> 55[3] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 05/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 05/0 : 101[1] -> 102[2] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 06/0 : 124[0] -> 127[3] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
 2: nid006497:227577:228746 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 03/0 : 84[0] -> 87[3] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 00/0 : 120[0] -> 121[1] via P2P/CUMEM
 2: nid006497:227576:228745 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 03/0 : 48[0] -> 51[3] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 06/0 : 60[0] -> 63[3] via P2P/CUMEM
 4: nid006499:254558:255753 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 01/0 : 38[2] -> 39[3] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[2] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
31: nid007342:59767:61251 [0] NCCL INFO Channel 07/0 : 124[0] -> 127[3] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 02/0 : 80[0] -> 83[3] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 05/0 : 76[0] -> 77[1] via P2P/CUMEM
 4: nid006499:254558:255753 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 06/0 : 84[0] -> 87[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 05/0 : 92[0] -> 93[1] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 06/0 : 48[0] -> 51[3] via P2P/CUMEM
14: nid006510:229444:230647 [2] NCCL INFO Channel 00/0 : 58[2] -> 59[3] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 05/0 : 37[1] -> 38[2] via P2P/CUMEM
13: nid006509:201790:202978 [2] NCCL INFO Channel 05/0 : 54[2] -> 55[3] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 02/0 : 100[0] -> 103[3] via P2P/CUMEM
12: nid006508:205768:207011 [2] NCCL INFO Channel 00/0 : 50[2] -> 51[3] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 00/0 : 57[1] -> 58[2] via P2P/CUMEM
 2: nid006497:227577:228746 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 05/0 : 38[2] -> 39[3] via P2P/CUMEM
15: nid006553:223923:225108 [0] NCCL INFO Channel 07/0 : 60[0] -> 63[3] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
 2: nid006497:227576:228745 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 07/0 : 84[0] -> 87[3] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 05/0 : 21[1] -> 22[2] via P2P/CUMEM
 3: nid006498:226765:227940 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 03/0 : 80[0] -> 83[3] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 07/0 : 48[0] -> 51[3] via P2P/CUMEM
 0: nid006495:241020:242232 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
12: nid006508:205768:207011 [2] NCCL INFO Channel 04/0 : 50[2] -> 51[3] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
14: nid006510:229444:230647 [2] NCCL INFO Channel 04/0 : 58[2] -> 59[3] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 03/0 : 100[0] -> 103[3] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 02/0 : 36[0] -> 39[3] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 06/0 : 80[0] -> 83[3] via P2P/CUMEM
 5: nid006500:260085:261302 [2] NCCL INFO Channel 01/0 : 22[2] -> 23[3] via P2P/CUMEM
11: nid006507:211335:212513 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[1] via P2P/CUMEM
 1: nid006496:242586:243809 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 01/0 : 109[1] -> 110[2] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 04/0 : 120[0] -> 121[1] via P2P/CUMEM
20: nid006558:215469:216705 [2] NCCL INFO Channel 00/0 : 82[2] -> 83[3] via P2P/CUMEM
 1: nid006496:242585:243810 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 03/0 : 36[0] -> 39[3] via P2P/CUMEM
20: nid006558:215467:216702 [0] NCCL INFO Channel 07/0 : 80[0] -> 83[3] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 00/0 : 97[1] -> 98[2] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 02/0 : 20[0] -> 23[3] via P2P/CUMEM
 2: nid006497:227575:228744 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
 0: nid006495:241020:242232 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
 5: nid006500:260085:261302 [2] NCCL INFO Channel 05/0 : 22[2] -> 23[3] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 06/0 : 100[0] -> 103[3] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 01/0 : 117[1] -> 118[2] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 01/0 : 118[2] -> 119[3] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 00/0 : 89[1] -> 90[2] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 04/0 : 57[1] -> 58[2] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 02/0 : 56[0] -> 59[3] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 04/0 : 97[1] -> 98[2] via P2P/CUMEM
22: nid006560:222273:223487 [2] NCCL INFO Channel 00/0 : 90[2] -> 91[3] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 06/0 : 36[0] -> 39[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[1] via P2P/CUMEM
 2: nid006497:227575:228744 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
20: nid006558:215469:216705 [2] NCCL INFO Channel 04/0 : 82[2] -> 83[3] via P2P/CUMEM
 1: nid006496:242586:243809 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
27: nid006566:216748:217925 [2] NCCL INFO Channel 01/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 05/0 : 109[1] -> 110[2] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 02/0 : 62[2] -> 66[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221582:222799 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[1] [receive] via NET/AWS Libfabric/1
 1: nid006496:242585:243810 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 03/0 : 20[0] -> 23[3] via P2P/CUMEM
 2: nid006497:227575:228744 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 06/0 : 62[2] -> 66[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:224214 [0] NCCL INFO Channel 07/0 : 100[0] -> 103[3] via P2P/CUMEM
 9: nid006505:249080:250303 [0] NCCL INFO Channel 07/0 : 36[0] -> 39[3] via P2P/CUMEM
16: nid006554:221582:222799 [1] NCCL INFO Channel 07/0 : 61[1] -> 65[1] [receive] via NET/AWS Libfabric/1
29: nid007305:27223:28381 [1] NCCL INFO Channel 05/0 : 117[1] -> 118[2] via P2P/CUMEM
24: nid006563:221140:222361 [0] NCCL INFO Channel 02/0 : 96[0] -> 99[3] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 05/0 : 118[2] -> 119[3] via P2P/CUMEM
19: nid006557:208419:209624 [2] NCCL INFO Channel 01/0 : 78[2] -> 79[3] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 04/0 : 89[1] -> 90[2] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 03/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:261300 [0] NCCL INFO Channel 06/0 : 20[0] -> 23[3] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
16: nid006554:221582:222799 [1] NCCL INFO Channel 02/0 : 65[1] -> 69[1] [send] via NET/AWS Libfabric/1
22: nid006560:222273:223487 [2] NCCL INFO Channel 04/0 : 90[2] -> 91[3] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 07/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:222799 [1] NCCL INFO Channel 06/0 : 65[1] -> 69[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227575:228744 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 07/0 : 20[0] -> 23[3] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 00/0 : 122[2] -> 123[3] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 01/0 : 93[1] -> 94[2] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/CUMEM
27: nid006566:216748:217925 [2] NCCL INFO Channel 05/0 : 110[2] -> 111[3] via P2P/CUMEM
30: nid007318:20581:21773 [1] NCCL INFO Channel 00/0 : 121[1] -> 122[2] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 00/0 : 41[1] -> 42[2] via P2P/CUMEM
23: nid006561:220713:221954 [2] NCCL INFO Channel 01/0 : 94[2] -> 95[3] via P2P/CUMEM
11: nid006507:211335:212513 [0] NCCL INFO Channel 05/0 : 44[0] -> 45[1] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 01/0 : 53[1] -> 54[2] via P2P/CUMEM
10: nid006506:263729:264927 [2] NCCL INFO Channel 00/0 : 42[2] -> 43[3] via P2P/CUMEM
24: nid006563:221140:222361 [0] NCCL INFO Channel 03/0 : 96[0] -> 99[3] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 05/0 : 28[0] -> 29[1] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 05/0 : 108[0] -> 109[1] via P2P/CUMEM
19: nid006557:208419:209624 [2] NCCL INFO Channel 05/0 : 78[2] -> 79[3] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 04/0 : 122[2] -> 123[3] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 01/0 : 77[1] -> 78[2] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 05/0 : 53[1] -> 54[2] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 02/0 : 116[0] -> 119[3] via P2P/CUMEM
30: nid007318:20581:21773 [1] NCCL INFO Channel 04/0 : 121[1] -> 122[2] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 02/0 : 88[0] -> 91[3] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 05/0 : 93[1] -> 94[2] via P2P/CUMEM
24: nid006563:221140:222361 [0] NCCL INFO Channel 06/0 : 96[0] -> 99[3] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 04/0 : 41[1] -> 42[2] via P2P/CUMEM
10: nid006506:263729:264927 [2] NCCL INFO Channel 04/0 : 42[2] -> 43[3] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
23: nid006561:220713:221954 [2] NCCL INFO Channel 05/0 : 94[2] -> 95[3] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 00/0 : 98[2] -> 99[3] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 02/0 : 108[0] -> 111[3] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 03/0 : 116[0] -> 119[3] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 02/0 : 52[0] -> 55[3] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 03/0 : 88[0] -> 91[3] via P2P/CUMEM
 8: nid006503:218411:219604 [1] NCCL INFO Channel 00/0 : 33[1] -> 34[2] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 05/0 : 77[1] -> 78[2] via P2P/CUMEM
16: nid006554:221584:222798 [3] NCCL INFO Channel 01/0 : 63[3] -> 67[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211337:212514 [2] NCCL INFO Channel 01/0 : 46[2] -> 47[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 00/0 : 60[0] -> 64[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222361 [0] NCCL INFO Channel 07/0 : 96[0] -> 99[3] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 02/0 : 120[0] -> 123[3] via P2P/CUMEM
16: nid006554:221584:222798 [3] NCCL INFO Channel 05/0 : 63[3] -> 67[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221142:222364 [2] NCCL INFO Channel 04/0 : 98[2] -> 99[3] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 03/0 : 108[0] -> 111[3] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 00/0 : 34[2] -> 35[3] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 03/0 : 52[0] -> 55[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 04/0 : 60[0] -> 64[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221584:222798 [3] NCCL INFO Channel 00/0 : 67[3] -> 71[3] [send] via NET/AWS Libfabric/3
 6: nid006501:221943:223145 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 02/0 : 92[0] -> 95[3] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 06/0 : 116[0] -> 119[3] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 01/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
16: nid006554:221584:222798 [3] NCCL INFO Channel 04/0 : 67[3] -> 71[3] [send] via NET/AWS Libfabric/3
10: nid006506:263727:264924 [0] NCCL INFO Channel 02/0 : 40[0] -> 43[3] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 06/0 : 88[0] -> 91[3] via P2P/CUMEM
11: nid006507:211337:212514 [2] NCCL INFO Channel 05/0 : 46[2] -> 47[3] via P2P/CUMEM
16: nid006554:221581:222797 [0] NCCL INFO Channel 05/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218411:219604 [1] NCCL INFO Channel 04/0 : 33[1] -> 34[2] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 06/0 : 108[0] -> 111[3] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 06/0 : 52[0] -> 55[3] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 01/0 : 45[1] -> 46[2] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 02/0 : 76[0] -> 79[3] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 01/0 : 29[1] -> 30[2] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 04/0 : 34[2] -> 35[3] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 03/0 : 120[0] -> 123[3] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 01/0 : 30[2] -> 31[3] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 03/0 : 40[0] -> 43[3] via P2P/CUMEM
 6: nid006501:221943:223145 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 03/0 : 92[0] -> 95[3] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 07/0 : 116[0] -> 119[3] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 07/0 : 52[0] -> 55[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 07/0 : 88[0] -> 91[3] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 05/0 : 45[1] -> 46[2] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 07/0 : 108[0] -> 111[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 03/0 : 24[0] -> 27[3] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 06/0 : 40[0] -> 43[3] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 06/0 : 120[0] -> 123[3] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 03/0 : 76[0] -> 79[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 06/0 : 92[0] -> 95[3] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 02/0 : 32[0] -> 35[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 06/0 : 24[0] -> 27[3] via P2P/CUMEM
11: nid006507:211335:212513 [0] NCCL INFO Channel 02/0 : 44[0] -> 47[3] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 05/0 : 29[1] -> 30[2] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 05/0 : 30[2] -> 31[3] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 07/0 : 40[0] -> 43[3] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 03/0 : 32[0] -> 35[3] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 07/0 : 120[0] -> 123[3] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 06/0 : 76[0] -> 79[3] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 07/0 : 92[0] -> 95[3] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 07/0 : 24[0] -> 27[3] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 06/0 : 32[0] -> 35[3] via P2P/CUMEM
11: nid006507:211335:212513 [0] NCCL INFO Channel 03/0 : 44[0] -> 47[3] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 07/0 : 76[0] -> 79[3] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 02/0 : 28[0] -> 31[3] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 07/0 : 32[0] -> 35[3] via P2P/CUMEM
17: nid006555:206594:207840 [1] NCCL INFO Channel 02/0 : 65[1] -> 69[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206595:207841 [2] NCCL INFO Channel 03/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206594:207840 [1] NCCL INFO Channel 06/0 : 65[1] -> 69[1] [receive] via NET/AWS Libfabric/1
11: nid006507:211335:212513 [0] NCCL INFO Channel 06/0 : 44[0] -> 47[3] via P2P/CUMEM
17: nid006555:206595:207841 [2] NCCL INFO Channel 07/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206594:207840 [1] NCCL INFO Channel 03/0 : 69[1] -> 73[1] [send] via NET/AWS Libfabric/1
18: nid006556:210775:212034 [1] NCCL INFO Channel 03/0 : 69[1] -> 73[1] [receive] via NET/AWS Libfabric/1
 7: nid006502:252584:253802 [0] NCCL INFO Channel 03/0 : 28[0] -> 31[3] via P2P/CUMEM
17: nid006555:206595:207841 [2] NCCL INFO Channel 02/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
17: nid006555:206594:207840 [1] NCCL INFO Channel 07/0 : 69[1] -> 73[1] [send] via NET/AWS Libfabric/1
18: nid006556:210775:212034 [1] NCCL INFO Channel 07/0 : 69[1] -> 73[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206595:207841 [2] NCCL INFO Channel 06/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
18: nid006556:210775:212034 [1] NCCL INFO Channel 02/0 : 73[1] -> 77[1] [send] via NET/AWS Libfabric/1
26: nid006565:222466:223696 [1] NCCL INFO Channel 03/0 : 101[1] -> 105[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210775:212034 [1] NCCL INFO Channel 06/0 : 73[1] -> 77[1] [send] via NET/AWS Libfabric/1
11: nid006507:211335:212513 [0] NCCL INFO Channel 07/0 : 44[0] -> 47[3] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 07/0 : 101[1] -> 105[1] [receive] via NET/AWS Libfabric/1
26: nid006565:222466:223696 [1] NCCL INFO Channel 02/0 : 105[1] -> 109[1] [send] via NET/AWS Libfabric/1
 7: nid006502:252584:253802 [0] NCCL INFO Channel 06/0 : 28[0] -> 31[3] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 02/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222466:223696 [1] NCCL INFO Channel 06/0 : 105[1] -> 109[1] [send] via NET/AWS Libfabric/1
26: nid006565:222467:223694 [2] NCCL INFO Channel 06/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223694 [2] NCCL INFO Channel 03/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212036 [2] NCCL INFO Channel 02/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212036 [2] NCCL INFO Channel 06/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72170:73388 [1] NCCL INFO Channel 03/0 : 109[1] -> 113[1] [receive] via NET/AWS Libfabric/1
 7: nid006502:252584:253802 [0] NCCL INFO Channel 07/0 : 28[0] -> 31[3] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 07/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225110 [2] NCCL INFO Channel 03/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212036 [2] NCCL INFO Channel 03/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
28: nid007251:72170:73388 [1] NCCL INFO Channel 07/0 : 109[1] -> 113[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210776:212036 [2] NCCL INFO Channel 07/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
31: nid007342:59768:61253 [1] NCCL INFO Channel 02/0 : 121[1] -> 125[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223925:225110 [2] NCCL INFO Channel 07/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72170:73388 [1] NCCL INFO Channel 02/0 : 113[1] -> 117[1] [send] via NET/AWS Libfabric/1
15: nid006553:223924:225109 [1] NCCL INFO Channel 02/0 : 57[1] -> 61[1] [receive] via NET/AWS Libfabric/1
31: nid007342:59769:61252 [2] NCCL INFO Channel 03/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225110 [2] NCCL INFO Channel 02/0 : 62[2] -> 66[2] [send] via NET/AWS Libfabric/2
28: nid007251:72170:73388 [1] NCCL INFO Channel 06/0 : 113[1] -> 117[1] [send] via NET/AWS Libfabric/1
31: nid007342:59768:61253 [1] NCCL INFO Channel 06/0 : 121[1] -> 125[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223924:225109 [1] NCCL INFO Channel 06/0 : 57[1] -> 61[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206595:207841 [2] NCCL INFO Channel 03/0 : 70[2] -> 68[0] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 06/0 : 62[2] -> 66[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:61252 [2] NCCL INFO Channel 07/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59768:61253 [1] NCCL INFO Channel 03/0 : 125[1] -> 1[1] [send] via NET/AWS Libfabric/1
15: nid006553:223924:225109 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[1] [send] via NET/AWS Libfabric/1
31: nid007342:59769:61252 [2] NCCL INFO Channel 02/0 : 126[2] -> 2[2] [send] via NET/AWS Libfabric/2
31: nid007342:59768:61253 [1] NCCL INFO Channel 07/0 : 125[1] -> 1[1] [send] via NET/AWS Libfabric/1
15: nid006553:223924:225109 [1] NCCL INFO Channel 07/0 : 61[1] -> 65[1] [send] via NET/AWS Libfabric/1
21: nid006559:211125:212335 [2] NCCL INFO Channel 03/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59769:61252 [2] NCCL INFO Channel 06/0 : 126[2] -> 2[2] [send] via NET/AWS Libfabric/2
21: nid006559:211124:212337 [1] NCCL INFO Channel 02/0 : 81[1] -> 85[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206595:207841 [2] NCCL INFO Channel 07/0 : 70[2] -> 68[0] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 00/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211125:212335 [2] NCCL INFO Channel 07/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210777:212037 [3] NCCL INFO Channel 01/0 : 71[3] -> 75[3] [receive] via NET/AWS Libfabric/3
17: nid006555:206593:207843 [0] NCCL INFO Channel 01/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230646 [0] NCCL INFO Channel 03/0 : 56[0] -> 59[3] via P2P/CUMEM
18: nid006556:210774:212035 [0] NCCL INFO Channel 00/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:207842 [3] NCCL INFO Channel 00/0 : 67[3] -> 71[3] [receive] via NET/AWS Libfabric/3
16: nid006554:221583:222800 [2] NCCL INFO Channel 02/0 : 66[2] -> 64[0] via P2P/CUMEM
26: nid006565:222465:223693 [0] NCCL INFO Channel 04/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211125:212335 [2] NCCL INFO Channel 02/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
21: nid006559:211124:212337 [1] NCCL INFO Channel 06/0 : 81[1] -> 85[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206596:207842 [3] NCCL INFO Channel 04/0 : 67[3] -> 71[3] [receive] via NET/AWS Libfabric/3
18: nid006556:210777:212037 [3] NCCL INFO Channel 05/0 : 71[3] -> 75[3] [receive] via NET/AWS Libfabric/3
18: nid006556:210774:212035 [0] NCCL INFO Channel 04/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72171:73389 [2] NCCL INFO Channel 02/0 : 110[2] -> 114[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254557:255751 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
14: nid006510:229442:230646 [0] NCCL INFO Channel 06/0 : 56[0] -> 59[3] via P2P/CUMEM
 3: nid006498:226767:227937 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222465:223693 [0] NCCL INFO Channel 01/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
21: nid006559:211125:212335 [2] NCCL INFO Channel 06/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
17: nid006555:206593:207843 [0] NCCL INFO Channel 05/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:207842 [3] NCCL INFO Channel 01/0 : 71[3] -> 75[3] [send] via NET/AWS Libfabric/3
28: nid007251:72171:73389 [2] NCCL INFO Channel 06/0 : 110[2] -> 114[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222465:223693 [0] NCCL INFO Channel 05/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
21: nid006559:211124:212337 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[1] [send] via NET/AWS Libfabric/1
18: nid006556:210774:212035 [0] NCCL INFO Channel 01/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
18: nid006556:210777:212037 [3] NCCL INFO Channel 00/0 : 75[3] -> 79[3] [send] via NET/AWS Libfabric/3
 3: nid006498:226767:227937 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59770:61254 [3] NCCL INFO Channel 00/0 : 123[3] -> 127[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254557:255751 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206596:207842 [3] NCCL INFO Channel 05/0 : 71[3] -> 75[3] [send] via NET/AWS Libfabric/3
28: nid007251:72171:73389 [2] NCCL INFO Channel 03/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
21: nid006559:211124:212337 [1] NCCL INFO Channel 07/0 : 85[1] -> 89[1] [send] via NET/AWS Libfabric/1
31: nid007342:59767:61251 [0] NCCL INFO Channel 01/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212035 [0] NCCL INFO Channel 05/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230646 [0] NCCL INFO Channel 07/0 : 56[0] -> 59[3] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 03/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:227937 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
26: nid006565:222468:223695 [3] NCCL INFO Channel 01/0 : 103[3] -> 107[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254557:255751 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226766:227939 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
26: nid006565:222468:223695 [3] NCCL INFO Channel 05/0 : 103[3] -> 107[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211126:212336 [3] NCCL INFO Channel 00/0 : 83[3] -> 87[3] [receive] via NET/AWS Libfabric/3
18: nid006556:210777:212037 [3] NCCL INFO Channel 04/0 : 75[3] -> 79[3] [send] via NET/AWS Libfabric/3
21: nid006559:211123:212334 [0] NCCL INFO Channel 01/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59770:61254 [3] NCCL INFO Channel 04/0 : 123[3] -> 127[3] [receive] via NET/AWS Libfabric/3
31: nid007342:59767:61251 [0] NCCL INFO Channel 05/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223023:224215 [2] NCCL INFO Channel 07/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:227937 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73389 [2] NCCL INFO Channel 07/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:224217 [1] NCCL INFO Channel 02/0 : 97[1] -> 101[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226766:227939 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
 4: nid006499:254558:255753 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:224215 [2] NCCL INFO Channel 02/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254557:255751 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
26: nid006565:222468:223695 [3] NCCL INFO Channel 00/0 : 107[3] -> 111[3] [send] via NET/AWS Libfabric/3
21: nid006559:211126:212336 [3] NCCL INFO Channel 04/0 : 83[3] -> 87[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211123:212334 [0] NCCL INFO Channel 05/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254558:255753 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223022:224217 [1] NCCL INFO Channel 06/0 : 97[1] -> 101[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72172:73390 [3] NCCL INFO Channel 01/0 : 111[3] -> 115[3] [receive] via NET/AWS Libfabric/3
26: nid006565:222468:223695 [3] NCCL INFO Channel 04/0 : 107[3] -> 111[3] [send] via NET/AWS Libfabric/3
31: nid007342:59767:61251 [0] NCCL INFO Channel 00/0 : 124[0] -> 0[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:255753 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:225108 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:222800 [2] NCCL INFO Channel 06/0 : 66[2] -> 64[0] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 06/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226768:227938 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
31: nid007342:59770:61254 [3] NCCL INFO Channel 01/0 : 127[3] -> 3[3] [send] via NET/AWS Libfabric/3
15: nid006553:223926:225111 [3] NCCL INFO Channel 00/0 : 59[3] -> 63[3] [receive] via NET/AWS Libfabric/3
17: nid006555:206593:207843 [0] NCCL INFO Channel 00/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226766:227939 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
31: nid007342:59767:61251 [0] NCCL INFO Channel 04/0 : 124[0] -> 0[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:255753 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:224217 [1] NCCL INFO Channel 03/0 : 101[1] -> 105[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226768:227938 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
 3: nid006498:226766:227939 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
31: nid007342:59770:61254 [3] NCCL INFO Channel 05/0 : 127[3] -> 3[3] [send] via NET/AWS Libfabric/3
17: nid006555:206596:207842 [3] NCCL INFO Channel 03/0 : 71[3] -> 69[1] via P2P/CUMEM
21: nid006559:211123:212334 [0] NCCL INFO Channel 00/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
28: nid007251:72172:73390 [3] NCCL INFO Channel 05/0 : 111[3] -> 115[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211126:212336 [3] NCCL INFO Channel 01/0 : 87[3] -> 91[3] [send] via NET/AWS Libfabric/3
25: nid006564:223022:224217 [1] NCCL INFO Channel 07/0 : 101[1] -> 105[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226765:227940 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212334 [0] NCCL INFO Channel 04/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211126:212336 [3] NCCL INFO Channel 05/0 : 87[3] -> 91[3] [send] via NET/AWS Libfabric/3
17: nid006555:206593:207843 [0] NCCL INFO Channel 04/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
28: nid007251:72172:73390 [3] NCCL INFO Channel 00/0 : 115[3] -> 119[3] [send] via NET/AWS Libfabric/3
28: nid007251:72169:73387 [0] NCCL INFO Channel 00/0 : 108[0] -> 112[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226768:227938 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249081:250305 [1] NCCL INFO Channel 02/0 : 33[1] -> 37[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223926:225111 [3] NCCL INFO Channel 04/0 : 59[3] -> 63[3] [receive] via NET/AWS Libfabric/3
15: nid006553:223923:225108 [0] NCCL INFO Channel 05/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226768:227938 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
15: nid006553:223926:225111 [3] NCCL INFO Channel 01/0 : 63[3] -> 67[3] [send] via NET/AWS Libfabric/3
28: nid007251:72172:73390 [3] NCCL INFO Channel 04/0 : 115[3] -> 119[3] [send] via NET/AWS Libfabric/3
15: nid006553:223923:225108 [0] NCCL INFO Channel 00/0 : 60[0] -> 64[0] [send] via NET/AWS Libfabric/0
12: nid006508:205767:207009 [1] NCCL INFO Channel 03/0 : 45[1] -> 49[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205768:207011 [2] NCCL INFO Channel 02/0 : 46[2] -> 50[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215468:216703 [1] NCCL INFO Channel 03/0 : 77[1] -> 81[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249081:250305 [1] NCCL INFO Channel 06/0 : 33[1] -> 37[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72169:73387 [0] NCCL INFO Channel 04/0 : 108[0] -> 112[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223926:225111 [3] NCCL INFO Channel 05/0 : 63[3] -> 67[3] [send] via NET/AWS Libfabric/3
28: nid007251:72169:73387 [0] NCCL INFO Channel 01/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
17: nid006555:206596:207842 [3] NCCL INFO Channel 07/0 : 71[3] -> 69[1] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 07/0 : 45[1] -> 49[1] [receive] via NET/AWS Libfabric/1
20: nid006558:215468:216703 [1] NCCL INFO Channel 07/0 : 77[1] -> 81[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249081:250305 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[1] [send] via NET/AWS Libfabric/1
28: nid007251:72169:73387 [0] NCCL INFO Channel 05/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:207011 [2] NCCL INFO Channel 06/0 : 46[2] -> 50[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215468:216703 [1] NCCL INFO Channel 02/0 : 81[1] -> 85[1] [send] via NET/AWS Libfabric/1
12: nid006508:205767:207009 [1] NCCL INFO Channel 02/0 : 49[1] -> 53[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227576:228745 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249082:250304 [2] NCCL INFO Channel 03/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249081:250305 [1] NCCL INFO Channel 07/0 : 37[1] -> 41[1] [send] via NET/AWS Libfabric/1
12: nid006508:205768:207011 [2] NCCL INFO Channel 03/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
20: nid006558:215468:216703 [1] NCCL INFO Channel 06/0 : 81[1] -> 85[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260084:261301 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205767:207009 [1] NCCL INFO Channel 06/0 : 49[1] -> 53[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227576:228745 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249082:250304 [2] NCCL INFO Channel 07/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229443:230648 [1] NCCL INFO Channel 03/0 : 53[1] -> 57[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226765:227940 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205768:207011 [2] NCCL INFO Channel 07/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
16: nid006554:221584:222798 [3] NCCL INFO Channel 02/0 : 67[3] -> 65[1] via P2P/CUMEM
 4: nid006499:254559:255752 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
 9: nid006505:249082:250304 [2] NCCL INFO Channel 02/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
17: nid006555:206596:207842 [3] NCCL INFO Channel 00/0 : 71[3] -> 70[2] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
 2: nid006497:227576:228745 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
15: nid006553:223923:225108 [0] NCCL INFO Channel 04/0 : 60[0] -> 64[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227576:228745 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260084:261301 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226765:227940 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249082:250304 [2] NCCL INFO Channel 06/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241020:242232 [1] NCCL INFO Channel 03/0 : 125[1] -> 1[1] [receive] via NET/AWS Libfabric/1
14: nid006510:229443:230648 [1] NCCL INFO Channel 07/0 : 53[1] -> 57[1] [receive] via NET/AWS Libfabric/1
 4: nid006499:254559:255752 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
 5: nid006500:260084:261301 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
 4: nid006499:254556:255750 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:227940 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254559:255752 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241020:242232 [1] NCCL INFO Channel 07/0 : 125[1] -> 1[1] [receive] via NET/AWS Libfabric/1
 4: nid006499:254556:255750 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260085:261302 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229443:230648 [1] NCCL INFO Channel 02/0 : 57[1] -> 61[1] [send] via NET/AWS Libfabric/1
 0: nid006495:241020:242232 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
16: nid006554:221584:222798 [3] NCCL INFO Channel 06/0 : 67[3] -> 65[1] via P2P/CUMEM
 4: nid006499:254559:255752 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
 4: nid006499:254556:255750 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
20: nid006558:215469:216705 [2] NCCL INFO Channel 02/0 : 78[2] -> 82[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241020:242232 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
 9: nid006505:249083:250306 [3] NCCL INFO Channel 00/0 : 35[3] -> 39[3] [receive] via NET/AWS Libfabric/3
 5: nid006500:260085:261302 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254556:255750 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227577:228746 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216705 [2] NCCL INFO Channel 06/0 : 78[2] -> 82[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230647 [2] NCCL INFO Channel 02/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261302 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:243810 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249083:250306 [3] NCCL INFO Channel 04/0 : 35[3] -> 39[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223024:224216 [3] NCCL INFO Channel 00/0 : 99[3] -> 103[3] [receive] via NET/AWS Libfabric/3
 2: nid006497:227577:228746 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249080:250303 [0] NCCL INFO Channel 01/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242586:243809 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261302 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216705 [2] NCCL INFO Channel 03/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:250306 [3] NCCL INFO Channel 01/0 : 39[3] -> 43[3] [send] via NET/AWS Libfabric/3
 1: nid006496:242585:243810 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
14: nid006510:229443:230648 [1] NCCL INFO Channel 06/0 : 57[1] -> 61[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227577:228746 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249080:250303 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:207842 [3] NCCL INFO Channel 02/0 : 71[3] -> 70[2] via P2P/CUMEM
 1: nid006496:242586:243809 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216705 [2] NCCL INFO Channel 07/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
25: nid006564:223024:224216 [3] NCCL INFO Channel 04/0 : 99[3] -> 103[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223021:224214 [0] NCCL INFO Channel 01/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227577:228746 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:250306 [3] NCCL INFO Channel 05/0 : 39[3] -> 43[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249080:250303 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242585:243810 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
16: nid006554:221584:222798 [3] NCCL INFO Channel 01/0 : 67[3] -> 66[2] via P2P/CUMEM
14: nid006510:229444:230647 [2] NCCL INFO Channel 06/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:243809 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:243810 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
25: nid006564:223024:224216 [3] NCCL INFO Channel 01/0 : 103[3] -> 107[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241021:242231 [2] NCCL INFO Channel 02/0 : 126[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249080:250303 [0] NCCL INFO Channel 04/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:255753 [2] NCCL INFO Channel 02/0 : 18[2] -> 16[0] via P2P/CUMEM
25: nid006564:223021:224214 [0] NCCL INFO Channel 05/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223024:224216 [3] NCCL INFO Channel 05/0 : 103[3] -> 107[3] [send] via NET/AWS Libfabric/3
17: nid006555:206594:207840 [1] NCCL INFO Channel 00/0 : 69[1] -> 68[0] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 02/0 : 105[1] -> 109[1] [receive] via NET/AWS Libfabric/1
 1: nid006496:242586:243809 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:242231 [2] NCCL INFO Channel 06/0 : 126[2] -> 2[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222272:223486 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[1] [receive] via NET/AWS Libfabric/1
27: nid006566:216747:217923 [1] NCCL INFO Channel 06/0 : 105[1] -> 109[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226767:227937 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
20: nid006558:215467:216702 [0] NCCL INFO Channel 00/0 : 76[0] -> 80[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229444:230647 [2] NCCL INFO Channel 03/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
20: nid006558:215470:216704 [3] NCCL INFO Channel 01/0 : 79[3] -> 83[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223021:224214 [0] NCCL INFO Channel 00/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
27: nid006566:216747:217923 [1] NCCL INFO Channel 03/0 : 109[1] -> 113[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227575:228744 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:207842 [3] NCCL INFO Channel 04/0 : 71[3] -> 70[2] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 07/0 : 85[1] -> 89[1] [receive] via NET/AWS Libfabric/1
13: nid006509:201790:202978 [2] NCCL INFO Channel 03/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216747:217923 [1] NCCL INFO Channel 07/0 : 109[1] -> 113[1] [send] via NET/AWS Libfabric/1
22: nid006560:222273:223487 [2] NCCL INFO Channel 02/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201789:202980 [1] NCCL INFO Channel 02/0 : 49[1] -> 53[1] [receive] via NET/AWS Libfabric/1
14: nid006510:229444:230647 [2] NCCL INFO Channel 07/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209624 [2] NCCL INFO Channel 03/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:217925 [2] NCCL INFO Channel 03/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222272:223486 [1] NCCL INFO Channel 02/0 : 89[1] -> 93[1] [send] via NET/AWS Libfabric/1
13: nid006509:201790:202978 [2] NCCL INFO Channel 07/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215467:216702 [0] NCCL INFO Channel 04/0 : 76[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254558:255753 [2] NCCL INFO Channel 06/0 : 18[2] -> 16[0] via P2P/CUMEM
 2: nid006497:227577:228746 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 05/0 : 79[3] -> 83[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223021:224214 [0] NCCL INFO Channel 04/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:209624 [2] NCCL INFO Channel 07/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227578:228747 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
20: nid006558:215467:216702 [0] NCCL INFO Channel 01/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
20: nid006558:215470:216704 [3] NCCL INFO Channel 00/0 : 83[3] -> 87[3] [send] via NET/AWS Libfabric/3
16: nid006554:221584:222798 [3] NCCL INFO Channel 03/0 : 67[3] -> 66[2] via P2P/CUMEM
 3: nid006498:226767:227937 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:217925 [2] NCCL INFO Channel 07/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222272:223486 [1] NCCL INFO Channel 06/0 : 89[1] -> 93[1] [send] via NET/AWS Libfabric/1
12: nid006508:205769:207010 [3] NCCL INFO Channel 01/0 : 47[3] -> 51[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201790:202978 [2] NCCL INFO Channel 02/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209624 [2] NCCL INFO Channel 02/0 : 78[2] -> 82[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223487 [2] NCCL INFO Channel 06/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227575:228744 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201789:202980 [1] NCCL INFO Channel 06/0 : 49[1] -> 53[1] [receive] via NET/AWS Libfabric/1
20: nid006558:215467:216702 [0] NCCL INFO Channel 05/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
15: nid006553:223925:225110 [2] NCCL INFO Channel 03/0 : 62[2] -> 60[0] via P2P/CUMEM
12: nid006508:205766:207008 [0] NCCL INFO Channel 00/0 : 44[0] -> 48[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215470:216704 [3] NCCL INFO Channel 04/0 : 83[3] -> 87[3] [send] via NET/AWS Libfabric/3
27: nid006566:216748:217925 [2] NCCL INFO Channel 02/0 : 110[2] -> 114[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223487 [2] NCCL INFO Channel 03/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227578:228747 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
19: nid006557:208419:209624 [2] NCCL INFO Channel 06/0 : 78[2] -> 82[2] [send] via NET/AWS Libfabric/2
12: nid006508:205769:207010 [3] NCCL INFO Channel 05/0 : 47[3] -> 51[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201790:202978 [2] NCCL INFO Channel 06/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
13: nid006509:201789:202980 [1] NCCL INFO Channel 03/0 : 53[1] -> 57[1] [send] via NET/AWS Libfabric/1
27: nid006566:216748:217925 [2] NCCL INFO Channel 06/0 : 110[2] -> 114[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:207008 [0] NCCL INFO Channel 04/0 : 44[0] -> 48[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222273:223487 [2] NCCL INFO Channel 07/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227575:228744 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
10: nid006506:263728:264925 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205769:207010 [3] NCCL INFO Channel 00/0 : 51[3] -> 55[3] [send] via NET/AWS Libfabric/3
13: nid006509:201789:202980 [1] NCCL INFO Channel 07/0 : 53[1] -> 57[1] [send] via NET/AWS Libfabric/1
17: nid006555:206594:207840 [1] NCCL INFO Channel 02/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206596:207842 [3] NCCL INFO Channel 06/0 : 71[3] -> 70[2] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 02/0 : 113[1] -> 117[1] [receive] via NET/AWS Libfabric/1
 2: nid006497:227578:228747 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
10: nid006506:263729:264927 [2] NCCL INFO Channel 02/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227575:228744 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
10: nid006506:263728:264925 [1] NCCL INFO Channel 07/0 : 37[1] -> 41[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205766:207008 [0] NCCL INFO Channel 01/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241021:242231 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 07/0 : 62[2] -> 60[0] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 04/0 : 51[3] -> 55[3] [send] via NET/AWS Libfabric/3
 6: nid006501:221942:223144 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
 1: nid006496:242586:243809 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 02/0 : 74[2] -> 72[0] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 02/0 : 94[2] -> 98[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:221954 [2] NCCL INFO Channel 03/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220712:221952 [1] NCCL INFO Channel 02/0 : 89[1] -> 93[1] [receive] via NET/AWS Libfabric/1
26: nid006565:222467:223694 [2] NCCL INFO Channel 02/0 : 106[2] -> 104[0] via P2P/CUMEM
21: nid006559:211125:212335 [2] NCCL INFO Channel 03/0 : 86[2] -> 84[0] via P2P/CUMEM
 2: nid006497:227578:228747 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
13: nid006509:201790:202978 [2] NCCL INFO Channel 03/0 : 54[2] -> 52[0] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
 1: nid006496:242587:243808 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
14: nid006510:229444:230647 [2] NCCL INFO Channel 02/0 : 58[2] -> 56[0] via P2P/CUMEM
27: nid006566:216748:217925 [2] NCCL INFO Channel 03/0 : 110[2] -> 108[0] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 01/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227577:228746 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
20: nid006558:215469:216705 [2] NCCL INFO Channel 02/0 : 82[2] -> 80[0] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 06/0 : 94[2] -> 98[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:221954 [2] NCCL INFO Channel 07/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220712:221952 [1] NCCL INFO Channel 06/0 : 89[1] -> 93[1] [receive] via NET/AWS Libfabric/1
19: nid006557:208419:209624 [2] NCCL INFO Channel 03/0 : 78[2] -> 76[0] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 06/0 : 113[1] -> 117[1] [receive] via NET/AWS Libfabric/1
10: nid006506:263728:264925 [1] NCCL INFO Channel 02/0 : 41[1] -> 45[1] [send] via NET/AWS Libfabric/1
10: nid006506:263729:264927 [2] NCCL INFO Channel 06/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205766:207008 [0] NCCL INFO Channel 05/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221942:223144 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
 1: nid006496:242587:243808 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221142:222364 [2] NCCL INFO Channel 03/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
29: nid007305:27224:28383 [2] NCCL INFO Channel 03/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:28380 [0] NCCL INFO Channel 05/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261300 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:221954 [2] NCCL INFO Channel 02/0 : 94[2] -> 98[2] [send] via NET/AWS Libfabric/2
29: nid007305:27223:28381 [1] NCCL INFO Channel 03/0 : 117[1] -> 121[1] [send] via NET/AWS Libfabric/1
30: nid007318:20581:21773 [1] NCCL INFO Channel 03/0 : 117[1] -> 121[1] [receive] via NET/AWS Libfabric/1
10: nid006506:263728:264925 [1] NCCL INFO Channel 06/0 : 41[1] -> 45[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221942:223144 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
 1: nid006496:242587:243808 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241021:242231 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
16: nid006554:221582:222799 [1] NCCL INFO Channel 01/0 : 65[1] -> 64[0] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 03/0 : 93[1] -> 97[1] [send] via NET/AWS Libfabric/1
27: nid006566:216746:217924 [0] NCCL INFO Channel 01/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263729:264927 [2] NCCL INFO Channel 03/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242587:243808 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
16: nid006554:221584:222798 [3] NCCL INFO Channel 05/0 : 67[3] -> 66[2] via P2P/CUMEM
 5: nid006500:260083:261300 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:222364 [2] NCCL INFO Channel 07/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
29: nid007305:27222:28380 [0] NCCL INFO Channel 00/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
30: nid007318:20581:21773 [1] NCCL INFO Channel 07/0 : 117[1] -> 121[1] [receive] via NET/AWS Libfabric/1
 1: nid006496:242586:243809 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 06/0 : 74[2] -> 72[0] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 03/0 : 93[1] -> 97[1] [receive] via NET/AWS Libfabric/1
23: nid006561:220713:221954 [2] NCCL INFO Channel 06/0 : 94[2] -> 98[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209624 [2] NCCL INFO Channel 07/0 : 78[2] -> 76[0] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 07/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27223:28381 [1] NCCL INFO Channel 07/0 : 117[1] -> 121[1] [send] via NET/AWS Libfabric/1
20: nid006558:215469:216705 [2] NCCL INFO Channel 06/0 : 82[2] -> 80[0] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 01/0 : 95[3] -> 99[3] [receive] via NET/AWS Libfabric/3
23: nid006561:220712:221952 [1] NCCL INFO Channel 07/0 : 93[1] -> 97[1] [send] via NET/AWS Libfabric/1
26: nid006565:222467:223694 [2] NCCL INFO Channel 06/0 : 106[2] -> 104[0] via P2P/CUMEM
27: nid006566:216746:217924 [0] NCCL INFO Channel 05/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27225:28382 [3] NCCL INFO Channel 00/0 : 115[3] -> 119[3] [receive] via NET/AWS Libfabric/3
30: nid007318:20581:21773 [1] NCCL INFO Channel 02/0 : 121[1] -> 125[1] [send] via NET/AWS Libfabric/1
10: nid006506:263729:264927 [2] NCCL INFO Channel 07/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
22: nid006560:222271:223485 [0] NCCL INFO Channel 00/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211337:212514 [2] NCCL INFO Channel 03/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218411:219604 [1] NCCL INFO Channel 03/0 : 29[1] -> 33[1] [receive] via NET/AWS Libfabric/1
14: nid006510:229444:230647 [2] NCCL INFO Channel 06/0 : 58[2] -> 56[0] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221141:222363 [1] NCCL INFO Channel 07/0 : 93[1] -> 97[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226768:227938 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
27: nid006566:216748:217925 [2] NCCL INFO Channel 07/0 : 110[2] -> 108[0] via P2P/CUMEM
29: nid007305:27222:28380 [0] NCCL INFO Channel 04/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
22: nid006560:222274:223484 [3] NCCL INFO Channel 01/0 : 87[3] -> 91[3] [receive] via NET/AWS Libfabric/3
 5: nid006500:260083:261300 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260086:261304 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221143:222362 [3] NCCL INFO Channel 05/0 : 95[3] -> 99[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211125:212335 [2] NCCL INFO Channel 07/0 : 86[2] -> 84[0] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 00/0 : 107[3] -> 111[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27224:28383 [2] NCCL INFO Channel 02/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
30: nid007318:20581:21773 [1] NCCL INFO Channel 06/0 : 121[1] -> 125[1] [send] via NET/AWS Libfabric/1
30: nid007318:20582:21772 [2] NCCL INFO Channel 02/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222274:223484 [3] NCCL INFO Channel 05/0 : 87[3] -> 91[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211337:212514 [2] NCCL INFO Channel 07/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:202978 [2] NCCL INFO Channel 07/0 : 54[2] -> 52[0] via P2P/CUMEM
17: nid006555:206594:207840 [1] NCCL INFO Channel 04/0 : 69[1] -> 68[0] via P2P/CUMEM
 1: nid006496:242584:243807 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218411:219604 [1] NCCL INFO Channel 07/0 : 29[1] -> 33[1] [receive] via NET/AWS Libfabric/1
24: nid006563:221141:222363 [1] NCCL INFO Channel 02/0 : 97[1] -> 101[1] [send] via NET/AWS Libfabric/1
27: nid006566:216746:217924 [0] NCCL INFO Channel 00/0 : 108[0] -> 112[0] [send] via NET/AWS Libfabric/0
29: nid007305:27225:28382 [3] NCCL INFO Channel 04/0 : 115[3] -> 119[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211337:212514 [2] NCCL INFO Channel 02/0 : 46[2] -> 50[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224215 [2] NCCL INFO Channel 03/0 : 102[2] -> 100[0] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 00/0 : 99[3] -> 103[3] [send] via NET/AWS Libfabric/3
23: nid006561:220713:221954 [2] NCCL INFO Channel 03/0 : 94[2] -> 92[0] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 02/0 : 73[1] -> 77[1] [receive] via NET/AWS Libfabric/1
27: nid006566:216749:217922 [3] NCCL INFO Channel 04/0 : 107[3] -> 111[3] [receive] via NET/AWS Libfabric/3
27: nid006566:216746:217924 [0] NCCL INFO Channel 04/0 : 108[0] -> 112[0] [send] via NET/AWS Libfabric/0
29: nid007305:27224:28383 [2] NCCL INFO Channel 06/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21772 [2] NCCL INFO Channel 06/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:223487 [2] NCCL INFO Channel 02/0 : 90[2] -> 88[0] via P2P/CUMEM
 2: nid006497:227578:228747 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
11: nid006507:211337:212514 [2] NCCL INFO Channel 06/0 : 46[2] -> 50[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:223145 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218411:219604 [1] NCCL INFO Channel 02/0 : 33[1] -> 37[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260083:261300 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260086:261304 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
24: nid006563:221141:222363 [1] NCCL INFO Channel 06/0 : 97[1] -> 101[1] [send] via NET/AWS Libfabric/1
19: nid006557:208418:209625 [1] NCCL INFO Channel 06/0 : 73[1] -> 77[1] [receive] via NET/AWS Libfabric/1
29: nid007305:27225:28382 [3] NCCL INFO Channel 01/0 : 119[3] -> 123[3] [send] via NET/AWS Libfabric/3
22: nid006560:222274:223484 [3] NCCL INFO Channel 00/0 : 91[3] -> 95[3] [send] via NET/AWS Libfabric/3
 6: nid006501:221943:223145 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:243807 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218412:219605 [2] NCCL INFO Channel 02/0 : 30[2] -> 34[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221143:222362 [3] NCCL INFO Channel 04/0 : 99[3] -> 103[3] [send] via NET/AWS Libfabric/3
27: nid006566:216749:217922 [3] NCCL INFO Channel 01/0 : 111[3] -> 115[3] [send] via NET/AWS Libfabric/3
29: nid007305:27225:28382 [3] NCCL INFO Channel 05/0 : 119[3] -> 123[3] [send] via NET/AWS Libfabric/3
30: nid007318:20582:21772 [2] NCCL INFO Channel 03/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
22: nid006560:222274:223484 [3] NCCL INFO Channel 04/0 : 91[3] -> 95[3] [send] via NET/AWS Libfabric/3
 8: nid006503:218411:219604 [1] NCCL INFO Channel 06/0 : 33[1] -> 37[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260086:261304 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
24: nid006563:221142:222364 [2] NCCL INFO Channel 02/0 : 98[2] -> 96[0] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 03/0 : 77[1] -> 81[1] [send] via NET/AWS Libfabric/1
28: nid007251:72171:73389 [2] NCCL INFO Channel 02/0 : 114[2] -> 112[0] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 07/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:223145 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242584:243807 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218412:219605 [2] NCCL INFO Channel 06/0 : 30[2] -> 34[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226768:227938 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 05/0 : 111[3] -> 115[3] [send] via NET/AWS Libfabric/3
13: nid006509:201791:202979 [3] NCCL INFO Channel 00/0 : 51[3] -> 55[3] [receive] via NET/AWS Libfabric/3
 8: nid006503:218412:219605 [2] NCCL INFO Channel 03/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224215 [2] NCCL INFO Channel 07/0 : 102[2] -> 100[0] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 07/0 : 77[1] -> 81[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221943:223145 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219605 [2] NCCL INFO Channel 07/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:222799 [1] NCCL INFO Channel 03/0 : 65[1] -> 64[0] via P2P/CUMEM
21: nid006559:211126:212336 [3] NCCL INFO Channel 03/0 : 87[3] -> 85[1] via P2P/CUMEM
23: nid006561:220713:221954 [2] NCCL INFO Channel 07/0 : 94[2] -> 92[0] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 00/0 : 75[3] -> 79[3] [receive] via NET/AWS Libfabric/3
10: nid006506:263729:264927 [2] NCCL INFO Channel 02/0 : 42[2] -> 40[0] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 02/0 : 41[1] -> 45[1] [receive] via NET/AWS Libfabric/1
11: nid006507:211337:212514 [2] NCCL INFO Channel 03/0 : 46[2] -> 44[0] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 01/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243807 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
16: nid006554:221584:222798 [3] NCCL INFO Channel 07/0 : 67[3] -> 66[2] via P2P/CUMEM
 2: nid006497:227578:228747 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
25: nid006564:223024:224216 [3] NCCL INFO Channel 03/0 : 103[3] -> 101[1] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 01/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206594:207840 [1] NCCL INFO Channel 06/0 : 69[1] -> 68[0] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 06/0 : 98[2] -> 96[0] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 03/0 : 118[2] -> 116[0] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 01/0 : 119[3] -> 123[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254559:255752 [3] NCCL INFO Channel 02/0 : 19[3] -> 17[1] via P2P/CUMEM
12: nid006508:205768:207011 [2] NCCL INFO Channel 02/0 : 50[2] -> 48[0] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 04/0 : 51[3] -> 55[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221140:222361 [0] NCCL INFO Channel 00/0 : 92[0] -> 96[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20582:21772 [2] NCCL INFO Channel 02/0 : 122[2] -> 120[0] via P2P/CUMEM
22: nid006560:222271:223485 [0] NCCL INFO Channel 04/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201791:202979 [3] NCCL INFO Channel 01/0 : 55[3] -> 59[3] [send] via NET/AWS Libfabric/3
27: nid006566:216749:217922 [3] NCCL INFO Channel 03/0 : 111[3] -> 109[1] via P2P/CUMEM
22: nid006560:222273:223487 [2] NCCL INFO Channel 06/0 : 90[2] -> 88[0] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 05/0 : 55[3] -> 59[3] [send] via NET/AWS Libfabric/3
19: nid006557:208420:209623 [3] NCCL INFO Channel 04/0 : 75[3] -> 79[3] [receive] via NET/AWS Libfabric/3
31: nid007342:59769:61252 [2] NCCL INFO Channel 03/0 : 126[2] -> 124[0] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 03/0 : 38[2] -> 36[0] via P2P/CUMEM
 5: nid006500:260085:261302 [2] NCCL INFO Channel 03/0 : 22[2] -> 20[0] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 00/0 : 91[3] -> 95[3] [receive] via NET/AWS Libfabric/3
19: nid006557:208420:209623 [3] NCCL INFO Channel 01/0 : 79[3] -> 83[3] [send] via NET/AWS Libfabric/3
28: nid007251:72171:73389 [2] NCCL INFO Channel 06/0 : 114[2] -> 112[0] via P2P/CUMEM
26: nid006565:222468:223695 [3] NCCL INFO Channel 02/0 : 107[3] -> 105[1] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 06/0 : 41[1] -> 45[1] [receive] via NET/AWS Libfabric/1
23: nid006561:220711:221951 [0] NCCL INFO Channel 05/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208420:209623 [3] NCCL INFO Channel 05/0 : 79[3] -> 83[3] [send] via NET/AWS Libfabric/3
28: nid007251:72172:73390 [3] NCCL INFO Channel 02/0 : 115[3] -> 113[1] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 05/0 : 119[3] -> 123[3] [receive] via NET/AWS Libfabric/3
22: nid006560:222271:223485 [0] NCCL INFO Channel 01/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222361 [0] NCCL INFO Channel 04/0 : 92[0] -> 96[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220714:221953 [3] NCCL INFO Channel 04/0 : 91[3] -> 95[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211126:212336 [3] NCCL INFO Channel 07/0 : 87[3] -> 85[1] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 00/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263729:264927 [2] NCCL INFO Channel 06/0 : 42[2] -> 40[0] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 03/0 : 45[1] -> 49[1] [send] via NET/AWS Libfabric/1
13: nid006509:201788:202977 [0] NCCL INFO Channel 05/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226768:227938 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
19: nid006557:208417:209622 [0] NCCL INFO Channel 01/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20583:21774 [3] NCCL INFO Channel 00/0 : 123[3] -> 127[3] [send] via NET/AWS Libfabric/3
10: nid006506:263730:264926 [3] NCCL INFO Channel 01/0 : 39[3] -> 43[3] [receive] via NET/AWS Libfabric/3
 7: nid006502:252586:253805 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221140:222361 [0] NCCL INFO Channel 01/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:221951 [0] NCCL INFO Channel 00/0 : 92[0] -> 96[0] [send] via NET/AWS Libfabric/0
30: nid007318:20583:21774 [3] NCCL INFO Channel 04/0 : 123[3] -> 127[3] [send] via NET/AWS Libfabric/3
 4: nid006499:254559:255752 [3] NCCL INFO Channel 06/0 : 19[3] -> 17[1] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223485 [0] NCCL INFO Channel 05/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227578:228747 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 07/0 : 45[1] -> 49[1] [send] via NET/AWS Libfabric/1
12: nid006508:205768:207011 [2] NCCL INFO Channel 06/0 : 50[2] -> 48[0] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 01/0 : 127[3] -> 3[3] [receive] via NET/AWS Libfabric/3
 7: nid006502:252585:253803 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
23: nid006561:220714:221953 [3] NCCL INFO Channel 01/0 : 95[3] -> 99[3] [send] via NET/AWS Libfabric/3
10: nid006506:263730:264926 [3] NCCL INFO Channel 05/0 : 39[3] -> 43[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211337:212514 [2] NCCL INFO Channel 07/0 : 46[2] -> 44[0] via P2P/CUMEM
18: nid006556:210777:212037 [3] NCCL INFO Channel 02/0 : 75[3] -> 73[1] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261302 [2] NCCL INFO Channel 07/0 : 22[2] -> 20[0] via P2P/CUMEM
23: nid006561:220711:221951 [0] NCCL INFO Channel 04/0 : 92[0] -> 96[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:209622 [0] NCCL INFO Channel 05/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21771 [0] NCCL INFO Channel 04/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:264924 [0] NCCL INFO Channel 04/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:202977 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252585:253803 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
24: nid006563:221140:222361 [0] NCCL INFO Channel 05/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
23: nid006561:220714:221953 [3] NCCL INFO Channel 05/0 : 95[3] -> 99[3] [send] via NET/AWS Libfabric/3
19: nid006557:208417:209622 [0] NCCL INFO Channel 00/0 : 76[0] -> 80[0] [send] via NET/AWS Libfabric/0
29: nid007305:27225:28382 [3] NCCL INFO Channel 03/0 : 119[3] -> 117[1] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 00/0 : 43[3] -> 47[3] [send] via NET/AWS Libfabric/3
17: nid006555:206595:207841 [2] NCCL INFO Channel 00/0 : 70[2] -> 69[1] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 02/0 : 83[3] -> 81[1] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 05/0 : 127[3] -> 3[3] [receive] via NET/AWS Libfabric/3
 7: nid006502:252586:253805 [2] NCCL INFO Channel 02/0 : 30[2] -> 34[2] [send] via NET/AWS Libfabric/2
25: nid006564:223024:224216 [3] NCCL INFO Channel 07/0 : 103[3] -> 101[1] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 07/0 : 118[2] -> 116[0] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 07/0 : 38[2] -> 36[0] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
 7: nid006502:252585:253803 [1] NCCL INFO Channel 03/0 : 29[1] -> 33[1] [send] via NET/AWS Libfabric/1
19: nid006557:208417:209622 [0] NCCL INFO Channel 04/0 : 76[0] -> 80[0] [send] via NET/AWS Libfabric/0
21: nid006559:211126:212336 [3] NCCL INFO Channel 00/0 : 87[3] -> 86[2] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 07/0 : 111[3] -> 109[1] via P2P/CUMEM
31: nid007342:59769:61252 [2] NCCL INFO Channel 07/0 : 126[2] -> 124[0] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 06/0 : 122[2] -> 120[0] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
10: nid006506:263730:264926 [3] NCCL INFO Channel 04/0 : 43[3] -> 47[3] [send] via NET/AWS Libfabric/3
22: nid006560:222274:223484 [3] NCCL INFO Channel 02/0 : 91[3] -> 89[1] via P2P/CUMEM
13: nid006509:201788:202977 [0] NCCL INFO Channel 04/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:253805 [2] NCCL INFO Channel 06/0 : 30[2] -> 34[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218413:219606 [3] NCCL INFO Channel 01/0 : 31[3] -> 35[3] [receive] via NET/AWS Libfabric/3
16: nid006554:221582:222799 [1] NCCL INFO Channel 05/0 : 65[1] -> 64[0] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 03/0 : 79[3] -> 77[1] via P2P/CUMEM
26: nid006565:222468:223695 [3] NCCL INFO Channel 06/0 : 107[3] -> 105[1] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 02/0 : 123[3] -> 121[1] via P2P/CUMEM
10: nid006506:263727:264924 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241022:242233 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
 7: nid006502:252585:253803 [1] NCCL INFO Channel 07/0 : 29[1] -> 33[1] [send] via NET/AWS Libfabric/1
 8: nid006503:218413:219606 [3] NCCL INFO Channel 05/0 : 31[3] -> 35[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221143:222362 [3] NCCL INFO Channel 02/0 : 99[3] -> 97[1] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 03/0 : 95[3] -> 93[1] via P2P/CUMEM
 4: nid006499:254559:255752 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/CUMEM
17: nid006555:206595:207841 [2] NCCL INFO Channel 04/0 : 70[2] -> 69[1] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 00/0 : 28[0] -> 32[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21771 [0] NCCL INFO Channel 01/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218413:219606 [3] NCCL INFO Channel 00/0 : 35[3] -> 39[3] [send] via NET/AWS Libfabric/3
18: nid006556:210777:212037 [3] NCCL INFO Channel 06/0 : 75[3] -> 73[1] via P2P/CUMEM
 6: nid006501:221943:223145 [2] NCCL INFO Channel 02/0 : 26[2] -> 24[0] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Channel 06/0 : 115[3] -> 113[1] via P2P/CUMEM
30: nid007318:20580:21771 [0] NCCL INFO Channel 05/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
22: nid006560:222274:223484 [3] NCCL INFO Channel 06/0 : 91[3] -> 89[1] via P2P/CUMEM
31: nid007342:59770:61254 [3] NCCL INFO Channel 03/0 : 127[3] -> 125[1] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 04/0 : 28[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218413:219606 [3] NCCL INFO Channel 04/0 : 35[3] -> 39[3] [send] via NET/AWS Libfabric/3
 2: nid006497:227578:228747 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 03/0 : 30[2] -> 28[0] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 00/0 : 111[3] -> 110[2] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 06/0 : 83[3] -> 81[1] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
25: nid006564:223024:224216 [3] NCCL INFO Channel 00/0 : 103[3] -> 102[2] via P2P/CUMEM
 1: nid006496:242587:243808 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 01/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
16: nid006554:221582:222799 [1] NCCL INFO Channel 07/0 : 65[1] -> 64[0] via P2P/CUMEM
 3: nid006498:226768:227938 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 02/0 : 34[2] -> 32[0] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 06/0 : 99[3] -> 97[1] via P2P/CUMEM
21: nid006559:211126:212336 [3] NCCL INFO Channel 02/0 : 87[3] -> 86[2] via P2P/CUMEM
29: nid007305:27225:28382 [3] NCCL INFO Channel 07/0 : 119[3] -> 117[1] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 06/0 : 123[3] -> 121[1] via P2P/CUMEM
 8: nid006503:218410:219603 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
23: nid006561:220714:221953 [3] NCCL INFO Channel 07/0 : 95[3] -> 93[1] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 07/0 : 79[3] -> 77[1] via P2P/CUMEM
 9: nid006505:249083:250306 [3] NCCL INFO Channel 03/0 : 39[3] -> 37[1] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 00/0 : 124[0] -> 0[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:223143 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222468:223695 [3] NCCL INFO Channel 01/0 : 107[3] -> 106[2] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 01/0 : 66[2] -> 65[1] via P2P/CUMEM
18: nid006556:210777:212037 [3] NCCL INFO Channel 01/0 : 75[3] -> 74[2] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Channel 01/0 : 115[3] -> 114[2] via P2P/CUMEM
31: nid007342:59770:61254 [3] NCCL INFO Channel 07/0 : 127[3] -> 125[1] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221943:223145 [2] NCCL INFO Channel 06/0 : 26[2] -> 24[0] via P2P/CUMEM
 1: nid006496:242587:243808 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
 4: nid006499:254559:255752 [3] NCCL INFO Channel 03/0 : 19[3] -> 18[2] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 01/0 : 123[3] -> 122[2] via P2P/CUMEM
 2: nid006497:227576:228745 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
 2: nid006497:227578:228747 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241022:242233 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
25: nid006564:223024:224216 [3] NCCL INFO Channel 02/0 : 103[3] -> 102[2] via P2P/CUMEM
29: nid007305:27225:28382 [3] NCCL INFO Channel 00/0 : 119[3] -> 118[2] via P2P/CUMEM
16: nid006554:221583:222800 [2] NCCL INFO Channel 05/0 : 66[2] -> 65[1] via P2P/CUMEM
22: nid006560:222274:223484 [3] NCCL INFO Channel 01/0 : 91[3] -> 90[2] via P2P/CUMEM
 6: nid006501:221941:223143 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:253805 [2] NCCL INFO Channel 07/0 : 30[2] -> 28[0] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 01/0 : 83[3] -> 82[2] via P2P/CUMEM
11: nid006507:211335:212513 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211124:212337 [1] NCCL INFO Channel 00/0 : 85[1] -> 84[0] via P2P/CUMEM
21: nid006559:211126:212336 [3] NCCL INFO Channel 04/0 : 87[3] -> 86[2] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 00/0 : 95[3] -> 94[2] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 00/0 : 79[3] -> 78[2] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 04/0 : 124[0] -> 0[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59770:61254 [3] NCCL INFO Channel 00/0 : 127[3] -> 126[2] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 06/0 : 34[2] -> 32[0] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 01/0 : 99[3] -> 98[2] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 00/0 : 43[3] -> 47[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211335:212513 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242587:243808 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
 9: nid006505:249083:250306 [3] NCCL INFO Channel 07/0 : 39[3] -> 37[1] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
 3: nid006498:226768:227938 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 02/0 : 111[3] -> 110[2] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
 3: nid006498:226766:227939 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211338:212512 [3] NCCL INFO Channel 04/0 : 43[3] -> 47[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211335:212513 [0] NCCL INFO Channel 00/0 : 44[0] -> 48[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221944:223146 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
18: nid006556:210777:212037 [3] NCCL INFO Channel 03/0 : 75[3] -> 74[2] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 00/0 : 101[1] -> 100[0] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 03/0 : 123[3] -> 122[2] via P2P/CUMEM
25: nid006564:223024:224216 [3] NCCL INFO Channel 04/0 : 103[3] -> 102[2] via P2P/CUMEM
29: nid007305:27225:28382 [3] NCCL INFO Channel 02/0 : 119[3] -> 118[2] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
 2: nid006497:227576:228745 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
22: nid006560:222274:223484 [3] NCCL INFO Channel 03/0 : 91[3] -> 90[2] via P2P/CUMEM
26: nid006565:222468:223695 [3] NCCL INFO Channel 03/0 : 107[3] -> 106[2] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 01/0 : 17[1] -> 16[0] via P2P/CUMEM
31: nid007342:59770:61254 [3] NCCL INFO Channel 02/0 : 127[3] -> 126[2] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Channel 03/0 : 115[3] -> 114[2] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 01/0 : 47[3] -> 51[3] [send] via NET/AWS Libfabric/3
11: nid006507:211335:212513 [0] NCCL INFO Channel 04/0 : 44[0] -> 48[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242587:243808 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
 4: nid006499:254559:255752 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227578:228747 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 03/0 : 23[3] -> 21[1] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 05/0 : 47[3] -> 51[3] [send] via NET/AWS Libfabric/3
20: nid006558:215470:216704 [3] NCCL INFO Channel 03/0 : 83[3] -> 82[2] via P2P/CUMEM
21: nid006559:211124:212337 [1] NCCL INFO Channel 02/0 : 85[1] -> 84[0] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 02/0 : 95[3] -> 94[2] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 03/0 : 99[3] -> 98[2] via P2P/CUMEM
21: nid006559:211126:212336 [3] NCCL INFO Channel 06/0 : 87[3] -> 86[2] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249083:250306 [3] NCCL INFO Channel 00/0 : 39[3] -> 38[2] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 00/0 : 117[1] -> 116[0] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 02/0 : 79[3] -> 78[2] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 00/0 : 109[1] -> 108[0] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 02/0 : 43[3] -> 41[1] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242585:243810 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
 3: nid006498:226768:227938 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 00/0 : 125[1] -> 124[0] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 04/0 : 111[3] -> 110[2] via P2P/CUMEM
 0: nid006495:241019:242230 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
11: nid006507:211338:212512 [3] NCCL INFO Channel 03/0 : 47[3] -> 45[1] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 02/0 : 51[3] -> 49[1] via P2P/CUMEM
 1: nid006496:242587:243808 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 00/0 : 28[0] -> 32[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226766:227939 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 02/0 : 101[1] -> 100[0] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 07/0 : 23[3] -> 21[1] via P2P/CUMEM
31: nid007342:59770:61254 [3] NCCL INFO Channel 04/0 : 127[3] -> 126[2] via P2P/CUMEM
29: nid007305:27225:28382 [3] NCCL INFO Channel 04/0 : 119[3] -> 118[2] via P2P/CUMEM
30: nid007318:20581:21773 [1] NCCL INFO Channel 01/0 : 121[1] -> 120[0] via P2P/CUMEM
 0: nid006495:241020:242232 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 01/0 : 73[1] -> 72[0] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223024:224216 [3] NCCL INFO Channel 06/0 : 103[3] -> 102[2] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 01/0 : 105[1] -> 104[0] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 05/0 : 123[3] -> 122[2] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
18: nid006556:210777:212037 [3] NCCL INFO Channel 05/0 : 75[3] -> 74[2] via P2P/CUMEM
 7: nid006502:252584:253802 [0] NCCL INFO Channel 04/0 : 28[0] -> 32[0] [send] via NET/AWS Libfabric/0
28: nid007251:72170:73388 [1] NCCL INFO Channel 01/0 : 113[1] -> 112[0] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 01/0 : 89[1] -> 88[0] via P2P/CUMEM
 2: nid006497:227576:228745 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 01/0 : 81[1] -> 80[0] via P2P/CUMEM
26: nid006565:222468:223695 [3] NCCL INFO Channel 05/0 : 107[3] -> 106[2] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Channel 05/0 : 115[3] -> 114[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254559:255752 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 06/0 : 43[3] -> 41[1] via P2P/CUMEM
22: nid006560:222274:223484 [3] NCCL INFO Channel 05/0 : 91[3] -> 90[2] via P2P/CUMEM
 1: nid006496:242585:243810 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 05/0 : 83[3] -> 82[2] via P2P/CUMEM
 1: nid006496:242587:243808 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 01/0 : 97[1] -> 96[0] via P2P/CUMEM
21: nid006559:211124:212337 [1] NCCL INFO Channel 04/0 : 85[1] -> 84[0] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 02/0 : 125[1] -> 124[0] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 00/0 : 93[1] -> 92[0] via P2P/CUMEM
31: nid007342:59770:61254 [3] NCCL INFO Channel 06/0 : 127[3] -> 126[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 01/0 : 31[3] -> 35[3] [send] via NET/AWS Libfabric/3
27: nid006566:216747:217923 [1] NCCL INFO Channel 02/0 : 109[1] -> 108[0] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 04/0 : 95[3] -> 94[2] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 05/0 : 99[3] -> 98[2] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 02/0 : 117[1] -> 116[0] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 07/0 : 47[3] -> 45[1] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 06/0 : 51[3] -> 49[1] via P2P/CUMEM
 0: nid006495:241020:242232 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 05/0 : 31[3] -> 35[3] [send] via NET/AWS Libfabric/3
 3: nid006498:226767:227937 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
 9: nid006505:249083:250306 [3] NCCL INFO Channel 02/0 : 39[3] -> 38[2] via P2P/CUMEM
 0: nid006495:241022:242233 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 00/0 : 77[1] -> 76[0] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 04/0 : 79[3] -> 78[2] via P2P/CUMEM
27: nid006566:216749:217922 [3] NCCL INFO Channel 06/0 : 111[3] -> 110[2] via P2P/CUMEM
29: nid007305:27225:28382 [3] NCCL INFO Channel 06/0 : 119[3] -> 118[2] via P2P/CUMEM
30: nid007318:20583:21774 [3] NCCL INFO Channel 07/0 : 123[3] -> 122[2] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 04/0 : 101[1] -> 100[0] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 00/0 : 23[3] -> 22[2] via P2P/CUMEM
 3: nid006498:226766:227939 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 02/0 : 27[3] -> 25[1] via P2P/CUMEM
 1: nid006496:242585:243810 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
 2: nid006497:227576:228745 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 03/0 : 105[1] -> 104[0] via P2P/CUMEM
30: nid007318:20581:21773 [1] NCCL INFO Channel 03/0 : 121[1] -> 120[0] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 01/0 : 43[3] -> 42[2] via P2P/CUMEM
26: nid006565:222468:223695 [3] NCCL INFO Channel 07/0 : 107[3] -> 106[2] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 03/0 : 81[1] -> 80[0] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 02/0 : 35[3] -> 33[1] via P2P/CUMEM
 3: nid006498:226767:227937 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 03/0 : 73[1] -> 72[0] via P2P/CUMEM
28: nid007251:72170:73388 [1] NCCL INFO Channel 03/0 : 113[1] -> 112[0] via P2P/CUMEM
18: nid006556:210777:212037 [3] NCCL INFO Channel 07/0 : 75[3] -> 74[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 03/0 : 31[3] -> 29[1] via P2P/CUMEM
21: nid006559:211124:212337 [1] NCCL INFO Channel 06/0 : 85[1] -> 84[0] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 04/0 : 125[1] -> 124[0] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 03/0 : 89[1] -> 88[0] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 01/0 : 51[3] -> 50[2] via P2P/CUMEM
 4: nid006499:254558:255753 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/CUMEM
 3: nid006498:226766:227939 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Channel 07/0 : 115[3] -> 114[2] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 03/0 : 97[1] -> 96[0] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/CUMEM
22: nid006560:222274:223484 [3] NCCL INFO Channel 07/0 : 91[3] -> 90[2] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 06/0 : 27[3] -> 25[1] via P2P/CUMEM
 1: nid006496:242585:243810 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
20: nid006558:215470:216704 [3] NCCL INFO Channel 07/0 : 83[3] -> 82[2] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241020:242232 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 01/0 : 55[3] -> 59[3] [receive] via NET/AWS Libfabric/3
23: nid006561:220712:221952 [1] NCCL INFO Channel 02/0 : 93[1] -> 92[0] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 04/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:217925 [2] NCCL INFO Channel 00/0 : 110[2] -> 109[1] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 00/0 : 47[3] -> 46[2] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 06/0 : 35[3] -> 33[1] via P2P/CUMEM
31: nid007342:59768:61253 [1] NCCL INFO Channel 06/0 : 125[1] -> 124[0] via P2P/CUMEM
25: nid006564:223022:224217 [1] NCCL INFO Channel 06/0 : 101[1] -> 100[0] via P2P/CUMEM
23: nid006561:220714:221953 [3] NCCL INFO Channel 06/0 : 95[3] -> 94[2] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 04/0 : 109[1] -> 108[0] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 04/0 : 117[1] -> 116[0] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 07/0 : 31[3] -> 29[1] via P2P/CUMEM
 0: nid006495:241020:242232 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 00/0 : 37[1] -> 36[0] via P2P/CUMEM
19: nid006557:208420:209623 [3] NCCL INFO Channel 06/0 : 79[3] -> 78[2] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 05/0 : 55[3] -> 59[3] [receive] via NET/AWS Libfabric/3
14: nid006510:229442:230646 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227577:228746 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
24: nid006563:221143:222362 [3] NCCL INFO Channel 07/0 : 99[3] -> 98[2] via P2P/CUMEM
 9: nid006505:249083:250306 [3] NCCL INFO Channel 04/0 : 39[3] -> 38[2] via P2P/CUMEM
28: nid007251:72171:73389 [2] NCCL INFO Channel 01/0 : 114[2] -> 113[1] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 02/0 : 77[1] -> 76[0] via P2P/CUMEM
 4: nid006499:254558:255753 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/CUMEM
 4: nid006499:254557:255751 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 05/0 : 105[1] -> 104[0] via P2P/CUMEM
14: nid006510:229442:230646 [0] NCCL INFO Channel 05/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
14: nid006510:229445:230649 [3] NCCL INFO Channel 00/0 : 59[3] -> 63[3] [send] via NET/AWS Libfabric/3
30: nid007318:20581:21773 [1] NCCL INFO Channel 05/0 : 121[1] -> 120[0] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 05/0 : 73[1] -> 72[0] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 02/0 : 23[3] -> 22[2] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 03/0 : 43[3] -> 42[2] via P2P/CUMEM
27: nid006566:216748:217925 [2] NCCL INFO Channel 04/0 : 110[2] -> 109[1] via P2P/CUMEM
 2: nid006497:227577:228746 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 01/0 : 106[2] -> 105[1] via P2P/CUMEM
21: nid006559:211125:212335 [2] NCCL INFO Channel 00/0 : 86[2] -> 85[1] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 05/0 : 81[1] -> 80[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 04/0 : 59[3] -> 63[3] [send] via NET/AWS Libfabric/3
28: nid007251:72170:73388 [1] NCCL INFO Channel 05/0 : 113[1] -> 112[0] via P2P/CUMEM
27: nid006566:216747:217923 [1] NCCL INFO Channel 06/0 : 109[1] -> 108[0] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 05/0 : 89[1] -> 88[0] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 05/0 : 97[1] -> 96[0] via P2P/CUMEM
21: nid006559:211125:212335 [2] NCCL INFO Channel 04/0 : 86[2] -> 85[1] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 03/0 : 51[3] -> 50[2] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 01/0 : 74[2] -> 73[1] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 03/0 : 55[3] -> 53[1] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/CUMEM
29: nid007305:27223:28381 [1] NCCL INFO Channel 06/0 : 117[1] -> 116[0] via P2P/CUMEM
18: nid006556:210775:212034 [1] NCCL INFO Channel 07/0 : 73[1] -> 72[0] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 00/0 : 102[2] -> 101[1] via P2P/CUMEM
28: nid007251:72171:73389 [2] NCCL INFO Channel 05/0 : 114[2] -> 113[1] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 02/0 : 59[3] -> 57[1] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 03/0 : 63[3] -> 61[1] via P2P/CUMEM
28: nid007251:72170:73388 [1] NCCL INFO Channel 07/0 : 113[1] -> 112[0] via P2P/CUMEM
20: nid006558:215469:216705 [2] NCCL INFO Channel 01/0 : 82[2] -> 81[1] via P2P/CUMEM
23: nid006561:220713:221954 [2] NCCL INFO Channel 00/0 : 94[2] -> 93[1] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 02/0 : 37[1] -> 36[0] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 01/0 : 35[3] -> 34[2] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 02/0 : 47[3] -> 46[2] via P2P/CUMEM
20: nid006558:215468:216703 [1] NCCL INFO Channel 07/0 : 81[1] -> 80[0] via P2P/CUMEM
30: nid007318:20581:21773 [1] NCCL INFO Channel 07/0 : 121[1] -> 120[0] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 04/0 : 93[1] -> 92[0] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 00/0 : 31[3] -> 30[2] via P2P/CUMEM
25: nid006564:223023:224215 [2] NCCL INFO Channel 04/0 : 102[2] -> 101[1] via P2P/CUMEM
26: nid006565:222466:223696 [1] NCCL INFO Channel 07/0 : 105[1] -> 104[0] via P2P/CUMEM
19: nid006557:208419:209624 [2] NCCL INFO Channel 00/0 : 78[2] -> 77[1] via P2P/CUMEM
26: nid006565:222467:223694 [2] NCCL INFO Channel 05/0 : 106[2] -> 105[1] via P2P/CUMEM
22: nid006560:222273:223487 [2] NCCL INFO Channel 01/0 : 90[2] -> 89[1] via P2P/CUMEM
 9: nid006505:249083:250306 [3] NCCL INFO Channel 06/0 : 39[3] -> 38[2] via P2P/CUMEM
22: nid006560:222272:223486 [1] NCCL INFO Channel 07/0 : 89[1] -> 88[0] via P2P/CUMEM
18: nid006556:210776:212036 [2] NCCL INFO Channel 05/0 : 74[2] -> 73[1] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 04/0 : 77[1] -> 76[0] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 01/0 : 41[1] -> 40[0] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 05/0 : 43[3] -> 42[2] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 00/0 : 21[1] -> 20[0] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 04/0 : 23[3] -> 22[2] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 07/0 : 55[3] -> 53[1] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 01/0 : 98[2] -> 97[1] via P2P/CUMEM
24: nid006563:221141:222363 [1] NCCL INFO Channel 07/0 : 97[1] -> 96[0] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 00/0 : 118[2] -> 117[1] via P2P/CUMEM
23: nid006561:220713:221954 [2] NCCL INFO Channel 04/0 : 94[2] -> 93[1] via P2P/CUMEM
 5: nid006500:260085:261302 [2] NCCL INFO Channel 00/0 : 22[2] -> 21[1] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 01/0 : 49[1] -> 48[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 06/0 : 59[3] -> 57[1] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 07/0 : 63[3] -> 61[1] via P2P/CUMEM
23: nid006561:220712:221952 [1] NCCL INFO Channel 06/0 : 93[1] -> 92[0] via P2P/CUMEM
22: nid006560:222273:223487 [2] NCCL INFO Channel 05/0 : 90[2] -> 89[1] via P2P/CUMEM
20: nid006558:215469:216705 [2] NCCL INFO Channel 05/0 : 82[2] -> 81[1] via P2P/CUMEM
19: nid006557:208419:209624 [2] NCCL INFO Channel 04/0 : 78[2] -> 77[1] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 05/0 : 51[3] -> 50[2] via P2P/CUMEM
29: nid007305:27224:28383 [2] NCCL INFO Channel 04/0 : 118[2] -> 117[1] via P2P/CUMEM
19: nid006557:208418:209625 [1] NCCL INFO Channel 06/0 : 77[1] -> 76[0] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 01/0 : 122[2] -> 121[1] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 03/0 : 27[3] -> 26[2] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 04/0 : 37[1] -> 36[0] via P2P/CUMEM
24: nid006563:221142:222364 [2] NCCL INFO Channel 05/0 : 98[2] -> 97[1] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 02/0 : 21[1] -> 20[0] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 03/0 : 35[3] -> 34[2] via P2P/CUMEM
30: nid007318:20582:21772 [2] NCCL INFO Channel 05/0 : 122[2] -> 121[1] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 00/0 : 45[1] -> 44[0] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 04/0 : 47[3] -> 46[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 02/0 : 31[3] -> 30[2] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 00/0 : 38[2] -> 37[1] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 00/0 : 55[3] -> 54[2] via P2P/CUMEM
 5: nid006500:260086:261304 [3] NCCL INFO Channel 06/0 : 23[3] -> 22[2] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 03/0 : 41[1] -> 40[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 01/0 : 59[3] -> 58[2] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 00/0 : 63[3] -> 62[2] via P2P/CUMEM
 5: nid006500:260085:261302 [2] NCCL INFO Channel 04/0 : 22[2] -> 21[1] via P2P/CUMEM
10: nid006506:263730:264926 [3] NCCL INFO Channel 07/0 : 43[3] -> 42[2] via P2P/CUMEM
 9: nid006505:249081:250305 [1] NCCL INFO Channel 06/0 : 37[1] -> 36[0] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 03/0 : 49[1] -> 48[0] via P2P/CUMEM
31: nid007342:59769:61252 [2] NCCL INFO Channel 00/0 : 126[2] -> 125[1] via P2P/CUMEM
 9: nid006505:249082:250304 [2] NCCL INFO Channel 04/0 : 38[2] -> 37[1] via P2P/CUMEM
 1: nid006496:242586:243809 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
12: nid006508:205769:207010 [3] NCCL INFO Channel 07/0 : 51[3] -> 50[2] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 04/0 : 21[1] -> 20[0] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
31: nid007342:59769:61252 [2] NCCL INFO Channel 04/0 : 126[2] -> 125[1] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/CUMEM
 1: nid006496:242586:243809 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 00/0 : 29[1] -> 28[0] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/CUMEM
 0: nid006495:241021:242231 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 02/0 : 45[1] -> 44[0] via P2P/CUMEM
 8: nid006503:218411:219604 [1] NCCL INFO Channel 01/0 : 33[1] -> 32[0] via P2P/CUMEM
11: nid006507:211338:212512 [3] NCCL INFO Channel 06/0 : 47[3] -> 46[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 04/0 : 31[3] -> 30[2] via P2P/CUMEM
10: nid006506:263729:264927 [2] NCCL INFO Channel 01/0 : 42[2] -> 41[1] via P2P/CUMEM
 6: nid006501:221943:223145 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 05/0 : 35[3] -> 34[2] via P2P/CUMEM
 5: nid006500:260084:261301 [1] NCCL INFO Channel 06/0 : 21[1] -> 20[0] via P2P/CUMEM
12: nid006508:205768:207011 [2] NCCL INFO Channel 01/0 : 50[2] -> 49[1] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 02/0 : 55[3] -> 54[2] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 01/0 : 34[2] -> 33[1] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 05/0 : 41[1] -> 40[0] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 02/0 : 63[3] -> 62[2] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 05/0 : 49[1] -> 48[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 03/0 : 59[3] -> 58[2] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 00/0 : 30[2] -> 29[1] via P2P/CUMEM
10: nid006506:263729:264927 [2] NCCL INFO Channel 05/0 : 42[2] -> 41[1] via P2P/CUMEM
10: nid006506:263728:264925 [1] NCCL INFO Channel 07/0 : 41[1] -> 40[0] via P2P/CUMEM
 6: nid006501:221944:223146 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/CUMEM
 6: nid006501:221943:223145 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/CUMEM
12: nid006508:205768:207011 [2] NCCL INFO Channel 05/0 : 50[2] -> 49[1] via P2P/CUMEM
 8: nid006503:218411:219604 [1] NCCL INFO Channel 03/0 : 33[1] -> 32[0] via P2P/CUMEM
11: nid006507:211337:212514 [2] NCCL INFO Channel 00/0 : 46[2] -> 45[1] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 02/0 : 29[1] -> 28[0] via P2P/CUMEM
12: nid006508:205767:207009 [1] NCCL INFO Channel 07/0 : 49[1] -> 48[0] via P2P/CUMEM
 8: nid006503:218413:219606 [3] NCCL INFO Channel 07/0 : 35[3] -> 34[2] via P2P/CUMEM
 7: nid006502:252587:253804 [3] NCCL INFO Channel 06/0 : 31[3] -> 30[2] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 04/0 : 45[1] -> 44[0] via P2P/CUMEM
 8: nid006503:218412:219605 [2] NCCL INFO Channel 05/0 : 34[2] -> 33[1] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 00/0 : 53[1] -> 52[0] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 04/0 : 55[3] -> 54[2] via P2P/CUMEM
11: nid006507:211337:212514 [2] NCCL INFO Channel 04/0 : 46[2] -> 45[1] via P2P/CUMEM
 7: nid006502:252586:253805 [2] NCCL INFO Channel 04/0 : 30[2] -> 29[1] via P2P/CUMEM
11: nid006507:211336:212515 [1] NCCL INFO Channel 06/0 : 45[1] -> 44[0] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 04/0 : 63[3] -> 62[2] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 01/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 05/0 : 59[3] -> 58[2] via P2P/CUMEM
 8: nid006503:218411:219604 [1] NCCL INFO Channel 05/0 : 33[1] -> 32[0] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 04/0 : 29[1] -> 28[0] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 00/0 : 61[1] -> 60[0] via P2P/CUMEM
 6: nid006501:221942:223144 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/CUMEM
 8: nid006503:218411:219604 [1] NCCL INFO Channel 07/0 : 33[1] -> 32[0] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 02/0 : 53[1] -> 52[0] via P2P/CUMEM
 7: nid006502:252585:253803 [1] NCCL INFO Channel 06/0 : 29[1] -> 28[0] via P2P/CUMEM
13: nid006509:201791:202979 [3] NCCL INFO Channel 06/0 : 55[3] -> 54[2] via P2P/CUMEM
15: nid006553:223926:225111 [3] NCCL INFO Channel 06/0 : 63[3] -> 62[2] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 03/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229445:230649 [3] NCCL INFO Channel 07/0 : 59[3] -> 58[2] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 02/0 : 61[1] -> 60[0] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 04/0 : 53[1] -> 52[0] via P2P/CUMEM
14: nid006510:229444:230647 [2] NCCL INFO Channel 01/0 : 58[2] -> 57[1] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 00/0 : 62[2] -> 61[1] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 04/0 : 61[1] -> 60[0] via P2P/CUMEM
13: nid006509:201790:202978 [2] NCCL INFO Channel 00/0 : 54[2] -> 53[1] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 05/0 : 57[1] -> 56[0] via P2P/CUMEM
13: nid006509:201789:202980 [1] NCCL INFO Channel 06/0 : 53[1] -> 52[0] via P2P/CUMEM
15: nid006553:223925:225110 [2] NCCL INFO Channel 04/0 : 62[2] -> 61[1] via P2P/CUMEM
14: nid006510:229444:230647 [2] NCCL INFO Channel 05/0 : 58[2] -> 57[1] via P2P/CUMEM
15: nid006553:223924:225109 [1] NCCL INFO Channel 06/0 : 61[1] -> 60[0] via P2P/CUMEM
14: nid006510:229443:230648 [1] NCCL INFO Channel 07/0 : 57[1] -> 56[0] via P2P/CUMEM
13: nid006509:201790:202978 [2] NCCL INFO Channel 04/0 : 54[2] -> 53[1] via P2P/CUMEM
28: nid007251:72172:73390 [3] NCCL INFO Connected all rings
28: nid007251:72169:73387 [0] NCCL INFO Connected all rings
28: nid007251:72170:73388 [1] NCCL INFO Connected all rings
28: nid007251:72171:73389 [2] NCCL INFO Connected all rings
 4: nid006499:254556:255750 [0] NCCL INFO Connected all rings
 4: nid006499:254557:255751 [1] NCCL INFO Connected all rings
 4: nid006499:254559:255752 [3] NCCL INFO Connected all rings
 4: nid006499:254558:255753 [2] NCCL INFO Connected all rings
 3: nid006498:226765:227940 [0] NCCL INFO Connected all rings
 3: nid006498:226766:227939 [1] NCCL INFO Connected all rings
 3: nid006498:226768:227938 [3] NCCL INFO Connected all rings
 3: nid006498:226767:227937 [2] NCCL INFO Connected all rings
16: nid006554:221581:222797 [0] NCCL INFO Connected all rings
16: nid006554:221582:222799 [1] NCCL INFO Connected all rings
16: nid006554:221584:222798 [3] NCCL INFO Connected all rings
20: nid006558:215467:216702 [0] NCCL INFO Connected all rings
20: nid006558:215468:216703 [1] NCCL INFO Connected all rings
20: nid006558:215470:216704 [3] NCCL INFO Connected all rings
16: nid006554:221583:222800 [2] NCCL INFO Connected all rings
20: nid006558:215469:216705 [2] NCCL INFO Connected all rings
30: nid007318:20580:21771 [0] NCCL INFO Connected all rings
30: nid007318:20581:21773 [1] NCCL INFO Connected all rings
29: nid007305:27225:28382 [3] NCCL INFO Connected all rings
29: nid007305:27222:28380 [0] NCCL INFO Connected all rings
30: nid007318:20583:21774 [3] NCCL INFO Connected all rings
30: nid007318:20582:21772 [2] NCCL INFO Connected all rings
29: nid007305:27224:28383 [2] NCCL INFO Connected all rings
29: nid007305:27223:28381 [1] NCCL INFO Connected all rings
18: nid006556:210774:212035 [0] NCCL INFO Connected all rings
18: nid006556:210777:212037 [3] NCCL INFO Connected all rings
 6: nid006501:221941:223143 [0] NCCL INFO Connected all rings
17: nid006555:206596:207842 [3] NCCL INFO Connected all rings
 5: nid006500:260083:261300 [0] NCCL INFO Connected all rings
17: nid006555:206593:207843 [0] NCCL INFO Connected all rings
 6: nid006501:221942:223144 [1] NCCL INFO Connected all rings
 5: nid006500:260084:261301 [1] NCCL INFO Connected all rings
18: nid006556:210776:212036 [2] NCCL INFO Connected all rings
18: nid006556:210775:212034 [1] NCCL INFO Connected all rings
 5: nid006500:260086:261304 [3] NCCL INFO Connected all rings
17: nid006555:206595:207841 [2] NCCL INFO Connected all rings
17: nid006555:206594:207840 [1] NCCL INFO Connected all rings
 6: nid006501:221944:223146 [3] NCCL INFO Connected all rings
 5: nid006500:260085:261302 [2] NCCL INFO Connected all rings
 6: nid006501:221943:223145 [2] NCCL INFO Connected all rings
19: nid006557:208420:209623 [3] NCCL INFO Connected all rings
19: nid006557:208417:209622 [0] NCCL INFO Connected all rings
19: nid006557:208419:209624 [2] NCCL INFO Connected all rings
19: nid006557:208418:209625 [1] NCCL INFO Connected all rings
 8: nid006503:218410:219603 [0] NCCL INFO Connected all rings
 8: nid006503:218413:219606 [3] NCCL INFO Connected all rings
 7: nid006502:252587:253804 [3] NCCL INFO Connected all rings
 7: nid006502:252584:253802 [0] NCCL INFO Connected all rings
 8: nid006503:218412:219605 [2] NCCL INFO Connected all rings
 8: nid006503:218411:219604 [1] NCCL INFO Connected all rings
 7: nid006502:252585:253803 [1] NCCL INFO Connected all rings
 7: nid006502:252586:253805 [2] NCCL INFO Connected all rings
27: nid006566:216749:217922 [3] NCCL INFO Connected all rings
24: nid006563:221143:222362 [3] NCCL INFO Connected all rings
24: nid006563:221140:222361 [0] NCCL INFO Connected all rings
24: nid006563:221141:222363 [1] NCCL INFO Connected all rings
24: nid006563:221142:222364 [2] NCCL INFO Connected all rings
27: nid006566:216746:217924 [0] NCCL INFO Connected all rings
27: nid006566:216748:217925 [2] NCCL INFO Connected all rings
27: nid006566:216747:217923 [1] NCCL INFO Connected all rings
23: nid006561:220711:221951 [0] NCCL INFO Connected all rings
23: nid006561:220714:221953 [3] NCCL INFO Connected all rings
22: nid006560:222274:223484 [3] NCCL INFO Connected all rings
23: nid006561:220713:221954 [2] NCCL INFO Connected all rings
23: nid006561:220712:221952 [1] NCCL INFO Connected all rings
21: nid006559:211126:212336 [3] NCCL INFO Connected all rings
22: nid006560:222271:223485 [0] NCCL INFO Connected all rings
21: nid006559:211123:212334 [0] NCCL INFO Connected all rings
22: nid006560:222273:223487 [2] NCCL INFO Connected all rings
21: nid006559:211124:212337 [1] NCCL INFO Connected all rings
21: nid006559:211125:212335 [2] NCCL INFO Connected all rings
22: nid006560:222272:223486 [1] NCCL INFO Connected all rings
31: nid007342:59767:61251 [0] NCCL INFO Connected all rings
31: nid007342:59768:61253 [1] NCCL INFO Connected all rings
31: nid007342:59770:61254 [3] NCCL INFO Connected all rings
31: nid007342:59769:61252 [2] NCCL INFO Connected all rings
26: nid006565:222465:223693 [0] NCCL INFO Connected all rings
25: nid006564:223021:224214 [0] NCCL INFO Connected all rings
25: nid006564:223022:224217 [1] NCCL INFO Connected all rings
25: nid006564:223024:224216 [3] NCCL INFO Connected all rings
25: nid006564:223023:224215 [2] NCCL INFO Connected all rings
 9: nid006505:249080:250303 [0] NCCL INFO Connected all rings
 9: nid006505:249083:250306 [3] NCCL INFO Connected all rings
26: nid006565:222466:223696 [1] NCCL INFO Connected all rings
 9: nid006505:249082:250304 [2] NCCL INFO Connected all rings
 9: nid006505:249081:250305 [1] NCCL INFO Connected all rings
26: nid006565:222468:223695 [3] NCCL INFO Connected all rings
26: nid006565:222467:223694 [2] NCCL INFO Connected all rings
12: nid006508:205766:207008 [0] NCCL INFO Connected all rings
12: nid006508:205769:207010 [3] NCCL INFO Connected all rings
10: nid006506:263727:264924 [0] NCCL INFO Connected all rings
12: nid006508:205767:207009 [1] NCCL INFO Connected all rings
12: nid006508:205768:207011 [2] NCCL INFO Connected all rings
11: nid006507:211335:212513 [0] NCCL INFO Connected all rings
10: nid006506:263728:264925 [1] NCCL INFO Connected all rings
10: nid006506:263730:264926 [3] NCCL INFO Connected all rings
11: nid006507:211336:212515 [1] NCCL INFO Connected all rings
11: nid006507:211338:212512 [3] NCCL INFO Connected all rings
10: nid006506:263729:264927 [2] NCCL INFO Connected all rings
11: nid006507:211337:212514 [2] NCCL INFO Connected all rings
15: nid006553:223923:225108 [0] NCCL INFO Connected all rings
15: nid006553:223926:225111 [3] NCCL INFO Connected all rings
14: nid006510:229442:230646 [0] NCCL INFO Connected all rings
14: nid006510:229445:230649 [3] NCCL INFO Connected all rings
15: nid006553:223925:225110 [2] NCCL INFO Connected all rings
14: nid006510:229444:230647 [2] NCCL INFO Connected all rings
15: nid006553:223924:225109 [1] NCCL INFO Connected all rings
14: nid006510:229443:230648 [1] NCCL INFO Connected all rings
 0: nid006495:241019:242230 [0] NCCL INFO Connected all rings
 1: nid006496:242584:243807 [0] NCCL INFO Connected all rings
 0: nid006495:241022:242233 [3] NCCL INFO Connected all rings
 0: nid006495:241020:242232 [1] NCCL INFO Connected all rings
 1: nid006496:242586:243809 [2] NCCL INFO Connected all rings
 0: nid006495:241021:242231 [2] NCCL INFO Connected all rings
 1: nid006496:242587:243808 [3] NCCL INFO Connected all rings
 2: nid006497:227575:228744 [0] NCCL INFO Connected all rings
 2: nid006497:227577:228746 [2] NCCL INFO Connected all rings
 2: nid006497:227576:228745 [1] NCCL INFO Connected all rings
 2: nid006497:227578:228747 [3] NCCL INFO Connected all rings
 1: nid006496:242585:243810 [1] NCCL INFO Connected all rings
13: nid006509:201788:202977 [0] NCCL INFO Connected all rings
13: nid006509:201791:202979 [3] NCCL INFO Connected all rings
13: nid006509:201790:202978 [2] NCCL INFO Connected all rings
13: nid006509:201789:202980 [1] NCCL INFO Connected all rings
 0: Rank 0:  Loading vision tower: google/siglip-so400m-patch14-384
 0: [2025-06-24 19:27:59,247] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 1131, num_elems = 15.68B
 1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 4: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
28: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
30: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
19: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
28: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
19: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 6: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 4: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
21: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
30: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
23: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
26: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
10: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 5: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
18: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
18: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
18: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 5: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
21: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
11: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 9: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 8: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
11: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
13: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
26: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
18: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
15: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
29: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
24: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
14: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
16: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
15: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
23: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
16: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
31: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
22: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
16: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
27: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
12: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 7: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
25: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
27: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
12: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
19: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 6: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
22: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 5: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 7: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
30: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 8: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
12: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
13: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
14: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
28: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 9: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
14: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
29: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
25: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 9: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 6: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
17: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
15: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
17: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
13: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
30: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 4: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
29: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
28: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
31: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 8: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
31: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
19: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
17: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
24: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 7: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
10: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
27: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
21: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
10: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
29: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
25: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 5: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 9: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
21: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 4: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
23: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
23: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
31: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
10: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
25: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
14: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 8: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
20: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
22: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 7: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 6: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
11: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
22: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
11: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
20: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
15: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
12: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
13: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
24: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
20: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
17: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
26: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
26: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
20: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
27: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
16: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
 1: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.86s/it]
18: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.86s/it]
14: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.86s/it]
28: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
30: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
 6: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
30: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
23: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
 1: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
28: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
23: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
18: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
29: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
14: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
27: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
31: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.87s/it]
21: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
13: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
26: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
27: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
 4: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
 4: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
26: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
31: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
29: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
 2: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 7: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.88s/it]
21: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
22: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 0: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 9: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
28: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 5: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 8: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
19: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 2: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
11: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 9: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.89s/it]
 0: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
11: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
25: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
24: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
22: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
 5: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
19: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
 7: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
25: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
24: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
19: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
12: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
17: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
30: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
10: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
12: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
15: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
 8: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
 3: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.90s/it]
15: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
17: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
 3: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
17: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
16: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
16: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.91s/it]
16: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.92s/it]
 0: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.92s/it]
 6: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.93s/it]
18: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.96s/it]
 1: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.97s/it]
18: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.98s/it]
 9: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.98s/it]
 3: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.98s/it]
 6: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.99s/it]
10: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.99s/it]
14: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.99s/it]
13: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.99s/it]
 0: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  5.00s/it]
29: Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  5.00s/it]
10: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.00s/it]
10: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.01s/it]
13: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.01s/it]
12: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.02s/it]
28: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.02s/it]
 5: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.03s/it]
30: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.03s/it]
 6: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.07s/it]
24: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.09s/it]
23: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.11s/it]
 8: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.11s/it]
 5: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.11s/it]
20: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.12s/it]
27: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.15s/it]
 9: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.15s/it]
31: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.16s/it]
 7: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.16s/it]
21: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.16s/it]
 7: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.16s/it]
 1: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.17s/it]
26: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.16s/it]
11: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.17s/it]
25: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.17s/it]
27: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.17s/it]
11: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.17s/it]
15: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.18s/it]
22: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.19s/it]
 8: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.20s/it]
31: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.21s/it]
23: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.21s/it]
20: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.22s/it]
17: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.24s/it]
22: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.24s/it]
21: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.24s/it]
12: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.25s/it]
15: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.28s/it]
16: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.29s/it]
25: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.30s/it]
 4: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:15,  5.30s/it]
29: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:16,  5.52s/it]
14: Loading checkpoint shards:  25%|       | 1/4 [00:05<00:17,  5.73s/it]
19: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.06s/it]
24: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.18s/it]
 4: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.19s/it]
 2: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.19s/it]
 2: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.20s/it]
13: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.22s/it]
26: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.22s/it]
 3: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.23s/it]
20: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.25s/it]
20: Loading checkpoint shards:  25%|       | 1/4 [00:06<00:18,  6.27s/it]
18: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.64s/it]
30: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 1: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
23: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.64s/it]
 6: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
14: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
18: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
28: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
30: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
28: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
23: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
29: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 1: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
31: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
14: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
26: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 4: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
27: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
21: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
13: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 4: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
31: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 7: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
26: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
27: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
21: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 2: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
29: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
22: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
24: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 2: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 9: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.65s/it]
 5: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
19: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
28: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
11: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 7: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 5: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 8: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 0: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
22: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
25: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
25: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
24: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
19: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 9: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
19: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 0: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
30: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
10: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
11: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
17: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 3: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
12: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
12: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 8: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
17: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
15: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
15: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
16: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.66s/it]
 3: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.67s/it]
17: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.67s/it]
16: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.67s/it]
16: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.67s/it]
 6: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.68s/it]
18: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.68s/it]
 1: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.68s/it]
18: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.68s/it]
 9: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.69s/it]
10: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.69s/it]
 6: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.69s/it]
 3: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.69s/it]
13: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.69s/it]
14: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
 0: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
10: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
 0: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.71s/it]
29: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
12: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
10: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
28: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
30: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.70s/it]
13: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.71s/it]
 5: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.71s/it]
 6: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.72s/it]
24: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.73s/it]
23: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.73s/it]
 5: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.75s/it]
 8: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.75s/it]
20: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.75s/it]
27: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
 9: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
25: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
 1: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
 7: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
11: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
 7: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.77s/it]
11: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
31: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.77s/it]
26: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.77s/it]
15: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.76s/it]
21: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.77s/it]
27: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.77s/it]
 8: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.78s/it]
22: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.78s/it]
31: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.78s/it]
23: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.79s/it]
17: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.79s/it]
22: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.79s/it]
21: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.80s/it]
20: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.80s/it]
12: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.80s/it]
15: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.81s/it]
16: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.81s/it]
 4: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.82s/it]
25: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.82s/it]
29: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  5.91s/it]
14: Loading checkpoint shards:  50%|     | 2/4 [00:11<00:11,  6.00s/it]
 3: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:11,  5.97s/it]
19: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.14s/it]
 2: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.17s/it]
24: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.18s/it]
 4: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.18s/it]
 2: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.18s/it]
13: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.19s/it]
26: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.20s/it]
20: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.21s/it]
20: Loading checkpoint shards:  50%|     | 2/4 [00:12<00:12,  6.22s/it]
 1: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
28: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
28: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
18: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
 1: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
 6: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
14: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
18: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
31: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
30: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
27: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
14: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
26: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
23: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.24s/it]
30: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.24s/it]
13: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
23: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.24s/it]
29: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.24s/it]
27: Loading checkpoint shards:  75%|  | 3/4 [00:17<00:06,  6.23s/it]
21: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
26: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
31: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 4: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 2: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
21: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
22: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
19: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 9: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 0: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
29: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 7: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
11: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
22: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
24: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 2: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
28: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 5: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 0: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 9: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 8: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
19: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 5: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
10: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
11: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 7: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
25: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
24: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
25: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 3: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
30: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
16: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
12: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
17: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
17: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
19: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
15: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
15: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 8: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
17: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 3: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
16: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.24s/it]
 0: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.23s/it]
 6: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.25s/it]
18: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.25s/it]
 1: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.25s/it]
18: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.25s/it]
 9: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
 6: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.25s/it]
10: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
 3: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
14: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
13: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
 0: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
10: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
29: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
10: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.26s/it]
12: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
28: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
30: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
13: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
 5: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
 6: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.27s/it]
23: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.28s/it]
24: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.28s/it]
 8: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.28s/it]
 5: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
20: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
27: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
 1: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
 7: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
 7: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
21: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
11: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
25: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
 9: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
11: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.29s/it]
26: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
31: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
15: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
27: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
22: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
23: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.30s/it]
31: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
 8: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
20: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
21: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
22: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.31s/it]
17: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.32s/it]
12: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.32s/it]
15: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.32s/it]
16: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.32s/it]
25: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.32s/it]
 4: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.33s/it]
29: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.37s/it]
16: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.58s/it]
14: Loading checkpoint shards:  75%|  | 3/4 [00:18<00:06,  6.41s/it]
19: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.50s/it]
24: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.52s/it]
 2: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.52s/it]
 2: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.52s/it]
 4: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.53s/it]
26: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.53s/it]
13: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.53s/it]
 3: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.58s/it]
20: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.54s/it]
20: Loading checkpoint shards:  75%|  | 3/4 [00:19<00:06,  6.55s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.87s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.19s/it]
 1: 
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 6: 
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
14: 
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.87s/it]
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
18: 
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
28: 
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
28: 
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
30: 
31: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
31: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
31: 
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
14: 
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
30: 
 1: 
31: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.87s/it]
23: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
31: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
31: 
23: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
23: 
23: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
23: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
23: 
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
13: 
27: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
27: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
27: 
26: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
26: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
26: 
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
29: 
21: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
21: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
21: 
27: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
27: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
27: 
21: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
21: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
21: 
26: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
26: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
26: 
 4: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 4: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 2: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 4: 
 2: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 2: 
 4: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 4: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 4: 
22: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
22: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 9: 
 2: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 2: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 2: 
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
28: 
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
29: 
 7: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 7: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 7: 
11: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
11: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
11: 
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 9: 
 8: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
24: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 8: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
24: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 8: 
24: 
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 5: 
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
19: 
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 5: 
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
19: 
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
19: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
19: 
22: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
25: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
22: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
24: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
22: 
25: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
25: 
24: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
24: 
11: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
11: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
11: 
 7: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 7: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 7: 
16: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
16: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
25: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
25: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
25: 
 8: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 8: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.20s/it]
 8: 
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
 3: 
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
30: 
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
 0: 
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
17: 
16: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
16: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
16: 
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
 3: 
12: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
12: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
12: 
12: 
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
17: 
15: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.88s/it]
15: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
15: 
17: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
17: 
15: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
15: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
15: 
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.21s/it]
 6: 
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.22s/it]
18: 
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.22s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.22s/it]
 9: 
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
18: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.22s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.22s/it]
10: 
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
 0: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
 0: 
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
14: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
14: 
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
 3: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
 3: 
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
29: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
29: 
12: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
12: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
10: 
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
10: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
10: 
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.89s/it]
30: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
28: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.23s/it]
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
 5: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.24s/it]
 5: 
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  4.90s/it]
 6: Loading checkpoint shards: 100%|| 4/4 [00:20<00:00,  5.25s/it]
 6: 
23: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.90s/it]
23: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.25s/it]
23: 
24: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
24: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.25s/it]
24: 
 8: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
 8: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.25s/it]
 8: 
 5: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
 5: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.26s/it]
 5: 
20: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
20: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.26s/it]
20: 
31: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
31: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.26s/it]
31: 
27: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
27: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
27: 
 9: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
 9: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
 1: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
 1: 
 7: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
 7: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
 7: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
 7: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
 7: 
26: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.91s/it]
26: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
26: 
11: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
11: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
11: 
11: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
11: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
11: 
25: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
25: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
21: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
21: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
21: 
27: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
27: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
27: 
15: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
15: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.27s/it]
15: 
22: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
22: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.28s/it]
22: 
31: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
31: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.28s/it]
31: 
23: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
23: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.28s/it]
23: 
 8: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
 8: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.28s/it]
 8: 
20: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
20: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.28s/it]
20: 
21: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
21: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.29s/it]
21: 
17: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
17: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.29s/it]
17: 
22: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.93s/it]
22: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.29s/it]
12: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.92s/it]
12: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.29s/it]
12: 
15: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.93s/it]
15: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.30s/it]
15: 
13: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
25: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.93s/it]
25: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.30s/it]
25: 
 4: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.93s/it]
 4: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.30s/it]
 4: 
31: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
31: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
16: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.94s/it]
16: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.31s/it]
16: 
 6: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
22: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
28: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
19: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 6: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
26: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
23: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
28: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
28: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
23: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
30: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 5: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
14: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
21: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 5: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
21: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
14: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 4: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 4: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
25: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
27: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 9: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
30: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
30: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
19: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 9: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
19: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
16: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
10: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
25: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
27: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
29: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
29: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 8: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
24: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 8: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 7: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 7: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
15: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
15: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
10: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
12: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
12: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
24: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
17: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
17: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
17: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 9: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
14: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
13: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 6: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
29: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
30: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
28: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
13: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
10: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 0: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.06s/it]
 0: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.33s/it]
 0: 
10: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 5: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 6: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
16: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
22: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
23: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
24: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 8: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 5: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 9: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
31: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
27: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
29: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.96s/it]
29: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.36s/it]
20: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 7: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 7: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
26: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
21: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
15: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
25: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
22: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
12: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
23: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
27: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
31: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 8: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
17: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
22: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
12: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
20: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
21: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
26: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
15: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 4: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
25: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
14: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  4.99s/it]
14: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.41s/it]
14: 
 0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 0: Rank 0:  Prompt version: qwen_1_5
 0: Rank 0:  google/siglip-so400m-patch14-384 is already loaded, `load_model` called again, skipping.
 0: Rank 0:  Using mm_tunable_parts: mm_vision_tower,mm_mlp_adapter,mm_language_model
 0: Rank 0:  Total parameters: ~8030.35 MB)
 0: Rank 0:  Trainable parameters: ~8030.35 MB)
 0: Rank 0:  Loading radvlm/data/llava_datasets/combined.json
18: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
18: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
29: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
18: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
18: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
16: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
19: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.03s/it]
19: Loading checkpoint shards: 100%|| 4/4 [00:21<00:00,  5.49s/it]
19: 
14: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 2: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.05s/it]
 2: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.52s/it]
 2: 
24: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.05s/it]
24: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.52s/it]
24: 
 2: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.05s/it]
 2: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.53s/it]
 2: 
 4: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.06s/it]
 4: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.53s/it]
 4: 
 3: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.08s/it]
 3: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.53s/it]
 3: 
13: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.06s/it]
13: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.53s/it]
13: 
26: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.06s/it]
26: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.53s/it]
16: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.32s/it]
16: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.54s/it]
16: 
20: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.07s/it]
20: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.55s/it]
20: 
20: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.08s/it]
20: Loading checkpoint shards: 100%|| 4/4 [00:22<00:00,  5.55s/it]
20: 
19: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
24: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
26: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
13: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
20: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
16: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
20: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 4: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 1:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 1:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
26: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
26:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
22: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
22:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
26: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
26:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 6: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 6:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 5: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 5:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
28: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
28:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 5: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 5:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
31: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
31:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
28: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
28:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
23: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
23:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 1:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
31: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
31:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
22: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
22:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
23: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
23:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 3:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
24: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
24:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
30: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
30: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
30:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 4: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 4:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
27: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
27: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
27:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
19: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
19:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
14: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
14:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
13: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
13:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 4: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 4:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
21: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
21: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
21:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
19: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
19:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 8: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 8:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
18: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
18:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
24: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
24:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
29: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
29:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
18: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
18:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
11: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
11:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
25: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
25:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
14: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
14:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 6: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 6:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
11: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
11:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 3:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 7: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 7:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
25: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
25:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 2:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 9: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 9: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 9:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
15: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
15:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
19: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
19:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
30: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
30:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
10: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
10:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 2:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 8: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 8:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
15: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
15:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 7: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 7:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
16: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
16:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
28: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
28:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
29: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
29:   warnings.warn(
 0: 
 0: Rank 0:  Loaded 1126105 samples from radvlm/data/llava_datasets/combined.json
 0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 0:   warnings.warn(
 0: Rank 0:  Loaded 1126105 samples from radvlm/data/llava_datasets/combined.json
 0: Rank 0:  Formatting inputs...Skip in lazy mode
12: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
12: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
12:   warnings.warn(
 0: Rank 0:  Setting NCCL timeout to INF to avoid running errors.
 0: 
 0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 0:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
17: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
17:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
16: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
16:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
10: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
10:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
17: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
17:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 0:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
12: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
12:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 9: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 9:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 5: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 5:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
17: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
17:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
18: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
18:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
13: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
13:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 6: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 6:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
10: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
10: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
10:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
14: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
14:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
13: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
13:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
12: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
12:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
18: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
18:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
28: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
28:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
30: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
30:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 0:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 5: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 5:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 6: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 6:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
24: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
24:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
29: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
29:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 1:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
23: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
23:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
31: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
31:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
20: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
20:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
26: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
26:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 8: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 8:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 7: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 7:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
31: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
31:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
21: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
21:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
27: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
27:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
22: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
22:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
27: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
27:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
11: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
11: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
11:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 9: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 9:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
25: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
25:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
15: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
15:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 7: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 7:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
23: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
23:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
22: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
22:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 8: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 8:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
20: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
20:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
21: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
21:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
15: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
15:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
17: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
17:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 4: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 4:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
12: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
12:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
16: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
16:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
25: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
25:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
29: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
29:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
14: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
14:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
19: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
19:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
24: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
24:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 4: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 4:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 3:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
26: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
26:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
20: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
20: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
20:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
13: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
13:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 2:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
16: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
16:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
 2:   warnings.warn(
 1: nid006496:242586:243932 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 01/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 05/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 06/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 07/0 : 106[2] -> 107[3] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 112[0] -> 113[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 02/0 : 112[0] -> 113[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 03/0 : 112[0] -> 113[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 05/0 : 112[0] -> 113[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 06/0 : 112[0] -> 113[1] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 07/0 : 112[0] -> 113[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 02/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 03/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 05/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 06/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 07/0 : 88[0] -> 89[1] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 02/0 : 107[3] -> 104[0] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 03/0 : 107[3] -> 104[0] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 06/0 : 107[3] -> 104[0] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 07/0 : 107[3] -> 104[0] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 01/0 : 113[1] -> 114[2] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 05/0 : 113[1] -> 114[2] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 04/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
30: nid007318:20581:21915 [1] NCCL INFO Channel 01/0 : 121[1] -> 122[2] via P2P/CUMEM
30: nid007318:20581:21915 [1] NCCL INFO Channel 05/0 : 121[1] -> 122[2] via P2P/CUMEM
 4: nid006499:254557:255854 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/CUMEM
 4: nid006499:254557:255854 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[1] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[1] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[1] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 20[0] -> 21[1] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 06/0 : 20[0] -> 21[1] via P2P/CUMEM
 5: nid006500:260083:261412 [0] NCCL INFO Channel 07/0 : 20[0] -> 21[1] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 00/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 02/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 03/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 04/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 06/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 07/0 : 126[2] -> 127[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 01/0 : 58[2] -> 59[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 58[2] -> 59[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 58[2] -> 59[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 05/0 : 58[2] -> 59[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 06/0 : 58[2] -> 59[3] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 07/0 : 58[2] -> 59[3] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 00/0 : 124[0] -> 125[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 02/0 : 124[0] -> 125[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 03/0 : 124[0] -> 125[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 04/0 : 124[0] -> 125[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 06/0 : 124[0] -> 125[1] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 07/0 : 124[0] -> 125[1] via P2P/CUMEM
18: nid006556:210775:212170 [1] NCCL INFO Channel 01/0 : 73[1] -> 74[2] via P2P/CUMEM
18: nid006556:210775:212170 [1] NCCL INFO Channel 05/0 : 73[1] -> 74[2] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[1] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[1] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 52[0] -> 53[1] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 06/0 : 52[0] -> 53[1] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 07/0 : 52[0] -> 53[1] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 00/0 : 22[2] -> 23[3] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 02/0 : 22[2] -> 23[3] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 03/0 : 22[2] -> 23[3] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 04/0 : 22[2] -> 23[3] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 22[2] -> 23[3] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 22[2] -> 23[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 00/0 : 86[2] -> 87[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 02/0 : 86[2] -> 87[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 03/0 : 86[2] -> 87[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 04/0 : 86[2] -> 87[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 86[2] -> 87[3] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 86[2] -> 87[3] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 00/0 : 109[1] -> 110[2] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 04/0 : 109[1] -> 110[2] via P2P/CUMEM
29: nid007305:27223:28486 [1] NCCL INFO Channel 00/0 : 117[1] -> 118[2] via P2P/CUMEM
29: nid007305:27223:28486 [1] NCCL INFO Channel 04/0 : 117[1] -> 118[2] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 96[0] -> 97[1] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 02/0 : 96[0] -> 97[1] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 03/0 : 96[0] -> 97[1] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 05/0 : 96[0] -> 97[1] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 06/0 : 96[0] -> 97[1] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 07/0 : 96[0] -> 97[1] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 00/0 : 94[2] -> 95[3] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 02/0 : 94[2] -> 95[3] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 03/0 : 94[2] -> 95[3] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 04/0 : 94[2] -> 95[3] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 94[2] -> 95[3] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 01/0 : 74[2] -> 75[3] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 94[2] -> 95[3] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 74[2] -> 75[3] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 74[2] -> 75[3] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 05/0 : 74[2] -> 75[3] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 02/0 : 95[3] -> 92[0] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 06/0 : 74[2] -> 75[3] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 07/0 : 74[2] -> 75[3] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 03/0 : 95[3] -> 92[0] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 06/0 : 95[3] -> 92[0] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 07/0 : 95[3] -> 92[0] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 00/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 02/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 03/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 06/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 07/0 : 84[0] -> 85[1] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 01/0 : 33[1] -> 34[2] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 05/0 : 33[1] -> 34[2] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 00/0 : 76[0] -> 77[1] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 02/0 : 76[0] -> 77[1] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 03/0 : 76[0] -> 77[1] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 76[0] -> 77[1] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 06/0 : 76[0] -> 77[1] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 07/0 : 76[0] -> 77[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 02/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 03/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 05/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 06/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 07/0 : 120[0] -> 121[1] via P2P/CUMEM
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 05/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 00/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227576:228867 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
 2: nid006497:227576:228867 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 00/0 : 46[2] -> 47[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 02/0 : 46[2] -> 47[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 03/0 : 46[2] -> 47[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 04/0 : 46[2] -> 47[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 46[2] -> 47[3] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 46[2] -> 47[3] via P2P/CUMEM
 9: nid006505:249081:250427 [1] NCCL INFO Channel 00/0 : 37[1] -> 38[2] via P2P/CUMEM
 9: nid006505:249081:250427 [1] NCCL INFO Channel 04/0 : 37[1] -> 38[2] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 02/0 : 47[3] -> 44[0] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 03/0 : 47[3] -> 44[0] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 06/0 : 47[3] -> 44[0] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 07/0 : 47[3] -> 44[0] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/CUMEM
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:209727 [2] NCCL INFO Channel 00/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 02/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 03/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 04/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 78[2] -> 79[3] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 02/0 : 79[3] -> 76[0] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 03/0 : 79[3] -> 76[0] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 06/0 : 79[3] -> 76[0] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 07/0 : 79[3] -> 76[0] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 00/0 : 38[2] -> 39[3] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 02/0 : 38[2] -> 39[3] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 03/0 : 38[2] -> 39[3] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 04/0 : 38[2] -> 39[3] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 38[2] -> 39[3] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 38[2] -> 39[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 00/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 02/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 03/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 04/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 102[2] -> 103[3] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 102[2] -> 103[3] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[1] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[1] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[1] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 05/0 : 40[0] -> 41[1] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 06/0 : 40[0] -> 41[1] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 07/0 : 40[0] -> 41[1] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 01/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 05/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 06/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 07/0 : 114[2] -> 115[3] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 00/0 : 113[1] -> 112[0] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 02/0 : 113[1] -> 112[0] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 04/0 : 113[1] -> 112[0] via P2P/CUMEM
28: nid007251:72170:73781 [1] NCCL INFO Channel 06/0 : 113[1] -> 112[0] via P2P/CUMEM
14: nid006510:229443:230763 [1] NCCL INFO Channel 01/0 : 57[1] -> 58[2] via P2P/CUMEM
14: nid006510:229443:230763 [1] NCCL INFO Channel 05/0 : 57[1] -> 58[2] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 02/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 03/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 05/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 06/0 : 64[0] -> 65[1] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 07/0 : 64[0] -> 65[1] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 00/0 : 62[2] -> 63[3] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 02/0 : 62[2] -> 63[3] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 03/0 : 62[2] -> 63[3] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 04/0 : 62[2] -> 63[3] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 62[2] -> 63[3] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 62[2] -> 63[3] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 05/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 06/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 07/0 : 32[0] -> 33[1] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 04/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
15: nid006553:223924:225253 [1] NCCL INFO Channel 00/0 : 61[1] -> 62[2] via P2P/CUMEM
15: nid006553:223924:225253 [1] NCCL INFO Channel 04/0 : 61[1] -> 62[2] via P2P/CUMEM
12: nid006508:205767:207134 [1] NCCL INFO Channel 01/0 : 49[1] -> 50[2] via P2P/CUMEM
12: nid006508:205767:207134 [1] NCCL INFO Channel 05/0 : 49[1] -> 50[2] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
 0: nid006495:241020:242328 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
 0: nid006495:241020:242328 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
24: nid006563:221141:222486 [1] NCCL INFO Channel 01/0 : 97[1] -> 98[2] via P2P/CUMEM
24: nid006563:221141:222486 [1] NCCL INFO Channel 05/0 : 97[1] -> 98[2] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 04/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:207135 [2] NCCL INFO Channel 01/0 : 50[2] -> 51[3] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 50[2] -> 51[3] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 50[2] -> 51[3] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 05/0 : 50[2] -> 51[3] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 06/0 : 50[2] -> 51[3] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 07/0 : 50[2] -> 51[3] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 02/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 03/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 05/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 06/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 07/0 : 72[0] -> 73[1] via P2P/CUMEM
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 05/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 00/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
18: nid006556:210775:212170 [1] NCCL INFO Channel 00/0 : 73[1] -> 72[0] via P2P/CUMEM
18: nid006556:210775:212170 [1] NCCL INFO Channel 02/0 : 73[1] -> 72[0] via P2P/CUMEM
18: nid006556:210775:212170 [1] NCCL INFO Channel 04/0 : 73[1] -> 72[0] via P2P/CUMEM
18: nid006556:210775:212170 [1] NCCL INFO Channel 06/0 : 73[1] -> 72[0] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 01/0 : 41[1] -> 42[2] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 05/0 : 41[1] -> 42[2] via P2P/CUMEM
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 05/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249083:250431 [3] NCCL INFO Channel 02/0 : 39[3] -> 36[0] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249083:250431 [3] NCCL INFO Channel 03/0 : 39[3] -> 36[0] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 03/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:250431 [3] NCCL INFO Channel 06/0 : 39[3] -> 36[0] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:250431 [3] NCCL INFO Channel 07/0 : 39[3] -> 36[0] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[1] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[1] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[1] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 28[0] -> 29[1] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 06/0 : 28[0] -> 29[1] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 07/0 : 28[0] -> 29[1] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 00/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 02/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 03/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 04/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 70[2] -> 71[3] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 02/0 : 71[3] -> 68[0] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 03/0 : 71[3] -> 68[0] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 06/0 : 71[3] -> 68[0] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 07/0 : 71[3] -> 68[0] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 00/0 : 100[0] -> 101[1] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 02/0 : 100[0] -> 101[1] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 03/0 : 100[0] -> 101[1] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 100[0] -> 101[1] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 06/0 : 100[0] -> 101[1] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 07/0 : 100[0] -> 101[1] via P2P/CUMEM
17: nid006555:206594:207957 [1] NCCL INFO Channel 00/0 : 69[1] -> 70[2] via P2P/CUMEM
17: nid006555:206594:207957 [1] NCCL INFO Channel 04/0 : 69[1] -> 70[2] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 03/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
12: nid006508:205769:207138 [3] NCCL INFO Channel 02/0 : 51[3] -> 48[0] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 06/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
12: nid006508:205769:207138 [3] NCCL INFO Channel 03/0 : 51[3] -> 48[0] via P2P/CUMEM
12: nid006508:205769:207138 [3] NCCL INFO Channel 06/0 : 51[3] -> 48[0] via P2P/CUMEM
12: nid006508:205769:207138 [3] NCCL INFO Channel 07/0 : 51[3] -> 48[0] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210777:212177 [3] NCCL INFO Channel 02/0 : 75[3] -> 72[0] via P2P/CUMEM
18: nid006556:210777:212177 [3] NCCL INFO Channel 03/0 : 75[3] -> 72[0] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 07/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210777:212177 [3] NCCL INFO Channel 06/0 : 75[3] -> 72[0] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
18: nid006556:210777:212177 [3] NCCL INFO Channel 07/0 : 75[3] -> 72[0] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 00/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 02/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 03/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 04/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 54[2] -> 55[3] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 02/0 : 55[3] -> 52[0] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 03/0 : 55[3] -> 52[0] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 06/0 : 55[3] -> 52[0] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 07/0 : 55[3] -> 52[0] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 02/0 : 115[3] -> 112[0] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 06/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
28: nid007251:72172:73787 [3] NCCL INFO Channel 03/0 : 115[3] -> 112[0] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 06/0 : 115[3] -> 112[0] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 07/0 : 115[3] -> 112[0] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 05/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 06/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 07/0 : 56[0] -> 57[1] via P2P/CUMEM
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 05/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
14: nid006510:229443:230763 [1] NCCL INFO Channel 00/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229443:230763 [1] NCCL INFO Channel 02/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229443:230763 [1] NCCL INFO Channel 04/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229443:230763 [1] NCCL INFO Channel 06/0 : 57[1] -> 56[0] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241022:242333 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 01/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 05/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 06/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 07/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 02/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 07/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
30: nid007318:20583:21914 [3] NCCL INFO Channel 03/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20581:21915 [1] NCCL INFO Channel 00/0 : 121[1] -> 120[0] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 06/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20581:21915 [1] NCCL INFO Channel 02/0 : 121[1] -> 120[0] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 07/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20581:21915 [1] NCCL INFO Channel 04/0 : 121[1] -> 120[0] via P2P/CUMEM
30: nid007318:20581:21915 [1] NCCL INFO Channel 06/0 : 121[1] -> 120[0] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 00/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 02/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 03/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 06/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 07/0 : 116[0] -> 117[1] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 01/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 05/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 104[0] -> 112[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 108[0] -> 116[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 104[0] -> 112[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 05/0 : 108[0] -> 116[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 112[0] -> 120[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 112[0] -> 120[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260086:261418 [3] NCCL INFO Channel 02/0 : 23[3] -> 20[0] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 03/0 : 23[3] -> 20[0] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 06/0 : 23[3] -> 20[0] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 07/0 : 23[3] -> 20[0] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 02/0 : 27[3] -> 24[0] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221944:223249 [3] NCCL INFO Channel 03/0 : 27[3] -> 24[0] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 06/0 : 27[3] -> 24[0] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 07/0 : 27[3] -> 24[0] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/CUMEM
 6: nid006501:221942:223244 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 01/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 05/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 06/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 07/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 02/0 : 43[3] -> 40[0] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 07/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
10: nid006506:263730:265068 [3] NCCL INFO Channel 03/0 : 43[3] -> 40[0] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 00/0 : 41[1] -> 40[0] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 06/0 : 43[3] -> 40[0] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 02/0 : 41[1] -> 40[0] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 07/0 : 43[3] -> 40[0] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 04/0 : 41[1] -> 40[0] via P2P/CUMEM
10: nid006506:263728:265063 [1] NCCL INFO Channel 06/0 : 41[1] -> 40[0] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 00/0 : 92[0] -> 93[1] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[2] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 04/0 : 21[1] -> 22[2] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[0] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 03/0 : 21[1] -> 20[0] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 05/0 : 21[1] -> 20[0] via P2P/CUMEM
 5: nid006500:260084:261419 [1] NCCL INFO Channel 07/0 : 21[1] -> 20[0] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 02/0 : 92[0] -> 93[1] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 03/0 : 92[0] -> 93[1] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 92[0] -> 93[1] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 06/0 : 92[0] -> 93[1] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 07/0 : 92[0] -> 93[1] via P2P/CUMEM
20: nid006558:215468:216802 [1] NCCL INFO Channel 01/0 : 81[1] -> 82[2] via P2P/CUMEM
20: nid006558:215468:216802 [1] NCCL INFO Channel 05/0 : 81[1] -> 82[2] via P2P/CUMEM
 1: nid006496:242585:243942 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
 1: nid006496:242585:243942 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:243933 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242586:243932 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:243942 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:243942 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
 1: nid006496:242585:243942 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
 1: nid006496:242585:243942 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 02/0 : 66[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241021:242327 [2] NCCL INFO Channel 03/0 : 66[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241021:242327 [2] NCCL INFO Channel 02/0 : 2[2] -> 66[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:242327 [2] NCCL INFO Channel 03/0 : 2[2] -> 66[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 00/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 02/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 03/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 04/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 30[2] -> 31[3] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 02/0 : 31[3] -> 28[0] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 03/0 : 31[3] -> 28[0] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 06/0 : 31[3] -> 28[0] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 07/0 : 31[3] -> 28[0] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 00/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 02/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 03/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 06/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 07/0 : 108[0] -> 109[1] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 00/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59770:61422 [3] NCCL INFO Channel 02/0 : 127[3] -> 124[0] via P2P/CUMEM
31: nid007342:59770:61422 [3] NCCL INFO Channel 03/0 : 127[3] -> 124[0] via P2P/CUMEM
31: nid007342:59770:61422 [3] NCCL INFO Channel 06/0 : 127[3] -> 124[0] via P2P/CUMEM
31: nid007342:59770:61422 [3] NCCL INFO Channel 07/0 : 127[3] -> 124[0] via P2P/CUMEM
11: nid006507:211336:212653 [1] NCCL INFO Channel 00/0 : 45[1] -> 46[2] via P2P/CUMEM
11: nid006507:211336:212653 [1] NCCL INFO Channel 04/0 : 45[1] -> 46[2] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 02/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 38[2] -> 46[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 42[2] -> 50[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 42[2] -> 50[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 01/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 05/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 06/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 07/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 02/0 : 91[3] -> 88[0] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 03/0 : 91[3] -> 88[0] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 06/0 : 91[3] -> 88[0] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 07/0 : 91[3] -> 88[0] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 02/0 : 103[3] -> 100[0] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 03/0 : 103[3] -> 100[0] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 06/0 : 103[3] -> 100[0] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 07/0 : 103[3] -> 100[0] via P2P/CUMEM
21: nid006559:211124:212433 [1] NCCL INFO Channel 00/0 : 85[1] -> 86[2] via P2P/CUMEM
21: nid006559:211124:212433 [1] NCCL INFO Channel 04/0 : 85[1] -> 86[2] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 01/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 05/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211124:212433 [1] NCCL INFO Channel 01/0 : 85[1] -> 84[0] via P2P/CUMEM
21: nid006559:211124:212433 [1] NCCL INFO Channel 03/0 : 85[1] -> 84[0] via P2P/CUMEM
21: nid006559:211124:212433 [1] NCCL INFO Channel 05/0 : 85[1] -> 84[0] via P2P/CUMEM
21: nid006559:211124:212433 [1] NCCL INFO Channel 07/0 : 85[1] -> 84[0] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[1] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[1] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 38[2] -> 46[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 46[2] -> 54[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:212654 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[1] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 46[2] -> 54[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 44[0] -> 45[1] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 06/0 : 44[0] -> 45[1] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 07/0 : 44[0] -> 45[1] via P2P/CUMEM
15: nid006553:223926:225256 [3] NCCL INFO Channel 02/0 : 63[3] -> 60[0] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 02/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:212654 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223926:225256 [3] NCCL INFO Channel 03/0 : 63[3] -> 60[0] via P2P/CUMEM
15: nid006553:223926:225256 [3] NCCL INFO Channel 06/0 : 63[3] -> 60[0] via P2P/CUMEM
11: nid006507:211336:212653 [1] NCCL INFO Channel 01/0 : 45[1] -> 44[0] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 36[0] -> 44[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223926:225256 [3] NCCL INFO Channel 07/0 : 63[3] -> 60[0] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 36[0] -> 44[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211336:212653 [1] NCCL INFO Channel 03/0 : 45[1] -> 44[0] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 44[0] -> 52[0] [send] via NET/AWS Libfabric/0
11: nid006507:211336:212653 [1] NCCL INFO Channel 05/0 : 45[1] -> 44[0] via P2P/CUMEM
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 44[0] -> 52[0] [send] via NET/AWS Libfabric/0
11: nid006507:211336:212653 [1] NCCL INFO Channel 07/0 : 45[1] -> 44[0] via P2P/CUMEM
 7: nid006502:252585:253928 [1] NCCL INFO Channel 00/0 : 29[1] -> 30[2] via P2P/CUMEM
 7: nid006502:252585:253928 [1] NCCL INFO Channel 04/0 : 29[1] -> 30[2] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252586:253925 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252585:253928 [1] NCCL INFO Channel 01/0 : 29[1] -> 28[0] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221941:223241 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252585:253928 [1] NCCL INFO Channel 03/0 : 29[1] -> 28[0] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 28[0] -> 44[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252585:253928 [1] NCCL INFO Channel 05/0 : 29[1] -> 28[0] via P2P/CUMEM
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 28[0] -> 44[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 30[2] -> 46[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252585:253928 [1] NCCL INFO Channel 07/0 : 29[1] -> 28[0] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 30[2] -> 46[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249080:250434 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 06/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 07/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 05/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249081:250427 [1] NCCL INFO Channel 01/0 : 37[1] -> 36[0] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 36[0] -> 44[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249081:250427 [1] NCCL INFO Channel 03/0 : 37[1] -> 36[0] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 05/0 : 36[0] -> 44[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249081:250427 [1] NCCL INFO Channel 05/0 : 37[1] -> 36[0] via P2P/CUMEM
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 44[0] -> 36[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249081:250427 [1] NCCL INFO Channel 07/0 : 37[1] -> 36[0] via P2P/CUMEM
 9: nid006505:249080:250434 [0] NCCL INFO Channel 05/0 : 44[0] -> 36[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220712:222052 [1] NCCL INFO Channel 00/0 : 93[1] -> 94[2] via P2P/CUMEM
23: nid006561:220712:222052 [1] NCCL INFO Channel 04/0 : 93[1] -> 94[2] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 00/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:222046 [2] NCCL INFO Channel 02/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220712:222052 [1] NCCL INFO Channel 01/0 : 93[1] -> 92[0] via P2P/CUMEM
23: nid006561:220712:222052 [1] NCCL INFO Channel 03/0 : 93[1] -> 92[0] via P2P/CUMEM
23: nid006561:220712:222052 [1] NCCL INFO Channel 05/0 : 93[1] -> 92[0] via P2P/CUMEM
23: nid006561:220712:222052 [1] NCCL INFO Channel 07/0 : 93[1] -> 92[0] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 01/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 05/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 06/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 07/0 : 34[2] -> 35[3] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 00/0 : 33[1] -> 32[0] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 02/0 : 33[1] -> 32[0] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 04/0 : 33[1] -> 32[0] via P2P/CUMEM
 8: nid006503:218411:219698 [1] NCCL INFO Channel 06/0 : 33[1] -> 32[0] via P2P/CUMEM
31: nid007342:59768:61425 [1] NCCL INFO Channel 00/0 : 125[1] -> 126[2] via P2P/CUMEM
31: nid007342:59768:61425 [1] NCCL INFO Channel 04/0 : 125[1] -> 126[2] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 00/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59769:61363 [2] NCCL INFO Channel 02/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59767:61364 [0] NCCL INFO Channel 04/0 : 60[0] -> 124[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59768:61425 [1] NCCL INFO Channel 01/0 : 125[1] -> 124[0] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 05/0 : 60[0] -> 124[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59768:61425 [1] NCCL INFO Channel 03/0 : 125[1] -> 124[0] via P2P/CUMEM
31: nid007342:59767:61364 [0] NCCL INFO Channel 04/0 : 124[0] -> 60[0] [send] via NET/AWS Libfabric/0
31: nid007342:59769:61363 [2] NCCL INFO Channel 06/0 : 62[2] -> 126[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59767:61364 [0] NCCL INFO Channel 05/0 : 124[0] -> 60[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 00/0 : 112[0] -> 120[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 112[0] -> 120[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59768:61425 [1] NCCL INFO Channel 05/0 : 125[1] -> 124[0] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 07/0 : 62[2] -> 126[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59769:61363 [2] NCCL INFO Channel 06/0 : 126[2] -> 62[2] [send] via NET/AWS Libfabric/2
31: nid007342:59768:61425 [1] NCCL INFO Channel 07/0 : 125[1] -> 124[0] via P2P/CUMEM
31: nid007342:59769:61363 [2] NCCL INFO Channel 07/0 : 126[2] -> 62[2] [send] via NET/AWS Libfabric/2
30: nid007318:20580:21918 [0] NCCL INFO Channel 00/0 : 120[0] -> 112[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 120[0] -> 112[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:218070 [2] NCCL INFO Channel 00/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 02/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 03/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 04/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 110[2] -> 111[3] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 02/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216749:218063 [3] NCCL INFO Channel 02/0 : 111[3] -> 108[0] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 03/0 : 111[3] -> 108[0] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 01/0 : 109[1] -> 108[0] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 06/0 : 111[3] -> 108[0] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 03/0 : 109[1] -> 108[0] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 07/0 : 111[3] -> 108[0] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 05/0 : 109[1] -> 108[0] via P2P/CUMEM
27: nid006566:216747:218064 [1] NCCL INFO Channel 07/0 : 109[1] -> 108[0] via P2P/CUMEM
22: nid006560:222272:223615 [1] NCCL INFO Channel 01/0 : 89[1] -> 90[2] via P2P/CUMEM
22: nid006560:222272:223615 [1] NCCL INFO Channel 05/0 : 89[1] -> 90[2] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 05/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222271:223606 [0] NCCL INFO Channel 00/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
22: nid006560:222273:223612 [2] NCCL INFO Channel 07/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222272:223615 [1] NCCL INFO Channel 00/0 : 89[1] -> 88[0] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
22: nid006560:222272:223615 [1] NCCL INFO Channel 02/0 : 89[1] -> 88[0] via P2P/CUMEM
22: nid006560:222272:223615 [1] NCCL INFO Channel 04/0 : 89[1] -> 88[0] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 00/0 : 80[0] -> 88[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 76[0] -> 92[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 80[0] -> 88[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 76[0] -> 92[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222272:223615 [1] NCCL INFO Channel 06/0 : 89[1] -> 88[0] via P2P/CUMEM
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 78[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 92[0] -> 108[0] [send] via NET/AWS Libfabric/0
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 78[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 92[0] -> 108[0] [send] via NET/AWS Libfabric/0
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 94[2] -> 110[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 94[2] -> 110[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218413:219707 [3] NCCL INFO Channel 02/0 : 35[3] -> 32[0] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 06/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218413:219707 [3] NCCL INFO Channel 03/0 : 35[3] -> 32[0] via P2P/CUMEM
 8: nid006503:218413:219707 [3] NCCL INFO Channel 06/0 : 35[3] -> 32[0] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 38[2] -> 46[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 38[2] -> 46[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218413:219707 [3] NCCL INFO Channel 07/0 : 35[3] -> 32[0] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 18[2] -> 34[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 34[2] -> 50[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 34[2] -> 50[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 46[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 46[2] -> 38[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223923:225259 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 06/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 07/0 : 60[0] -> 61[1] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 28[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 28[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223924:225253 [1] NCCL INFO Channel 01/0 : 61[1] -> 60[0] via P2P/CUMEM
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 60[0] -> 92[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 60[0] -> 92[0] [send] via NET/AWS Libfabric/0
15: nid006553:223924:225253 [1] NCCL INFO Channel 03/0 : 61[1] -> 60[0] via P2P/CUMEM
15: nid006553:223924:225253 [1] NCCL INFO Channel 05/0 : 61[1] -> 60[0] via P2P/CUMEM
15: nid006553:223924:225253 [1] NCCL INFO Channel 07/0 : 61[1] -> 60[0] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 02/0 : 87[3] -> 84[0] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 03/0 : 87[3] -> 84[0] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 03/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
21: nid006559:211126:212436 [3] NCCL INFO Channel 06/0 : 87[3] -> 84[0] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
21: nid006559:211126:212436 [3] NCCL INFO Channel 07/0 : 87[3] -> 84[0] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 82[2] -> 90[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 82[2] -> 90[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206593:207960 [0] NCCL INFO Channel 00/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 02/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 03/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 06/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 07/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 01/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 05/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
17: nid006555:206594:207957 [1] NCCL INFO Channel 01/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206594:207957 [1] NCCL INFO Channel 03/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206594:207957 [1] NCCL INFO Channel 05/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206594:207957 [1] NCCL INFO Channel 07/0 : 69[1] -> 68[0] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 05/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 06/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 07/0 : 48[0] -> 49[1] via P2P/CUMEM
12: nid006508:205766:207141 [0] NCCL INFO Channel 04/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
12: nid006508:205767:207134 [1] NCCL INFO Channel 00/0 : 49[1] -> 48[0] via P2P/CUMEM
12: nid006508:205767:207134 [1] NCCL INFO Channel 02/0 : 49[1] -> 48[0] via P2P/CUMEM
12: nid006508:205767:207134 [1] NCCL INFO Channel 04/0 : 49[1] -> 48[0] via P2P/CUMEM
12: nid006508:205767:207134 [1] NCCL INFO Channel 06/0 : 49[1] -> 48[0] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 01/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 05/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 06/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 07/0 : 66[2] -> 67[3] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 02/0 : 67[3] -> 64[0] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 03/0 : 67[3] -> 64[0] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 06/0 : 67[3] -> 64[0] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 07/0 : 67[3] -> 64[0] via P2P/CUMEM
25: nid006564:223022:224342 [1] NCCL INFO Channel 00/0 : 101[1] -> 102[2] via P2P/CUMEM
25: nid006564:223022:224342 [1] NCCL INFO Channel 04/0 : 101[1] -> 102[2] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 01/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 05/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223022:224342 [1] NCCL INFO Channel 01/0 : 101[1] -> 100[0] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 03/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:224342 [1] NCCL INFO Channel 03/0 : 101[1] -> 100[0] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:224342 [1] NCCL INFO Channel 05/0 : 101[1] -> 100[0] via P2P/CUMEM
25: nid006564:223022:224342 [1] NCCL INFO Channel 07/0 : 101[1] -> 100[0] via P2P/CUMEM
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 80[0] -> 96[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 80[0] -> 96[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 96[0] -> 112[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 96[0] -> 112[0] [send] via NET/AWS Libfabric/0
29: nid007305:27224:28495 [2] NCCL INFO Channel 00/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 02/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 03/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 04/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 118[2] -> 119[3] via P2P/CUMEM
29: nid007305:27225:28489 [3] NCCL INFO Channel 02/0 : 119[3] -> 116[0] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 03/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 110[2] -> 118[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 106[2] -> 114[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27225:28489 [3] NCCL INFO Channel 03/0 : 119[3] -> 116[0] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 110[2] -> 118[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 114[2] -> 122[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27223:28486 [1] NCCL INFO Channel 01/0 : 117[1] -> 116[0] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 114[2] -> 122[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 106[2] -> 114[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 114[2] -> 122[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 114[2] -> 122[2] [send] via NET/AWS Libfabric/2
29: nid007305:27225:28489 [3] NCCL INFO Channel 06/0 : 119[3] -> 116[0] via P2P/CUMEM
29: nid007305:27223:28486 [1] NCCL INFO Channel 03/0 : 117[1] -> 116[0] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 122[2] -> 114[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 122[2] -> 114[2] [send] via NET/AWS Libfabric/2
29: nid007305:27225:28489 [3] NCCL INFO Channel 07/0 : 119[3] -> 116[0] via P2P/CUMEM
29: nid007305:27223:28486 [1] NCCL INFO Channel 05/0 : 117[1] -> 116[0] via P2P/CUMEM
29: nid007305:27223:28486 [1] NCCL INFO Channel 07/0 : 117[1] -> 116[0] via P2P/CUMEM
14: nid006510:229445:230769 [3] NCCL INFO Channel 02/0 : 59[3] -> 56[0] via P2P/CUMEM
14: nid006510:229445:230769 [3] NCCL INFO Channel 03/0 : 59[3] -> 56[0] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 07/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229445:230769 [3] NCCL INFO Channel 06/0 : 59[3] -> 56[0] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
14: nid006510:229445:230769 [3] NCCL INFO Channel 07/0 : 59[3] -> 56[0] via P2P/CUMEM
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 30[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 30[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 62[2] -> 94[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 62[2] -> 94[2] [send] via NET/AWS Libfabric/2
26: nid006565:222466:223838 [1] NCCL INFO Channel 01/0 : 105[1] -> 106[2] via P2P/CUMEM
26: nid006565:222466:223838 [1] NCCL INFO Channel 05/0 : 105[1] -> 106[2] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 07/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 106[2] -> 114[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 106[2] -> 114[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 102[2] -> 110[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 102[2] -> 110[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 114[2] -> 106[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 110[2] -> 118[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 98[2] -> 114[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 114[2] -> 106[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 110[2] -> 118[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 98[2] -> 114[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 118[2] -> 110[2] [send] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 118[2] -> 110[2] [send] via NET/AWS Libfabric/2
19: nid006557:208418:209751 [1] NCCL INFO Channel 00/0 : 77[1] -> 78[2] via P2P/CUMEM
19: nid006557:208418:209751 [1] NCCL INFO Channel 04/0 : 77[1] -> 78[2] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 02/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:209724 [0] NCCL INFO Channel 00/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208418:209751 [1] NCCL INFO Channel 01/0 : 77[1] -> 76[0] via P2P/CUMEM
19: nid006557:208418:209751 [1] NCCL INFO Channel 03/0 : 77[1] -> 76[0] via P2P/CUMEM
19: nid006557:208418:209751 [1] NCCL INFO Channel 05/0 : 77[1] -> 76[0] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 70[2] -> 78[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 68[0] -> 76[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 68[0] -> 76[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208418:209751 [1] NCCL INFO Channel 07/0 : 77[1] -> 76[0] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 70[2] -> 78[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 74[2] -> 82[2] [send] via NET/AWS Libfabric/2
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 76[0] -> 84[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 78[2] -> 86[2] [send] via NET/AWS Libfabric/2
18: nid006556:210774:212174 [0] NCCL INFO Channel 00/0 : 72[0] -> 80[0] [send] via NET/AWS Libfabric/0
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 74[2] -> 82[2] [send] via NET/AWS Libfabric/2
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 72[0] -> 80[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 76[0] -> 84[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 78[2] -> 86[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 01/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 05/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 06/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 07/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 02/0 : 99[3] -> 96[0] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 06/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
24: nid006563:221143:222489 [3] NCCL INFO Channel 03/0 : 99[3] -> 96[0] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 82[2] -> 98[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 82[2] -> 98[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 102[2] -> 110[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 102[2] -> 110[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 98[2] -> 114[2] [send] via NET/AWS Libfabric/2
24: nid006563:221141:222486 [1] NCCL INFO Channel 00/0 : 97[1] -> 96[0] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 06/0 : 99[3] -> 96[0] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 98[2] -> 114[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 110[2] -> 102[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 110[2] -> 102[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221141:222486 [1] NCCL INFO Channel 02/0 : 97[1] -> 96[0] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 94[2] -> 110[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221143:222489 [3] NCCL INFO Channel 07/0 : 99[3] -> 96[0] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 94[2] -> 110[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 114[2] -> 98[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 114[2] -> 98[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 110[2] -> 94[2] [send] via NET/AWS Libfabric/2
24: nid006563:221141:222486 [1] NCCL INFO Channel 04/0 : 97[1] -> 96[0] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 110[2] -> 94[2] [send] via NET/AWS Libfabric/2
24: nid006563:221141:222486 [1] NCCL INFO Channel 06/0 : 97[1] -> 96[0] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226768:228077 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
 3: nid006498:226766:228078 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254557:255854 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/CUMEM
 4: nid006499:254557:255854 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/CUMEM
 4: nid006499:254557:255854 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/CUMEM
 4: nid006499:254557:255854 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 02/0 : 19[3] -> 16[0] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 03/0 : 19[3] -> 16[0] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 06/0 : 19[3] -> 16[0] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 07/0 : 19[3] -> 16[0] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 01/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 05/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 06/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 07/0 : 82[2] -> 83[3] via P2P/CUMEM
20: nid006558:215470:216828 [3] NCCL INFO Channel 02/0 : 83[3] -> 80[0] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 06/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 74[2] -> 82[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 74[2] -> 82[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 78[2] -> 86[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 78[2] -> 86[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 82[2] -> 90[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 82[2] -> 90[2] [send] via NET/AWS Libfabric/2
20: nid006558:215470:216828 [3] NCCL INFO Channel 03/0 : 83[3] -> 80[0] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 86[2] -> 78[2] [send] via NET/AWS Libfabric/2
20: nid006558:215470:216828 [3] NCCL INFO Channel 06/0 : 83[3] -> 80[0] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 86[2] -> 78[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 82[2] -> 98[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 82[2] -> 98[2] [send] via NET/AWS Libfabric/2
20: nid006558:215470:216828 [3] NCCL INFO Channel 07/0 : 83[3] -> 80[0] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 98[2] -> 82[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 66[2] -> 98[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 98[2] -> 82[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 82[2] -> 74[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 66[2] -> 98[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 82[2] -> 74[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 90[2] -> 82[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 90[2] -> 82[2] [send] via NET/AWS Libfabric/2
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 02/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 03/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 05/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 06/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 07/0 : 80[0] -> 81[1] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 04/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 72[0] -> 80[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 72[0] -> 80[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 76[0] -> 84[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 80[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 05/0 : 76[0] -> 84[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 80[0] -> 88[0] [send] via NET/AWS Libfabric/0
20: nid006558:215468:216802 [1] NCCL INFO Channel 00/0 : 81[1] -> 80[0] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 84[0] -> 76[0] [send] via NET/AWS Libfabric/0
20: nid006558:215468:216802 [1] NCCL INFO Channel 02/0 : 81[1] -> 80[0] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 05/0 : 84[0] -> 76[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 80[0] -> 96[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 00/0 : 80[0] -> 72[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 80[0] -> 96[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 80[0] -> 72[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215468:216802 [1] NCCL INFO Channel 04/0 : 81[1] -> 80[0] via P2P/CUMEM
22: nid006560:222271:223606 [0] NCCL INFO Channel 00/0 : 88[0] -> 80[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 88[0] -> 80[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 96[0] -> 80[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215468:216802 [1] NCCL INFO Channel 06/0 : 81[1] -> 80[0] via P2P/CUMEM
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 96[0] -> 80[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 02/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 03/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 05/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 06/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 07/0 : 104[0] -> 105[1] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 05/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 00/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
26: nid006565:222466:223838 [1] NCCL INFO Channel 00/0 : 105[1] -> 104[0] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 100[0] -> 108[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 05/0 : 100[0] -> 108[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 00/0 : 104[0] -> 112[0] [send] via NET/AWS Libfabric/0
26: nid006565:222466:223838 [1] NCCL INFO Channel 02/0 : 105[1] -> 104[0] via P2P/CUMEM
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 104[0] -> 112[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 100[0] -> 108[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 100[0] -> 108[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222466:223838 [1] NCCL INFO Channel 04/0 : 105[1] -> 104[0] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 108[0] -> 116[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 00/0 : 112[0] -> 104[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 108[0] -> 116[0] [send] via NET/AWS Libfabric/0
26: nid006565:222466:223838 [1] NCCL INFO Channel 06/0 : 105[1] -> 104[0] via P2P/CUMEM
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 96[0] -> 112[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 112[0] -> 104[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 96[0] -> 112[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 108[0] -> 100[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 112[0] -> 96[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 112[0] -> 96[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 92[0] -> 108[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 05/0 : 108[0] -> 100[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 64[0] -> 96[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 92[0] -> 108[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 116[0] -> 108[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 64[0] -> 96[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 05/0 : 116[0] -> 108[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 108[0] -> 92[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 108[0] -> 92[0] [send] via NET/AWS Libfabric/0
13: nid006509:201789:203117 [1] NCCL INFO Channel 00/0 : 53[1] -> 54[2] via P2P/CUMEM
13: nid006509:201789:203117 [1] NCCL INFO Channel 04/0 : 53[1] -> 54[2] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201790:203114 [2] NCCL INFO Channel 03/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:203108 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
13: nid006509:201789:203117 [1] NCCL INFO Channel 01/0 : 53[1] -> 52[0] via P2P/CUMEM
13: nid006509:201788:203108 [0] NCCL INFO Channel 05/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
13: nid006509:201789:203117 [1] NCCL INFO Channel 03/0 : 53[1] -> 52[0] via P2P/CUMEM
13: nid006509:201789:203117 [1] NCCL INFO Channel 05/0 : 53[1] -> 52[0] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 46[2] -> 54[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 42[2] -> 50[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 44[0] -> 52[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201789:203117 [1] NCCL INFO Channel 07/0 : 53[1] -> 52[0] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 46[2] -> 54[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 50[2] -> 58[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 42[2] -> 50[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201788:203108 [0] NCCL INFO Channel 05/0 : 44[0] -> 52[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 50[2] -> 58[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229442:230766 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 50[2] -> 58[2] [send] via NET/AWS Libfabric/2
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [send] via NET/AWS Libfabric/0
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 54[2] -> 46[2] [send] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 50[2] -> 58[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 52[0] -> 44[0] [send] via NET/AWS Libfabric/0
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 54[2] -> 46[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:203108 [0] NCCL INFO Channel 05/0 : 52[0] -> 44[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 28[0] -> 44[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 30[2] -> 46[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 28[0] -> 44[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [send] via NET/AWS Libfabric/0
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 30[2] -> 46[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263727:265060 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 44[0] -> 28[0] [send] via NET/AWS Libfabric/0
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 50[2] -> 42[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 34[2] -> 50[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 58[2] -> 50[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 44[0] -> 28[0] [send] via NET/AWS Libfabric/0
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 50[2] -> 42[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 34[2] -> 50[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 46[2] -> 30[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [send] via NET/AWS Libfabric/0
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 58[2] -> 50[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 46[2] -> 30[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 50[2] -> 34[2] [send] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 50[2] -> 34[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227578:228893 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
 2: nid006497:227578:228893 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227578:228893 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227578:228893 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 18[2] -> 34[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 34[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 34[2] -> 66[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 34[2] -> 66[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 30[2] -> 62[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 30[2] -> 62[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 62[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 62[2] -> 30[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221582:222947 [1] NCCL INFO Channel 01/0 : 65[1] -> 66[2] via P2P/CUMEM
16: nid006554:221582:222947 [1] NCCL INFO Channel 05/0 : 65[1] -> 66[2] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 04/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 06/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:222947 [1] NCCL INFO Channel 00/0 : 65[1] -> 64[0] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 32[0] -> 64[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 32[0] -> 64[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 34[2] -> 66[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221582:222947 [1] NCCL INFO Channel 02/0 : 65[1] -> 64[0] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 68[0] -> 76[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 64[0] -> 96[0] [send] via NET/AWS Libfabric/0
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 70[2] -> 78[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 34[2] -> 66[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206593:207960 [0] NCCL INFO Channel 05/0 : 68[0] -> 76[0] [send] via NET/AWS Libfabric/0
16: nid006554:221582:222947 [1] NCCL INFO Channel 04/0 : 65[1] -> 64[0] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 64[0] -> 96[0] [send] via NET/AWS Libfabric/0
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 70[2] -> 78[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 66[2] -> 98[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 66[2] -> 98[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:222947 [1] NCCL INFO Channel 06/0 : 65[1] -> 64[0] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 76[0] -> 68[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 76[0] -> 92[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 05/0 : 76[0] -> 68[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 76[0] -> 92[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 96[0] -> 64[0] [send] via NET/AWS Libfabric/0
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 78[2] -> 70[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 78[2] -> 94[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 66[2] -> 34[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 78[2] -> 70[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 96[0] -> 64[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 78[2] -> 94[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 66[2] -> 34[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 2[2] -> 66[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 92[0] -> 76[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 2[2] -> 66[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 92[0] -> 76[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 60[0] -> 92[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 66[2] -> 2[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 66[2] -> 2[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 94[2] -> 78[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 60[0] -> 92[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 94[2] -> 78[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 62[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 92[0] -> 60[0] [send] via NET/AWS Libfabric/0
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 62[2] -> 94[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 98[2] -> 66[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 92[0] -> 60[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 98[2] -> 66[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 02/0 : 66[2] -> 34[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 94[2] -> 62[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 03/0 : 66[2] -> 34[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 94[2] -> 62[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:242327 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241021:242327 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 98[2] -> 66[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 50[2] -> 34[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 98[2] -> 66[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 50[2] -> 34[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 114[2] -> 98[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 03/0 : 34[2] -> 18[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 06/0 : 70[2] -> 66[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 114[2] -> 98[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:222944 [2] NCCL INFO Channel 07/0 : 70[2] -> 66[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 02/0 : 98[2] -> 82[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 03/0 : 98[2] -> 82[2] [send] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 58[2] -> 50[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 58[2] -> 50[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 06/0 : 38[2] -> 34[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:219704 [2] NCCL INFO Channel 07/0 : 38[2] -> 34[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 02/0 : 50[2] -> 42[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 122[2] -> 114[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 03/0 : 50[2] -> 42[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 06/0 : 102[2] -> 98[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 122[2] -> 114[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 90[2] -> 82[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:222492 [2] NCCL INFO Channel 07/0 : 102[2] -> 98[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 02/0 : 114[2] -> 106[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 90[2] -> 82[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 03/0 : 114[2] -> 106[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 02/0 : 82[2] -> 74[2] [send] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 03/0 : 82[2] -> 74[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 62[2] -> 58[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 06/0 : 54[2] -> 50[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 62[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:207135 [2] NCCL INFO Channel 07/0 : 54[2] -> 50[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 02/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 06/0 : 118[2] -> 114[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 03/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 126[2] -> 122[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 06/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:255881 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:73784 [2] NCCL INFO Channel 07/0 : 118[2] -> 114[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 06/0 : 86[2] -> 82[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 126[2] -> 122[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 94[2] -> 90[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 110[2] -> 106[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 02/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 94[2] -> 90[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:216833 [2] NCCL INFO Channel 07/0 : 86[2] -> 82[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:230760 [2] NCCL INFO Channel 07/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 03/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 78[2] -> 74[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:223250 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 110[2] -> 106[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 02/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 06/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 03/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 02/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 78[2] -> 74[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:21921 [2] NCCL INFO Channel 07/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 06/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 02/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 03/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:228868 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:223612 [2] NCCL INFO Channel 07/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 03/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 06/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:223813 [2] NCCL INFO Channel 07/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 06/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:212171 [2] NCCL INFO Channel 07/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 126[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 126[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 62[2] -> 126[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 62[2] -> 126[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 94[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 94[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 06/0 : 62[2] -> 30[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:61363 [2] NCCL INFO Channel 02/0 : 126[2] -> 122[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:61363 [2] NCCL INFO Channel 03/0 : 126[2] -> 122[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 07/0 : 62[2] -> 30[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 110[2] -> 94[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 02/0 : 62[2] -> 58[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 110[2] -> 94[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 03/0 : 62[2] -> 58[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 06/0 : 94[2] -> 78[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 07/0 : 94[2] -> 78[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 46[2] -> 42[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 46[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 46[2] -> 30[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 118[2] -> 110[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 46[2] -> 42[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 86[2] -> 78[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 02/0 : 94[2] -> 90[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 118[2] -> 110[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 86[2] -> 78[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 02/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 03/0 : 94[2] -> 90[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 06/0 : 110[2] -> 102[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 03/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 06/0 : 78[2] -> 70[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 07/0 : 78[2] -> 70[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 07/0 : 110[2] -> 102[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 06/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:265069 [2] NCCL INFO Channel 07/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 54[2] -> 46[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59769:61363 [2] NCCL INFO Channel 01/0 : 126[2] -> 125[1] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 54[2] -> 46[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 02/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 02/0 : 78[2] -> 74[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 02/0 : 110[2] -> 106[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:253925 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 02/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 02/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 03/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 06/0 : 46[2] -> 38[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:209727 [2] NCCL INFO Channel 03/0 : 78[2] -> 74[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:61363 [2] NCCL INFO Channel 05/0 : 126[2] -> 125[1] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 03/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:218070 [2] NCCL INFO Channel 03/0 : 110[2] -> 106[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 07/0 : 46[2] -> 38[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 02/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 03/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 01/0 : 62[2] -> 61[1] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 03/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 06/0 : 86[2] -> 82[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:225252 [2] NCCL INFO Channel 05/0 : 62[2] -> 61[1] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 06/0 : 118[2] -> 114[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 06/0 : 70[2] -> 66[2] [send] via NET/AWS Libfabric/2
21: nid006559:211125:212429 [2] NCCL INFO Channel 07/0 : 86[2] -> 82[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:28495 [2] NCCL INFO Channel 07/0 : 118[2] -> 114[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 02/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:207956 [2] NCCL INFO Channel 07/0 : 70[2] -> 66[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 03/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 02/0 : 46[2] -> 42[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 06/0 : 102[2] -> 98[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:228084 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:212647 [2] NCCL INFO Channel 03/0 : 46[2] -> 42[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:224335 [2] NCCL INFO Channel 07/0 : 102[2] -> 98[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 02/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 03/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 06/0 : 54[2] -> 50[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:203114 [2] NCCL INFO Channel 07/0 : 54[2] -> 50[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 01/0 : 94[2] -> 93[1] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:261413 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 06/0 : 38[2] -> 34[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:243932 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:250428 [2] NCCL INFO Channel 07/0 : 38[2] -> 34[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:222046 [2] NCCL INFO Channel 05/0 : 94[2] -> 93[1] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 01/0 : 110[2] -> 109[1] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 01/0 : 78[2] -> 77[1] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 01/0 : 30[2] -> 29[1] via P2P/CUMEM
27: nid006566:216748:218070 [2] NCCL INFO Channel 05/0 : 110[2] -> 109[1] via P2P/CUMEM
19: nid006557:208419:209727 [2] NCCL INFO Channel 05/0 : 78[2] -> 77[1] via P2P/CUMEM
 7: nid006502:252586:253925 [2] NCCL INFO Channel 05/0 : 30[2] -> 29[1] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 00/0 : 82[2] -> 81[1] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 01/0 : 86[2] -> 85[1] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 00/0 : 66[2] -> 65[1] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 01/0 : 118[2] -> 117[1] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 00/0 : 122[2] -> 121[1] via P2P/CUMEM
 3: nid006498:226767:228084 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 00/0 : 98[2] -> 97[1] via P2P/CUMEM
20: nid006558:215469:216833 [2] NCCL INFO Channel 04/0 : 82[2] -> 81[1] via P2P/CUMEM
21: nid006559:211125:212429 [2] NCCL INFO Channel 05/0 : 86[2] -> 85[1] via P2P/CUMEM
29: nid007305:27224:28495 [2] NCCL INFO Channel 05/0 : 118[2] -> 117[1] via P2P/CUMEM
30: nid007318:20582:21921 [2] NCCL INFO Channel 04/0 : 122[2] -> 121[1] via P2P/CUMEM
16: nid006554:221583:222944 [2] NCCL INFO Channel 04/0 : 66[2] -> 65[1] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 00/0 : 106[2] -> 105[1] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 00/0 : 74[2] -> 73[1] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 01/0 : 46[2] -> 45[1] via P2P/CUMEM
24: nid006563:221142:222492 [2] NCCL INFO Channel 04/0 : 98[2] -> 97[1] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 01/0 : 70[2] -> 69[1] via P2P/CUMEM
26: nid006565:222467:223813 [2] NCCL INFO Channel 04/0 : 106[2] -> 105[1] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/CUMEM
18: nid006556:210776:212171 [2] NCCL INFO Channel 04/0 : 74[2] -> 73[1] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 00/0 : 90[2] -> 89[1] via P2P/CUMEM
11: nid006507:211337:212647 [2] NCCL INFO Channel 05/0 : 46[2] -> 45[1] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 01/0 : 54[2] -> 53[1] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 00/0 : 50[2] -> 49[1] via P2P/CUMEM
 6: nid006501:221943:223250 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/CUMEM
17: nid006555:206595:207956 [2] NCCL INFO Channel 05/0 : 70[2] -> 69[1] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 01/0 : 22[2] -> 21[1] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 01/0 : 102[2] -> 101[1] via P2P/CUMEM
13: nid006509:201790:203114 [2] NCCL INFO Channel 05/0 : 54[2] -> 53[1] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 01/0 : 38[2] -> 37[1] via P2P/CUMEM
22: nid006560:222273:223612 [2] NCCL INFO Channel 04/0 : 90[2] -> 89[1] via P2P/CUMEM
 4: nid006499:254558:255881 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/CUMEM
12: nid006508:205768:207135 [2] NCCL INFO Channel 04/0 : 50[2] -> 49[1] via P2P/CUMEM
 5: nid006500:260085:261413 [2] NCCL INFO Channel 05/0 : 22[2] -> 21[1] via P2P/CUMEM
25: nid006564:223023:224335 [2] NCCL INFO Channel 05/0 : 102[2] -> 101[1] via P2P/CUMEM
 0: nid006495:241021:242327 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 00/0 : 58[2] -> 57[1] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 00/0 : 42[2] -> 41[1] via P2P/CUMEM
 9: nid006505:249082:250428 [2] NCCL INFO Channel 05/0 : 38[2] -> 37[1] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 00/0 : 34[2] -> 33[1] via P2P/CUMEM
14: nid006510:229444:230760 [2] NCCL INFO Channel 04/0 : 58[2] -> 57[1] via P2P/CUMEM
10: nid006506:263729:265069 [2] NCCL INFO Channel 04/0 : 42[2] -> 41[1] via P2P/CUMEM
 8: nid006503:218412:219704 [2] NCCL INFO Channel 04/0 : 34[2] -> 33[1] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
 2: nid006497:227577:228868 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
 1: nid006496:242586:243932 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 00/0 : 114[2] -> 113[1] via P2P/CUMEM
28: nid007251:72171:73784 [2] NCCL INFO Channel 04/0 : 114[2] -> 113[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227576:228867 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227576:228867 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227576:228867 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227576:228867 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 32[0] -> 64[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 32[0] -> 64[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 64[0] -> 32[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 0[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 64[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 0[0] -> 64[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 64[0] -> 0[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 64[0] -> 0[0] [send] via NET/AWS Libfabric/0
 0: Parameter Offload: Total persistent parameters: 728992 in 407 params
 0: 
 0: nid006495:241019:242351 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241019:242351 [0] NCCL INFO Channel 00/0 : 64[0] -> 0[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241019:242351 [0] NCCL INFO Channel 01/0 : 64[0] -> 0[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241019:242351 [0] NCCL INFO Channel 00/0 : 0[0] -> 64[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241019:242351 [0] NCCL INFO Channel 01/0 : 0[0] -> 64[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:242328 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241020:242328 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
 0: nid006495:241019:242351 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241019:242351 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 96[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:242328 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 96[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 00/0 : 64[0] -> 32[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:242328 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Channel 01/0 : 64[0] -> 32[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 04/0 : 68[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 28[0] -> 60[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 112[0] -> 96[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:222938 [0] NCCL INFO Channel 05/0 : 68[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 28[0] -> 60[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 112[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 00/0 : 96[0] -> 80[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 01/0 : 96[0] -> 80[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 60[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 60[0] -> 28[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 124[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 124[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 60[0] -> 124[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 04/0 : 100[0] -> 96[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 120[0] -> 112[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 60[0] -> 124[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 04/0 : 36[0] -> 32[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 120[0] -> 112[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:222483 [0] NCCL INFO Channel 05/0 : 100[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:219701 [0] NCCL INFO Channel 05/0 : 36[0] -> 32[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 00/0 : 112[0] -> 104[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 01/0 : 112[0] -> 104[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 92[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 92[0] -> 60[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:61364 [0] NCCL INFO Channel 00/0 : 124[0] -> 120[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 04/0 : 60[0] -> 28[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 04/0 : 116[0] -> 112[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:61364 [0] NCCL INFO Channel 01/0 : 124[0] -> 120[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 05/0 : 60[0] -> 28[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:73780 [0] NCCL INFO Channel 05/0 : 116[0] -> 112[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 00/0 : 124[0] -> 120[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 124[0] -> 120[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 00/0 : 108[0] -> 104[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 108[0] -> 104[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:255860 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 44[0] -> 28[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 00/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:225259 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 01/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 44[0] -> 28[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 04/0 : 52[0] -> 48[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 04/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:207141 [0] NCCL INFO Channel 05/0 : 52[0] -> 48[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:223841 [0] NCCL INFO Channel 05/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:223241 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:228915 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 04/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:230766 [0] NCCL INFO Channel 05/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 52[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 04/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 00/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 52[0] -> 44[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 88[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:253922 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 01/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 04/0 : 44[0] -> 36[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 88[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:265060 [0] NCCL INFO Channel 05/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 00/0 : 80[0] -> 72[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 04/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 05/0 : 44[0] -> 36[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 01/0 : 80[0] -> 72[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:21918 [0] NCCL INFO Channel 05/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [send] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 04/0 : 84[0] -> 80[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211335:212654 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:228081 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:216834 [0] NCCL INFO Channel 05/0 : 84[0] -> 80[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 00/0 : 92[0] -> 88[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 92[0] -> 88[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 05/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 00/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 00/0 : 76[0] -> 72[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 04/0 : 52[0] -> 48[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 76[0] -> 72[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 00/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 01/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
13: nid006509:201788:203108 [0] NCCL INFO Channel 05/0 : 52[0] -> 48[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 04/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 01/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 04/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:261412 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:223606 [0] NCCL INFO Channel 05/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 05/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:243933 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:212174 [0] NCCL INFO Channel 05/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 04/0 : 36[0] -> 32[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:250434 [0] NCCL INFO Channel 05/0 : 36[0] -> 32[0] [send] via NET/AWS Libfabric/0
15: nid006553:223926:225256 [3] NCCL INFO Channel 01/0 : 63[3] -> 62[2] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 01/0 : 31[3] -> 30[2] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 108[0] -> 92[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 108[0] -> 92[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 04/0 : 92[0] -> 76[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 05/0 : 92[0] -> 76[0] [send] via NET/AWS Libfabric/0
31: nid007342:59770:61422 [3] NCCL INFO Channel 01/0 : 127[3] -> 126[2] via P2P/CUMEM
15: nid006553:223926:225256 [3] NCCL INFO Channel 03/0 : 63[3] -> 62[2] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
 7: nid006502:252587:253919 [3] NCCL INFO Channel 03/0 : 31[3] -> 30[2] via P2P/CUMEM
23: nid006561:220711:222049 [0] NCCL INFO Channel 00/0 : 92[0] -> 88[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 116[0] -> 108[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:222049 [0] NCCL INFO Channel 01/0 : 92[0] -> 88[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 116[0] -> 108[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 84[0] -> 76[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211338:212648 [3] NCCL INFO Channel 01/0 : 47[3] -> 46[2] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 04/0 : 108[0] -> 100[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 84[0] -> 76[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223926:225256 [3] NCCL INFO Channel 05/0 : 63[3] -> 62[2] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 05/0 : 108[0] -> 100[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 04/0 : 76[0] -> 68[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252587:253919 [3] NCCL INFO Channel 05/0 : 31[3] -> 30[2] via P2P/CUMEM
14: nid006510:229445:230769 [3] NCCL INFO Channel 00/0 : 59[3] -> 58[2] via P2P/CUMEM
 2: nid006497:227578:228893 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 05/0 : 76[0] -> 68[0] [send] via NET/AWS Libfabric/0
12: nid006508:205769:207138 [3] NCCL INFO Channel 00/0 : 51[3] -> 50[2] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
15: nid006553:223926:225256 [3] NCCL INFO Channel 07/0 : 63[3] -> 62[2] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/CUMEM
31: nid007342:59770:61422 [3] NCCL INFO Channel 03/0 : 127[3] -> 126[2] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 01/0 : 55[3] -> 54[2] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 01/0 : 23[3] -> 22[2] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 00/0 : 108[0] -> 104[0] [send] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 00/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252587:253919 [3] NCCL INFO Channel 07/0 : 31[3] -> 30[2] via P2P/CUMEM
 9: nid006505:249083:250431 [3] NCCL INFO Channel 01/0 : 39[3] -> 38[2] via P2P/CUMEM
 3: nid006498:226768:228077 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
27: nid006566:216746:218067 [0] NCCL INFO Channel 01/0 : 108[0] -> 104[0] [send] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 01/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:209724 [0] NCCL INFO Channel 00/0 : 76[0] -> 72[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218413:219707 [3] NCCL INFO Channel 00/0 : 35[3] -> 34[2] via P2P/CUMEM
19: nid006557:208417:209724 [0] NCCL INFO Channel 01/0 : 76[0] -> 72[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 00/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 00/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 01/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226768:228077 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 00/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 01/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 05/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59770:61422 [3] NCCL INFO Channel 05/0 : 127[3] -> 126[2] via P2P/CUMEM
 2: nid006497:227578:228893 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 00/0 : 43[3] -> 42[2] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 01/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:28492 [0] NCCL INFO Channel 04/0 : 116[0] -> 112[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:224336 [0] NCCL INFO Channel 05/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229445:230769 [3] NCCL INFO Channel 02/0 : 59[3] -> 58[2] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 03/0 : 47[3] -> 46[2] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/CUMEM
29: nid007305:27222:28492 [0] NCCL INFO Channel 05/0 : 116[0] -> 112[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242587:243939 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 05/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59770:61422 [3] NCCL INFO Channel 07/0 : 127[3] -> 126[2] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 04/0 : 100[0] -> 96[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 05/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:212430 [0] NCCL INFO Channel 04/0 : 84[0] -> 80[0] [send] via NET/AWS Libfabric/0
12: nid006508:205769:207138 [3] NCCL INFO Channel 02/0 : 51[3] -> 50[2] via P2P/CUMEM
 2: nid006497:227578:228893 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 03/0 : 55[3] -> 54[2] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 03/0 : 23[3] -> 22[2] via P2P/CUMEM
25: nid006564:223021:224336 [0] NCCL INFO Channel 05/0 : 100[0] -> 96[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:207960 [0] NCCL INFO Channel 04/0 : 68[0] -> 64[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249083:250431 [3] NCCL INFO Channel 03/0 : 39[3] -> 38[2] via P2P/CUMEM
21: nid006559:211123:212430 [0] NCCL INFO Channel 05/0 : 84[0] -> 80[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241022:242333 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 05/0 : 47[3] -> 46[2] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/CUMEM
17: nid006555:206593:207960 [0] NCCL INFO Channel 05/0 : 68[0] -> 64[0] [send] via NET/AWS Libfabric/0
14: nid006510:229445:230769 [3] NCCL INFO Channel 04/0 : 59[3] -> 58[2] via P2P/CUMEM
 8: nid006503:218413:219707 [3] NCCL INFO Channel 02/0 : 35[3] -> 34[2] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
 2: nid006497:227578:228893 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 05/0 : 23[3] -> 22[2] via P2P/CUMEM
12: nid006508:205769:207138 [3] NCCL INFO Channel 04/0 : 51[3] -> 50[2] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 05/0 : 55[3] -> 54[2] via P2P/CUMEM
 4: nid006499:254559:255857 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/CUMEM
 6: nid006501:221944:223249 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 01/0 : 95[3] -> 94[2] via P2P/CUMEM
14: nid006510:229445:230769 [3] NCCL INFO Channel 06/0 : 59[3] -> 58[2] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 02/0 : 43[3] -> 42[2] via P2P/CUMEM
 9: nid006505:249083:250431 [3] NCCL INFO Channel 05/0 : 39[3] -> 38[2] via P2P/CUMEM
12: nid006508:205769:207138 [3] NCCL INFO Channel 06/0 : 51[3] -> 50[2] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
 5: nid006500:260086:261418 [3] NCCL INFO Channel 07/0 : 23[3] -> 22[2] via P2P/CUMEM
11: nid006507:211338:212648 [3] NCCL INFO Channel 07/0 : 47[3] -> 46[2] via P2P/CUMEM
 8: nid006503:218413:219707 [3] NCCL INFO Channel 04/0 : 35[3] -> 34[2] via P2P/CUMEM
 1: nid006496:242587:243939 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
13: nid006509:201791:203111 [3] NCCL INFO Channel 07/0 : 55[3] -> 54[2] via P2P/CUMEM
 9: nid006505:249083:250431 [3] NCCL INFO Channel 07/0 : 39[3] -> 38[2] via P2P/CUMEM
 0: nid006495:241022:242333 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 04/0 : 43[3] -> 42[2] via P2P/CUMEM
 8: nid006503:218413:219707 [3] NCCL INFO Channel 06/0 : 35[3] -> 34[2] via P2P/CUMEM
10: nid006506:263730:265068 [3] NCCL INFO Channel 06/0 : 43[3] -> 42[2] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 01/0 : 111[3] -> 110[2] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 03/0 : 95[3] -> 94[2] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 01/0 : 79[3] -> 78[2] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 03/0 : 111[3] -> 110[2] via P2P/CUMEM
29: nid007305:27225:28489 [3] NCCL INFO Channel 01/0 : 119[3] -> 118[2] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 05/0 : 95[3] -> 94[2] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 00/0 : 107[3] -> 106[2] via P2P/CUMEM
20: nid006558:215470:216828 [3] NCCL INFO Channel 00/0 : 83[3] -> 82[2] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 01/0 : 103[3] -> 102[2] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 00/0 : 123[3] -> 122[2] via P2P/CUMEM
23: nid006561:220714:222043 [3] NCCL INFO Channel 07/0 : 95[3] -> 94[2] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 00/0 : 67[3] -> 66[2] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 00/0 : 91[3] -> 90[2] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 01/0 : 87[3] -> 86[2] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 05/0 : 111[3] -> 110[2] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 01/0 : 71[3] -> 70[2] via P2P/CUMEM
20: nid006558:215470:216828 [3] NCCL INFO Channel 02/0 : 83[3] -> 82[2] via P2P/CUMEM
18: nid006556:210777:212177 [3] NCCL INFO Channel 00/0 : 75[3] -> 74[2] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 00/0 : 99[3] -> 98[2] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 03/0 : 79[3] -> 78[2] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 00/0 : 115[3] -> 114[2] via P2P/CUMEM
29: nid007305:27225:28489 [3] NCCL INFO Channel 03/0 : 119[3] -> 118[2] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 03/0 : 103[3] -> 102[2] via P2P/CUMEM
20: nid006558:215470:216828 [3] NCCL INFO Channel 04/0 : 83[3] -> 82[2] via P2P/CUMEM
27: nid006566:216749:218063 [3] NCCL INFO Channel 07/0 : 111[3] -> 110[2] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 02/0 : 67[3] -> 66[2] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 05/0 : 79[3] -> 78[2] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 03/0 : 71[3] -> 70[2] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 02/0 : 91[3] -> 90[2] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 02/0 : 99[3] -> 98[2] via P2P/CUMEM
29: nid007305:27225:28489 [3] NCCL INFO Channel 05/0 : 119[3] -> 118[2] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 02/0 : 123[3] -> 122[2] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 02/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 03/0 : 87[3] -> 86[2] via P2P/CUMEM
19: nid006557:208420:209721 [3] NCCL INFO Channel 07/0 : 79[3] -> 78[2] via P2P/CUMEM
20: nid006558:215470:216828 [3] NCCL INFO Channel 06/0 : 83[3] -> 82[2] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 05/0 : 71[3] -> 70[2] via P2P/CUMEM
29: nid007305:27225:28489 [3] NCCL INFO Channel 07/0 : 119[3] -> 118[2] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 05/0 : 103[3] -> 102[2] via P2P/CUMEM
18: nid006556:210777:212177 [3] NCCL INFO Channel 02/0 : 75[3] -> 74[2] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 04/0 : 91[3] -> 90[2] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 02/0 : 115[3] -> 114[2] via P2P/CUMEM
17: nid006555:206596:207953 [3] NCCL INFO Channel 07/0 : 71[3] -> 70[2] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 04/0 : 123[3] -> 122[2] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 04/0 : 99[3] -> 98[2] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 04/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 05/0 : 87[3] -> 86[2] via P2P/CUMEM
30: nid007318:20583:21914 [3] NCCL INFO Channel 06/0 : 123[3] -> 122[2] via P2P/CUMEM
18: nid006556:210777:212177 [3] NCCL INFO Channel 04/0 : 75[3] -> 74[2] via P2P/CUMEM
25: nid006564:223024:224339 [3] NCCL INFO Channel 07/0 : 103[3] -> 102[2] via P2P/CUMEM
22: nid006560:222274:223609 [3] NCCL INFO Channel 06/0 : 91[3] -> 90[2] via P2P/CUMEM
26: nid006565:222468:223816 [3] NCCL INFO Channel 06/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211126:212436 [3] NCCL INFO Channel 07/0 : 87[3] -> 86[2] via P2P/CUMEM
18: nid006556:210777:212177 [3] NCCL INFO Channel 06/0 : 75[3] -> 74[2] via P2P/CUMEM
24: nid006563:221143:222489 [3] NCCL INFO Channel 06/0 : 99[3] -> 98[2] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 04/0 : 115[3] -> 114[2] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 04/0 : 67[3] -> 66[2] via P2P/CUMEM
16: nid006554:221584:222941 [3] NCCL INFO Channel 06/0 : 67[3] -> 66[2] via P2P/CUMEM
28: nid007251:72172:73787 [3] NCCL INFO Channel 06/0 : 115[3] -> 114[2] via P2P/CUMEM
16: nid006554:221581:222938 [0] NCCL INFO Connected all trees
 4: nid006499:254556:255860 [0] NCCL INFO Connected all trees
 1: nid006496:242584:243933 [0] NCCL INFO Connected all trees
 2: nid006497:227575:228915 [0] NCCL INFO Connected all trees
 5: nid006500:260083:261412 [0] NCCL INFO Connected all trees
17: nid006555:206593:207960 [0] NCCL INFO Connected all trees
 3: nid006498:226765:228081 [0] NCCL INFO Connected all trees
19: nid006557:208417:209724 [0] NCCL INFO Connected all trees
18: nid006556:210774:212174 [0] NCCL INFO Connected all trees
27: nid006566:216746:218067 [0] NCCL INFO Connected all trees
20: nid006558:215467:216834 [0] NCCL INFO Connected all trees
15: nid006553:223923:225259 [0] NCCL INFO Connected all trees
31: nid007342:59767:61364 [0] NCCL INFO Connected all trees
 6: nid006501:221941:223241 [0] NCCL INFO Connected all trees
 7: nid006502:252584:253922 [0] NCCL INFO Connected all trees
22: nid006560:222271:223606 [0] NCCL INFO Connected all trees
25: nid006564:223021:224336 [0] NCCL INFO Connected all trees
29: nid007305:27222:28492 [0] NCCL INFO Connected all trees
30: nid007318:20580:21918 [0] NCCL INFO Connected all trees
26: nid006565:222465:223841 [0] NCCL INFO Connected all trees
28: nid007251:72169:73780 [0] NCCL INFO Connected all trees
 0: nid006495:241019:242351 [0] NCCL INFO Connected all trees
 8: nid006503:218410:219701 [0] NCCL INFO Connected all trees
11: nid006507:211335:212654 [0] NCCL INFO Connected all trees
 9: nid006505:249080:250434 [0] NCCL INFO Connected all trees
10: nid006506:263727:265060 [0] NCCL INFO Connected all trees
12: nid006508:205766:207141 [0] NCCL INFO Connected all trees
14: nid006510:229442:230766 [0] NCCL INFO Connected all trees
13: nid006509:201788:203108 [0] NCCL INFO Connected all trees
23: nid006561:220711:222049 [0] NCCL INFO Connected all trees
24: nid006563:221140:222483 [0] NCCL INFO Connected all trees
21: nid006559:211123:212430 [0] NCCL INFO Connected all trees
 0: nid006495:241021:242327 [2] NCCL INFO Connected all trees
16: nid006554:221582:222947 [1] NCCL INFO Connected all trees
16: nid006554:221584:222941 [3] NCCL INFO Connected all trees
16: nid006554:221583:222944 [2] NCCL INFO Connected all trees
 8: nid006503:218411:219698 [1] NCCL INFO Connected all trees
24: nid006563:221141:222486 [1] NCCL INFO Connected all trees
17: nid006555:206594:207957 [1] NCCL INFO Connected all trees
 1: nid006496:242585:243942 [1] NCCL INFO Connected all trees
 8: nid006503:218413:219707 [3] NCCL INFO Connected all trees
 8: nid006503:218412:219704 [2] NCCL INFO Connected all trees
24: nid006563:221143:222489 [3] NCCL INFO Connected all trees
 9: nid006505:249081:250427 [1] NCCL INFO Connected all trees
20: nid006558:215468:216802 [1] NCCL INFO Connected all trees
18: nid006556:210775:212170 [1] NCCL INFO Connected all trees
17: nid006555:206596:207953 [3] NCCL INFO Connected all trees
20: nid006558:215470:216828 [3] NCCL INFO Connected all trees
24: nid006563:221142:222492 [2] NCCL INFO Connected all trees
 9: nid006505:249083:250431 [3] NCCL INFO Connected all trees
21: nid006559:211124:212433 [1] NCCL INFO Connected all trees
22: nid006560:222272:223615 [1] NCCL INFO Connected all trees
 2: nid006497:227576:228867 [1] NCCL INFO Connected all trees
20: nid006558:215469:216833 [2] NCCL INFO Connected all trees
 4: nid006499:254558:255881 [2] NCCL INFO Connected all trees
10: nid006506:263728:265063 [1] NCCL INFO Connected all trees
 9: nid006505:249082:250428 [2] NCCL INFO Connected all trees
18: nid006556:210777:212177 [3] NCCL INFO Connected all trees
 5: nid006500:260084:261419 [1] NCCL INFO Connected all trees
23: nid006561:220712:222052 [1] NCCL INFO Connected all trees
19: nid006557:208418:209751 [1] NCCL INFO Connected all trees
19: nid006557:208420:209721 [3] NCCL INFO Connected all trees
28: nid007251:72170:73781 [1] NCCL INFO Connected all trees
25: nid006564:223022:224342 [1] NCCL INFO Connected all trees
28: nid007251:72171:73784 [2] NCCL INFO Connected all trees
12: nid006508:205767:207134 [1] NCCL INFO Connected all trees
25: nid006564:223024:224339 [3] NCCL INFO Connected all trees
27: nid006566:216747:218064 [1] NCCL INFO Connected all trees
12: nid006508:205769:207138 [3] NCCL INFO Connected all trees
13: nid006509:201789:203117 [1] NCCL INFO Connected all trees
 6: nid006501:221942:223244 [1] NCCL INFO Connected all trees
 1: nid006496:242587:243939 [3] NCCL INFO Connected all trees
14: nid006510:229443:230763 [1] NCCL INFO Connected all trees
 3: nid006498:226766:228078 [1] NCCL INFO Connected all trees
15: nid006553:223924:225253 [1] NCCL INFO Connected all trees
11: nid006507:211336:212653 [1] NCCL INFO Connected all trees
12: nid006508:205768:207135 [2] NCCL INFO Connected all trees
 1: nid006496:242586:243932 [2] NCCL INFO Connected all trees
 3: nid006498:226768:228077 [3] NCCL INFO Connected all trees
11: nid006507:211338:212648 [3] NCCL INFO Connected all trees
17: nid006555:206595:207956 [2] NCCL INFO Connected all trees
 3: nid006498:226767:228084 [2] NCCL INFO Connected all trees
26: nid006565:222466:223838 [1] NCCL INFO Connected all trees
27: nid006566:216749:218063 [3] NCCL INFO Connected all trees
29: nid007305:27223:28486 [1] NCCL INFO Connected all trees
11: nid006507:211337:212647 [2] NCCL INFO Connected all trees
26: nid006565:222468:223816 [3] NCCL INFO Connected all trees
21: nid006559:211126:212436 [3] NCCL INFO Connected all trees
29: nid007305:27225:28489 [3] NCCL INFO Connected all trees
30: nid007318:20581:21915 [1] NCCL INFO Connected all trees
26: nid006565:222467:223813 [2] NCCL INFO Connected all trees
21: nid006559:211125:212429 [2] NCCL INFO Connected all trees
27: nid006566:216748:218070 [2] NCCL INFO Connected all trees
30: nid007318:20582:21921 [2] NCCL INFO Connected all trees
22: nid006560:222274:223609 [3] NCCL INFO Connected all trees
 2: nid006497:227578:228893 [3] NCCL INFO Connected all trees
 7: nid006502:252585:253928 [1] NCCL INFO Connected all trees
 4: nid006499:254559:255857 [3] NCCL INFO Connected all trees
10: nid006506:263730:265068 [3] NCCL INFO Connected all trees
22: nid006560:222273:223612 [2] NCCL INFO Connected all trees
 2: nid006497:227577:228868 [2] NCCL INFO Connected all trees
18: nid006556:210776:212171 [2] NCCL INFO Connected all trees
 7: nid006502:252587:253919 [3] NCCL INFO Connected all trees
 5: nid006500:260086:261418 [3] NCCL INFO Connected all trees
19: nid006557:208419:209727 [2] NCCL INFO Connected all trees
10: nid006506:263729:265069 [2] NCCL INFO Connected all trees
 7: nid006502:252586:253925 [2] NCCL INFO Connected all trees
25: nid006564:223023:224335 [2] NCCL INFO Connected all trees
 5: nid006500:260085:261413 [2] NCCL INFO Connected all trees
23: nid006561:220714:222043 [3] NCCL INFO Connected all trees
31: nid007342:59768:61425 [1] NCCL INFO Connected all trees
13: nid006509:201790:203114 [2] NCCL INFO Connected all trees
13: nid006509:201791:203111 [3] NCCL INFO Connected all trees
 6: nid006501:221944:223249 [3] NCCL INFO Connected all trees
14: nid006510:229445:230769 [3] NCCL INFO Connected all trees
23: nid006561:220713:222046 [2] NCCL INFO Connected all trees
31: nid007342:59770:61422 [3] NCCL INFO Connected all trees
15: nid006553:223926:225256 [3] NCCL INFO Connected all trees
 6: nid006501:221943:223250 [2] NCCL INFO Connected all trees
14: nid006510:229444:230760 [2] NCCL INFO Connected all trees
31: nid007342:59769:61363 [2] NCCL INFO Connected all trees
29: nid007305:27224:28495 [2] NCCL INFO Connected all trees
30: nid007318:20583:21914 [3] NCCL INFO Connected all trees
15: nid006553:223925:225252 [2] NCCL INFO Connected all trees
 0: nid006495:241022:242333 [3] NCCL INFO Connected all trees
 0: nid006495:241020:242328 [1] NCCL INFO Connected all trees
 4: nid006499:254557:255854 [1] NCCL INFO Connected all trees
28: nid007251:72172:73787 [3] NCCL INFO Connected all trees
 0: wandb: Currently logged in as: nicolas-deperrois (krauthammerlab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
 0: wandb: Tracking run with wandb version 0.19.11
 0: wandb: Run data is saved locally in /capstor/scratch/cscs/ndeperr/code/RadVLM/wandb/run-20250624_192900-5zockb19
 0: wandb: Run `wandb offline` to turn off syncing.
 0: wandb: Syncing run radvlm-sft
 0: wandb:  View project at https://wandb.ai/krauthammerlab/huggingface
 0: wandb:  View run at https://wandb.ai/krauthammerlab/huggingface/runs/5zockb19
 0:   0%|          | 0/4398 [00:00<?, ?it/s]
22: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
22:   return fn(*args, **kwargs)
23: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
23:   return fn(*args, **kwargs)
23: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
23:   return fn(*args, **kwargs)
20: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
20:   return fn(*args, **kwargs)
21: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
21:   return fn(*args, **kwargs)
28: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
28:   return fn(*args, **kwargs)
 4: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 4:   return fn(*args, **kwargs)
 4: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 4:   return fn(*args, **kwargs)
24: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
24:   return fn(*args, **kwargs)
 5: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 5:   return fn(*args, **kwargs)
13: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
13:   return fn(*args, **kwargs)
23: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
23:   return fn(*args, **kwargs)
23: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
23:   return fn(*args, **kwargs)
21: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
21:   return fn(*args, **kwargs)
 6: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 6:   return fn(*args, **kwargs)
21: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
21:   return fn(*args, **kwargs)
19: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
19:   return fn(*args, **kwargs)
28: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
28:   return fn(*args, **kwargs)
19: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
19:   return fn(*args, **kwargs)
28: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
28:   return fn(*args, **kwargs)
26: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
26:   return fn(*args, **kwargs)
24: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
24:   return fn(*args, **kwargs)
25: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
25:   return fn(*args, **kwargs)
27: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
27:   return fn(*args, **kwargs)
27: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
27:   return fn(*args, **kwargs)
26: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
26:   return fn(*args, **kwargs)
29: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
29:   return fn(*args, **kwargs)
29: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
29:   return fn(*args, **kwargs)
10: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
10:   return fn(*args, **kwargs)
21: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
21:   return fn(*args, **kwargs)
30: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
30:   return fn(*args, **kwargs)
20: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
20:   return fn(*args, **kwargs)
19: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
19:   return fn(*args, **kwargs)
24: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
24:   return fn(*args, **kwargs)
22: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
22:   return fn(*args, **kwargs)
28: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
28:   return fn(*args, **kwargs)
 1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 1:   return fn(*args, **kwargs)
25: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
25:   return fn(*args, **kwargs)
27: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
27:   return fn(*args, **kwargs)
26: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
26:   return fn(*args, **kwargs)
30: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
30:   return fn(*args, **kwargs)
25: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
25:   return fn(*args, **kwargs)
29: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
29:   return fn(*args, **kwargs)
 1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 1:   return fn(*args, **kwargs)
20: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
20:   return fn(*args, **kwargs)
24: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
24:   return fn(*args, **kwargs)
22: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
22:   return fn(*args, **kwargs)
 2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 2:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 2:   return fn(*args, **kwargs)
19: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
19:   return fn(*args, **kwargs)
31: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
31:   return fn(*args, **kwargs)
 3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 3:   return fn(*args, **kwargs)
26: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
26:   return fn(*args, **kwargs)
 3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 3:   return fn(*args, **kwargs)
27: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
27:   return fn(*args, **kwargs)
20: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
20:   return fn(*args, **kwargs)
29: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
29:   return fn(*args, **kwargs)
 1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 1:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
22: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
22:   return fn(*args, **kwargs)
30: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
30:   return fn(*args, **kwargs)
30: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
30:   return fn(*args, **kwargs)
25: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
25:   return fn(*args, **kwargs)
 8: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 8:   return fn(*args, **kwargs)
 2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 2:   return fn(*args, **kwargs)
 9: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 9:   return fn(*args, **kwargs)
 5: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 5:   return fn(*args, **kwargs)
 6: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 6:   return fn(*args, **kwargs)
31: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
31:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 7: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 7:   return fn(*args, **kwargs)
 4: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 4:   return fn(*args, **kwargs)
 3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 3:   return fn(*args, **kwargs)
 1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 1:   return fn(*args, **kwargs)
 7: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 7:   return fn(*args, **kwargs)
 5: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 5:   return fn(*args, **kwargs)
11: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
11:   return fn(*args, **kwargs)
31: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
31:   return fn(*args, **kwargs)
 2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 2:   return fn(*args, **kwargs)
 6: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 6:   return fn(*args, **kwargs)
 4: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 4:   return fn(*args, **kwargs)
11: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
11:   return fn(*args, **kwargs)
 5: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 5:   return fn(*args, **kwargs)
10: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
10:   return fn(*args, **kwargs)
 3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 3:   return fn(*args, **kwargs)
 9: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 9:   return fn(*args, **kwargs)
 8: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 8:   return fn(*args, **kwargs)
 8: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 8:   return fn(*args, **kwargs)
 6: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 6:   return fn(*args, **kwargs)
15: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
15:   return fn(*args, **kwargs)
10: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
10:   return fn(*args, **kwargs)
16: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
16:   return fn(*args, **kwargs)
31: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
31:   return fn(*args, **kwargs)
13: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
13:   return fn(*args, **kwargs)
17: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
17:   return fn(*args, **kwargs)
12: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
12:   return fn(*args, **kwargs)
12: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
12:   return fn(*args, **kwargs)
16: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
16:   return fn(*args, **kwargs)
 7: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 7:   return fn(*args, **kwargs)
 9: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 9:   return fn(*args, **kwargs)
17: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
17:   return fn(*args, **kwargs)
14: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
14:   return fn(*args, **kwargs)
15: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
15:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 9: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 9:   return fn(*args, **kwargs)
17: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
17:   return fn(*args, **kwargs)
10: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
10:   return fn(*args, **kwargs)
11: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
11:   return fn(*args, **kwargs)
12: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
12:   return fn(*args, **kwargs)
 7: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 7:   return fn(*args, **kwargs)
 8: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 8:   return fn(*args, **kwargs)
14: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
14:   return fn(*args, **kwargs)
13: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
13:   return fn(*args, **kwargs)
11: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
11:   return fn(*args, **kwargs)
16: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
16:   return fn(*args, **kwargs)
14: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
14:   return fn(*args, **kwargs)
15: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
15:   return fn(*args, **kwargs)
13: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
13:   return fn(*args, **kwargs)
16: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
16:   return fn(*args, **kwargs)
15: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
15:   return fn(*args, **kwargs)
17: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
17:   return fn(*args, **kwargs)
14: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
14:   return fn(*args, **kwargs)
12: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
12:   return fn(*args, **kwargs)
18: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
18:   return fn(*args, **kwargs)
18: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
18:   return fn(*args, **kwargs)
18: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
18:   return fn(*args, **kwargs)
18: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
18:   return fn(*args, **kwargs)
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
26:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
25:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
30:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 9:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
28:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
29:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
21:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
11:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
16:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
31:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
20:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
27:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
18:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
14:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
13:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 7:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 5:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 6:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
19:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
23:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
12:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 1:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
22:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
15:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
24:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 8:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
10:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 3:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
17:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 4:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 2:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
25: nid006564:223024:225759 [3] NCCL INFO Using network AWS Libfabric
17: nid006555:206595:209409 [2] NCCL INFO Using network AWS Libfabric
17: nid006555:206596:209407 [3] NCCL INFO Using network AWS Libfabric
30: nid007318:20583:23369 [3] NCCL INFO Using network AWS Libfabric
30: nid007318:20583:23369 [3] NCCL INFO DMA-BUF is available on GPU device 3
25: nid006564:223024:225759 [3] NCCL INFO DMA-BUF is available on GPU device 3
23: nid006561:220714:223485 [3] NCCL INFO Using network AWS Libfabric
23: nid006561:220713:223486 [2] NCCL INFO Using network AWS Libfabric
27: nid006566:216747:219509 [1] NCCL INFO Using network AWS Libfabric
25: nid006564:223021:225757 [0] NCCL INFO Using network AWS Libfabric
25: nid006564:223023:225756 [2] NCCL INFO Using network AWS Libfabric
23: nid006561:220713:223486 [2] NCCL INFO DMA-BUF is available on GPU device 2
23: nid006561:220714:223485 [3] NCCL INFO DMA-BUF is available on GPU device 3
27: nid006566:216747:219509 [1] NCCL INFO DMA-BUF is available on GPU device 1
27: nid006566:216749:219512 [3] NCCL INFO Using network AWS Libfabric
17: nid006555:206594:209408 [1] NCCL INFO Using network AWS Libfabric
17: nid006555:206593:209410 [0] NCCL INFO Using network AWS Libfabric
17: nid006555:206596:209407 [3] NCCL INFO DMA-BUF is available on GPU device 3
17: nid006555:206595:209409 [2] NCCL INFO DMA-BUF is available on GPU device 2
17: nid006555:206594:209408 [1] NCCL INFO DMA-BUF is available on GPU device 1
20: nid006558:215470:218259 [3] NCCL INFO Using network AWS Libfabric
18: nid006556:210776:213633 [2] NCCL INFO Using network AWS Libfabric
23: nid006561:220711:223488 [0] NCCL INFO Using network AWS Libfabric
23: nid006561:220712:223487 [1] NCCL INFO Using network AWS Libfabric
28: nid007251:72172:75238 [3] NCCL INFO Using network AWS Libfabric
27: nid006566:216748:219511 [2] NCCL INFO Using network AWS Libfabric
30: nid007318:20581:23371 [1] NCCL INFO Using network AWS Libfabric
 9: nid006505:249081:251864 [1] NCCL INFO Using network AWS Libfabric
18: nid006556:210775:213634 [1] NCCL INFO Using network AWS Libfabric
28: nid007251:72169:75235 [0] NCCL INFO Using network AWS Libfabric
28: nid007251:72170:75236 [1] NCCL INFO Using network AWS Libfabric
27: nid006566:216746:219510 [0] NCCL INFO Using network AWS Libfabric
27: nid006566:216749:219512 [3] NCCL INFO DMA-BUF is available on GPU device 3
30: nid007318:20582:23370 [2] NCCL INFO Using network AWS Libfabric
30: nid007318:20580:23372 [0] NCCL INFO Using network AWS Libfabric
 9: nid006505:249082:251866 [2] NCCL INFO Using network AWS Libfabric
18: nid006556:210774:213632 [0] NCCL INFO Using network AWS Libfabric
25: nid006564:223022:225758 [1] NCCL INFO Using network AWS Libfabric
28: nid007251:72171:75237 [2] NCCL INFO Using network AWS Libfabric
27: nid006566:216748:219511 [2] NCCL INFO DMA-BUF is available on GPU device 2
30: nid007318:20581:23371 [1] NCCL INFO DMA-BUF is available on GPU device 1
13: nid006509:201791:204556 [3] NCCL INFO Using network AWS Libfabric
18: nid006556:210777:213635 [3] NCCL INFO Using network AWS Libfabric
18: nid006556:210776:213633 [2] NCCL INFO DMA-BUF is available on GPU device 2
18: nid006556:210775:213634 [1] NCCL INFO DMA-BUF is available on GPU device 1
14: nid006510:229444:232220 [2] NCCL INFO Using network AWS Libfabric
25: nid006564:223023:225756 [2] NCCL INFO DMA-BUF is available on GPU device 2
25: nid006564:223021:225757 [0] NCCL INFO DMA-BUF is available on GPU device 0
28: nid007251:72172:75238 [3] NCCL INFO DMA-BUF is available on GPU device 3
28: nid007251:72169:75235 [0] NCCL INFO DMA-BUF is available on GPU device 0
27: nid006566:216746:219510 [0] NCCL INFO DMA-BUF is available on GPU device 0
 1: nid006496:242586:245386 [2] NCCL INFO Using network AWS Libfabric
20: nid006558:215468:218258 [1] NCCL INFO Using network AWS Libfabric
20: nid006558:215467:218261 [0] NCCL INFO Using network AWS Libfabric
 0: nid006495:241020:244121 [1] NCCL INFO Using network AWS Libfabric
 0: nid006495:241019:244118 [0] NCCL INFO Using network AWS Libfabric
16: nid006554:221584:224381 [3] NCCL INFO Using network AWS Libfabric
14: nid006510:229443:232223 [1] NCCL INFO Using network AWS Libfabric
25: nid006564:223022:225758 [1] NCCL INFO DMA-BUF is available on GPU device 1
 9: nid006505:249083:251865 [3] NCCL INFO Using network AWS Libfabric
 9: nid006505:249081:251864 [1] NCCL INFO DMA-BUF is available on GPU device 1
 1: nid006496:242584:245387 [0] NCCL INFO Using network AWS Libfabric
20: nid006558:215469:218260 [2] NCCL INFO Using network AWS Libfabric
20: nid006558:215470:218259 [3] NCCL INFO DMA-BUF is available on GPU device 3
 0: nid006495:241021:244119 [2] NCCL INFO Using network AWS Libfabric
 0: nid006495:241022:244120 [3] NCCL INFO Using network AWS Libfabric
 8: nid006503:218412:221169 [2] NCCL INFO Using network AWS Libfabric
 8: nid006503:218411:221167 [1] NCCL INFO Using network AWS Libfabric
16: nid006554:221582:224382 [1] NCCL INFO Using network AWS Libfabric
16: nid006554:221581:224383 [0] NCCL INFO Using network AWS Libfabric
 3: nid006498:226768:229503 [3] NCCL INFO Using network AWS Libfabric
 9: nid006505:249080:251867 [0] NCCL INFO Using network AWS Libfabric
 9: nid006505:249082:251866 [2] NCCL INFO DMA-BUF is available on GPU device 2
15: nid006553:223924:226695 [1] NCCL INFO Using network AWS Libfabric
15: nid006553:223925:226694 [2] NCCL INFO Using network AWS Libfabric
11: nid006507:211338:214084 [3] NCCL INFO Using network AWS Libfabric
 1: nid006496:242587:245388 [3] NCCL INFO Using network AWS Libfabric
 1: nid006496:242586:245386 [2] NCCL INFO DMA-BUF is available on GPU device 2
 1: nid006496:242585:245389 [1] NCCL INFO Using network AWS Libfabric
20: nid006558:215468:218258 [1] NCCL INFO DMA-BUF is available on GPU device 1
 0: nid006495:241020:244121 [1] NCCL INFO DMA-BUF is available on GPU device 1
 0: nid006495:241019:244118 [0] NCCL INFO DMA-BUF is available on GPU device 0
 0: nid006495:241021:244119 [2] NCCL INFO DMA-BUF is available on GPU device 2
 8: nid006503:218410:221166 [0] NCCL INFO Using network AWS Libfabric
 8: nid006503:218413:221168 [3] NCCL INFO Using network AWS Libfabric
 3: nid006498:226765:229505 [0] NCCL INFO Using network AWS Libfabric
 3: nid006498:226766:229504 [1] NCCL INFO Using network AWS Libfabric
15: nid006553:223926:226696 [3] NCCL INFO Using network AWS Libfabric
15: nid006553:223923:226697 [0] NCCL INFO Using network AWS Libfabric
11: nid006507:211337:214086 [2] NCCL INFO Using network AWS Libfabric
 1: nid006496:242584:245387 [0] NCCL INFO DMA-BUF is available on GPU device 0
20: nid006558:215469:218260 [2] NCCL INFO DMA-BUF is available on GPU device 2
20: nid006558:215467:218261 [0] NCCL INFO DMA-BUF is available on GPU device 0
 0: nid006495:241022:244120 [3] NCCL INFO DMA-BUF is available on GPU device 3
 3: nid006498:226767:229506 [2] NCCL INFO Using network AWS Libfabric
 3: nid006498:226768:229503 [3] NCCL INFO DMA-BUF is available on GPU device 3
15: nid006553:223925:226694 [2] NCCL INFO DMA-BUF is available on GPU device 2
15: nid006553:223924:226695 [1] NCCL INFO DMA-BUF is available on GPU device 1
11: nid006507:211338:214084 [3] NCCL INFO DMA-BUF is available on GPU device 3
11: nid006507:211337:214086 [2] NCCL INFO DMA-BUF is available on GPU device 2
11: nid006507:211335:214085 [0] NCCL INFO Using network AWS Libfabric
 1: nid006496:242587:245388 [3] NCCL INFO DMA-BUF is available on GPU device 3
 9: nid006505:249083:251865 [3] NCCL INFO DMA-BUF is available on GPU device 3
15: nid006553:223926:226696 [3] NCCL INFO DMA-BUF is available on GPU device 3
11: nid006507:211336:214087 [1] NCCL INFO Using network AWS Libfabric
 1: nid006496:242585:245389 [1] NCCL INFO DMA-BUF is available on GPU device 1
 2: nid006497:227577:230334 [2] NCCL INFO Using network AWS Libfabric
 9: nid006505:249080:251867 [0] NCCL INFO DMA-BUF is available on GPU device 0
15: nid006553:223923:226697 [0] NCCL INFO DMA-BUF is available on GPU device 0
11: nid006507:211335:214085 [0] NCCL INFO DMA-BUF is available on GPU device 0
11: nid006507:211336:214087 [1] NCCL INFO DMA-BUF is available on GPU device 1
 2: nid006497:227577:230334 [2] NCCL INFO DMA-BUF is available on GPU device 2
 7: nid006502:252584:255362 [0] NCCL INFO Using network AWS Libfabric
 7: nid006502:252587:255364 [3] NCCL INFO Using network AWS Libfabric
10: nid006506:263730:266490 [3] NCCL INFO Using network AWS Libfabric
 2: nid006497:227578:230336 [3] NCCL INFO Using network AWS Libfabric
 2: nid006497:227575:230335 [0] NCCL INFO Using network AWS Libfabric
 7: nid006502:252586:255365 [2] NCCL INFO Using network AWS Libfabric
 7: nid006502:252587:255364 [3] NCCL INFO DMA-BUF is available on GPU device 3
 7: nid006502:252584:255362 [0] NCCL INFO DMA-BUF is available on GPU device 0
10: nid006506:263729:266489 [2] NCCL INFO Using network AWS Libfabric
10: nid006506:263728:266492 [1] NCCL INFO Using network AWS Libfabric
 2: nid006497:227576:230337 [1] NCCL INFO Using network AWS Libfabric
 7: nid006502:252585:255363 [1] NCCL INFO Using network AWS Libfabric
 5: nid006500:260083:262846 [0] NCCL INFO Using network AWS Libfabric
 4: nid006499:254559:257317 [3] NCCL INFO Using network AWS Libfabric
10: nid006506:263727:266491 [0] NCCL INFO Using network AWS Libfabric
 2: nid006497:227578:230336 [3] NCCL INFO DMA-BUF is available on GPU device 3
 7: nid006502:252586:255365 [2] NCCL INFO DMA-BUF is available on GPU device 2
 5: nid006500:260085:262848 [2] NCCL INFO Using network AWS Libfabric
 4: nid006499:254557:257316 [1] NCCL INFO Using network AWS Libfabric
 4: nid006499:254558:257318 [2] NCCL INFO Using network AWS Libfabric
10: nid006506:263730:266490 [3] NCCL INFO DMA-BUF is available on GPU device 3
10: nid006506:263729:266489 [2] NCCL INFO DMA-BUF is available on GPU device 2
 2: nid006497:227575:230335 [0] NCCL INFO DMA-BUF is available on GPU device 0
 2: nid006497:227576:230337 [1] NCCL INFO DMA-BUF is available on GPU device 1
17: nid006555:206593:209410 [0] NCCL INFO DMA-BUF is available on GPU device 0
 7: nid006502:252585:255363 [1] NCCL INFO DMA-BUF is available on GPU device 1
 5: nid006500:260086:262847 [3] NCCL INFO Using network AWS Libfabric
 5: nid006500:260084:262849 [1] NCCL INFO Using network AWS Libfabric
 5: nid006500:260085:262848 [2] NCCL INFO DMA-BUF is available on GPU device 2
 5: nid006500:260083:262846 [0] NCCL INFO DMA-BUF is available on GPU device 0
29: nid007305:27225:29913 [3] NCCL INFO Using network AWS Libfabric
 4: nid006499:254556:257319 [0] NCCL INFO Using network AWS Libfabric
 4: nid006499:254559:257317 [3] NCCL INFO DMA-BUF is available on GPU device 3
10: nid006506:263728:266492 [1] NCCL INFO DMA-BUF is available on GPU device 1
 5: nid006500:260086:262847 [3] NCCL INFO DMA-BUF is available on GPU device 3
29: nid007305:27222:29915 [0] NCCL INFO Using network AWS Libfabric
29: nid007305:27223:29914 [1] NCCL INFO Using network AWS Libfabric
 4: nid006499:254557:257316 [1] NCCL INFO DMA-BUF is available on GPU device 1
 4: nid006499:254558:257318 [2] NCCL INFO DMA-BUF is available on GPU device 2
 4: nid006499:254556:257319 [0] NCCL INFO DMA-BUF is available on GPU device 0
10: nid006506:263727:266491 [0] NCCL INFO DMA-BUF is available on GPU device 0
 5: nid006500:260084:262849 [1] NCCL INFO DMA-BUF is available on GPU device 1
24: nid006563:221142:223915 [2] NCCL INFO Using network AWS Libfabric
26: nid006565:222465:225268 [0] NCCL INFO Using network AWS Libfabric
26: nid006565:222466:225266 [1] NCCL INFO Using network AWS Libfabric
29: nid007305:27224:29916 [2] NCCL INFO Using network AWS Libfabric
29: nid007305:27225:29913 [3] NCCL INFO DMA-BUF is available on GPU device 3
24: nid006563:221143:223916 [3] NCCL INFO Using network AWS Libfabric
24: nid006563:221140:223917 [0] NCCL INFO Using network AWS Libfabric
26: nid006565:222467:225267 [2] NCCL INFO Using network AWS Libfabric
29: nid007305:27222:29915 [0] NCCL INFO DMA-BUF is available on GPU device 0
29: nid007305:27223:29914 [1] NCCL INFO DMA-BUF is available on GPU device 1
24: nid006563:221141:223918 [1] NCCL INFO Using network AWS Libfabric
24: nid006563:221142:223915 [2] NCCL INFO DMA-BUF is available on GPU device 2
26: nid006565:222468:225265 [3] NCCL INFO Using network AWS Libfabric
29: nid007305:27224:29916 [2] NCCL INFO DMA-BUF is available on GPU device 2
24: nid006563:221140:223917 [0] NCCL INFO DMA-BUF is available on GPU device 0
24: nid006563:221143:223916 [3] NCCL INFO DMA-BUF is available on GPU device 3
26: nid006565:222466:225266 [1] NCCL INFO DMA-BUF is available on GPU device 1
26: nid006565:222468:225265 [3] NCCL INFO DMA-BUF is available on GPU device 3
26: nid006565:222467:225267 [2] NCCL INFO DMA-BUF is available on GPU device 2
21: nid006559:211125:213871 [2] NCCL INFO Using network AWS Libfabric
24: nid006563:221141:223918 [1] NCCL INFO DMA-BUF is available on GPU device 1
26: nid006565:222465:225268 [0] NCCL INFO DMA-BUF is available on GPU device 0
21: nid006559:211123:213872 [0] NCCL INFO Using network AWS Libfabric
21: nid006559:211125:213871 [2] NCCL INFO DMA-BUF is available on GPU device 2
30: nid007318:20580:23372 [0] NCCL INFO DMA-BUF is available on GPU device 0
30: nid007318:20582:23370 [2] NCCL INFO DMA-BUF is available on GPU device 2
22: nid006560:222274:225056 [3] NCCL INFO Using network AWS Libfabric
18: nid006556:210774:213632 [0] NCCL INFO DMA-BUF is available on GPU device 0
21: nid006559:211124:213873 [1] NCCL INFO Using network AWS Libfabric
21: nid006559:211126:213874 [3] NCCL INFO Using network AWS Libfabric
22: nid006560:222272:225053 [1] NCCL INFO Using network AWS Libfabric
22: nid006560:222273:225054 [2] NCCL INFO Using network AWS Libfabric
18: nid006556:210777:213635 [3] NCCL INFO DMA-BUF is available on GPU device 3
19: nid006557:208417:211182 [0] NCCL INFO Using network AWS Libfabric
19: nid006557:208420:211184 [3] NCCL INFO Using network AWS Libfabric
21: nid006559:211123:213872 [0] NCCL INFO DMA-BUF is available on GPU device 0
22: nid006560:222271:225055 [0] NCCL INFO Using network AWS Libfabric
22: nid006560:222274:225056 [3] NCCL INFO DMA-BUF is available on GPU device 3
19: nid006557:208419:211183 [2] NCCL INFO Using network AWS Libfabric
19: nid006557:208418:211185 [1] NCCL INFO Using network AWS Libfabric
21: nid006559:211124:213873 [1] NCCL INFO DMA-BUF is available on GPU device 1
31: nid007342:59770:62998 [3] NCCL INFO Using network AWS Libfabric
22: nid006560:222272:225053 [1] NCCL INFO DMA-BUF is available on GPU device 1
23: nid006561:220711:223488 [0] NCCL INFO DMA-BUF is available on GPU device 0
19: nid006557:208417:211182 [0] NCCL INFO DMA-BUF is available on GPU device 0
19: nid006557:208420:211184 [3] NCCL INFO DMA-BUF is available on GPU device 3
21: nid006559:211126:213874 [3] NCCL INFO DMA-BUF is available on GPU device 3
31: nid007342:59767:62995 [0] NCCL INFO Using network AWS Libfabric
31: nid007342:59768:62996 [1] NCCL INFO Using network AWS Libfabric
22: nid006560:222273:225054 [2] NCCL INFO DMA-BUF is available on GPU device 2
22: nid006560:222271:225055 [0] NCCL INFO DMA-BUF is available on GPU device 0
23: nid006561:220712:223487 [1] NCCL INFO DMA-BUF is available on GPU device 1
19: nid006557:208419:211183 [2] NCCL INFO DMA-BUF is available on GPU device 2
28: nid007251:72170:75236 [1] NCCL INFO DMA-BUF is available on GPU device 1
31: nid007342:59769:62997 [2] NCCL INFO Using network AWS Libfabric
19: nid006557:208418:211185 [1] NCCL INFO DMA-BUF is available on GPU device 1
28: nid007251:72171:75237 [2] NCCL INFO DMA-BUF is available on GPU device 2
31: nid007342:59770:62998 [3] NCCL INFO DMA-BUF is available on GPU device 3
12: nid006508:205767:208640 [1] NCCL INFO Using network AWS Libfabric
12: nid006508:205769:208641 [3] NCCL INFO Using network AWS Libfabric
13: nid006509:201789:204557 [1] NCCL INFO Using network AWS Libfabric
13: nid006509:201788:204558 [0] NCCL INFO Using network AWS Libfabric
13: nid006509:201790:204559 [2] NCCL INFO Using network AWS Libfabric
31: nid007342:59767:62995 [0] NCCL INFO DMA-BUF is available on GPU device 0
31: nid007342:59769:62997 [2] NCCL INFO DMA-BUF is available on GPU device 2
31: nid007342:59768:62996 [1] NCCL INFO DMA-BUF is available on GPU device 1
12: nid006508:205766:208643 [0] NCCL INFO Using network AWS Libfabric
12: nid006508:205768:208642 [2] NCCL INFO Using network AWS Libfabric
13: nid006509:201791:204556 [3] NCCL INFO DMA-BUF is available on GPU device 3
12: nid006508:205767:208640 [1] NCCL INFO DMA-BUF is available on GPU device 1
13: nid006509:201789:204557 [1] NCCL INFO DMA-BUF is available on GPU device 1
13: nid006509:201788:204558 [0] NCCL INFO DMA-BUF is available on GPU device 0
13: nid006509:201790:204559 [2] NCCL INFO DMA-BUF is available on GPU device 2
14: nid006510:229445:232221 [3] NCCL INFO Using network AWS Libfabric
14: nid006510:229442:232222 [0] NCCL INFO Using network AWS Libfabric
12: nid006508:205769:208641 [3] NCCL INFO DMA-BUF is available on GPU device 3
14: nid006510:229444:232220 [2] NCCL INFO DMA-BUF is available on GPU device 2
12: nid006508:205766:208643 [0] NCCL INFO DMA-BUF is available on GPU device 0
12: nid006508:205768:208642 [2] NCCL INFO DMA-BUF is available on GPU device 2
 6: nid006501:221941:224722 [0] NCCL INFO Using network AWS Libfabric
 6: nid006501:221942:224719 [1] NCCL INFO Using network AWS Libfabric
 6: nid006501:221943:224720 [2] NCCL INFO Using network AWS Libfabric
 8: nid006503:218411:221167 [1] NCCL INFO DMA-BUF is available on GPU device 1
 8: nid006503:218412:221169 [2] NCCL INFO DMA-BUF is available on GPU device 2
16: nid006554:221583:224384 [2] NCCL INFO Using network AWS Libfabric
16: nid006554:221584:224381 [3] NCCL INFO DMA-BUF is available on GPU device 3
14: nid006510:229443:232223 [1] NCCL INFO DMA-BUF is available on GPU device 1
 3: nid006498:226765:229505 [0] NCCL INFO DMA-BUF is available on GPU device 0
 3: nid006498:226766:229504 [1] NCCL INFO DMA-BUF is available on GPU device 1
 6: nid006501:221944:224721 [3] NCCL INFO Using network AWS Libfabric
 8: nid006503:218410:221166 [0] NCCL INFO DMA-BUF is available on GPU device 0
16: nid006554:221581:224383 [0] NCCL INFO DMA-BUF is available on GPU device 0
14: nid006510:229442:232222 [0] NCCL INFO DMA-BUF is available on GPU device 0
14: nid006510:229445:232221 [3] NCCL INFO DMA-BUF is available on GPU device 3
 3: nid006498:226767:229506 [2] NCCL INFO DMA-BUF is available on GPU device 2
 6: nid006501:221941:224722 [0] NCCL INFO DMA-BUF is available on GPU device 0
 8: nid006503:218413:221168 [3] NCCL INFO DMA-BUF is available on GPU device 3
16: nid006554:221582:224382 [1] NCCL INFO DMA-BUF is available on GPU device 1
 6: nid006501:221942:224719 [1] NCCL INFO DMA-BUF is available on GPU device 1
 6: nid006501:221943:224720 [2] NCCL INFO DMA-BUF is available on GPU device 2
16: nid006554:221583:224384 [2] NCCL INFO DMA-BUF is available on GPU device 2
 6: nid006501:221944:224721 [3] NCCL INFO DMA-BUF is available on GPU device 3
27: nid006566:216749:219512 [3] NCCL INFO bootstrapSplit: comm 0x4006f159d530 parent 0xaaaaf3be1d00 rank 111 nranks 128 color 315732477 key 111 prev 110 next 112 - DONE
27: nid006566:216749:219512 [3] NCCL INFO ncclCommSplit comm 0x4006f159d530 rank 111 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf3be1d00 color 315732477 key 111 commId 0xd4eed6c1342b1de7 - Init START
30: nid007318:20583:23369 [3] NCCL INFO bootstrapSplit: comm 0x4006cc0d5940 parent 0xaaaaee514e10 rank 123 nranks 128 color 315732477 key 123 prev 122 next 124 - DONE
30: nid007318:20581:23371 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d360 parent 0xaaab01d20530 rank 121 nranks 128 color 315732477 key 121 prev 120 next 122 - DONE
20: nid006558:215467:218261 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d340 parent 0xaaab024c49d0 rank 80 nranks 128 color 315732477 key 80 prev 79 next 81 - DONE
20: nid006558:215467:218261 [0] NCCL INFO ncclCommSplit comm 0x40065d59d340 rank 80 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab024c49d0 color 315732477 key 80 commId 0xd4eed6c1342b1de7 - Init START
27: nid006566:216748:219511 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d2d0 parent 0xaaab0ae01a50 rank 110 nranks 128 color 315732477 key 110 prev 109 next 111 - DONE
27: nid006566:216748:219511 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d2d0 rank 110 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab0ae01a50 color 315732477 key 110 commId 0xd4eed6c1342b1de7 - Init START
27: nid006566:216746:219510 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d1d0 parent 0xaaab08db5770 rank 108 nranks 128 color 315732477 key 108 prev 107 next 109 - DONE
27: nid006566:216747:219509 [1] NCCL INFO bootstrapSplit: comm 0x4006a559d640 parent 0xaaab06140ab0 rank 109 nranks 128 color 315732477 key 109 prev 108 next 110 - DONE
20: nid006558:215469:218260 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d3c0 parent 0xaaaaf4921880 rank 82 nranks 128 color 315732477 key 82 prev 81 next 83 - DONE
20: nid006558:215468:218258 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d590 parent 0xaaaae4530560 rank 81 nranks 128 color 315732477 key 81 prev 80 next 82 - DONE
20: nid006558:215469:218260 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 82 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf4921880 color 315732477 key 82 commId 0xd4eed6c1342b1de7 - Init START
 0: nid006495:241019:244118 [0] NCCL INFO bootstrapSplit: comm 0x4006b00d5c40 parent 0xaaaaca364de0 rank 0 nranks 128 color 315732477 key 0 prev 127 next 1 - DONE
 0: nid006495:241021:244119 [2] NCCL INFO bootstrapSplit: comm 0x4006ad59d440 parent 0xaaab287e25d0 rank 2 nranks 128 color 315732477 key 2 prev 1 next 3 - DONE
19: nid006557:208420:211184 [3] NCCL INFO bootstrapSplit: comm 0x4006f159d540 parent 0xaaab123500f0 rank 79 nranks 128 color 315732477 key 79 prev 78 next 80 - DONE
19: nid006557:208418:211185 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d540 parent 0xaaab2b67f850 rank 77 nranks 128 color 315732477 key 77 prev 76 next 78 - DONE
27: nid006566:216746:219510 [0] NCCL INFO ncclCommSplit comm 0x40065d59d1d0 rank 108 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab08db5770 color 315732477 key 108 commId 0xd4eed6c1342b1de7 - Init START
27: nid006566:216747:219509 [1] NCCL INFO ncclCommSplit comm 0x4006a559d640 rank 109 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab06140ab0 color 315732477 key 109 commId 0xd4eed6c1342b1de7 - Init START
 9: nid006505:249083:251865 [3] NCCL INFO bootstrapSplit: comm 0x4006b0abf8a0 parent 0xaaab25b11310 rank 39 nranks 128 color 315732477 key 39 prev 38 next 40 - DONE
 9: nid006505:249083:251865 [3] NCCL INFO ncclCommSplit comm 0x4006b0abf8a0 rank 39 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab25b11310 color 315732477 key 39 commId 0xd4eed6c1342b1de7 - Init START
20: nid006558:215470:218259 [3] NCCL INFO bootstrapSplit: comm 0x4006c959d3c0 parent 0xaaab04942a10 rank 83 nranks 128 color 315732477 key 83 prev 82 next 84 - DONE
 0: nid006495:241022:244120 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d340 parent 0xaaaaf2c322a0 rank 3 nranks 128 color 315732477 key 3 prev 2 next 4 - DONE
 0: nid006495:241020:244121 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d380 parent 0xaaaad1d00bf0 rank 1 nranks 128 color 315732477 key 1 prev 0 next 2 - DONE
19: nid006557:208419:211183 [2] NCCL INFO bootstrapSplit: comm 0x4006ad59d3a0 parent 0xaaab15322000 rank 78 nranks 128 color 315732477 key 78 prev 77 next 79 - DONE
19: nid006557:208417:211182 [0] NCCL INFO bootstrapSplit: comm 0x4006a159d2c0 parent 0xaaaafeab5cd0 rank 76 nranks 128 color 315732477 key 76 prev 75 next 77 - DONE
19: nid006557:208419:211183 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d3a0 rank 78 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab15322000 color 315732477 key 78 commId 0xd4eed6c1342b1de7 - Init START
19: nid006557:208420:211184 [3] NCCL INFO ncclCommSplit comm 0x4006f159d540 rank 79 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab123500f0 color 315732477 key 79 commId 0xd4eed6c1342b1de7 - Init START
31: nid007342:59767:62995 [0] NCCL INFO bootstrapSplit: comm 0x4006a159d420 parent 0xaaab02e43cb0 rank 124 nranks 128 color 315732477 key 124 prev 123 next 125 - DONE
31: nid007342:59767:62995 [0] NCCL INFO ncclCommSplit comm 0x4006a159d420 rank 124 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab02e43cb0 color 315732477 key 124 commId 0xd4eed6c1342b1de7 - Init START
 9: nid006505:249082:251866 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d2a0 parent 0xaaaaddaa10c0 rank 38 nranks 128 color 315732477 key 38 prev 37 next 39 - DONE
20: nid006558:215468:218258 [1] NCCL INFO ncclCommSplit comm 0x4006c559d590 rank 81 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae4530560 color 315732477 key 81 commId 0xd4eed6c1342b1de7 - Init START
20: nid006558:215470:218259 [3] NCCL INFO ncclCommSplit comm 0x4006c959d3c0 rank 83 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab04942a10 color 315732477 key 83 commId 0xd4eed6c1342b1de7 - Init START
 0: nid006495:241019:244118 [0] NCCL INFO ncclCommSplit comm 0x4006b00d5c40 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaca364de0 color 315732477 key 0 commId 0xd4eed6c1342b1de7 - Init START
 0: nid006495:241021:244119 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d440 rank 2 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab287e25d0 color 315732477 key 2 commId 0xd4eed6c1342b1de7 - Init START
 0: nid006495:241022:244120 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d340 rank 3 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf2c322a0 color 315732477 key 3 commId 0xd4eed6c1342b1de7 - Init START
23: nid006561:220714:223485 [3] NCCL INFO bootstrapSplit: comm 0x4006e959d520 parent 0xaaaaf7d8fec0 rank 95 nranks 128 color 315732477 key 95 prev 94 next 96 - DONE
23: nid006561:220711:223488 [0] NCCL INFO bootstrapSplit: comm 0x4006e159d4a0 parent 0xaaab1f303c10 rank 92 nranks 128 color 315732477 key 92 prev 91 next 93 - DONE
28: nid007251:72170:75236 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d300 parent 0xaaaae3cff410 rank 113 nranks 128 color 315732477 key 113 prev 112 next 114 - DONE
28: nid007251:72169:75235 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d260 parent 0xaaab09955090 rank 112 nranks 128 color 315732477 key 112 prev 111 next 113 - DONE
31: nid007342:59768:62996 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d3c0 parent 0xaaaaebad4d50 rank 125 nranks 128 color 315732477 key 125 prev 124 next 126 - DONE
31: nid007342:59769:62997 [2] NCCL INFO bootstrapSplit: comm 0x4006c959d480 parent 0xaaaacd751c60 rank 126 nranks 128 color 315732477 key 126 prev 125 next 127 - DONE
31: nid007342:59770:62998 [3] NCCL INFO bootstrapSplit: comm 0x4006e959d690 parent 0xaaaae96a06d0 rank 127 nranks 128 color 315732477 key 127 prev 126 next 0 - DONE
31: nid007342:59768:62996 [1] NCCL INFO ncclCommSplit comm 0x4006e559d3c0 rank 125 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaebad4d50 color 315732477 key 125 commId 0xd4eed6c1342b1de7 - Init START
31: nid007342:59769:62997 [2] NCCL INFO ncclCommSplit comm 0x4006c959d480 rank 126 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaacd751c60 color 315732477 key 126 commId 0xd4eed6c1342b1de7 - Init START
 9: nid006505:249082:251866 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2a0 rank 38 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaddaa10c0 color 315732477 key 38 commId 0xd4eed6c1342b1de7 - Init START
 0: nid006495:241020:244121 [1] NCCL INFO ncclCommSplit comm 0x4006e559d380 rank 1 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad1d00bf0 color 315732477 key 1 commId 0xd4eed6c1342b1de7 - Init START
25: nid006564:223023:225756 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d430 parent 0xaaab04f72cc0 rank 102 nranks 128 color 315732477 key 102 prev 101 next 103 - DONE
25: nid006564:223024:225759 [3] NCCL INFO bootstrapSplit: comm 0x4006c959d490 parent 0xaaaafc951420 rank 103 nranks 128 color 315732477 key 103 prev 102 next 104 - DONE
23: nid006561:220714:223485 [3] NCCL INFO ncclCommSplit comm 0x4006e959d520 rank 95 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf7d8fec0 color 315732477 key 95 commId 0xd4eed6c1342b1de7 - Init START
23: nid006561:220711:223488 [0] NCCL INFO ncclCommSplit comm 0x4006e159d4a0 rank 92 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab1f303c10 color 315732477 key 92 commId 0xd4eed6c1342b1de7 - Init START
28: nid007251:72171:75237 [2] NCCL INFO bootstrapSplit: comm 0x4006a959d610 parent 0xaaab01150380 rank 114 nranks 128 color 315732477 key 114 prev 113 next 115 - DONE
28: nid007251:72170:75236 [1] NCCL INFO ncclCommSplit comm 0x4006c559d300 rank 113 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae3cff410 color 315732477 key 113 commId 0xd4eed6c1342b1de7 - Init START
28: nid007251:72169:75235 [0] NCCL INFO ncclCommSplit comm 0x4006c159d260 rank 112 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab09955090 color 315732477 key 112 commId 0xd4eed6c1342b1de7 - Init START
28: nid007251:72172:75238 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d550 parent 0xaaaaf6d30140 rank 115 nranks 128 color 315732477 key 115 prev 114 next 116 - DONE
28: nid007251:72171:75237 [2] NCCL INFO ncclCommSplit comm 0x4006a959d610 rank 114 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab01150380 color 315732477 key 114 commId 0xd4eed6c1342b1de7 - Init START
31: nid007342:59770:62998 [3] NCCL INFO ncclCommSplit comm 0x4006e959d690 rank 127 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaae96a06d0 color 315732477 key 127 commId 0xd4eed6c1342b1de7 - Init START
 9: nid006505:249081:251864 [1] NCCL INFO bootstrapSplit: comm 0x4006a559d100 parent 0xaaab0bfe2b70 rank 37 nranks 128 color 315732477 key 37 prev 36 next 38 - DONE
17: nid006555:206595:209409 [2] NCCL INFO bootstrapSplit: comm 0x4006d559d410 parent 0xaaaaf5a54290 rank 70 nranks 128 color 315732477 key 70 prev 69 next 71 - DONE
17: nid006555:206596:209407 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d4a0 parent 0xaaaafe0d1bf0 rank 71 nranks 128 color 315732477 key 71 prev 70 next 72 - DONE
17: nid006555:206595:209409 [2] NCCL INFO ncclCommSplit comm 0x4006d559d410 rank 70 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf5a54290 color 315732477 key 70 commId 0xd4eed6c1342b1de7 - Init START
17: nid006555:206596:209407 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d4a0 rank 71 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafe0d1bf0 color 315732477 key 71 commId 0xd4eed6c1342b1de7 - Init START
25: nid006564:223021:225757 [0] NCCL INFO bootstrapSplit: comm 0x40061d59d2c0 parent 0xaaaaf17f5630 rank 100 nranks 128 color 315732477 key 100 prev 99 next 101 - DONE
25: nid006564:223023:225756 [2] NCCL INFO ncclCommSplit comm 0x4006e959d430 rank 102 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab04f72cc0 color 315732477 key 102 commId 0xd4eed6c1342b1de7 - Init START
23: nid006561:220713:223486 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d580 parent 0xaaab19e1fc40 rank 94 nranks 128 color 315732477 key 94 prev 93 next 95 - DONE
23: nid006561:220712:223487 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d610 parent 0xaaaafae71f80 rank 93 nranks 128 color 315732477 key 93 prev 92 next 94 - DONE
28: nid007251:72172:75238 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d550 rank 115 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf6d30140 color 315732477 key 115 commId 0xd4eed6c1342b1de7 - Init START
29: nid007305:27224:29916 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d310 parent 0xaaab02ed3070 rank 118 nranks 128 color 315732477 key 118 prev 117 next 119 - DONE
29: nid007305:27225:29913 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d660 parent 0xaaaafdcaec70 rank 119 nranks 128 color 315732477 key 119 prev 118 next 120 - DONE
 9: nid006505:249081:251864 [1] NCCL INFO ncclCommSplit comm 0x4006a559d100 rank 37 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0bfe2b70 color 315732477 key 37 commId 0xd4eed6c1342b1de7 - Init START
17: nid006555:206594:209408 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d320 parent 0xaaab0a7e34b0 rank 69 nranks 128 color 315732477 key 69 prev 68 next 70 - DONE
17: nid006555:206594:209408 [1] NCCL INFO ncclCommSplit comm 0x4006e559d320 rank 69 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0a7e34b0 color 315732477 key 69 commId 0xd4eed6c1342b1de7 - Init START
25: nid006564:223022:225758 [1] NCCL INFO bootstrapSplit: comm 0x4006c40d5be0 parent 0xaaaadab71590 rank 101 nranks 128 color 315732477 key 101 prev 100 next 102 - DONE
24: nid006563:221143:223916 [3] NCCL INFO bootstrapSplit: comm 0x4006d159d690 parent 0xaaab126b1510 rank 99 nranks 128 color 315732477 key 99 prev 98 next 100 - DONE
24: nid006563:221143:223916 [3] NCCL INFO ncclCommSplit comm 0x4006d159d690 rank 99 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab126b1510 color 315732477 key 99 commId 0xd4eed6c1342b1de7 - Init START
23: nid006561:220713:223486 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d580 rank 94 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab19e1fc40 color 315732477 key 94 commId 0xd4eed6c1342b1de7 - Init START
23: nid006561:220712:223487 [1] NCCL INFO ncclCommSplit comm 0x4006e559d610 rank 93 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafae71f80 color 315732477 key 93 commId 0xd4eed6c1342b1de7 - Init START
26: nid006565:222467:225267 [2] NCCL INFO bootstrapSplit: comm 0x4006ad59d4c0 parent 0xaaab1e0113c0 rank 106 nranks 128 color 315732477 key 106 prev 105 next 107 - DONE
26: nid006565:222468:225265 [3] NCCL INFO bootstrapSplit: comm 0x4006b159d550 parent 0xaaaacd640a00 rank 107 nranks 128 color 315732477 key 107 prev 106 next 108 - DONE
29: nid007305:27224:29916 [2] NCCL INFO ncclCommSplit comm 0x4006e959d310 rank 118 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab02ed3070 color 315732477 key 118 commId 0xd4eed6c1342b1de7 - Init START
29: nid007305:27222:29915 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d200 parent 0xaaaae8e75560 rank 116 nranks 128 color 315732477 key 116 prev 115 next 117 - DONE
29: nid007305:27223:29914 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d440 parent 0xaaaadd180740 rank 117 nranks 128 color 315732477 key 117 prev 116 next 118 - DONE
29: nid007305:27225:29913 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d660 rank 119 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafdcaec70 color 315732477 key 119 commId 0xd4eed6c1342b1de7 - Init START
 9: nid006505:249080:251867 [0] NCCL INFO bootstrapSplit: comm 0x4006e159d370 parent 0xaaaac2b051b0 rank 36 nranks 128 color 315732477 key 36 prev 35 next 37 - DONE
 9: nid006505:249080:251867 [0] NCCL INFO ncclCommSplit comm 0x4006e159d370 rank 36 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac2b051b0 color 315732477 key 36 commId 0xd4eed6c1342b1de7 - Init START
17: nid006555:206593:209410 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d230 parent 0xaaab06a54c60 rank 68 nranks 128 color 315732477 key 68 prev 67 next 69 - DONE
17: nid006555:206593:209410 [0] NCCL INFO ncclCommSplit comm 0x4006c159d230 rank 68 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab06a54c60 color 315732477 key 68 commId 0xd4eed6c1342b1de7 - Init START
25: nid006564:223024:225759 [3] NCCL INFO ncclCommSplit comm 0x4006c959d490 rank 103 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafc951420 color 315732477 key 103 commId 0xd4eed6c1342b1de7 - Init START
25: nid006564:223021:225757 [0] NCCL INFO ncclCommSplit comm 0x40061d59d2c0 rank 100 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf17f5630 color 315732477 key 100 commId 0xd4eed6c1342b1de7 - Init START
24: nid006563:221142:223915 [2] NCCL INFO bootstrapSplit: comm 0x4006ad59d540 parent 0xaaab146907e0 rank 98 nranks 128 color 315732477 key 98 prev 97 next 99 - DONE
24: nid006563:221141:223918 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d3e0 parent 0xaaaaf69b0d70 rank 97 nranks 128 color 315732477 key 97 prev 96 next 98 - DONE
26: nid006565:222466:225266 [1] NCCL INFO bootstrapSplit: comm 0x4006a40d5be0 parent 0xaaaadd720280 rank 105 nranks 128 color 315732477 key 105 prev 104 next 106 - DONE
26: nid006565:222467:225267 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d4c0 rank 106 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1e0113c0 color 315732477 key 106 commId 0xd4eed6c1342b1de7 - Init START
26: nid006565:222465:225268 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d4d0 parent 0xaaaaf61a4fc0 rank 104 nranks 128 color 315732477 key 104 prev 103 next 105 - DONE
21: nid006559:211126:213874 [3] NCCL INFO bootstrapSplit: comm 0x4006a959d420 parent 0xaaaaf4030670 rank 87 nranks 128 color 315732477 key 87 prev 86 next 88 - DONE
21: nid006559:211125:213871 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d1e0 parent 0xaaaad6db0e40 rank 86 nranks 128 color 315732477 key 86 prev 85 next 87 - DONE
21: nid006559:211126:213874 [3] NCCL INFO ncclCommSplit comm 0x4006a959d420 rank 87 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf4030670 color 315732477 key 87 commId 0xd4eed6c1342b1de7 - Init START
21: nid006559:211124:213873 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d290 parent 0xaaaae87e0f50 rank 85 nranks 128 color 315732477 key 85 prev 84 next 86 - DONE
29: nid007305:27222:29915 [0] NCCL INFO ncclCommSplit comm 0x40063d59d200 rank 116 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae8e75560 color 315732477 key 116 commId 0xd4eed6c1342b1de7 - Init START
29: nid007305:27223:29914 [1] NCCL INFO ncclCommSplit comm 0x4006e559d440 rank 117 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadd180740 color 315732477 key 117 commId 0xd4eed6c1342b1de7 - Init START
30: nid007318:20582:23370 [2] NCCL INFO bootstrapSplit: comm 0x4006c959d300 parent 0xaaab18742680 rank 122 nranks 128 color 315732477 key 122 prev 121 next 123 - DONE
30: nid007318:20580:23372 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d340 parent 0xaaab05f55b40 rank 120 nranks 128 color 315732477 key 120 prev 119 next 121 - DONE
30: nid007318:20583:23369 [3] NCCL INFO ncclCommSplit comm 0x4006cc0d5940 rank 123 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaee514e10 color 315732477 key 123 commId 0xd4eed6c1342b1de7 - Init START
30: nid007318:20581:23371 [1] NCCL INFO ncclCommSplit comm 0x4006c559d360 rank 121 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab01d20530 color 315732477 key 121 commId 0xd4eed6c1342b1de7 - Init START
22: nid006560:222274:225056 [3] NCCL INFO bootstrapSplit: comm 0x4006ed59d300 parent 0xaaaad06e0ae0 rank 91 nranks 128 color 315732477 key 91 prev 90 next 92 - DONE
22: nid006560:222274:225056 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d300 rank 91 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad06e0ae0 color 315732477 key 91 commId 0xd4eed6c1342b1de7 - Init START
12: nid006508:205768:208642 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d5e0 parent 0xaaaae60828b0 rank 50 nranks 128 color 315732477 key 50 prev 49 next 51 - DONE
12: nid006508:205769:208641 [3] NCCL INFO bootstrapSplit: comm 0x4006ed59d3e0 parent 0xaaaafffd57b0 rank 51 nranks 128 color 315732477 key 51 prev 50 next 52 - DONE
13: nid006509:201790:204559 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d3c0 parent 0xaaaac07c29c0 rank 54 nranks 128 color 315732477 key 54 prev 53 next 55 - DONE
13: nid006509:201791:204556 [3] NCCL INFO bootstrapSplit: comm 0x4006d159d3b0 parent 0xaaaafc8b2560 rank 55 nranks 128 color 315732477 key 55 prev 54 next 56 - DONE
25: nid006564:223022:225758 [1] NCCL INFO ncclCommSplit comm 0x4006c40d5be0 rank 101 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadab71590 color 315732477 key 101 commId 0xd4eed6c1342b1de7 - Init START
24: nid006563:221140:223917 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d480 parent 0xaaaafb2f59b0 rank 96 nranks 128 color 315732477 key 96 prev 95 next 97 - DONE
24: nid006563:221142:223915 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d540 rank 98 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab146907e0 color 315732477 key 98 commId 0xd4eed6c1342b1de7 - Init START
26: nid006565:222468:225265 [3] NCCL INFO ncclCommSplit comm 0x4006b159d550 rank 107 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaacd640a00 color 315732477 key 107 commId 0xd4eed6c1342b1de7 - Init START
26: nid006565:222466:225266 [1] NCCL INFO ncclCommSplit comm 0x4006a40d5be0 rank 105 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadd720280 color 315732477 key 105 commId 0xd4eed6c1342b1de7 - Init START
21: nid006559:211125:213871 [2] NCCL INFO ncclCommSplit comm 0x4006e959d1e0 rank 86 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaad6db0e40 color 315732477 key 86 commId 0xd4eed6c1342b1de7 - Init START
21: nid006559:211123:213872 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d270 parent 0xaaaacd464770 rank 84 nranks 128 color 315732477 key 84 prev 83 next 85 - DONE
30: nid007318:20582:23370 [2] NCCL INFO ncclCommSplit comm 0x4006c959d300 rank 122 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab18742680 color 315732477 key 122 commId 0xd4eed6c1342b1de7 - Init START
30: nid007318:20580:23372 [0] NCCL INFO ncclCommSplit comm 0x40063d59d340 rank 120 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab05f55b40 color 315732477 key 120 commId 0xd4eed6c1342b1de7 - Init START
22: nid006560:222273:225054 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d580 parent 0xaaaae7423530 rank 90 nranks 128 color 315732477 key 90 prev 89 next 91 - DONE
22: nid006560:222271:225055 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d4c0 parent 0xaaaaf8db5160 rank 88 nranks 128 color 315732477 key 88 prev 87 next 89 - DONE
12: nid006508:205766:208643 [0] NCCL INFO bootstrapSplit: comm 0x40061d59d500 parent 0xaaab01b649c0 rank 48 nranks 128 color 315732477 key 48 prev 47 next 49 - DONE
12: nid006508:205767:208640 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d610 parent 0xaaaaf9abf4e0 rank 49 nranks 128 color 315732477 key 49 prev 48 next 50 - DONE
13: nid006509:201789:204557 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d490 parent 0xaaab23f6f620 rank 53 nranks 128 color 315732477 key 53 prev 52 next 54 - DONE
13: nid006509:201788:204558 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d290 parent 0xaaab17976000 rank 52 nranks 128 color 315732477 key 52 prev 51 next 53 - DONE
16: nid006554:221584:224381 [3] NCCL INFO bootstrapSplit: comm 0x4006ed59d3e0 parent 0xaaaafdce2b10 rank 67 nranks 128 color 315732477 key 67 prev 66 next 68 - DONE
16: nid006554:221581:224383 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d330 parent 0xaaaae2836550 rank 64 nranks 128 color 315732477 key 64 prev 63 next 65 - DONE
14: nid006510:229444:232220 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d3e0 parent 0xaaab050af830 rank 58 nranks 128 color 315732477 key 58 prev 57 next 59 - DONE
14: nid006510:229445:232221 [3] NCCL INFO bootstrapSplit: comm 0x4006ad59d550 parent 0xaaaad4ac1400 rank 59 nranks 128 color 315732477 key 59 prev 58 next 60 - DONE
14: nid006510:229444:232220 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 58 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab050af830 color 315732477 key 58 commId 0xd4eed6c1342b1de7 - Init START
24: nid006563:221141:223918 [1] NCCL INFO ncclCommSplit comm 0x4006c559d3e0 rank 97 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf69b0d70 color 315732477 key 97 commId 0xd4eed6c1342b1de7 - Init START
24: nid006563:221140:223917 [0] NCCL INFO ncclCommSplit comm 0x4006c159d480 rank 96 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafb2f59b0 color 315732477 key 96 commId 0xd4eed6c1342b1de7 - Init START
26: nid006565:222465:225268 [0] NCCL INFO ncclCommSplit comm 0x4006c159d4d0 rank 104 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf61a4fc0 color 315732477 key 104 commId 0xd4eed6c1342b1de7 - Init START
21: nid006559:211124:213873 [1] NCCL INFO ncclCommSplit comm 0x4006e559d290 rank 85 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae87e0f50 color 315732477 key 85 commId 0xd4eed6c1342b1de7 - Init START
21: nid006559:211123:213872 [0] NCCL INFO ncclCommSplit comm 0x40065d59d270 rank 84 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacd464770 color 315732477 key 84 commId 0xd4eed6c1342b1de7 - Init START
22: nid006560:222272:225053 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d540 parent 0xaaaafbb630b0 rank 89 nranks 128 color 315732477 key 89 prev 88 next 90 - DONE
22: nid006560:222273:225054 [2] NCCL INFO ncclCommSplit comm 0x4006e959d580 rank 90 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae7423530 color 315732477 key 90 commId 0xd4eed6c1342b1de7 - Init START
22: nid006560:222271:225055 [0] NCCL INFO ncclCommSplit comm 0x4006c159d4c0 rank 88 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8db5160 color 315732477 key 88 commId 0xd4eed6c1342b1de7 - Init START
22: nid006560:222272:225053 [1] NCCL INFO ncclCommSplit comm 0x4006e559d540 rank 89 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafbb630b0 color 315732477 key 89 commId 0xd4eed6c1342b1de7 - Init START
12: nid006508:205768:208642 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d5e0 rank 50 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae60828b0 color 315732477 key 50 commId 0xd4eed6c1342b1de7 - Init START
13: nid006509:201790:204559 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 54 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaac07c29c0 color 315732477 key 54 commId 0xd4eed6c1342b1de7 - Init START
13: nid006509:201791:204556 [3] NCCL INFO ncclCommSplit comm 0x4006d159d3b0 rank 55 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafc8b2560 color 315732477 key 55 commId 0xd4eed6c1342b1de7 - Init START
13: nid006509:201789:204557 [1] NCCL INFO ncclCommSplit comm 0x4006e559d490 rank 53 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab23f6f620 color 315732477 key 53 commId 0xd4eed6c1342b1de7 - Init START
 6: nid006501:221944:224721 [3] NCCL INFO bootstrapSplit: comm 0x4006ed59d380 parent 0xaaab07c52630 rank 27 nranks 128 color 315732477 key 27 prev 26 next 28 - DONE
 6: nid006501:221944:224721 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 27 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab07c52630 color 315732477 key 27 commId 0xd4eed6c1342b1de7 - Init START
 6: nid006501:221943:224720 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d2d0 parent 0xaaaafac13400 rank 26 nranks 128 color 315732477 key 26 prev 25 next 27 - DONE
 1: nid006496:242585:245389 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d4f0 parent 0xaaab017914b0 rank 5 nranks 128 color 315732477 key 5 prev 4 next 6 - DONE
 1: nid006496:242586:245386 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d300 parent 0xaaaae9db1a00 rank 6 nranks 128 color 315732477 key 6 prev 5 next 7 - DONE
 8: nid006503:218413:221168 [3] NCCL INFO bootstrapSplit: comm 0x4006c959d440 parent 0xaaaad50f1540 rank 35 nranks 128 color 315732477 key 35 prev 34 next 36 - DONE
 8: nid006503:218413:221168 [3] NCCL INFO ncclCommSplit comm 0x4006c959d440 rank 35 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad50f1540 color 315732477 key 35 commId 0xd4eed6c1342b1de7 - Init START
16: nid006554:221583:224384 [2] NCCL INFO bootstrapSplit: comm 0x4006c959d390 parent 0xaaaadd7bfd30 rank 66 nranks 128 color 315732477 key 66 prev 65 next 67 - DONE
16: nid006554:221582:224382 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d650 parent 0xaaaad7c30460 rank 65 nranks 128 color 315732477 key 65 prev 64 next 66 - DONE
16: nid006554:221584:224381 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 67 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafdce2b10 color 315732477 key 67 commId 0xd4eed6c1342b1de7 - Init START
14: nid006510:229445:232221 [3] NCCL INFO ncclCommSplit comm 0x4006ad59d550 rank 59 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad4ac1400 color 315732477 key 59 commId 0xd4eed6c1342b1de7 - Init START
14: nid006510:229443:232223 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d450 parent 0xaaab02930260 rank 57 nranks 128 color 315732477 key 57 prev 56 next 58 - DONE
14: nid006510:229442:232222 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d400 parent 0xaaaadd8b3cb0 rank 56 nranks 128 color 315732477 key 56 prev 55 next 57 - DONE
14: nid006510:229443:232223 [1] NCCL INFO ncclCommSplit comm 0x4006c559d450 rank 57 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab02930260 color 315732477 key 57 commId 0xd4eed6c1342b1de7 - Init START
14: nid006510:229442:232222 [0] NCCL INFO ncclCommSplit comm 0x40065d59d400 rank 56 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadd8b3cb0 color 315732477 key 56 commId 0xd4eed6c1342b1de7 - Init START
12: nid006508:205769:208641 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 51 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafffd57b0 color 315732477 key 51 commId 0xd4eed6c1342b1de7 - Init START
12: nid006508:205766:208643 [0] NCCL INFO ncclCommSplit comm 0x40061d59d500 rank 48 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab01b649c0 color 315732477 key 48 commId 0xd4eed6c1342b1de7 - Init START
13: nid006509:201788:204558 [0] NCCL INFO ncclCommSplit comm 0x40065d59d290 rank 52 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab17976000 color 315732477 key 52 commId 0xd4eed6c1342b1de7 - Init START
 6: nid006501:221943:224720 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2d0 rank 26 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaafac13400 color 315732477 key 26 commId 0xd4eed6c1342b1de7 - Init START
 1: nid006496:242587:245388 [3] NCCL INFO bootstrapSplit: comm 0x4006ed59d380 parent 0xaaaad33e0d00 rank 7 nranks 128 color 315732477 key 7 prev 6 next 8 - DONE
 1: nid006496:242585:245389 [1] NCCL INFO ncclCommSplit comm 0x4006c559d4f0 rank 5 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab017914b0 color 315732477 key 5 commId 0xd4eed6c1342b1de7 - Init START
 1: nid006496:242586:245386 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d300 rank 6 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae9db1a00 color 315732477 key 6 commId 0xd4eed6c1342b1de7 - Init START
 1: nid006496:242584:245387 [0] NCCL INFO bootstrapSplit: comm 0x4006c00d5ce0 parent 0xaaaafa2855c0 rank 4 nranks 128 color 315732477 key 4 prev 3 next 5 - DONE
 8: nid006503:218411:221167 [1] NCCL INFO bootstrapSplit: comm 0x4006a559d440 parent 0xaaab0c010470 rank 33 nranks 128 color 315732477 key 33 prev 32 next 34 - DONE
 8: nid006503:218410:221166 [0] NCCL INFO bootstrapSplit: comm 0x4006c159d410 parent 0xaaaaf8554fd0 rank 32 nranks 128 color 315732477 key 32 prev 31 next 33 - DONE
 8: nid006503:218411:221167 [1] NCCL INFO ncclCommSplit comm 0x4006a559d440 rank 33 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0c010470 color 315732477 key 33 commId 0xd4eed6c1342b1de7 - Init START
16: nid006554:221581:224383 [0] NCCL INFO ncclCommSplit comm 0x40063d59d330 rank 64 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae2836550 color 315732477 key 64 commId 0xd4eed6c1342b1de7 - Init START
16: nid006554:221583:224384 [2] NCCL INFO ncclCommSplit comm 0x4006c959d390 rank 66 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaadd7bfd30 color 315732477 key 66 commId 0xd4eed6c1342b1de7 - Init START
 3: nid006498:226767:229506 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d2c0 parent 0xaaaaf026f8b0 rank 14 nranks 128 color 315732477 key 14 prev 13 next 15 - DONE
 3: nid006498:226767:229506 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2c0 rank 14 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf026f8b0 color 315732477 key 14 commId 0xd4eed6c1342b1de7 - Init START
12: nid006508:205767:208640 [1] NCCL INFO ncclCommSplit comm 0x4006c559d610 rank 49 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf9abf4e0 color 315732477 key 49 commId 0xd4eed6c1342b1de7 - Init START
 6: nid006501:221942:224719 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d220 parent 0xaaaaf2b5ecd0 rank 25 nranks 128 color 315732477 key 25 prev 24 next 26 - DONE
 6: nid006501:221941:224722 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d2e0 parent 0xaaab18cffb10 rank 24 nranks 128 color 315732477 key 24 prev 23 next 25 - DONE
 6: nid006501:221942:224719 [1] NCCL INFO ncclCommSplit comm 0x4006e559d220 rank 25 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf2b5ecd0 color 315732477 key 25 commId 0xd4eed6c1342b1de7 - Init START
 6: nid006501:221941:224722 [0] NCCL INFO ncclCommSplit comm 0x40065d59d2e0 rank 24 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab18cffb10 color 315732477 key 24 commId 0xd4eed6c1342b1de7 - Init START
 1: nid006496:242587:245388 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 7 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad33e0d00 color 315732477 key 7 commId 0xd4eed6c1342b1de7 - Init START
 1: nid006496:242584:245387 [0] NCCL INFO ncclCommSplit comm 0x4006c00d5ce0 rank 4 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa2855c0 color 315732477 key 4 commId 0xd4eed6c1342b1de7 - Init START
 8: nid006503:218410:221166 [0] NCCL INFO ncclCommSplit comm 0x4006c159d410 rank 32 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8554fd0 color 315732477 key 32 commId 0xd4eed6c1342b1de7 - Init START
16: nid006554:221582:224382 [1] NCCL INFO ncclCommSplit comm 0x4006c559d650 rank 65 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad7c30460 color 315732477 key 65 commId 0xd4eed6c1342b1de7 - Init START
 3: nid006498:226765:229505 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d400 parent 0xaaaaf9754bd0 rank 12 nranks 128 color 315732477 key 12 prev 11 next 13 - DONE
 3: nid006498:226766:229504 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d3a0 parent 0xaaaafc6c1d20 rank 13 nranks 128 color 315732477 key 13 prev 12 next 14 - DONE
 3: nid006498:226765:229505 [0] NCCL INFO ncclCommSplit comm 0x40063d59d400 rank 12 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf9754bd0 color 315732477 key 12 commId 0xd4eed6c1342b1de7 - Init START
15: nid006553:223926:226696 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d340 parent 0xaaab186224e0 rank 63 nranks 128 color 315732477 key 63 prev 62 next 64 - DONE
15: nid006553:223925:226694 [2] NCCL INFO bootstrapSplit: comm 0x4006e559d570 parent 0xaaab1b0b02b0 rank 62 nranks 128 color 315732477 key 62 prev 61 next 63 - DONE
11: nid006507:211338:214084 [3] NCCL INFO bootstrapSplit: comm 0x4006f559d200 parent 0xaaab1f201250 rank 47 nranks 128 color 315732477 key 47 prev 46 next 48 - DONE
11: nid006507:211338:214084 [3] NCCL INFO ncclCommSplit comm 0x4006f559d200 rank 47 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab1f201250 color 315732477 key 47 commId 0xd4eed6c1342b1de7 - Init START
11: nid006507:211337:214086 [2] NCCL INFO bootstrapSplit: comm 0x4006e959d290 parent 0xaaab1716ee10 rank 46 nranks 128 color 315732477 key 46 prev 45 next 47 - DONE
 8: nid006503:218412:221169 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d300 parent 0xaaaadb064120 rank 34 nranks 128 color 315732477 key 34 prev 33 next 35 - DONE
 8: nid006503:218412:221169 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d300 rank 34 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaadb064120 color 315732477 key 34 commId 0xd4eed6c1342b1de7 - Init START
 3: nid006498:226768:229503 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d320 parent 0xaaaae06b1570 rank 15 nranks 128 color 315732477 key 15 prev 14 next 16 - DONE
15: nid006553:223926:226696 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d340 rank 63 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab186224e0 color 315732477 key 63 commId 0xd4eed6c1342b1de7 - Init START
15: nid006553:223925:226694 [2] NCCL INFO ncclCommSplit comm 0x4006e559d570 rank 62 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1b0b02b0 color 315732477 key 62 commId 0xd4eed6c1342b1de7 - Init START
11: nid006507:211337:214086 [2] NCCL INFO ncclCommSplit comm 0x4006e959d290 rank 46 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1716ee10 color 315732477 key 46 commId 0xd4eed6c1342b1de7 - Init START
18: nid006556:210776:213633 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d2c0 parent 0xaaab24c608a0 rank 74 nranks 128 color 315732477 key 74 prev 73 next 75 - DONE
18: nid006556:210777:213635 [3] NCCL INFO bootstrapSplit: comm 0x4006ad59d430 parent 0xaaab001d2d70 rank 75 nranks 128 color 315732477 key 75 prev 74 next 76 - DONE
 3: nid006498:226766:229504 [1] NCCL INFO ncclCommSplit comm 0x4006c559d3a0 rank 13 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafc6c1d20 color 315732477 key 13 commId 0xd4eed6c1342b1de7 - Init START
 3: nid006498:226768:229503 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d320 rank 15 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaae06b1570 color 315732477 key 15 commId 0xd4eed6c1342b1de7 - Init START
 2: nid006497:227575:230335 [0] NCCL INFO bootstrapSplit: comm 0x40061d59d270 parent 0xaaab34a73550 rank 8 nranks 128 color 315732477 key 8 prev 7 next 9 - DONE
 2: nid006497:227577:230334 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d470 parent 0xaaab266a1d50 rank 10 nranks 128 color 315732477 key 10 prev 9 next 11 - DONE
15: nid006553:223924:226695 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d560 parent 0xaaaae1630990 rank 61 nranks 128 color 315732477 key 61 prev 60 next 62 - DONE
11: nid006507:211336:214087 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d480 parent 0xaaab0a883290 rank 45 nranks 128 color 315732477 key 45 prev 44 next 46 - DONE
11: nid006507:211335:214085 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d500 parent 0xaaaadbe34c90 rank 44 nranks 128 color 315732477 key 44 prev 43 next 45 - DONE
18: nid006556:210774:213632 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d540 parent 0xaaab00fe4b80 rank 72 nranks 128 color 315732477 key 72 prev 71 next 73 - DONE
18: nid006556:210776:213633 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d2c0 rank 74 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab24c608a0 color 315732477 key 74 commId 0xd4eed6c1342b1de7 - Init START
18: nid006556:210774:213632 [0] NCCL INFO ncclCommSplit comm 0x40063d59d540 rank 72 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab00fe4b80 color 315732477 key 72 commId 0xd4eed6c1342b1de7 - Init START
18: nid006556:210775:213634 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d570 parent 0xaaab1c4215c0 rank 73 nranks 128 color 315732477 key 73 prev 72 next 74 - DONE
 7: nid006502:252587:255364 [3] NCCL INFO bootstrapSplit: comm 0x4006b159d3a0 parent 0xaaab12a7f380 rank 31 nranks 128 color 315732477 key 31 prev 30 next 32 - DONE
 7: nid006502:252586:255365 [2] NCCL INFO bootstrapSplit: comm 0x4006c959d340 parent 0xaaab16820860 rank 30 nranks 128 color 315732477 key 30 prev 29 next 31 - DONE
 2: nid006497:227576:230337 [1] NCCL INFO bootstrapSplit: comm 0x4006c959d080 parent 0xaaaade5e19c0 rank 9 nranks 128 color 315732477 key 9 prev 8 next 10 - DONE
 2: nid006497:227578:230336 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d3c0 parent 0xaaaaf3bd2ec0 rank 11 nranks 128 color 315732477 key 11 prev 10 next 12 - DONE
 2: nid006497:227575:230335 [0] NCCL INFO ncclCommSplit comm 0x40061d59d270 rank 8 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab34a73550 color 315732477 key 8 commId 0xd4eed6c1342b1de7 - Init START
15: nid006553:223923:226697 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d3b0 parent 0xaaab008f4e30 rank 60 nranks 128 color 315732477 key 60 prev 59 next 61 - DONE
15: nid006553:223924:226695 [1] NCCL INFO ncclCommSplit comm 0x4006e559d560 rank 61 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae1630990 color 315732477 key 61 commId 0xd4eed6c1342b1de7 - Init START
15: nid006553:223923:226697 [0] NCCL INFO ncclCommSplit comm 0x40065d59d3b0 rank 60 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab008f4e30 color 315732477 key 60 commId 0xd4eed6c1342b1de7 - Init START
11: nid006507:211336:214087 [1] NCCL INFO ncclCommSplit comm 0x4006c559d480 rank 45 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0a883290 color 315732477 key 45 commId 0xd4eed6c1342b1de7 - Init START
11: nid006507:211335:214085 [0] NCCL INFO ncclCommSplit comm 0x40065d59d500 rank 44 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadbe34c90 color 315732477 key 44 commId 0xd4eed6c1342b1de7 - Init START
18: nid006556:210777:213635 [3] NCCL INFO ncclCommSplit comm 0x4006ad59d430 rank 75 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab001d2d70 color 315732477 key 75 commId 0xd4eed6c1342b1de7 - Init START
18: nid006556:210775:213634 [1] NCCL INFO ncclCommSplit comm 0x4006c559d570 rank 73 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1c4215c0 color 315732477 key 73 commId 0xd4eed6c1342b1de7 - Init START
 7: nid006502:252585:255363 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d280 parent 0xaaab03122220 rank 29 nranks 128 color 315732477 key 29 prev 28 next 30 - DONE
 7: nid006502:252584:255362 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d560 parent 0xaaaafb714860 rank 28 nranks 128 color 315732477 key 28 prev 27 next 29 - DONE
10: nid006506:263730:266490 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d520 parent 0xaaaadbd63f10 rank 43 nranks 128 color 315732477 key 43 prev 42 next 44 - DONE
10: nid006506:263729:266489 [2] NCCL INFO bootstrapSplit: comm 0x4006cd59d320 parent 0xaaab0c9e0740 rank 42 nranks 128 color 315732477 key 42 prev 41 next 43 - DONE
10: nid006506:263730:266490 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d520 rank 43 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaadbd63f10 color 315732477 key 43 commId 0xd4eed6c1342b1de7 - Init START
10: nid006506:263728:266492 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d2f0 parent 0xaaab066b0b90 rank 41 nranks 128 color 315732477 key 41 prev 40 next 42 - DONE
10: nid006506:263729:266489 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d320 rank 42 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab0c9e0740 color 315732477 key 42 commId 0xd4eed6c1342b1de7 - Init START
 2: nid006497:227577:230334 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d470 rank 10 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab266a1d50 color 315732477 key 10 commId 0xd4eed6c1342b1de7 - Init START
 2: nid006497:227576:230337 [1] NCCL INFO ncclCommSplit comm 0x4006c959d080 rank 9 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaade5e19c0 color 315732477 key 9 commId 0xd4eed6c1342b1de7 - Init START
 7: nid006502:252587:255364 [3] NCCL INFO ncclCommSplit comm 0x4006b159d3a0 rank 31 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab12a7f380 color 315732477 key 31 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260085:262848 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d380 parent 0xaaab1f1aeb40 rank 22 nranks 128 color 315732477 key 22 prev 21 next 23 - DONE
 5: nid006500:260084:262849 [1] NCCL INFO bootstrapSplit: comm 0x4006c559d540 parent 0xaaaaebd115d0 rank 21 nranks 128 color 315732477 key 21 prev 20 next 22 - DONE
 4: nid006499:254558:257318 [2] NCCL INFO bootstrapSplit: comm 0x4006ed59d070 parent 0xaaab14912c30 rank 18 nranks 128 color 315732477 key 18 prev 17 next 19 - DONE
 4: nid006499:254559:257317 [3] NCCL INFO bootstrapSplit: comm 0x4006d159d380 parent 0xaaaaffcbe690 rank 19 nranks 128 color 315732477 key 19 prev 18 next 20 - DONE
10: nid006506:263727:266491 [0] NCCL INFO bootstrapSplit: comm 0x4006e159d2a0 parent 0xaaaacce51d30 rank 40 nranks 128 color 315732477 key 40 prev 39 next 41 - DONE
10: nid006506:263728:266492 [1] NCCL INFO ncclCommSplit comm 0x4006e559d2f0 rank 41 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab066b0b90 color 315732477 key 41 commId 0xd4eed6c1342b1de7 - Init START
 2: nid006497:227578:230336 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 11 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf3bd2ec0 color 315732477 key 11 commId 0xd4eed6c1342b1de7 - Init START
 7: nid006502:252586:255365 [2] NCCL INFO ncclCommSplit comm 0x4006c959d340 rank 30 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab16820860 color 315732477 key 30 commId 0xd4eed6c1342b1de7 - Init START
 7: nid006502:252585:255363 [1] NCCL INFO ncclCommSplit comm 0x4006c559d280 rank 29 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab03122220 color 315732477 key 29 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260083:262846 [0] NCCL INFO bootstrapSplit: comm 0x40063d59d550 parent 0xaaaaddd64a90 rank 20 nranks 128 color 315732477 key 20 prev 19 next 21 - DONE
19: nid006557:208418:211185 [1] NCCL INFO ncclCommSplit comm 0x4006e559d540 rank 77 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab2b67f850 color 315732477 key 77 commId 0xd4eed6c1342b1de7 - Init START
19: nid006557:208417:211182 [0] NCCL INFO ncclCommSplit comm 0x4006a159d2c0 rank 76 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafeab5cd0 color 315732477 key 76 commId 0xd4eed6c1342b1de7 - Init START
 4: nid006499:254556:257319 [0] NCCL INFO bootstrapSplit: comm 0x40065d59d1d0 parent 0xaaaafd9c3e70 rank 16 nranks 128 color 315732477 key 16 prev 15 next 17 - DONE
 4: nid006499:254558:257318 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d070 rank 18 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab14912c30 color 315732477 key 18 commId 0xd4eed6c1342b1de7 - Init START
 4: nid006499:254557:257316 [1] NCCL INFO bootstrapSplit: comm 0x4006e559d3c0 parent 0xaaaaf4721fc0 rank 17 nranks 128 color 315732477 key 17 prev 16 next 18 - DONE
10: nid006506:263727:266491 [0] NCCL INFO ncclCommSplit comm 0x4006e159d2a0 rank 40 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacce51d30 color 315732477 key 40 commId 0xd4eed6c1342b1de7 - Init START
 7: nid006502:252584:255362 [0] NCCL INFO ncclCommSplit comm 0x40065d59d560 rank 28 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafb714860 color 315732477 key 28 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260086:262847 [3] NCCL INFO bootstrapSplit: comm 0x4006cd59d250 parent 0xaaaaf70716c0 rank 23 nranks 128 color 315732477 key 23 prev 22 next 24 - DONE
 5: nid006500:260085:262848 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 22 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1f1aeb40 color 315732477 key 22 commId 0xd4eed6c1342b1de7 - Init START
 4: nid006499:254559:257317 [3] NCCL INFO ncclCommSplit comm 0x4006d159d380 rank 19 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaffcbe690 color 315732477 key 19 commId 0xd4eed6c1342b1de7 - Init START
 4: nid006499:254556:257319 [0] NCCL INFO ncclCommSplit comm 0x40065d59d1d0 rank 16 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafd9c3e70 color 315732477 key 16 commId 0xd4eed6c1342b1de7 - Init START
 4: nid006499:254557:257316 [1] NCCL INFO ncclCommSplit comm 0x4006e559d3c0 rank 17 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf4721fc0 color 315732477 key 17 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260084:262849 [1] NCCL INFO ncclCommSplit comm 0x4006c559d540 rank 21 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaebd115d0 color 315732477 key 21 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260083:262846 [0] NCCL INFO ncclCommSplit comm 0x40063d59d550 rank 20 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaddd64a90 color 315732477 key 20 commId 0xd4eed6c1342b1de7 - Init START
 5: nid006500:260086:262847 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d250 rank 23 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf70716c0 color 315732477 key 23 commId 0xd4eed6c1342b1de7 - Init START
17: nid006555:206596:209407 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
17: nid006555:206596:209407 [3] NCCL INFO NVLS multicast support is not available on dev 3
17: nid006555:206595:209409 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
17: nid006555:206595:209409 [2] NCCL INFO NVLS multicast support is not available on dev 2
17: nid006555:206594:209408 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
17: nid006555:206594:209408 [1] NCCL INFO NVLS multicast support is not available on dev 1
17: nid006555:206593:209410 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
17: nid006555:206593:209410 [0] NCCL INFO NVLS multicast support is not available on dev 0
28: nid007251:72170:75236 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
28: nid007251:72170:75236 [1] NCCL INFO NVLS multicast support is not available on dev 1
28: nid007251:72169:75235 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
28: nid007251:72169:75235 [0] NCCL INFO NVLS multicast support is not available on dev 0
28: nid007251:72171:75237 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
28: nid007251:72172:75238 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
28: nid007251:72171:75237 [2] NCCL INFO NVLS multicast support is not available on dev 2
28: nid007251:72172:75238 [3] NCCL INFO NVLS multicast support is not available on dev 3
16: nid006554:221581:224383 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
16: nid006554:221581:224383 [0] NCCL INFO NVLS multicast support is not available on dev 0
16: nid006554:221583:224384 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
16: nid006554:221583:224384 [2] NCCL INFO NVLS multicast support is not available on dev 2
16: nid006554:221584:224381 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
16: nid006554:221584:224381 [3] NCCL INFO NVLS multicast support is not available on dev 3
16: nid006554:221582:224382 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
16: nid006554:221582:224382 [1] NCCL INFO NVLS multicast support is not available on dev 1
 9: nid006505:249082:251866 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 6: nid006501:221943:224720 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 9: nid006505:249082:251866 [2] NCCL INFO NVLS multicast support is not available on dev 2
 6: nid006501:221943:224720 [2] NCCL INFO NVLS multicast support is not available on dev 2
26: nid006565:222468:225265 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
26: nid006565:222468:225265 [3] NCCL INFO NVLS multicast support is not available on dev 3
 9: nid006505:249081:251864 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 9: nid006505:249080:251867 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 6: nid006501:221942:224719 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
26: nid006565:222466:225266 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
26: nid006565:222467:225267 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
26: nid006565:222466:225266 [1] NCCL INFO NVLS multicast support is not available on dev 1
13: nid006509:201789:204557 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 9: nid006505:249081:251864 [1] NCCL INFO NVLS multicast support is not available on dev 1
 9: nid006505:249080:251867 [0] NCCL INFO NVLS multicast support is not available on dev 0
 6: nid006501:221941:224722 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 6: nid006501:221942:224719 [1] NCCL INFO NVLS multicast support is not available on dev 1
13: nid006509:201789:204557 [1] NCCL INFO NVLS multicast support is not available on dev 1
26: nid006565:222467:225267 [2] NCCL INFO NVLS multicast support is not available on dev 2
 9: nid006505:249083:251865 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 6: nid006501:221941:224722 [0] NCCL INFO NVLS multicast support is not available on dev 0
13: nid006509:201788:204558 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
13: nid006509:201788:204558 [0] NCCL INFO NVLS multicast support is not available on dev 0
 9: nid006505:249083:251865 [3] NCCL INFO NVLS multicast support is not available on dev 3
 6: nid006501:221944:224721 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
29: nid007305:27222:29915 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
29: nid007305:27222:29915 [0] NCCL INFO NVLS multicast support is not available on dev 0
26: nid006565:222465:225268 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 6: nid006501:221944:224721 [3] NCCL INFO NVLS multicast support is not available on dev 3
26: nid006565:222465:225268 [0] NCCL INFO NVLS multicast support is not available on dev 0
13: nid006509:201791:204556 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
13: nid006509:201791:204556 [3] NCCL INFO NVLS multicast support is not available on dev 3
29: nid007305:27223:29914 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
29: nid007305:27224:29916 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
29: nid007305:27225:29913 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
13: nid006509:201790:204559 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
29: nid007305:27224:29916 [2] NCCL INFO NVLS multicast support is not available on dev 2
29: nid007305:27223:29914 [1] NCCL INFO NVLS multicast support is not available on dev 1
29: nid007305:27225:29913 [3] NCCL INFO NVLS multicast support is not available on dev 3
13: nid006509:201790:204559 [2] NCCL INFO NVLS multicast support is not available on dev 2
 3: nid006498:226767:229506 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 3: nid006498:226765:229505 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 3: nid006498:226766:229504 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 3: nid006498:226768:229503 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 3: nid006498:226767:229506 [2] NCCL INFO NVLS multicast support is not available on dev 2
 3: nid006498:226768:229503 [3] NCCL INFO NVLS multicast support is not available on dev 3
 3: nid006498:226766:229504 [1] NCCL INFO NVLS multicast support is not available on dev 1
 3: nid006498:226765:229505 [0] NCCL INFO NVLS multicast support is not available on dev 0
31: nid007342:59768:62996 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 7: nid006502:252584:255362 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
31: nid007342:59768:62996 [1] NCCL INFO NVLS multicast support is not available on dev 1
 7: nid006502:252584:255362 [0] NCCL INFO NVLS multicast support is not available on dev 0
31: nid007342:59769:62997 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
31: nid007342:59767:62995 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
31: nid007342:59770:62998 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
31: nid007342:59770:62998 [3] NCCL INFO NVLS multicast support is not available on dev 3
31: nid007342:59769:62997 [2] NCCL INFO NVLS multicast support is not available on dev 2
31: nid007342:59767:62995 [0] NCCL INFO NVLS multicast support is not available on dev 0
25: nid006564:223021:225757 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
25: nid006564:223023:225756 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
25: nid006564:223024:225759 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
25: nid006564:223021:225757 [0] NCCL INFO NVLS multicast support is not available on dev 0
25: nid006564:223023:225756 [2] NCCL INFO NVLS multicast support is not available on dev 2
25: nid006564:223024:225759 [3] NCCL INFO NVLS multicast support is not available on dev 3
25: nid006564:223022:225758 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 7: nid006502:252587:255364 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
25: nid006564:223022:225758 [1] NCCL INFO NVLS multicast support is not available on dev 1
 7: nid006502:252586:255365 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 7: nid006502:252587:255364 [3] NCCL INFO NVLS multicast support is not available on dev 3
 5: nid006500:260086:262847 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 5: nid006500:260083:262846 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 5: nid006500:260084:262849 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 5: nid006500:260085:262848 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 7: nid006502:252586:255365 [2] NCCL INFO NVLS multicast support is not available on dev 2
 5: nid006500:260086:262847 [3] NCCL INFO NVLS multicast support is not available on dev 3
 7: nid006502:252585:255363 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 5: nid006500:260084:262849 [1] NCCL INFO NVLS multicast support is not available on dev 1
 5: nid006500:260085:262848 [2] NCCL INFO NVLS multicast support is not available on dev 2
 7: nid006502:252585:255363 [1] NCCL INFO NVLS multicast support is not available on dev 1
 5: nid006500:260083:262846 [0] NCCL INFO NVLS multicast support is not available on dev 0
19: nid006557:208417:211182 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
19: nid006557:208417:211182 [0] NCCL INFO NVLS multicast support is not available on dev 0
19: nid006557:208420:211184 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
15: nid006553:223925:226694 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
19: nid006557:208420:211184 [3] NCCL INFO NVLS multicast support is not available on dev 3
18: nid006556:210776:213633 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
19: nid006557:208419:211183 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
15: nid006553:223925:226694 [2] NCCL INFO NVLS multicast support is not available on dev 2
18: nid006556:210776:213633 [2] NCCL INFO NVLS multicast support is not available on dev 2
19: nid006557:208419:211183 [2] NCCL INFO NVLS multicast support is not available on dev 2
19: nid006557:208418:211185 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
19: nid006557:208418:211185 [1] NCCL INFO NVLS multicast support is not available on dev 1
22: nid006560:222274:225056 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
15: nid006553:223924:226695 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
22: nid006560:222274:225056 [3] NCCL INFO NVLS multicast support is not available on dev 3
22: nid006560:222271:225055 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
18: nid006556:210774:213632 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
18: nid006556:210774:213632 [0] NCCL INFO NVLS multicast support is not available on dev 0
18: nid006556:210775:213634 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
22: nid006560:222272:225053 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
22: nid006560:222271:225055 [0] NCCL INFO NVLS multicast support is not available on dev 0
15: nid006553:223923:226697 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
15: nid006553:223924:226695 [1] NCCL INFO NVLS multicast support is not available on dev 1
15: nid006553:223923:226697 [0] NCCL INFO NVLS multicast support is not available on dev 0
18: nid006556:210775:213634 [1] NCCL INFO NVLS multicast support is not available on dev 1
15: nid006553:223926:226696 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
22: nid006560:222272:225053 [1] NCCL INFO NVLS multicast support is not available on dev 1
22: nid006560:222273:225054 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
15: nid006553:223926:226696 [3] NCCL INFO NVLS multicast support is not available on dev 3
18: nid006556:210777:213635 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
22: nid006560:222273:225054 [2] NCCL INFO NVLS multicast support is not available on dev 2
18: nid006556:210777:213635 [3] NCCL INFO NVLS multicast support is not available on dev 3
 2: nid006497:227575:230335 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 4: nid006499:254558:257318 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 4: nid006499:254558:257318 [2] NCCL INFO NVLS multicast support is not available on dev 2
 2: nid006497:227578:230336 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 2: nid006497:227577:230334 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 2: nid006497:227576:230337 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 2: nid006497:227575:230335 [0] NCCL INFO NVLS multicast support is not available on dev 0
 2: nid006497:227578:230336 [3] NCCL INFO NVLS multicast support is not available on dev 3
 2: nid006497:227577:230334 [2] NCCL INFO NVLS multicast support is not available on dev 2
 2: nid006497:227576:230337 [1] NCCL INFO NVLS multicast support is not available on dev 1
 4: nid006499:254556:257319 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 4: nid006499:254556:257319 [0] NCCL INFO NVLS multicast support is not available on dev 0
 4: nid006499:254557:257316 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 4: nid006499:254559:257317 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 4: nid006499:254557:257316 [1] NCCL INFO NVLS multicast support is not available on dev 1
 4: nid006499:254559:257317 [3] NCCL INFO NVLS multicast support is not available on dev 3
20: nid006558:215470:218259 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
20: nid006558:215467:218261 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
20: nid006558:215469:218260 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
20: nid006558:215470:218259 [3] NCCL INFO NVLS multicast support is not available on dev 3
20: nid006558:215468:218258 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
20: nid006558:215468:218258 [1] NCCL INFO NVLS multicast support is not available on dev 1
20: nid006558:215469:218260 [2] NCCL INFO NVLS multicast support is not available on dev 2
20: nid006558:215467:218261 [0] NCCL INFO NVLS multicast support is not available on dev 0
 8: nid006503:218411:221167 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 8: nid006503:218411:221167 [1] NCCL INFO NVLS multicast support is not available on dev 1
 8: nid006503:218410:221166 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 8: nid006503:218412:221169 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 8: nid006503:218413:221168 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 8: nid006503:218410:221166 [0] NCCL INFO NVLS multicast support is not available on dev 0
 8: nid006503:218413:221168 [3] NCCL INFO NVLS multicast support is not available on dev 3
 8: nid006503:218412:221169 [2] NCCL INFO NVLS multicast support is not available on dev 2
24: nid006563:221142:223915 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
24: nid006563:221142:223915 [2] NCCL INFO NVLS multicast support is not available on dev 2
24: nid006563:221140:223917 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
24: nid006563:221143:223916 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
24: nid006563:221140:223917 [0] NCCL INFO NVLS multicast support is not available on dev 0
24: nid006563:221143:223916 [3] NCCL INFO NVLS multicast support is not available on dev 3
24: nid006563:221141:223918 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
24: nid006563:221141:223918 [1] NCCL INFO NVLS multicast support is not available on dev 1
30: nid007318:20581:23371 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
30: nid007318:20583:23369 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
30: nid007318:20580:23372 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
30: nid007318:20582:23370 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
30: nid007318:20583:23369 [3] NCCL INFO NVLS multicast support is not available on dev 3
30: nid007318:20581:23371 [1] NCCL INFO NVLS multicast support is not available on dev 1
30: nid007318:20582:23370 [2] NCCL INFO NVLS multicast support is not available on dev 2
30: nid007318:20580:23372 [0] NCCL INFO NVLS multicast support is not available on dev 0
11: nid006507:211338:214084 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
11: nid006507:211338:214084 [3] NCCL INFO NVLS multicast support is not available on dev 3
11: nid006507:211337:214086 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
11: nid006507:211336:214087 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
11: nid006507:211337:214086 [2] NCCL INFO NVLS multicast support is not available on dev 2
11: nid006507:211336:214087 [1] NCCL INFO NVLS multicast support is not available on dev 1
11: nid006507:211335:214085 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
11: nid006507:211335:214085 [0] NCCL INFO NVLS multicast support is not available on dev 0
 1: nid006496:242586:245386 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 1: nid006496:242586:245386 [2] NCCL INFO NVLS multicast support is not available on dev 2
 1: nid006496:242587:245388 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 1: nid006496:242587:245388 [3] NCCL INFO NVLS multicast support is not available on dev 3
 1: nid006496:242584:245387 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
12: nid006508:205768:208642 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
12: nid006508:205768:208642 [2] NCCL INFO NVLS multicast support is not available on dev 2
 1: nid006496:242584:245387 [0] NCCL INFO NVLS multicast support is not available on dev 0
 1: nid006496:242585:245389 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 1: nid006496:242585:245389 [1] NCCL INFO NVLS multicast support is not available on dev 1
12: nid006508:205766:208643 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
12: nid006508:205767:208640 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
12: nid006508:205769:208641 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
12: nid006508:205766:208643 [0] NCCL INFO NVLS multicast support is not available on dev 0
12: nid006508:205767:208640 [1] NCCL INFO NVLS multicast support is not available on dev 1
12: nid006508:205769:208641 [3] NCCL INFO NVLS multicast support is not available on dev 3
23: nid006561:220714:223485 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
23: nid006561:220714:223485 [3] NCCL INFO NVLS multicast support is not available on dev 3
23: nid006561:220713:223486 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
23: nid006561:220713:223486 [2] NCCL INFO NVLS multicast support is not available on dev 2
23: nid006561:220712:223487 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
23: nid006561:220712:223487 [1] NCCL INFO NVLS multicast support is not available on dev 1
23: nid006561:220711:223488 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
23: nid006561:220711:223488 [0] NCCL INFO NVLS multicast support is not available on dev 0
21: nid006559:211125:213871 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
21: nid006559:211123:213872 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
21: nid006559:211125:213871 [2] NCCL INFO NVLS multicast support is not available on dev 2
21: nid006559:211123:213872 [0] NCCL INFO NVLS multicast support is not available on dev 0
21: nid006559:211126:213874 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
21: nid006559:211124:213873 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
21: nid006559:211126:213874 [3] NCCL INFO NVLS multicast support is not available on dev 3
21: nid006559:211124:213873 [1] NCCL INFO NVLS multicast support is not available on dev 1
10: nid006506:263730:266490 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
10: nid006506:263730:266490 [3] NCCL INFO NVLS multicast support is not available on dev 3
10: nid006506:263729:266489 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
10: nid006506:263728:266492 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
10: nid006506:263729:266489 [2] NCCL INFO NVLS multicast support is not available on dev 2
10: nid006506:263728:266492 [1] NCCL INFO NVLS multicast support is not available on dev 1
10: nid006506:263727:266491 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
10: nid006506:263727:266491 [0] NCCL INFO NVLS multicast support is not available on dev 0
27: nid006566:216747:219509 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
27: nid006566:216746:219510 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
27: nid006566:216748:219511 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
27: nid006566:216747:219509 [1] NCCL INFO NVLS multicast support is not available on dev 1
27: nid006566:216746:219510 [0] NCCL INFO NVLS multicast support is not available on dev 0
27: nid006566:216749:219512 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
27: nid006566:216748:219511 [2] NCCL INFO NVLS multicast support is not available on dev 2
27: nid006566:216749:219512 [3] NCCL INFO NVLS multicast support is not available on dev 3
14: nid006510:229444:232220 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
14: nid006510:229444:232220 [2] NCCL INFO NVLS multicast support is not available on dev 2
14: nid006510:229443:232223 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
14: nid006510:229442:232222 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
14: nid006510:229443:232223 [1] NCCL INFO NVLS multicast support is not available on dev 1
14: nid006510:229442:232222 [0] NCCL INFO NVLS multicast support is not available on dev 0
14: nid006510:229445:232221 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
14: nid006510:229445:232221 [3] NCCL INFO NVLS multicast support is not available on dev 3
 0: nid006495:241019:244118 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
 0: nid006495:241022:244120 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
 0: nid006495:241021:244119 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
 0: nid006495:241019:244118 [0] NCCL INFO NVLS multicast support is not available on dev 0
 0: nid006495:241020:244121 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
 0: nid006495:241022:244120 [3] NCCL INFO NVLS multicast support is not available on dev 3
 0: nid006495:241021:244119 [2] NCCL INFO NVLS multicast support is not available on dev 2
 0: nid006495:241020:244121 [1] NCCL INFO NVLS multicast support is not available on dev 1
31: nid007342:59770:62998 [3] NCCL INFO comm 0x4006e959d690 rank 127 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
31: nid007342:59769:62997 [2] NCCL INFO comm 0x4006c959d480 rank 126 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
31: nid007342:59770:62998 [3] NCCL INFO Trees [0] -1/-1/-1->127->126 [1] -1/-1/-1->127->126 [2] 124/-1/-1->127->126 [3] 124/-1/-1->127->126 [4] -1/-1/-1->127->126 [5] -1/-1/-1->127->126 [6] 124/-1/-1->127->126 [7] 124/-1/-1->127->126
31: nid007342:59770:62998 [3] NCCL INFO P2P Chunksize set to 131072
31: nid007342:59768:62996 [1] NCCL INFO comm 0x4006e559d3c0 rank 125 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
31: nid007342:59767:62995 [0] NCCL INFO comm 0x4006a159d420 rank 124 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
31: nid007342:59769:62997 [2] NCCL INFO Trees [0] 127/-1/-1->126->125 [1] 127/-1/-1->126->125 [2] 127/-1/-1->126->122 [3] 127/-1/-1->126->122 [4] 127/-1/-1->126->125 [5] 127/-1/-1->126->125 [6] 127/62/-1->126->-1 [7] 127/62/-1->126->-1
31: nid007342:59769:62997 [2] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20583:23369 [3] NCCL INFO comm 0x4006cc0d5940 rank 123 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
31: nid007342:59768:62996 [1] NCCL INFO Trees [0] 126/-1/-1->125->124 [1] 126/-1/-1->125->124 [2] -1/-1/-1->125->124 [3] -1/-1/-1->125->124 [4] 126/-1/-1->125->124 [5] 126/-1/-1->125->124 [6] -1/-1/-1->125->124 [7] -1/-1/-1->125->124
31: nid007342:59768:62996 [1] NCCL INFO P2P Chunksize set to 131072
31: nid007342:59767:62995 [0] NCCL INFO Trees [0] 125/-1/-1->124->120 [1] 125/-1/-1->124->120 [2] 125/-1/-1->124->127 [3] 125/-1/-1->124->127 [4] 125/60/-1->124->-1 [5] 125/60/-1->124->-1 [6] 125/-1/-1->124->127 [7] 125/-1/-1->124->127
31: nid007342:59767:62995 [0] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20581:23371 [1] NCCL INFO comm 0x4006c559d360 rank 121 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
30: nid007318:20580:23372 [0] NCCL INFO comm 0x40063d59d340 rank 120 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
30: nid007318:20582:23370 [2] NCCL INFO comm 0x4006c959d300 rank 122 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
30: nid007318:20583:23369 [3] NCCL INFO Trees [0] -1/-1/-1->123->122 [1] -1/-1/-1->123->122 [2] 120/-1/-1->123->122 [3] 120/-1/-1->123->122 [4] -1/-1/-1->123->122 [5] -1/-1/-1->123->122 [6] 120/-1/-1->123->122 [7] 120/-1/-1->123->122
30: nid007318:20583:23369 [3] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27225:29913 [3] NCCL INFO comm 0x4006cd59d660 rank 119 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
29: nid007305:27224:29916 [2] NCCL INFO comm 0x4006e959d310 rank 118 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
30: nid007318:20581:23371 [1] NCCL INFO Trees [0] 122/-1/-1->121->120 [1] 122/-1/-1->121->120 [2] -1/-1/-1->121->120 [3] -1/-1/-1->121->120 [4] 122/-1/-1->121->120 [5] 122/-1/-1->121->120 [6] -1/-1/-1->121->120 [7] -1/-1/-1->121->120
30: nid007318:20580:23372 [0] NCCL INFO Trees [0] 121/116/124->120->112 [1] 121/116/124->120->112 [2] 121/-1/-1->120->123 [3] 121/-1/-1->120->123 [4] 121/-1/-1->120->116 [5] 121/-1/-1->120->116 [6] 121/-1/-1->120->123 [7] 121/-1/-1->120->123
30: nid007318:20581:23371 [1] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20582:23370 [2] NCCL INFO Trees [0] 123/-1/-1->122->121 [1] 123/-1/-1->122->121 [2] 123/118/126->122->114 [3] 123/118/126->122->114 [4] 123/-1/-1->122->121 [5] 123/-1/-1->122->121 [6] 123/-1/-1->122->118 [7] 123/-1/-1->122->118
30: nid007318:20580:23372 [0] NCCL INFO P2P Chunksize set to 131072
30: nid007318:20582:23370 [2] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27223:29914 [1] NCCL INFO comm 0x4006e559d440 rank 117 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
28: nid007251:72172:75238 [3] NCCL INFO comm 0x4006cd59d550 rank 115 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
29: nid007305:27225:29913 [3] NCCL INFO Trees [0] -1/-1/-1->119->118 [1] -1/-1/-1->119->118 [2] 116/-1/-1->119->118 [3] 116/-1/-1->119->118 [4] -1/-1/-1->119->118 [5] -1/-1/-1->119->118 [6] 116/-1/-1->119->118 [7] 116/-1/-1->119->118
29: nid007305:27224:29916 [2] NCCL INFO Trees [0] 119/-1/-1->118->117 [1] 119/-1/-1->118->117 [2] 119/-1/-1->118->122 [3] 119/-1/-1->118->122 [4] 119/-1/-1->118->117 [5] 119/-1/-1->118->117 [6] 119/122/114->118->110 [7] 119/122/114->118->110
29: nid007305:27225:29913 [3] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27222:29915 [0] NCCL INFO comm 0x40063d59d200 rank 116 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
29: nid007305:27224:29916 [2] NCCL INFO P2P Chunksize set to 131072
29: nid007305:27223:29914 [1] NCCL INFO Trees [0] 118/-1/-1->117->116 [1] 118/-1/-1->117->116 [2] -1/-1/-1->117->116 [3] -1/-1/-1->117->116 [4] 118/-1/-1->117->116 [5] 118/-1/-1->117->116 [6] -1/-1/-1->117->116 [7] -1/-1/-1->117->116
29: nid007305:27223:29914 [1] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72171:75237 [2] NCCL INFO comm 0x4006a959d610 rank 114 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
29: nid007305:27222:29915 [0] NCCL INFO Trees [0] 117/-1/-1->116->120 [1] 117/-1/-1->116->120 [2] 117/-1/-1->116->119 [3] 117/-1/-1->116->119 [4] 117/120/112->116->108 [5] 117/120/112->116->108 [6] 117/-1/-1->116->119 [7] 117/-1/-1->116->119
29: nid007305:27222:29915 [0] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72169:75235 [0] NCCL INFO comm 0x4006c159d260 rank 112 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
28: nid007251:72170:75236 [1] NCCL INFO comm 0x4006c559d300 rank 113 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
28: nid007251:72172:75238 [3] NCCL INFO Trees [0] -1/-1/-1->115->114 [1] -1/-1/-1->115->114 [2] 112/-1/-1->115->114 [3] 112/-1/-1->115->114 [4] -1/-1/-1->115->114 [5] -1/-1/-1->115->114 [6] 112/-1/-1->115->114 [7] 112/-1/-1->115->114
28: nid007251:72172:75238 [3] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216749:219512 [3] NCCL INFO comm 0x4006f159d530 rank 111 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
27: nid006566:216748:219511 [2] NCCL INFO comm 0x4006ed59d2d0 rank 110 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
27: nid006566:216747:219509 [1] NCCL INFO comm 0x4006a559d640 rank 109 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
28: nid007251:72169:75235 [0] NCCL INFO Trees [0] 113/104/120->112->96 [1] 113/104/120->112->96 [2] 113/-1/-1->112->115 [3] 113/-1/-1->112->115 [4] 113/-1/-1->112->116 [5] 113/-1/-1->112->116 [6] 113/-1/-1->112->115 [7] 113/-1/-1->112->115
28: nid007251:72169:75235 [0] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72170:75236 [1] NCCL INFO Trees [0] 114/-1/-1->113->112 [1] 114/-1/-1->113->112 [2] -1/-1/-1->113->112 [3] -1/-1/-1->113->112 [4] 114/-1/-1->113->112 [5] 114/-1/-1->113->112 [6] -1/-1/-1->113->112 [7] -1/-1/-1->113->112
28: nid007251:72170:75236 [1] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72171:75237 [2] NCCL INFO Trees [0] 115/-1/-1->114->113 [1] 115/-1/-1->114->113 [2] 115/106/122->114->98 [3] 115/106/122->114->98 [4] 115/-1/-1->114->113 [5] 115/-1/-1->114->113 [6] 115/-1/-1->114->118 [7] 115/-1/-1->114->118
28: nid007251:72171:75237 [2] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216749:219512 [3] NCCL INFO Trees [0] -1/-1/-1->111->110 [1] -1/-1/-1->111->110 [2] 108/-1/-1->111->110 [3] 108/-1/-1->111->110 [4] -1/-1/-1->111->110 [5] -1/-1/-1->111->110 [6] 108/-1/-1->111->110 [7] 108/-1/-1->111->110
27: nid006566:216749:219512 [3] NCCL INFO P2P Chunksize set to 131072
27: nid006566:216748:219511 [2] NCCL INFO Trees [0] 111/-1/-1->110->109 [1] 111/-1/-1->110->109 [2] 111/-1/-1->110->106 [3] 111/-1/-1->110->106 [4] 111/-1/-1->110->109 [5] 111/-1/-1->110->109 [6] 111/118/102->110->94 [7] 111/118/102->110->94
27: nid006566:216747:219509 [1] NCCL INFO Trees [0] 110/-1/-1->109->108 [1] 110/-1/-1->109->108 [2] -1/-1/-1->109->108 [3] -1/-1/-1->109->108 [4] 110/-1/-1->109->108 [5] 110/-1/-1->109->108 [6] -1/-1/-1->109->108 [7] -1/-1/-1->109->108
27: nid006566:216746:219510 [0] NCCL INFO comm 0x40065d59d1d0 rank 108 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
27: nid006566:216748:219511 [2] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222466:225266 [1] NCCL INFO comm 0x4006a40d5be0 rank 105 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
26: nid006565:222467:225267 [2] NCCL INFO comm 0x4006ad59d4c0 rank 106 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
27: nid006566:216747:219509 [1] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222468:225265 [3] NCCL INFO comm 0x4006b159d550 rank 107 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
27: nid006566:216746:219510 [0] NCCL INFO Trees [0] 109/-1/-1->108->104 [1] 109/-1/-1->108->104 [2] 109/-1/-1->108->111 [3] 109/-1/-1->108->111 [4] 109/116/100->108->92 [5] 109/116/100->108->92 [6] 109/-1/-1->108->111 [7] 109/-1/-1->108->111
27: nid006566:216746:219510 [0] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222466:225266 [1] NCCL INFO Trees [0] 106/-1/-1->105->104 [1] 106/-1/-1->105->104 [2] -1/-1/-1->105->104 [3] -1/-1/-1->105->104 [4] 106/-1/-1->105->104 [5] 106/-1/-1->105->104 [6] -1/-1/-1->105->104 [7] -1/-1/-1->105->104
26: nid006565:222466:225266 [1] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222465:225268 [0] NCCL INFO comm 0x4006c159d4d0 rank 104 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
25: nid006564:223024:225759 [3] NCCL INFO comm 0x4006c959d490 rank 103 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
25: nid006564:223023:225756 [2] NCCL INFO comm 0x4006e959d430 rank 102 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
26: nid006565:222467:225267 [2] NCCL INFO Trees [0] 107/-1/-1->106->105 [1] 107/-1/-1->106->105 [2] 107/102/110->106->114 [3] 107/102/110->106->114 [4] 107/-1/-1->106->105 [5] 107/-1/-1->106->105 [6] 107/-1/-1->106->102 [7] 107/-1/-1->106->102
26: nid006565:222468:225265 [3] NCCL INFO Trees [0] -1/-1/-1->107->106 [1] -1/-1/-1->107->106 [2] 104/-1/-1->107->106 [3] 104/-1/-1->107->106 [4] -1/-1/-1->107->106 [5] -1/-1/-1->107->106 [6] 104/-1/-1->107->106 [7] 104/-1/-1->107->106
26: nid006565:222467:225267 [2] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222468:225265 [3] NCCL INFO P2P Chunksize set to 131072
26: nid006565:222465:225268 [0] NCCL INFO Trees [0] 105/100/108->104->112 [1] 105/100/108->104->112 [2] 105/-1/-1->104->107 [3] 105/-1/-1->104->107 [4] 105/-1/-1->104->100 [5] 105/-1/-1->104->100 [6] 105/-1/-1->104->107 [7] 105/-1/-1->104->107
26: nid006565:222465:225268 [0] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223024:225759 [3] NCCL INFO Trees [0] -1/-1/-1->103->102 [1] -1/-1/-1->103->102 [2] 100/-1/-1->103->102 [3] 100/-1/-1->103->102 [4] -1/-1/-1->103->102 [5] -1/-1/-1->103->102 [6] 100/-1/-1->103->102 [7] 100/-1/-1->103->102
25: nid006564:223024:225759 [3] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223023:225756 [2] NCCL INFO Trees [0] 103/-1/-1->102->101 [1] 103/-1/-1->102->101 [2] 103/-1/-1->102->106 [3] 103/-1/-1->102->106 [4] 103/-1/-1->102->101 [5] 103/-1/-1->102->101 [6] 103/106/98->102->110 [7] 103/106/98->102->110
25: nid006564:223023:225756 [2] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223021:225757 [0] NCCL INFO comm 0x40061d59d2c0 rank 100 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
25: nid006564:223022:225758 [1] NCCL INFO comm 0x4006c40d5be0 rank 101 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
24: nid006563:221143:223916 [3] NCCL INFO comm 0x4006d159d690 rank 99 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
24: nid006563:221142:223915 [2] NCCL INFO comm 0x4006ad59d540 rank 98 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
25: nid006564:223021:225757 [0] NCCL INFO Trees [0] 101/-1/-1->100->104 [1] 101/-1/-1->100->104 [2] 101/-1/-1->100->103 [3] 101/-1/-1->100->103 [4] 101/104/96->100->108 [5] 101/104/96->100->108 [6] 101/-1/-1->100->103 [7] 101/-1/-1->100->103
25: nid006564:223022:225758 [1] NCCL INFO Trees [0] 102/-1/-1->101->100 [1] 102/-1/-1->101->100 [2] -1/-1/-1->101->100 [3] -1/-1/-1->101->100 [4] 102/-1/-1->101->100 [5] 102/-1/-1->101->100 [6] -1/-1/-1->101->100 [7] -1/-1/-1->101->100
25: nid006564:223021:225757 [0] NCCL INFO P2P Chunksize set to 131072
25: nid006564:223022:225758 [1] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221140:223917 [0] NCCL INFO comm 0x4006c159d480 rank 96 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
24: nid006563:221143:223916 [3] NCCL INFO Trees [0] -1/-1/-1->99->98 [1] -1/-1/-1->99->98 [2] 96/-1/-1->99->98 [3] 96/-1/-1->99->98 [4] -1/-1/-1->99->98 [5] -1/-1/-1->99->98 [6] 96/-1/-1->99->98 [7] 96/-1/-1->99->98
24: nid006563:221141:223918 [1] NCCL INFO comm 0x4006c559d3e0 rank 97 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
24: nid006563:221142:223915 [2] NCCL INFO Trees [0] 99/-1/-1->98->97 [1] 99/-1/-1->98->97 [2] 99/82/114->98->66 [3] 99/82/114->98->66 [4] 99/-1/-1->98->97 [5] 99/-1/-1->98->97 [6] 99/-1/-1->98->102 [7] 99/-1/-1->98->102
24: nid006563:221143:223916 [3] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221142:223915 [2] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221140:223917 [0] NCCL INFO Trees [0] 97/80/112->96->64 [1] 97/80/112->96->64 [2] 97/-1/-1->96->99 [3] 97/-1/-1->96->99 [4] 97/-1/-1->96->100 [5] 97/-1/-1->96->100 [6] 97/-1/-1->96->99 [7] 97/-1/-1->96->99
24: nid006563:221140:223917 [0] NCCL INFO P2P Chunksize set to 131072
24: nid006563:221141:223918 [1] NCCL INFO Trees [0] 98/-1/-1->97->96 [1] 98/-1/-1->97->96 [2] -1/-1/-1->97->96 [3] -1/-1/-1->97->96 [4] 98/-1/-1->97->96 [5] 98/-1/-1->97->96 [6] -1/-1/-1->97->96 [7] -1/-1/-1->97->96
24: nid006563:221141:223918 [1] NCCL INFO P2P Chunksize set to 131072
 7: nid006502:252587:255364 [3] NCCL INFO comm 0x4006b159d3a0 rank 31 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
23: nid006561:220714:223485 [3] NCCL INFO comm 0x4006e959d520 rank 95 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
23: nid006561:220713:223486 [2] NCCL INFO comm 0x4006ed59d580 rank 94 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 7: nid006502:252586:255365 [2] NCCL INFO comm 0x4006c959d340 rank 30 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
23: nid006561:220712:223487 [1] NCCL INFO comm 0x4006e559d610 rank 93 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 7: nid006502:252587:255364 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 28/-1/-1->31->30 [3] 28/-1/-1->31->30 [4] -1/-1/-1->31->30 [5] -1/-1/-1->31->30 [6] 28/-1/-1->31->30 [7] 28/-1/-1->31->30
 7: nid006502:252587:255364 [3] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220714:223485 [3] NCCL INFO Trees [0] -1/-1/-1->95->94 [1] -1/-1/-1->95->94 [2] 92/-1/-1->95->94 [3] 92/-1/-1->95->94 [4] -1/-1/-1->95->94 [5] -1/-1/-1->95->94 [6] 92/-1/-1->95->94 [7] 92/-1/-1->95->94
23: nid006561:220714:223485 [3] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220713:223486 [2] NCCL INFO Trees [0] 95/-1/-1->94->93 [1] 95/-1/-1->94->93 [2] 95/-1/-1->94->90 [3] 95/-1/-1->94->90 [4] 95/-1/-1->94->93 [5] 95/-1/-1->94->93 [6] 95/110/78->94->62 [7] 95/110/78->94->62
22: nid006560:222274:225056 [3] NCCL INFO comm 0x4006ed59d300 rank 91 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
22: nid006560:222273:225054 [2] NCCL INFO comm 0x4006e959d580 rank 90 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
23: nid006561:220712:223487 [1] NCCL INFO Trees [0] 94/-1/-1->93->92 [1] 94/-1/-1->93->92 [2] -1/-1/-1->93->92 [3] -1/-1/-1->93->92 [4] 94/-1/-1->93->92 [5] 94/-1/-1->93->92 [6] -1/-1/-1->93->92 [7] -1/-1/-1->93->92
22: nid006560:222272:225053 [1] NCCL INFO comm 0x4006e559d540 rank 89 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
23: nid006561:220713:223486 [2] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220712:223487 [1] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220711:223488 [0] NCCL INFO comm 0x4006e159d4a0 rank 92 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
22: nid006560:222274:225056 [3] NCCL INFO Trees [0] -1/-1/-1->91->90 [1] -1/-1/-1->91->90 [2] 88/-1/-1->91->90 [3] 88/-1/-1->91->90 [4] -1/-1/-1->91->90 [5] -1/-1/-1->91->90 [6] 88/-1/-1->91->90 [7] 88/-1/-1->91->90
22: nid006560:222274:225056 [3] NCCL INFO P2P Chunksize set to 131072
 7: nid006502:252586:255365 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->26 [3] 31/-1/-1->30->26 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/46/14->30->62 [7] 31/46/14->30->62
 7: nid006502:252585:255363 [1] NCCL INFO comm 0x4006c559d280 rank 29 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 7: nid006502:252584:255362 [0] NCCL INFO comm 0x40065d59d560 rank 28 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 7: nid006502:252586:255365 [2] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211126:213874 [3] NCCL INFO comm 0x4006a959d420 rank 87 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
21: nid006559:211125:213871 [2] NCCL INFO comm 0x4006e959d1e0 rank 86 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
21: nid006559:211124:213873 [1] NCCL INFO comm 0x4006e559d290 rank 85 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
22: nid006560:222273:225054 [2] NCCL INFO Trees [0] 91/-1/-1->90->89 [1] 91/-1/-1->90->89 [2] 91/86/94->90->82 [3] 91/86/94->90->82 [4] 91/-1/-1->90->89 [5] 91/-1/-1->90->89 [6] 91/-1/-1->90->86 [7] 91/-1/-1->90->86
22: nid006560:222271:225055 [0] NCCL INFO comm 0x4006c159d4c0 rank 88 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
22: nid006560:222272:225053 [1] NCCL INFO Trees [0] 90/-1/-1->89->88 [1] 90/-1/-1->89->88 [2] -1/-1/-1->89->88 [3] -1/-1/-1->89->88 [4] 90/-1/-1->89->88 [5] 90/-1/-1->89->88 [6] -1/-1/-1->89->88 [7] -1/-1/-1->89->88
22: nid006560:222273:225054 [2] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222272:225053 [1] NCCL INFO P2P Chunksize set to 131072
23: nid006561:220711:223488 [0] NCCL INFO Trees [0] 93/-1/-1->92->88 [1] 93/-1/-1->92->88 [2] 93/-1/-1->92->95 [3] 93/-1/-1->92->95 [4] 93/108/76->92->60 [5] 93/108/76->92->60 [6] 93/-1/-1->92->95 [7] 93/-1/-1->92->95
23: nid006561:220711:223488 [0] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211126:213874 [3] NCCL INFO Trees [0] -1/-1/-1->87->86 [1] -1/-1/-1->87->86 [2] 84/-1/-1->87->86 [3] 84/-1/-1->87->86 [4] -1/-1/-1->87->86 [5] -1/-1/-1->87->86 [6] 84/-1/-1->87->86 [7] 84/-1/-1->87->86
21: nid006559:211125:213871 [2] NCCL INFO Trees [0] 87/-1/-1->86->85 [1] 87/-1/-1->86->85 [2] 87/-1/-1->86->90 [3] 87/-1/-1->86->90 [4] 87/-1/-1->86->85 [5] 87/-1/-1->86->85 [6] 87/90/82->86->78 [7] 87/90/82->86->78
20: nid006558:215470:218259 [3] NCCL INFO comm 0x4006c959d3c0 rank 83 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
20: nid006558:215469:218260 [2] NCCL INFO comm 0x4006cd59d3c0 rank 82 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 7: nid006502:252585:255363 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] -1/-1/-1->29->28 [3] -1/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->28 [6] -1/-1/-1->29->28 [7] -1/-1/-1->29->28
 7: nid006502:252585:255363 [1] NCCL INFO P2P Chunksize set to 131072
22: nid006560:222271:225055 [0] NCCL INFO Trees [0] 89/84/92->88->80 [1] 89/84/92->88->80 [2] 89/-1/-1->88->91 [3] 89/-1/-1->88->91 [4] 89/-1/-1->88->84 [5] 89/-1/-1->88->84 [6] 89/-1/-1->88->91 [7] 89/-1/-1->88->91
22: nid006560:222271:225055 [0] NCCL INFO P2P Chunksize set to 131072
 7: nid006502:252584:255362 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/-1/-1->28->31 [3] 29/-1/-1->28->31 [4] 29/44/12->28->60 [5] 29/44/12->28->60 [6] 29/-1/-1->28->31 [7] 29/-1/-1->28->31
 7: nid006502:252584:255362 [0] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211124:213873 [1] NCCL INFO Trees [0] 86/-1/-1->85->84 [1] 86/-1/-1->85->84 [2] -1/-1/-1->85->84 [3] -1/-1/-1->85->84 [4] 86/-1/-1->85->84 [5] 86/-1/-1->85->84 [6] -1/-1/-1->85->84 [7] -1/-1/-1->85->84
21: nid006559:211123:213872 [0] NCCL INFO comm 0x40065d59d270 rank 84 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
21: nid006559:211126:213874 [3] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211125:213871 [2] NCCL INFO P2P Chunksize set to 131072
21: nid006559:211124:213873 [1] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221944:224721 [3] NCCL INFO comm 0x4006ed59d380 rank 27 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 6: nid006501:221943:224720 [2] NCCL INFO comm 0x4006e959d2d0 rank 26 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
20: nid006558:215470:218259 [3] NCCL INFO Trees [0] -1/-1/-1->83->82 [1] -1/-1/-1->83->82 [2] 80/-1/-1->83->82 [3] 80/-1/-1->83->82 [4] -1/-1/-1->83->82 [5] -1/-1/-1->83->82 [6] 80/-1/-1->83->82 [7] 80/-1/-1->83->82
20: nid006558:215470:218259 [3] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215469:218260 [2] NCCL INFO Trees [0] 83/-1/-1->82->81 [1] 83/-1/-1->82->81 [2] 83/74/90->82->98 [3] 83/74/90->82->98 [4] 83/-1/-1->82->81 [5] 83/-1/-1->82->81 [6] 83/-1/-1->82->86 [7] 83/-1/-1->82->86
20: nid006558:215468:218258 [1] NCCL INFO comm 0x4006c559d590 rank 81 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
21: nid006559:211123:213872 [0] NCCL INFO Trees [0] 85/-1/-1->84->88 [1] 85/-1/-1->84->88 [2] 85/-1/-1->84->87 [3] 85/-1/-1->84->87 [4] 85/88/80->84->76 [5] 85/88/80->84->76 [6] 85/-1/-1->84->87 [7] 85/-1/-1->84->87
21: nid006559:211123:213872 [0] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215469:218260 [2] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208420:211184 [3] NCCL INFO comm 0x4006f159d540 rank 79 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
19: nid006557:208419:211183 [2] NCCL INFO comm 0x4006ad59d3a0 rank 78 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
20: nid006558:215467:218261 [0] NCCL INFO comm 0x40065d59d340 rank 80 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 6: nid006501:221944:224721 [3] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 24/-1/-1->27->26 [3] 24/-1/-1->27->26 [4] -1/-1/-1->27->26 [5] -1/-1/-1->27->26 [6] 24/-1/-1->27->26 [7] 24/-1/-1->27->26
 6: nid006501:221944:224721 [3] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221943:224720 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/22/30->26->18 [3] 27/22/30->26->18 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->22 [7] 27/-1/-1->26->22
 6: nid006501:221943:224720 [2] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215468:218258 [1] NCCL INFO Trees [0] 82/-1/-1->81->80 [1] 82/-1/-1->81->80 [2] -1/-1/-1->81->80 [3] -1/-1/-1->81->80 [4] 82/-1/-1->81->80 [5] 82/-1/-1->81->80 [6] -1/-1/-1->81->80 [7] -1/-1/-1->81->80
20: nid006558:215468:218258 [1] NCCL INFO P2P Chunksize set to 131072
20: nid006558:215467:218261 [0] NCCL INFO Trees [0] 81/72/88->80->96 [1] 81/72/88->80->96 [2] 81/-1/-1->80->83 [3] 81/-1/-1->80->83 [4] 81/-1/-1->80->84 [5] 81/-1/-1->80->84 [6] 81/-1/-1->80->83 [7] 81/-1/-1->80->83
20: nid006558:215467:218261 [0] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210777:213635 [3] NCCL INFO comm 0x4006ad59d430 rank 75 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
19: nid006557:208417:211182 [0] NCCL INFO comm 0x4006a159d2c0 rank 76 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
19: nid006557:208418:211185 [1] NCCL INFO comm 0x4006e559d540 rank 77 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
18: nid006556:210777:213635 [3] NCCL INFO Trees [0] -1/-1/-1->75->74 [1] -1/-1/-1->75->74 [2] 72/-1/-1->75->74 [3] 72/-1/-1->75->74 [4] -1/-1/-1->75->74 [5] -1/-1/-1->75->74 [6] 72/-1/-1->75->74 [7] 72/-1/-1->75->74
18: nid006556:210777:213635 [3] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208420:211184 [3] NCCL INFO Trees [0] -1/-1/-1->79->78 [1] -1/-1/-1->79->78 [2] 76/-1/-1->79->78 [3] 76/-1/-1->79->78 [4] -1/-1/-1->79->78 [5] -1/-1/-1->79->78 [6] 76/-1/-1->79->78 [7] 76/-1/-1->79->78
19: nid006557:208419:211183 [2] NCCL INFO Trees [0] 79/-1/-1->78->77 [1] 79/-1/-1->78->77 [2] 79/-1/-1->78->74 [3] 79/-1/-1->78->74 [4] 79/-1/-1->78->77 [5] 79/-1/-1->78->77 [6] 79/86/70->78->94 [7] 79/86/70->78->94
19: nid006557:208420:211184 [3] NCCL INFO P2P Chunksize set to 131072
19: nid006557:208419:211183 [2] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210776:213633 [2] NCCL INFO comm 0x4006cd59d2c0 rank 74 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
17: nid006555:206595:209409 [2] NCCL INFO comm 0x4006d559d410 rank 70 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
19: nid006557:208418:211185 [1] NCCL INFO Trees [0] 78/-1/-1->77->76 [1] 78/-1/-1->77->76 [2] -1/-1/-1->77->76 [3] -1/-1/-1->77->76 [4] 78/-1/-1->77->76 [5] 78/-1/-1->77->76 [6] -1/-1/-1->77->76 [7] -1/-1/-1->77->76
19: nid006557:208418:211185 [1] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206596:209407 [3] NCCL INFO comm 0x4006cd59d4a0 rank 71 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
17: nid006555:206594:209408 [1] NCCL INFO comm 0x4006e559d320 rank 69 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
19: nid006557:208417:211182 [0] NCCL INFO Trees [0] 77/-1/-1->76->72 [1] 77/-1/-1->76->72 [2] 77/-1/-1->76->79 [3] 77/-1/-1->76->79 [4] 77/84/68->76->92 [5] 77/84/68->76->92 [6] 77/-1/-1->76->79 [7] 77/-1/-1->76->79
19: nid006557:208417:211182 [0] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221942:224719 [1] NCCL INFO comm 0x4006e559d220 rank 25 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
18: nid006556:210774:213632 [0] NCCL INFO comm 0x40063d59d540 rank 72 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
18: nid006556:210775:213634 [1] NCCL INFO comm 0x4006c559d570 rank 73 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
16: nid006554:221584:224381 [3] NCCL INFO comm 0x4006ed59d3e0 rank 67 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
16: nid006554:221583:224384 [2] NCCL INFO comm 0x4006c959d390 rank 66 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
17: nid006555:206595:209409 [2] NCCL INFO Trees [0] 71/-1/-1->70->69 [1] 71/-1/-1->70->69 [2] 71/-1/-1->70->74 [3] 71/-1/-1->70->74 [4] 71/-1/-1->70->69 [5] 71/-1/-1->70->69 [6] 71/74/66->70->78 [7] 71/74/66->70->78
17: nid006555:206595:209409 [2] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210776:213633 [2] NCCL INFO Trees [0] 75/-1/-1->74->73 [1] 75/-1/-1->74->73 [2] 75/70/78->74->82 [3] 75/70/78->74->82 [4] 75/-1/-1->74->73 [5] 75/-1/-1->74->73 [6] 75/-1/-1->74->70 [7] 75/-1/-1->74->70
18: nid006556:210776:213633 [2] NCCL INFO P2P Chunksize set to 131072
 6: nid006501:221942:224719 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->24 [2] -1/-1/-1->25->24 [3] -1/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] -1/-1/-1->25->24 [7] -1/-1/-1->25->24
 6: nid006501:221942:224719 [1] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210774:213632 [0] NCCL INFO Trees [0] 73/68/76->72->80 [1] 73/68/76->72->80 [2] 73/-1/-1->72->75 [3] 73/-1/-1->72->75 [4] 73/-1/-1->72->68 [5] 73/-1/-1->72->68 [6] 73/-1/-1->72->75 [7] 73/-1/-1->72->75
18: nid006556:210775:213634 [1] NCCL INFO Trees [0] 74/-1/-1->73->72 [1] 74/-1/-1->73->72 [2] -1/-1/-1->73->72 [3] -1/-1/-1->73->72 [4] 74/-1/-1->73->72 [5] 74/-1/-1->73->72 [6] -1/-1/-1->73->72 [7] -1/-1/-1->73->72
18: nid006556:210774:213632 [0] NCCL INFO P2P Chunksize set to 131072
18: nid006556:210775:213634 [1] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206596:209407 [3] NCCL INFO Trees [0] -1/-1/-1->71->70 [1] -1/-1/-1->71->70 [2] 68/-1/-1->71->70 [3] 68/-1/-1->71->70 [4] -1/-1/-1->71->70 [5] -1/-1/-1->71->70 [6] 68/-1/-1->71->70 [7] 68/-1/-1->71->70
17: nid006555:206594:209408 [1] NCCL INFO Trees [0] 70/-1/-1->69->68 [1] 70/-1/-1->69->68 [2] -1/-1/-1->69->68 [3] -1/-1/-1->69->68 [4] 70/-1/-1->69->68 [5] 70/-1/-1->69->68 [6] -1/-1/-1->69->68 [7] -1/-1/-1->69->68
17: nid006555:206596:209407 [3] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206594:209408 [1] NCCL INFO P2P Chunksize set to 131072
17: nid006555:206593:209410 [0] NCCL INFO comm 0x4006c159d230 rank 68 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
16: nid006554:221582:224382 [1] NCCL INFO comm 0x4006c559d650 rank 65 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
16: nid006554:221584:224381 [3] NCCL INFO Trees [0] -1/-1/-1->67->66 [1] -1/-1/-1->67->66 [2] 64/-1/-1->67->66 [3] 64/-1/-1->67->66 [4] -1/-1/-1->67->66 [5] -1/-1/-1->67->66 [6] 64/-1/-1->67->66 [7] 64/-1/-1->67->66
15: nid006553:223926:226696 [3] NCCL INFO comm 0x4006cd59d340 rank 63 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
15: nid006553:223925:226694 [2] NCCL INFO comm 0x4006e559d570 rank 62 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
15: nid006553:223924:226695 [1] NCCL INFO comm 0x4006e559d560 rank 61 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
16: nid006554:221581:224383 [0] NCCL INFO comm 0x40063d59d330 rank 64 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
16: nid006554:221583:224384 [2] NCCL INFO Trees [0] 67/-1/-1->66->65 [1] 67/-1/-1->66->65 [2] 67/34/98->66->2 [3] 67/34/98->66->2 [4] 67/-1/-1->66->65 [5] 67/-1/-1->66->65 [6] 67/-1/-1->66->70 [7] 67/-1/-1->66->70
16: nid006554:221584:224381 [3] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223923:226697 [0] NCCL INFO comm 0x40065d59d3b0 rank 60 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
16: nid006554:221583:224384 [2] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221582:224382 [1] NCCL INFO Trees [0] 66/-1/-1->65->64 [1] 66/-1/-1->65->64 [2] -1/-1/-1->65->64 [3] -1/-1/-1->65->64 [4] 66/-1/-1->65->64 [5] 66/-1/-1->65->64 [6] -1/-1/-1->65->64 [7] -1/-1/-1->65->64
16: nid006554:221582:224382 [1] NCCL INFO P2P Chunksize set to 131072
16: nid006554:221581:224383 [0] NCCL INFO Trees [0] 65/32/96->64->0 [1] 65/32/96->64->0 [2] 65/-1/-1->64->67 [3] 65/-1/-1->64->67 [4] 65/-1/-1->64->68 [5] 65/-1/-1->64->68 [6] 65/-1/-1->64->67 [7] 65/-1/-1->64->67
16: nid006554:221581:224383 [0] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201791:204556 [3] NCCL INFO comm 0x4006d159d3b0 rank 55 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
13: nid006509:201790:204559 [2] NCCL INFO comm 0x4006cd59d3c0 rank 54 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 6: nid006501:221941:224722 [0] NCCL INFO comm 0x40065d59d2e0 rank 24 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
14: nid006510:229445:232221 [3] NCCL INFO comm 0x4006ad59d550 rank 59 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
14: nid006510:229442:232222 [0] NCCL INFO comm 0x40065d59d400 rank 56 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
15: nid006553:223926:226696 [3] NCCL INFO Trees [0] -1/-1/-1->63->62 [1] -1/-1/-1->63->62 [2] 60/-1/-1->63->62 [3] 60/-1/-1->63->62 [4] -1/-1/-1->63->62 [5] -1/-1/-1->63->62 [6] 60/-1/-1->63->62 [7] 60/-1/-1->63->62
15: nid006553:223925:226694 [2] NCCL INFO Trees [0] 63/-1/-1->62->61 [1] 63/-1/-1->62->61 [2] 63/-1/-1->62->58 [3] 63/-1/-1->62->58 [4] 63/-1/-1->62->61 [5] 63/-1/-1->62->61 [6] 63/94/30->62->126 [7] 63/94/30->62->126
17: nid006555:206593:209410 [0] NCCL INFO Trees [0] 69/-1/-1->68->72 [1] 69/-1/-1->68->72 [2] 69/-1/-1->68->71 [3] 69/-1/-1->68->71 [4] 69/72/64->68->76 [5] 69/72/64->68->76 [6] 69/-1/-1->68->71 [7] 69/-1/-1->68->71
17: nid006555:206593:209410 [0] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223926:226696 [3] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223925:226694 [2] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223924:226695 [1] NCCL INFO Trees [0] 62/-1/-1->61->60 [1] 62/-1/-1->61->60 [2] -1/-1/-1->61->60 [3] -1/-1/-1->61->60 [4] 62/-1/-1->61->60 [5] 62/-1/-1->61->60 [6] -1/-1/-1->61->60 [7] -1/-1/-1->61->60
15: nid006553:223923:226697 [0] NCCL INFO Trees [0] 61/-1/-1->60->56 [1] 61/-1/-1->60->56 [2] 61/-1/-1->60->63 [3] 61/-1/-1->60->63 [4] 61/92/28->60->124 [5] 61/92/28->60->124 [6] 61/-1/-1->60->63 [7] 61/-1/-1->60->63
15: nid006553:223924:226695 [1] NCCL INFO P2P Chunksize set to 131072
15: nid006553:223923:226697 [0] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205769:208641 [3] NCCL INFO comm 0x4006ed59d3e0 rank 51 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
13: nid006509:201791:204556 [3] NCCL INFO Trees [0] -1/-1/-1->55->54 [1] -1/-1/-1->55->54 [2] 52/-1/-1->55->54 [3] 52/-1/-1->55->54 [4] -1/-1/-1->55->54 [5] -1/-1/-1->55->54 [6] 52/-1/-1->55->54 [7] 52/-1/-1->55->54
13: nid006509:201791:204556 [3] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201790:204559 [2] NCCL INFO Trees [0] 55/-1/-1->54->53 [1] 55/-1/-1->54->53 [2] 55/-1/-1->54->58 [3] 55/-1/-1->54->58 [4] 55/-1/-1->54->53 [5] 55/-1/-1->54->53 [6] 55/58/50->54->46 [7] 55/58/50->54->46
13: nid006509:201789:204557 [1] NCCL INFO comm 0x4006e559d490 rank 53 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
13: nid006509:201790:204559 [2] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229444:232220 [2] NCCL INFO comm 0x4006ed59d3e0 rank 58 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
14: nid006510:229443:232223 [1] NCCL INFO comm 0x4006c559d450 rank 57 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
13: nid006509:201788:204558 [0] NCCL INFO comm 0x40065d59d290 rank 52 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 6: nid006501:221941:224722 [0] NCCL INFO Trees [0] 25/20/28->24->16 [1] 25/20/28->24->16 [2] 25/-1/-1->24->27 [3] 25/-1/-1->24->27 [4] 25/-1/-1->24->20 [5] 25/-1/-1->24->20 [6] 25/-1/-1->24->27 [7] 25/-1/-1->24->27
 6: nid006501:221941:224722 [0] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229445:232221 [3] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] -1/-1/-1->59->58 [2] 56/-1/-1->59->58 [3] 56/-1/-1->59->58 [4] -1/-1/-1->59->58 [5] -1/-1/-1->59->58 [6] 56/-1/-1->59->58 [7] 56/-1/-1->59->58
14: nid006510:229445:232221 [3] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249083:251865 [3] NCCL INFO comm 0x4006b0abf8a0 rank 39 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 9: nid006505:249082:251866 [2] NCCL INFO comm 0x4006e959d2a0 rank 38 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
12: nid006508:205768:208642 [2] NCCL INFO comm 0x4006cd59d5e0 rank 50 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
12: nid006508:205767:208640 [1] NCCL INFO comm 0x4006c559d610 rank 49 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
14: nid006510:229444:232220 [2] NCCL INFO Trees [0] 59/-1/-1->58->57 [1] 59/-1/-1->58->57 [2] 59/54/62->58->50 [3] 59/54/62->58->50 [4] 59/-1/-1->58->57 [5] 59/-1/-1->58->57 [6] 59/-1/-1->58->54 [7] 59/-1/-1->58->54
14: nid006510:229443:232223 [1] NCCL INFO Trees [0] 58/-1/-1->57->56 [1] 58/-1/-1->57->56 [2] -1/-1/-1->57->56 [3] -1/-1/-1->57->56 [4] 58/-1/-1->57->56 [5] 58/-1/-1->57->56 [6] -1/-1/-1->57->56 [7] -1/-1/-1->57->56
 5: nid006500:260086:262847 [3] NCCL INFO comm 0x4006cd59d250 rank 23 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 5: nid006500:260085:262848 [2] NCCL INFO comm 0x4006ed59d380 rank 22 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 9: nid006505:249080:251867 [0] NCCL INFO comm 0x4006e159d370 rank 36 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 9: nid006505:249081:251864 [1] NCCL INFO comm 0x4006a559d100 rank 37 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
12: nid006508:205769:208641 [3] NCCL INFO Trees [0] -1/-1/-1->51->50 [1] -1/-1/-1->51->50 [2] 48/-1/-1->51->50 [3] 48/-1/-1->51->50 [4] -1/-1/-1->51->50 [5] -1/-1/-1->51->50 [6] 48/-1/-1->51->50 [7] 48/-1/-1->51->50
12: nid006508:205769:208641 [3] NCCL INFO P2P Chunksize set to 131072
13: nid006509:201789:204557 [1] NCCL INFO Trees [0] 54/-1/-1->53->52 [1] 54/-1/-1->53->52 [2] -1/-1/-1->53->52 [3] -1/-1/-1->53->52 [4] 54/-1/-1->53->52 [5] 54/-1/-1->53->52 [6] -1/-1/-1->53->52 [7] -1/-1/-1->53->52
13: nid006509:201788:204558 [0] NCCL INFO Trees [0] 53/-1/-1->52->56 [1] 53/-1/-1->52->56 [2] 53/-1/-1->52->55 [3] 53/-1/-1->52->55 [4] 53/56/48->52->44 [5] 53/56/48->52->44 [6] 53/-1/-1->52->55 [7] 53/-1/-1->52->55
13: nid006509:201789:204557 [1] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229444:232220 [2] NCCL INFO P2P Chunksize set to 131072
14: nid006510:229443:232223 [1] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211338:214084 [3] NCCL INFO comm 0x4006f559d200 rank 47 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
14: nid006510:229442:232222 [0] NCCL INFO Trees [0] 57/52/60->56->48 [1] 57/52/60->56->48 [2] 57/-1/-1->56->59 [3] 57/-1/-1->56->59 [4] 57/-1/-1->56->52 [5] 57/-1/-1->56->52 [6] 57/-1/-1->56->59 [7] 57/-1/-1->56->59
14: nid006510:229442:232222 [0] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260086:262847 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 20/-1/-1->23->22 [3] 20/-1/-1->23->22 [4] -1/-1/-1->23->22 [5] -1/-1/-1->23->22 [6] 20/-1/-1->23->22 [7] 20/-1/-1->23->22
 5: nid006500:260086:262847 [3] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249083:251865 [3] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] 36/-1/-1->39->38 [3] 36/-1/-1->39->38 [4] -1/-1/-1->39->38 [5] -1/-1/-1->39->38 [6] 36/-1/-1->39->38 [7] 36/-1/-1->39->38
 9: nid006505:249083:251865 [3] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205766:208643 [0] NCCL INFO comm 0x40061d59d500 rank 48 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
12: nid006508:205768:208642 [2] NCCL INFO Trees [0] 51/-1/-1->50->49 [1] 51/-1/-1->50->49 [2] 51/42/58->50->34 [3] 51/42/58->50->34 [4] 51/-1/-1->50->49 [5] 51/-1/-1->50->49 [6] 51/-1/-1->50->54 [7] 51/-1/-1->50->54
13: nid006509:201788:204558 [0] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260085:262848 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->26 [3] 23/-1/-1->22->26 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/26/18->22->14 [7] 23/26/18->22->14
 5: nid006500:260085:262848 [2] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249082:251866 [2] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] 39/-1/-1->38->37 [2] 39/-1/-1->38->42 [3] 39/-1/-1->38->42 [4] 39/-1/-1->38->37 [5] 39/-1/-1->38->37 [6] 39/42/34->38->46 [7] 39/42/34->38->46
 9: nid006505:249082:251866 [2] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211337:214086 [2] NCCL INFO comm 0x4006e959d290 rank 46 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
11: nid006507:211338:214084 [3] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] 44/-1/-1->47->46 [3] 44/-1/-1->47->46 [4] -1/-1/-1->47->46 [5] -1/-1/-1->47->46 [6] 44/-1/-1->47->46 [7] 44/-1/-1->47->46
11: nid006507:211335:214085 [0] NCCL INFO comm 0x40065d59d500 rank 44 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
12: nid006508:205768:208642 [2] NCCL INFO P2P Chunksize set to 131072
 9: nid006505:249080:251867 [0] NCCL INFO Trees [0] 37/-1/-1->36->40 [1] 37/-1/-1->36->40 [2] 37/-1/-1->36->39 [3] 37/-1/-1->36->39 [4] 37/40/32->36->44 [5] 37/40/32->36->44 [6] 37/-1/-1->36->39 [7] 37/-1/-1->36->39
 9: nid006505:249081:251864 [1] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] -1/-1/-1->37->36 [3] -1/-1/-1->37->36 [4] 38/-1/-1->37->36 [5] 38/-1/-1->37->36 [6] -1/-1/-1->37->36 [7] -1/-1/-1->37->36
 9: nid006505:249080:251867 [0] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205767:208640 [1] NCCL INFO Trees [0] 50/-1/-1->49->48 [1] 50/-1/-1->49->48 [2] -1/-1/-1->49->48 [3] -1/-1/-1->49->48 [4] 50/-1/-1->49->48 [5] 50/-1/-1->49->48 [6] -1/-1/-1->49->48 [7] -1/-1/-1->49->48
12: nid006508:205767:208640 [1] NCCL INFO P2P Chunksize set to 131072
10: nid006506:263730:266490 [3] NCCL INFO comm 0x4006cd59d520 rank 43 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
10: nid006506:263729:266489 [2] NCCL INFO comm 0x4006cd59d320 rank 42 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 9: nid006505:249081:251864 [1] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211336:214087 [1] NCCL INFO comm 0x4006c559d480 rank 45 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
11: nid006507:211338:214084 [3] NCCL INFO P2P Chunksize set to 131072
12: nid006508:205766:208643 [0] NCCL INFO Trees [0] 49/40/56->48->32 [1] 49/40/56->48->32 [2] 49/-1/-1->48->51 [3] 49/-1/-1->48->51 [4] 49/-1/-1->48->52 [5] 49/-1/-1->48->52 [6] 49/-1/-1->48->51 [7] 49/-1/-1->48->51
12: nid006508:205766:208643 [0] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218410:221166 [0] NCCL INFO comm 0x4006c159d410 rank 32 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 8: nid006503:218413:221168 [3] NCCL INFO comm 0x4006c959d440 rank 35 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
10: nid006506:263727:266491 [0] NCCL INFO comm 0x4006e159d2a0 rank 40 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
11: nid006507:211337:214086 [2] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->42 [3] 47/-1/-1->46->42 [4] 47/-1/-1->46->45 [5] 47/-1/-1->46->45 [6] 47/54/38->46->30 [7] 47/54/38->46->30
11: nid006507:211337:214086 [2] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218412:221169 [2] NCCL INFO comm 0x4006cd59d300 rank 34 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 8: nid006503:218411:221167 [1] NCCL INFO comm 0x4006a559d440 rank 33 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
10: nid006506:263728:266492 [1] NCCL INFO comm 0x4006e559d2f0 rank 41 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
10: nid006506:263730:266490 [3] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] 40/-1/-1->43->42 [3] 40/-1/-1->43->42 [4] -1/-1/-1->43->42 [5] -1/-1/-1->43->42 [6] 40/-1/-1->43->42 [7] 40/-1/-1->43->42
 8: nid006503:218413:221168 [3] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] 32/-1/-1->35->34 [3] 32/-1/-1->35->34 [4] -1/-1/-1->35->34 [5] -1/-1/-1->35->34 [6] 32/-1/-1->35->34 [7] 32/-1/-1->35->34
 8: nid006503:218413:221168 [3] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260084:262849 [1] NCCL INFO comm 0x4006c559d540 rank 21 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
10: nid006506:263729:266489 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/38/46->42->50 [3] 43/38/46->42->50 [4] 43/-1/-1->42->41 [5] 43/-1/-1->42->41 [6] 43/-1/-1->42->38 [7] 43/-1/-1->42->38
10: nid006506:263730:266490 [3] NCCL INFO P2P Chunksize set to 131072
10: nid006506:263729:266489 [2] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260083:262846 [0] NCCL INFO comm 0x40063d59d550 rank 20 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
11: nid006507:211335:214085 [0] NCCL INFO Trees [0] 45/-1/-1->44->40 [1] 45/-1/-1->44->40 [2] 45/-1/-1->44->47 [3] 45/-1/-1->44->47 [4] 45/52/36->44->28 [5] 45/52/36->44->28 [6] 45/-1/-1->44->47 [7] 45/-1/-1->44->47
11: nid006507:211336:214087 [1] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] -1/-1/-1->45->44 [3] -1/-1/-1->45->44 [4] 46/-1/-1->45->44 [5] 46/-1/-1->45->44 [6] -1/-1/-1->45->44 [7] -1/-1/-1->45->44
 8: nid006503:218412:221169 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/18/50->34->66 [3] 35/18/50->34->66 [4] 35/-1/-1->34->33 [5] 35/-1/-1->34->33 [6] 35/-1/-1->34->38 [7] 35/-1/-1->34->38
 8: nid006503:218412:221169 [2] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211336:214087 [1] NCCL INFO P2P Chunksize set to 131072
11: nid006507:211335:214085 [0] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218411:221167 [1] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 34/-1/-1->33->32 [2] -1/-1/-1->33->32 [3] -1/-1/-1->33->32 [4] 34/-1/-1->33->32 [5] 34/-1/-1->33->32 [6] -1/-1/-1->33->32 [7] -1/-1/-1->33->32
 8: nid006503:218410:221166 [0] NCCL INFO Trees [0] 33/16/48->32->64 [1] 33/16/48->32->64 [2] 33/-1/-1->32->35 [3] 33/-1/-1->32->35 [4] 33/-1/-1->32->36 [5] 33/-1/-1->32->36 [6] 33/-1/-1->32->35 [7] 33/-1/-1->32->35
 5: nid006500:260084:262849 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] -1/-1/-1->21->20 [3] -1/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/-1/-1->21->20 [6] -1/-1/-1->21->20 [7] -1/-1/-1->21->20
 5: nid006500:260084:262849 [1] NCCL INFO P2P Chunksize set to 131072
10: nid006506:263728:266492 [1] NCCL INFO Trees [0] 42/-1/-1->41->40 [1] 42/-1/-1->41->40 [2] -1/-1/-1->41->40 [3] -1/-1/-1->41->40 [4] 42/-1/-1->41->40 [5] 42/-1/-1->41->40 [6] -1/-1/-1->41->40 [7] -1/-1/-1->41->40
10: nid006506:263728:266492 [1] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218411:221167 [1] NCCL INFO P2P Chunksize set to 131072
10: nid006506:263727:266491 [0] NCCL INFO Trees [0] 41/36/44->40->48 [1] 41/36/44->40->48 [2] 41/-1/-1->40->43 [3] 41/-1/-1->40->43 [4] 41/-1/-1->40->36 [5] 41/-1/-1->40->36 [6] 41/-1/-1->40->43 [7] 41/-1/-1->40->43
10: nid006506:263727:266491 [0] NCCL INFO P2P Chunksize set to 131072
 8: nid006503:218410:221166 [0] NCCL INFO P2P Chunksize set to 131072
 5: nid006500:260083:262846 [0] NCCL INFO Trees [0] 21/-1/-1->20->24 [1] 21/-1/-1->20->24 [2] 21/-1/-1->20->23 [3] 21/-1/-1->20->23 [4] 21/24/16->20->12 [5] 21/24/16->20->12 [6] 21/-1/-1->20->23 [7] 21/-1/-1->20->23
 5: nid006500:260083:262846 [0] NCCL INFO P2P Chunksize set to 131072
 4: nid006499:254559:257317 [3] NCCL INFO comm 0x4006d159d380 rank 19 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 4: nid006499:254559:257317 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 16/-1/-1->19->18 [3] 16/-1/-1->19->18 [4] -1/-1/-1->19->18 [5] -1/-1/-1->19->18 [6] 16/-1/-1->19->18 [7] 16/-1/-1->19->18
 4: nid006499:254559:257317 [3] NCCL INFO P2P Chunksize set to 131072
 4: nid006499:254558:257318 [2] NCCL INFO comm 0x4006ed59d070 rank 18 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 4: nid006499:254557:257316 [1] NCCL INFO comm 0x4006e559d3c0 rank 17 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 4: nid006499:254558:257318 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->34 [3] 19/10/26->18->34 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->22 [7] 19/-1/-1->18->22
 4: nid006499:254558:257318 [2] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226768:229503 [3] NCCL INFO comm 0x4006cd59d320 rank 15 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 4: nid006499:254556:257319 [0] NCCL INFO comm 0x40065d59d1d0 rank 16 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 3: nid006498:226767:229506 [2] NCCL INFO comm 0x4006e959d2c0 rank 14 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 4: nid006499:254557:257316 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16 [2] -1/-1/-1->17->16 [3] -1/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] -1/-1/-1->17->16 [7] -1/-1/-1->17->16
 4: nid006499:254557:257316 [1] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226766:229504 [1] NCCL INFO comm 0x4006c559d3a0 rank 13 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 4: nid006499:254556:257319 [0] NCCL INFO Trees [0] 17/8/24->16->32 [1] 17/8/24->16->32 [2] 17/-1/-1->16->19 [3] 17/-1/-1->16->19 [4] 17/-1/-1->16->20 [5] 17/-1/-1->16->20 [6] 17/-1/-1->16->19 [7] 17/-1/-1->16->19
 4: nid006499:254556:257319 [0] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226768:229503 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
 3: nid006498:226768:229503 [3] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226767:229506 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/22/6->14->30 [7] 15/22/6->14->30
 3: nid006498:226767:229506 [2] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226766:229504 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
 3: nid006498:226766:229504 [1] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227577:230334 [2] NCCL INFO comm 0x4006cd59d470 rank 10 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 2: nid006497:227578:230336 [3] NCCL INFO comm 0x4006cd59d3c0 rank 11 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 2: nid006497:227576:230337 [1] NCCL INFO comm 0x4006c959d080 rank 9 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 3: nid006498:226765:229505 [0] NCCL INFO comm 0x40063d59d400 rank 12 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 2: nid006497:227575:230335 [0] NCCL INFO comm 0x40061d59d270 rank 8 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 2: nid006497:227577:230334 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->18 [3] 11/6/14->10->18 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
 2: nid006497:227578:230336 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
 2: nid006497:227577:230334 [2] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227576:230337 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
 2: nid006497:227578:230336 [3] NCCL INFO P2P Chunksize set to 131072
 3: nid006498:226765:229505 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/20/4->12->28 [5] 13/20/4->12->28 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
 3: nid006498:226765:229505 [0] NCCL INFO P2P Chunksize set to 131072
 2: nid006497:227576:230337 [1] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242587:245388 [3] NCCL INFO comm 0x4006ed59d380 rank 7 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 1: nid006496:242586:245386 [2] NCCL INFO comm 0x4006ed59d300 rank 6 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 2: nid006497:227575:230335 [0] NCCL INFO Trees [0] 9/4/12->8->16 [1] 9/4/12->8->16 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
 2: nid006497:227575:230335 [0] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242587:245388 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
 1: nid006496:242587:245388 [3] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241022:244120 [3] NCCL INFO comm 0x4006cd59d340 rank 3 nRanks 128 nNodes 32 localRanks 4 localRank 3 MNNVL 0
 1: nid006496:242585:245389 [1] NCCL INFO comm 0x4006c559d4f0 rank 5 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 1: nid006496:242586:245386 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
 1: nid006496:242586:245386 [2] NCCL INFO P2P Chunksize set to 131072
 1: nid006496:242584:245387 [0] NCCL INFO comm 0x4006c00d5ce0 rank 4 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 1: nid006496:242585:245389 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
 1: nid006496:242585:245389 [1] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241022:244120 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
 0: nid006495:241022:244120 [3] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241021:244119 [2] NCCL INFO comm 0x4006ad59d440 rank 2 nRanks 128 nNodes 32 localRanks 4 localRank 2 MNNVL 0
 0: nid006495:241020:244121 [1] NCCL INFO comm 0x4006e559d380 rank 1 nRanks 128 nNodes 32 localRanks 4 localRank 1 MNNVL 0
 0: nid006495:241019:244118 [0] NCCL INFO comm 0x4006b00d5c40 rank 0 nRanks 128 nNodes 32 localRanks 4 localRank 0 MNNVL 0
 0: nid006495:241021:244119 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/66/-1->2->-1 [3] 3/66/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
 0: nid006495:241021:244119 [2] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241019:244118 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
 1: nid006496:242584:245387 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
 1: nid006496:242584:245387 [0] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241020:244121 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
 0: nid006495:241019:244118 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
 0: nid006495:241020:244121 [1] NCCL INFO P2P Chunksize set to 131072
 0: nid006495:241019:244118 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
 0: nid006495:241019:244118 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
 0: nid006495:241019:244118 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
 0: nid006495:241019:244118 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
 0: nid006495:241019:244118 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
 0: nid006495:241019:244118 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
 0: nid006495:241019:244118 [0] NCCL INFO Trees [0] 1/64/-1->0->-1 [1] 1/64/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
 0: nid006495:241019:244118 [0] NCCL INFO P2P Chunksize set to 131072
28: nid007251:72169:75235 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72169:75235 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59770:62998 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59770:62998 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20583:23369 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20583:23369 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27222:29915 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27222:29915 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59767:62995 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59767:62995 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229445:232221 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229445:232221 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72172:75238 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72172:75238 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223022:225758 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223022:225758 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27224:29916 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27224:29916 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222466:225266 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222466:225266 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220712:223487 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220712:223487 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59768:62996 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59768:62996 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222274:225056 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222274:225056 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216749:219512 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216749:219512 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59769:62997 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
31: nid007342:59769:62997 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211126:213874 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211126:213874 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20581:23371 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20581:23371 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223024:225759 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223024:225759 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222465:225268 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222465:225268 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252585:255363 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252585:255363 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221143:223916 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221143:223916 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221584:224381 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221584:224381 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206594:209408 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206594:209408 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72170:75236 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72170:75236 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215467:218261 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215467:218261 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20582:23370 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20582:23370 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215468:218258 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215468:218258 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208419:211183 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208419:211183 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20580:23372 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
30: nid007318:20580:23372 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210777:213635 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210777:213635 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220714:223485 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220714:223485 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223023:225756 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223023:225756 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221944:224721 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221944:224721 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72171:75237 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
28: nid007251:72171:75237 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27225:29913 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27225:29913 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222272:225053 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222272:225053 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216747:219509 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216747:219509 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208420:211184 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208420:211184 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221141:223918 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221141:223918 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27223:29914 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
29: nid007305:27223:29914 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223924:226695 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223924:226695 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223021:225757 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
25: nid006564:223021:225757 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222468:225265 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222468:225265 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206595:209409 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206595:209409 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
10: nid006506:263729:266489 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263729:266489 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252586:255365 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252586:255365 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222467:225267 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
26: nid006565:222467:225267 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59768:62996 [1] NCCL INFO ncclCommSplit comm 0x4006e559d3c0 rank 125 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaebad4d50 color 315732477 key 125 commId 0xd4eed6c1342b1de7 - Init COMPLETE
31: nid007342:59770:62998 [3] NCCL INFO ncclCommSplit comm 0x4006e959d690 rank 127 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaae96a06d0 color 315732477 key 127 commId 0xd4eed6c1342b1de7 - Init COMPLETE
23: nid006561:220713:223486 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220713:223486 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254556:257319 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254556:257319 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220711:223488 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
23: nid006561:220711:223488 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210775:213634 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210775:213634 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59768:62996 [1] NCCL INFO Init timings: rank 125 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59770:62998 [3] NCCL INFO Init timings: rank 127 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216748:219511 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216748:219511 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59767:62995 [0] NCCL INFO ncclCommSplit comm 0x4006a159d420 rank 124 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab02e43cb0 color 315732477 key 124 commId 0xd4eed6c1342b1de7 - Init COMPLETE
31: nid007342:59769:62997 [2] NCCL INFO ncclCommSplit comm 0x4006c959d480 rank 126 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaacd751c60 color 315732477 key 126 commId 0xd4eed6c1342b1de7 - Init COMPLETE
31: nid007342:59767:62995 [0] NCCL INFO Init timings: rank 124 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59769:62997 [2] NCCL INFO Init timings: rank 126 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216746:219510 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
27: nid006566:216746:219510 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
18: nid006556:210774:213632 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210774:213632 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205769:208641 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205769:208641 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201791:204556 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201791:204556 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221583:224384 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221583:224384 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221943:224720 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221943:224720 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59767:63010 [0] NCCL INFO Channel 01/0 : 124[0] -> 125[1] via P2P/CUMEM
20: nid006558:215470:218259 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215470:218259 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211338:214084 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211338:214084 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218413:221168 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218413:221168 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221140:223917 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221140:223917 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221142:223915 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
24: nid006563:221142:223915 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249082:251866 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249082:251866 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249080:251867 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249080:251867 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223925:226694 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223925:226694 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 8: nid006503:218412:221169 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218412:221169 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226767:229506 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226767:229506 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201790:204559 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201790:204559 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205768:208642 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205768:208642 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201789:204557 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201789:204557 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222271:225055 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222271:225055 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222273:225054 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
22: nid006560:222273:225054 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221942:224719 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221942:224719 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208418:211185 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208418:211185 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20583:23369 [3] NCCL INFO ncclCommSplit comm 0x4006cc0d5940 rank 123 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaee514e10 color 315732477 key 123 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20583:23369 [3] NCCL INFO Init timings: rank 123 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221582:224382 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221582:224382 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221581:224383 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
16: nid006554:221581:224383 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20581:23371 [1] NCCL INFO ncclCommSplit comm 0x4006c559d360 rank 121 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab01d20530 color 315732477 key 121 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20582:23370 [2] NCCL INFO ncclCommSplit comm 0x4006c959d300 rank 122 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab18742680 color 315732477 key 122 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20580:23372 [0] NCCL INFO ncclCommSplit comm 0x40063d59d340 rank 120 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab05f55b40 color 315732477 key 120 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20581:23371 [1] NCCL INFO Init timings: rank 121 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72170:75236 [1] NCCL INFO ncclCommSplit comm 0x4006c559d300 rank 113 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae3cff410 color 315732477 key 113 commId 0xd4eed6c1342b1de7 - Init COMPLETE
28: nid007251:72172:75238 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d550 rank 115 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf6d30140 color 315732477 key 115 commId 0xd4eed6c1342b1de7 - Init COMPLETE
28: nid007251:72170:75236 [1] NCCL INFO Init timings: rank 113 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215469:218260 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
20: nid006558:215469:218260 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72172:75238 [3] NCCL INFO Init timings: rank 115 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254557:257316 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254557:257316 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
13: nid006509:201788:204558 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
13: nid006509:201788:204558 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260084:262849 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260084:262849 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
12: nid006508:205766:208643 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205766:208643 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27222:29915 [0] NCCL INFO ncclCommSplit comm 0x40063d59d200 rank 116 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae8e75560 color 315732477 key 116 commId 0xd4eed6c1342b1de7 - Init COMPLETE
29: nid007305:27222:29915 [0] NCCL INFO Init timings: rank 116 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
30: nid007318:20582:23370 [2] NCCL INFO Init timings: rank 122 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210776:213633 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
18: nid006556:210776:213633 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72169:75235 [0] NCCL INFO ncclCommSplit comm 0x4006c159d260 rank 112 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab09955090 color 315732477 key 112 commId 0xd4eed6c1342b1de7 - Init COMPLETE
28: nid007251:72171:75237 [2] NCCL INFO ncclCommSplit comm 0x4006a959d610 rank 114 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab01150380 color 315732477 key 114 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20580:23372 [0] NCCL INFO Init timings: rank 120 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249083:251865 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249083:251865 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223024:225759 [3] NCCL INFO ncclCommSplit comm 0x4006c959d490 rank 103 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafc951420 color 315732477 key 103 commId 0xd4eed6c1342b1de7 - Init COMPLETE
25: nid006564:223024:225759 [3] NCCL INFO Init timings: rank 103 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72169:75235 [0] NCCL INFO Init timings: rank 112 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223022:225758 [1] NCCL INFO ncclCommSplit comm 0x4006c40d5be0 rank 101 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadab71590 color 315732477 key 101 commId 0xd4eed6c1342b1de7 - Init COMPLETE
28: nid007251:72171:75237 [2] NCCL INFO Init timings: rank 114 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263728:266492 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263728:266492 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252587:255364 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252587:255364 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223022:225758 [1] NCCL INFO Init timings: rank 101 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27224:29916 [2] NCCL INFO ncclCommSplit comm 0x4006e959d310 rank 118 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab02ed3070 color 315732477 key 118 commId 0xd4eed6c1342b1de7 - Init COMPLETE
29: nid007305:27224:29916 [2] NCCL INFO Init timings: rank 118 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223023:225756 [2] NCCL INFO ncclCommSplit comm 0x4006e959d430 rank 102 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab04f72cc0 color 315732477 key 102 commId 0xd4eed6c1342b1de7 - Init COMPLETE
25: nid006564:223023:225756 [2] NCCL INFO Init timings: rank 102 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27225:29913 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d660 rank 119 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafdcaec70 color 315732477 key 119 commId 0xd4eed6c1342b1de7 - Init COMPLETE
29: nid007305:27225:29913 [3] NCCL INFO Init timings: rank 119 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27223:29914 [1] NCCL INFO ncclCommSplit comm 0x4006e559d440 rank 117 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadd180740 color 315732477 key 117 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222466:225266 [1] NCCL INFO ncclCommSplit comm 0x4006a40d5be0 rank 105 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadd720280 color 315732477 key 105 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222468:225265 [3] NCCL INFO ncclCommSplit comm 0x4006b159d550 rank 107 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaacd640a00 color 315732477 key 107 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211124:213873 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211124:213873 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27223:29914 [1] NCCL INFO Init timings: rank 117 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
25: nid006564:223021:225757 [0] NCCL INFO ncclCommSplit comm 0x40061d59d2c0 rank 100 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf17f5630 color 315732477 key 100 commId 0xd4eed6c1342b1de7 - Init COMPLETE
25: nid006564:223021:225757 [0] NCCL INFO Init timings: rank 100 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208417:211182 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
19: nid006557:208417:211182 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222466:225266 [1] NCCL INFO Init timings: rank 105 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222465:225268 [0] NCCL INFO ncclCommSplit comm 0x4006c159d4d0 rank 104 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf61a4fc0 color 315732477 key 104 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211125:213871 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211125:213871 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252584:255362 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 7: nid006502:252584:255362 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229444:232220 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229444:232220 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211123:213872 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
21: nid006559:211123:213872 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
15: nid006553:223923:226697 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223923:226697 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
11: nid006507:211337:214086 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211337:214086 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222468:225265 [3] NCCL INFO Init timings: rank 107 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223926:226696 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
15: nid006553:223926:226696 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
28: nid007251:72169:75267 [0] NCCL INFO Channel 00/0 : 112[0] -> 113[1] via P2P/CUMEM
26: nid006565:222467:225267 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d4c0 rank 106 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1e0113c0 color 315732477 key 106 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222465:225268 [0] NCCL INFO Init timings: rank 104 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229443:232223 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229443:232223 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
26: nid006565:222467:225267 [2] NCCL INFO Init timings: rank 106 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263727:266491 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263727:266491 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206596:209407 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
17: nid006555:206593:209410 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205767:208640 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
12: nid006508:205767:208640 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206596:209407 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206593:209410 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
29: nid007305:27222:29926 [0] NCCL INFO Channel 01/0 : 116[0] -> 117[1] via P2P/CUMEM
31: nid007342:59767:63010 [0] NCCL INFO Channel 05/0 : 124[0] -> 125[1] via P2P/CUMEM
27: nid006566:216749:219512 [3] NCCL INFO ncclCommSplit comm 0x4006f159d530 rank 111 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf3be1d00 color 315732477 key 111 commId 0xd4eed6c1342b1de7 - Init COMPLETE
30: nid007318:20580:23385 [0] NCCL INFO Channel 00/0 : 120[0] -> 121[1] via P2P/CUMEM
23: nid006561:220714:223485 [3] NCCL INFO ncclCommSplit comm 0x4006e959d520 rank 95 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf7d8fec0 color 315732477 key 95 commId 0xd4eed6c1342b1de7 - Init COMPLETE
23: nid006561:220714:223485 [3] NCCL INFO Init timings: rank 95 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220712:223487 [1] NCCL INFO ncclCommSplit comm 0x4006e559d610 rank 93 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafae71f80 color 315732477 key 93 commId 0xd4eed6c1342b1de7 - Init COMPLETE
27: nid006566:216748:219511 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d2d0 rank 110 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab0ae01a50 color 315732477 key 110 commId 0xd4eed6c1342b1de7 - Init COMPLETE
27: nid006566:216749:219512 [3] NCCL INFO Init timings: rank 111 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263730:266490 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
10: nid006506:263730:266490 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242587:245388 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242587:245388 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220712:223487 [1] NCCL INFO Init timings: rank 93 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216747:219509 [1] NCCL INFO ncclCommSplit comm 0x4006a559d640 rank 109 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab06140ab0 color 315732477 key 109 commId 0xd4eed6c1342b1de7 - Init COMPLETE
27: nid006566:216748:219511 [2] NCCL INFO Init timings: rank 110 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216747:219509 [1] NCCL INFO Init timings: rank 109 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216746:219510 [0] NCCL INFO ncclCommSplit comm 0x40065d59d1d0 rank 108 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab08db5770 color 315732477 key 108 commId 0xd4eed6c1342b1de7 - Init COMPLETE
25: nid006564:223021:225771 [0] NCCL INFO Channel 01/0 : 100[0] -> 101[1] via P2P/CUMEM
23: nid006561:220713:223486 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d580 rank 94 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab19e1fc40 color 315732477 key 94 commId 0xd4eed6c1342b1de7 - Init COMPLETE
23: nid006561:220713:223486 [2] NCCL INFO Init timings: rank 94 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
27: nid006566:216746:219510 [0] NCCL INFO Init timings: rank 108 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260086:262847 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260086:262847 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 6: nid006501:221941:224722 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 6: nid006501:221941:224722 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220711:223488 [0] NCCL INFO ncclCommSplit comm 0x4006e159d4a0 rank 92 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab1f303c10 color 315732477 key 92 commId 0xd4eed6c1342b1de7 - Init COMPLETE
23: nid006561:220711:223488 [0] NCCL INFO Init timings: rank 92 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222465:225281 [0] NCCL INFO Channel 00/0 : 104[0] -> 105[1] via P2P/CUMEM
 5: nid006500:260083:262846 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260083:262846 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 1: nid006496:242585:245389 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242585:245389 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229442:232222 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
14: nid006510:229442:232222 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221141:223918 [1] NCCL INFO ncclCommSplit comm 0x4006c559d3e0 rank 97 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf69b0d70 color 315732477 key 97 commId 0xd4eed6c1342b1de7 - Init COMPLETE
24: nid006563:221140:223917 [0] NCCL INFO ncclCommSplit comm 0x4006c159d480 rank 96 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafb2f59b0 color 315732477 key 96 commId 0xd4eed6c1342b1de7 - Init COMPLETE
24: nid006563:221141:223918 [1] NCCL INFO Init timings: rank 97 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:214087 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211336:214087 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
24: nid006563:221142:223915 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d540 rank 98 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab146907e0 color 315732477 key 98 commId 0xd4eed6c1342b1de7 - Init COMPLETE
24: nid006563:221140:223917 [0] NCCL INFO Init timings: rank 96 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221143:223916 [3] NCCL INFO ncclCommSplit comm 0x4006d159d690 rank 99 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab126b1510 color 315732477 key 99 commId 0xd4eed6c1342b1de7 - Init COMPLETE
24: nid006563:221142:223915 [2] NCCL INFO Init timings: rank 98 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211335:214085 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
11: nid006507:211335:214085 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
27: nid006566:216746:219525 [0] NCCL INFO Channel 01/0 : 108[0] -> 109[1] via P2P/CUMEM
24: nid006563:221143:223916 [3] NCCL INFO Init timings: rank 99 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 8: nid006503:218410:221166 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218410:221166 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 4: nid006499:254558:257318 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254558:257318 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 5: nid006500:260085:262848 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 5: nid006500:260085:262848 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226768:229503 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226768:229503 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
23: nid006561:220711:223500 [0] NCCL INFO Channel 01/0 : 92[0] -> 93[1] via P2P/CUMEM
20: nid006558:215468:218258 [1] NCCL INFO ncclCommSplit comm 0x4006c559d590 rank 81 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae4530560 color 315732477 key 81 commId 0xd4eed6c1342b1de7 - Init COMPLETE
20: nid006558:215470:218259 [3] NCCL INFO ncclCommSplit comm 0x4006c959d3c0 rank 83 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab04942a10 color 315732477 key 83 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 2: nid006497:227578:230336 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227578:230336 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
20: nid006558:215468:218258 [1] NCCL INFO Init timings: rank 81 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215470:218259 [3] NCCL INFO Init timings: rank 83 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215467:218261 [0] NCCL INFO ncclCommSplit comm 0x40065d59d340 rank 80 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab024c49d0 color 315732477 key 80 commId 0xd4eed6c1342b1de7 - Init COMPLETE
20: nid006558:215469:218260 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 82 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf4921880 color 315732477 key 82 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210775:213634 [1] NCCL INFO ncclCommSplit comm 0x4006c559d570 rank 73 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1c4215c0 color 315732477 key 73 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210775:213634 [1] NCCL INFO Init timings: rank 73 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210777:213635 [3] NCCL INFO ncclCommSplit comm 0x4006ad59d430 rank 75 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab001d2d70 color 315732477 key 75 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210777:213635 [3] NCCL INFO Init timings: rank 75 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
28: nid007251:72169:75267 [0] NCCL INFO Channel 04/0 : 112[0] -> 113[1] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 01/0 : 125[1] -> 126[2] via P2P/CUMEM
20: nid006558:215467:218261 [0] NCCL INFO Init timings: rank 80 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221583:224384 [2] NCCL INFO ncclCommSplit comm 0x4006c959d390 rank 66 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaadd7bfd30 color 315732477 key 66 commId 0xd4eed6c1342b1de7 - Init COMPLETE
16: nid006554:221583:224384 [2] NCCL INFO Init timings: rank 66 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221584:224381 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 67 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafdce2b10 color 315732477 key 67 commId 0xd4eed6c1342b1de7 - Init COMPLETE
31: nid007342:59769:63009 [2] NCCL INFO Channel 01/0 : 126[2] -> 127[3] via P2P/CUMEM
22: nid006560:222272:225053 [1] NCCL INFO ncclCommSplit comm 0x4006e559d540 rank 89 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafbb630b0 color 315732477 key 89 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222272:225053 [1] NCCL INFO Init timings: rank 89 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215469:218260 [2] NCCL INFO Init timings: rank 82 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210774:213632 [0] NCCL INFO ncclCommSplit comm 0x40063d59d540 rank 72 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab00fe4b80 color 315732477 key 72 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210774:213632 [0] NCCL INFO Init timings: rank 72 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221582:224382 [1] NCCL INFO ncclCommSplit comm 0x4006c559d650 rank 65 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad7c30460 color 315732477 key 65 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222274:225056 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d300 rank 91 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad06e0ae0 color 315732477 key 91 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222274:225056 [3] NCCL INFO Init timings: rank 91 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210776:213633 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d2c0 rank 74 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab24c608a0 color 315732477 key 74 commId 0xd4eed6c1342b1de7 - Init COMPLETE
16: nid006554:221581:224383 [0] NCCL INFO ncclCommSplit comm 0x40063d59d330 rank 64 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae2836550 color 315732477 key 64 commId 0xd4eed6c1342b1de7 - Init COMPLETE
16: nid006554:221584:224381 [3] NCCL INFO Init timings: rank 67 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221582:224382 [1] NCCL INFO Init timings: rank 65 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221581:224383 [0] NCCL INFO Init timings: rank 64 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221140:223930 [0] NCCL INFO Channel 00/0 : 96[0] -> 97[1] via P2P/CUMEM
29: nid007305:27222:29926 [0] NCCL INFO Channel 05/0 : 116[0] -> 117[1] via P2P/CUMEM
18: nid006556:210776:213633 [2] NCCL INFO Init timings: rank 74 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254559:257317 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 4: nid006499:254559:257317 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
22: nid006560:222271:225055 [0] NCCL INFO ncclCommSplit comm 0x4006c159d4c0 rank 88 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8db5160 color 315732477 key 88 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222271:225055 [0] NCCL INFO Init timings: rank 88 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
22: nid006560:222273:225054 [2] NCCL INFO ncclCommSplit comm 0x4006e959d580 rank 90 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae7423530 color 315732477 key 90 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222273:225054 [2] NCCL INFO Init timings: rank 90 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 9: nid006505:249081:251864 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 9: nid006505:249081:251864 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 3: nid006498:226766:229504 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226766:229504 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
30: nid007318:20580:23385 [0] NCCL INFO Channel 04/0 : 120[0] -> 121[1] via P2P/CUMEM
13: nid006509:201791:204556 [3] NCCL INFO ncclCommSplit comm 0x4006d159d3b0 rank 55 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafc8b2560 color 315732477 key 55 commId 0xd4eed6c1342b1de7 - Init COMPLETE
13: nid006509:201789:204557 [1] NCCL INFO ncclCommSplit comm 0x4006e559d490 rank 53 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab23f6f620 color 315732477 key 53 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210774:213649 [0] NCCL INFO Channel 00/0 : 72[0] -> 73[1] via P2P/CUMEM
19: nid006557:208420:211184 [3] NCCL INFO ncclCommSplit comm 0x4006f159d540 rank 79 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab123500f0 color 315732477 key 79 commId 0xd4eed6c1342b1de7 - Init COMPLETE
19: nid006557:208418:211185 [1] NCCL INFO ncclCommSplit comm 0x4006e559d540 rank 77 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab2b67f850 color 315732477 key 77 commId 0xd4eed6c1342b1de7 - Init COMPLETE
19: nid006557:208420:211184 [3] NCCL INFO Init timings: rank 79 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208418:211185 [1] NCCL INFO Init timings: rank 77 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252587:255364 [3] NCCL INFO ncclCommSplit comm 0x4006b159d3a0 rank 31 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab12a7f380 color 315732477 key 31 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252587:255364 [3] NCCL INFO Init timings: rank 31 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208419:211183 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d3a0 rank 78 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab15322000 color 315732477 key 78 commId 0xd4eed6c1342b1de7 - Init COMPLETE
19: nid006557:208419:211183 [2] NCCL INFO Init timings: rank 78 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201790:204559 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 54 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaac07c29c0 color 315732477 key 54 commId 0xd4eed6c1342b1de7 - Init COMPLETE
13: nid006509:201788:204558 [0] NCCL INFO ncclCommSplit comm 0x40065d59d290 rank 52 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab17976000 color 315732477 key 52 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252585:255363 [1] NCCL INFO ncclCommSplit comm 0x4006c559d280 rank 29 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab03122220 color 315732477 key 29 commId 0xd4eed6c1342b1de7 - Init COMPLETE
13: nid006509:201791:204556 [3] NCCL INFO Init timings: rank 55 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201789:204557 [1] NCCL INFO Init timings: rank 53 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201790:204559 [2] NCCL INFO Init timings: rank 54 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201788:204558 [0] NCCL INFO Init timings: rank 52 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 7: nid006502:252585:255363 [1] NCCL INFO Init timings: rank 29 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 8: nid006503:218411:221167 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 8: nid006503:218411:221167 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
19: nid006557:208417:211182 [0] NCCL INFO ncclCommSplit comm 0x4006a159d2c0 rank 76 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafeab5cd0 color 315732477 key 76 commId 0xd4eed6c1342b1de7 - Init COMPLETE
20: nid006558:215467:218273 [0] NCCL INFO Channel 00/0 : 80[0] -> 81[1] via P2P/CUMEM
 0: nid006495:241022:244120 [3] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241022:244120 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
25: nid006564:223021:225771 [0] NCCL INFO Channel 05/0 : 100[0] -> 101[1] via P2P/CUMEM
19: nid006557:208417:211182 [0] NCCL INFO Init timings: rank 76 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205766:208643 [0] NCCL INFO ncclCommSplit comm 0x40061d59d500 rank 48 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab01b649c0 color 315732477 key 48 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 0: nid006495:241021:244119 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241021:244119 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221581:224396 [0] NCCL INFO Channel 00/0 : 64[0] -> 65[1] via P2P/CUMEM
15: nid006553:223925:226694 [2] NCCL INFO ncclCommSplit comm 0x4006e559d570 rank 62 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1b0b02b0 color 315732477 key 62 commId 0xd4eed6c1342b1de7 - Init COMPLETE
15: nid006553:223925:226694 [2] NCCL INFO Init timings: rank 62 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221944:224721 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 27 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab07c52630 color 315732477 key 27 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 6: nid006501:221944:224721 [3] NCCL INFO Init timings: rank 27 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 2: nid006497:227576:230337 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227576:230337 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252586:255365 [2] NCCL INFO ncclCommSplit comm 0x4006c959d340 rank 30 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab16820860 color 315732477 key 30 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252586:255365 [2] NCCL INFO Init timings: rank 30 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59768:63008 [1] NCCL INFO Channel 05/0 : 125[1] -> 126[2] via P2P/CUMEM
17: nid006555:206596:209407 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d4a0 rank 71 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafe0d1bf0 color 315732477 key 71 commId 0xd4eed6c1342b1de7 - Init COMPLETE
17: nid006555:206594:209408 [1] NCCL INFO ncclCommSplit comm 0x4006e559d320 rank 69 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0a7e34b0 color 315732477 key 69 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252584:255362 [0] NCCL INFO ncclCommSplit comm 0x40065d59d560 rank 28 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafb714860 color 315732477 key 28 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222465:225281 [0] NCCL INFO Channel 04/0 : 104[0] -> 105[1] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 05/0 : 126[2] -> 127[3] via P2P/CUMEM
17: nid006555:206596:209407 [3] NCCL INFO Init timings: rank 71 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206594:209408 [1] NCCL INFO Init timings: rank 69 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241020:244121 [1] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241020:244121 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 7: nid006502:252584:255362 [0] NCCL INFO Init timings: rank 28 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211126:213874 [3] NCCL INFO ncclCommSplit comm 0x4006a959d420 rank 87 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf4030670 color 315732477 key 87 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211124:213873 [1] NCCL INFO ncclCommSplit comm 0x4006e559d290 rank 85 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae87e0f50 color 315732477 key 85 commId 0xd4eed6c1342b1de7 - Init COMPLETE
22: nid006560:222271:225068 [0] NCCL INFO Channel 00/0 : 88[0] -> 89[1] via P2P/CUMEM
15: nid006553:223923:226697 [0] NCCL INFO ncclCommSplit comm 0x40065d59d3b0 rank 60 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab008f4e30 color 315732477 key 60 commId 0xd4eed6c1342b1de7 - Init COMPLETE
12: nid006508:205766:208643 [0] NCCL INFO Init timings: rank 48 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221942:224719 [1] NCCL INFO ncclCommSplit comm 0x4006e559d220 rank 25 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf2b5ecd0 color 315732477 key 25 commId 0xd4eed6c1342b1de7 - Init COMPLETE
17: nid006555:206595:209409 [2] NCCL INFO ncclCommSplit comm 0x4006d559d410 rank 70 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf5a54290 color 315732477 key 70 commId 0xd4eed6c1342b1de7 - Init COMPLETE
17: nid006555:206595:209409 [2] NCCL INFO Init timings: rank 70 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226765:229505 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 3: nid006498:226765:229505 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
21: nid006559:211126:213874 [3] NCCL INFO Init timings: rank 87 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223923:226697 [0] NCCL INFO Init timings: rank 60 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205768:208642 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d5e0 rank 50 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae60828b0 color 315732477 key 50 commId 0xd4eed6c1342b1de7 - Init COMPLETE
12: nid006508:205768:208642 [2] NCCL INFO Init timings: rank 50 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205767:208640 [1] NCCL INFO ncclCommSplit comm 0x4006c559d610 rank 49 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf9abf4e0 color 315732477 key 49 commId 0xd4eed6c1342b1de7 - Init COMPLETE
12: nid006508:205769:208641 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 51 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafffd57b0 color 315732477 key 51 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 6: nid006501:221943:224720 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2d0 rank 26 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaafac13400 color 315732477 key 26 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 6: nid006501:221942:224719 [1] NCCL INFO Init timings: rank 25 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 6: nid006501:221943:224720 [2] NCCL INFO Init timings: rank 26 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206593:209410 [0] NCCL INFO ncclCommSplit comm 0x4006c159d230 rank 68 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab06a54c60 color 315732477 key 68 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222466:225278 [1] NCCL INFO Channel 00/0 : 105[1] -> 106[2] via P2P/CUMEM
21: nid006559:211124:213873 [1] NCCL INFO Init timings: rank 85 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223924:226695 [1] NCCL INFO ncclCommSplit comm 0x4006e559d560 rank 61 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae1630990 color 315732477 key 61 commId 0xd4eed6c1342b1de7 - Init COMPLETE
15: nid006553:223924:226695 [1] NCCL INFO Init timings: rank 61 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205767:208640 [1] NCCL INFO Init timings: rank 49 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
12: nid006508:205769:208641 [3] NCCL INFO Init timings: rank 51 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
13: nid006509:201788:204571 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[1] via P2P/CUMEM
 6: nid006501:221941:224722 [0] NCCL INFO ncclCommSplit comm 0x40065d59d2e0 rank 24 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab18cffb10 color 315732477 key 24 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 6: nid006501:221941:224722 [0] NCCL INFO Init timings: rank 24 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206593:209410 [0] NCCL INFO Init timings: rank 68 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211123:213872 [0] NCCL INFO ncclCommSplit comm 0x40065d59d270 rank 84 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacd464770 color 315732477 key 84 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211123:213872 [0] NCCL INFO Init timings: rank 84 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223926:226696 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d340 rank 63 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab186224e0 color 315732477 key 63 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211125:213871 [2] NCCL INFO ncclCommSplit comm 0x4006e959d1e0 rank 86 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaad6db0e40 color 315732477 key 86 commId 0xd4eed6c1342b1de7 - Init COMPLETE
21: nid006559:211125:213871 [2] NCCL INFO Init timings: rank 86 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223926:226696 [3] NCCL INFO Init timings: rank 63 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
19: nid006557:208417:211197 [0] NCCL INFO Channel 01/0 : 76[0] -> 77[1] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 00/0 : 121[1] -> 122[2] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 05/0 : 92[0] -> 93[1] via P2P/CUMEM
27: nid006566:216746:219525 [0] NCCL INFO Channel 05/0 : 108[0] -> 109[1] via P2P/CUMEM
14: nid006510:229445:232221 [3] NCCL INFO ncclCommSplit comm 0x4006ad59d550 rank 59 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad4ac1400 color 315732477 key 59 commId 0xd4eed6c1342b1de7 - Init COMPLETE
14: nid006510:229443:232223 [1] NCCL INFO ncclCommSplit comm 0x4006c559d450 rank 57 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab02930260 color 315732477 key 57 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252584:255377 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[1] via P2P/CUMEM
14: nid006510:229445:232221 [3] NCCL INFO Init timings: rank 59 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.29, topo 0.34, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263729:266489 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d320 rank 42 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab0c9e0740 color 315732477 key 42 commId 0xd4eed6c1342b1de7 - Init COMPLETE
12: nid006508:205766:208660 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[1] via P2P/CUMEM
14: nid006510:229444:232220 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d3e0 rank 58 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab050af830 color 315732477 key 58 commId 0xd4eed6c1342b1de7 - Init COMPLETE
10: nid006506:263728:266492 [1] NCCL INFO ncclCommSplit comm 0x4006e559d2f0 rank 41 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab066b0b90 color 315732477 key 41 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 6: nid006501:221941:224736 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/CUMEM
 1: nid006496:242584:245387 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242584:245387 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
14: nid006510:229443:232223 [1] NCCL INFO Init timings: rank 57 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.29, topo 0.34, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229444:232220 [2] NCCL INFO Init timings: rank 58 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.29, topo 0.34, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263729:266489 [2] NCCL INFO Init timings: rank 42 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263728:266492 [1] NCCL INFO Init timings: rank 41 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
10: nid006506:263727:266491 [0] NCCL INFO ncclCommSplit comm 0x4006e159d2a0 rank 40 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacce51d30 color 315732477 key 40 commId 0xd4eed6c1342b1de7 - Init COMPLETE
14: nid006510:229442:232222 [0] NCCL INFO ncclCommSplit comm 0x40065d59d400 rank 56 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadd8b3cb0 color 315732477 key 56 commId 0xd4eed6c1342b1de7 - Init COMPLETE
14: nid006510:229442:232222 [0] NCCL INFO Init timings: rank 56 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.29, topo 0.34, graphs 0.02, connections 0.01, rest 0.00)
21: nid006559:211123:213886 [0] NCCL INFO Channel 01/0 : 84[0] -> 85[1] via P2P/CUMEM
10: nid006506:263730:266490 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d520 rank 43 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaadbd63f10 color 315732477 key 43 commId 0xd4eed6c1342b1de7 - Init COMPLETE
15: nid006553:223923:226710 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[1] via P2P/CUMEM
10: nid006506:263730:266490 [3] NCCL INFO Init timings: rank 43 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27224:29927 [2] NCCL INFO Channel 01/0 : 118[2] -> 119[3] via P2P/CUMEM
10: nid006506:263727:266491 [0] NCCL INFO Init timings: rank 40 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.30, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241019:244118 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 0: nid006495:241019:244118 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
31: nid007342:59767:63010 [0] NCCL INFO Channel 02/0 : 124[0] -> 127[3] via P2P/CUMEM
 2: nid006497:227577:230334 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227577:230334 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
17: nid006555:206593:209422 [0] NCCL INFO Channel 01/0 : 68[0] -> 69[1] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 01/0 : 101[1] -> 102[2] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 00/0 : 113[1] -> 114[2] via P2P/CUMEM
30: nid007318:20582:23383 [2] NCCL INFO Channel 00/0 : 122[2] -> 123[3] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 01/0 : 102[2] -> 103[3] via P2P/CUMEM
24: nid006563:221140:223930 [0] NCCL INFO Channel 04/0 : 96[0] -> 97[1] via P2P/CUMEM
28: nid007251:72171:75269 [2] NCCL INFO Channel 00/0 : 114[2] -> 115[3] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 01/0 : 117[1] -> 118[2] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 04/0 : 121[1] -> 122[2] via P2P/CUMEM
11: nid006507:211337:214086 [2] NCCL INFO ncclCommSplit comm 0x4006e959d290 rank 46 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1716ee10 color 315732477 key 46 commId 0xd4eed6c1342b1de7 - Init COMPLETE
11: nid006507:211335:214085 [0] NCCL INFO ncclCommSplit comm 0x40065d59d500 rank 44 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadbe34c90 color 315732477 key 44 commId 0xd4eed6c1342b1de7 - Init COMPLETE
11: nid006507:211337:214086 [2] NCCL INFO Init timings: rank 46 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222467:225279 [2] NCCL INFO Channel 00/0 : 106[2] -> 107[3] via P2P/CUMEM
 4: nid006499:254558:257318 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d070 rank 18 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab14912c30 color 315732477 key 18 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 4: nid006499:254558:257318 [2] NCCL INFO Init timings: rank 18 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211335:214085 [0] NCCL INFO Init timings: rank 44 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254556:257319 [0] NCCL INFO ncclCommSplit comm 0x40065d59d1d0 rank 16 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafd9c3e70 color 315732477 key 16 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 4: nid006499:254556:257319 [0] NCCL INFO Init timings: rank 16 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254557:257316 [1] NCCL INFO ncclCommSplit comm 0x4006e559d3c0 rank 17 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf4721fc0 color 315732477 key 17 commId 0xd4eed6c1342b1de7 - Init COMPLETE
11: nid006507:211338:214084 [3] NCCL INFO ncclCommSplit comm 0x4006f559d200 rank 47 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab1f201250 color 315732477 key 47 commId 0xd4eed6c1342b1de7 - Init COMPLETE
11: nid006507:211338:214084 [3] NCCL INFO Init timings: rank 47 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
29: nid007305:27224:29927 [2] NCCL INFO Channel 05/0 : 118[2] -> 119[3] via P2P/CUMEM
 9: nid006505:249080:251867 [0] NCCL INFO ncclCommSplit comm 0x4006e159d370 rank 36 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac2b051b0 color 315732477 key 36 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 9: nid006505:249080:251867 [0] NCCL INFO Init timings: rank 36 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
14: nid006510:229442:232235 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[1] via P2P/CUMEM
 1: nid006496:242586:245386 [2] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 1: nid006496:242586:245386 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
 9: nid006505:249082:251866 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2a0 rank 38 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaddaa10c0 color 315732477 key 38 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 9: nid006505:249082:251866 [2] NCCL INFO Init timings: rank 38 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:214087 [1] NCCL INFO ncclCommSplit comm 0x4006c559d480 rank 45 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0a883290 color 315732477 key 45 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 4: nid006499:254559:257317 [3] NCCL INFO ncclCommSplit comm 0x4006d159d380 rank 19 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaffcbe690 color 315732477 key 19 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 4: nid006499:254557:257316 [1] NCCL INFO Init timings: rank 17 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 4: nid006499:254559:257317 [3] NCCL INFO Init timings: rank 19 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
18: nid006556:210774:213649 [0] NCCL INFO Channel 04/0 : 72[0] -> 73[1] via P2P/CUMEM
 5: nid006500:260086:262847 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d250 rank 23 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf70716c0 color 315732477 key 23 commId 0xd4eed6c1342b1de7 - Init COMPLETE
31: nid007342:59767:63010 [0] NCCL INFO Channel 03/0 : 124[0] -> 127[3] via P2P/CUMEM
 9: nid006505:249083:251865 [3] NCCL INFO ncclCommSplit comm 0x4006b0abf8a0 rank 39 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab25b11310 color 315732477 key 39 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 9: nid006505:249083:251865 [3] NCCL INFO Init timings: rank 39 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220712:223498 [1] NCCL INFO Channel 01/0 : 93[1] -> 94[2] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 04/0 : 113[1] -> 114[2] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 04/0 : 105[1] -> 106[2] via P2P/CUMEM
 9: nid006505:249081:251864 [1] NCCL INFO ncclCommSplit comm 0x4006a559d100 rank 37 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0bfe2b70 color 315732477 key 37 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 9: nid006505:249081:251864 [1] NCCL INFO Init timings: rank 37 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
11: nid006507:211336:214087 [1] NCCL INFO Init timings: rank 45 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241019:244118 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
16: nid006554:221581:224396 [0] NCCL INFO Channel 04/0 : 64[0] -> 65[1] via P2P/CUMEM
 5: nid006500:260086:262847 [3] NCCL INFO Init timings: rank 23 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215467:218273 [0] NCCL INFO Channel 04/0 : 80[0] -> 81[1] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 05/0 : 101[1] -> 102[2] via P2P/CUMEM
 5: nid006500:260084:262849 [1] NCCL INFO ncclCommSplit comm 0x4006c559d540 rank 21 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaebd115d0 color 315732477 key 21 commId 0xd4eed6c1342b1de7 - Init COMPLETE
28: nid007251:72171:75269 [2] NCCL INFO Channel 04/0 : 114[2] -> 115[3] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 05/0 : 117[1] -> 118[2] via P2P/CUMEM
30: nid007318:20582:23383 [2] NCCL INFO Channel 04/0 : 122[2] -> 123[3] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 00/0 : 81[1] -> 82[2] via P2P/CUMEM
 8: nid006503:218410:221166 [0] NCCL INFO ncclCommSplit comm 0x4006c159d410 rank 32 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8554fd0 color 315732477 key 32 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 8: nid006503:218412:221169 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d300 rank 34 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaadb064120 color 315732477 key 34 commId 0xd4eed6c1342b1de7 - Init COMPLETE
25: nid006564:223023:225770 [2] NCCL INFO Channel 05/0 : 102[2] -> 103[3] via P2P/CUMEM
 5: nid006500:260085:262848 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 22 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab1f1aeb40 color 315732477 key 22 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 5: nid006500:260084:262849 [1] NCCL INFO Init timings: rank 21 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260085:262848 [2] NCCL INFO Init timings: rank 22 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
23: nid006561:220713:223499 [2] NCCL INFO Channel 01/0 : 94[2] -> 95[3] via P2P/CUMEM
 8: nid006503:218410:221166 [0] NCCL INFO Init timings: rank 32 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 8: nid006503:218412:221169 [2] NCCL INFO Init timings: rank 34 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260083:262846 [0] NCCL INFO ncclCommSplit comm 0x40063d59d550 rank 20 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaddd64a90 color 315732477 key 20 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222467:225279 [2] NCCL INFO Channel 04/0 : 106[2] -> 107[3] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 01/0 : 109[1] -> 110[2] via P2P/CUMEM
 4: nid006499:254556:257329 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/CUMEM
10: nid006506:263727:266505 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[1] via P2P/CUMEM
13: nid006509:201788:204571 [0] NCCL INFO Channel 05/0 : 52[0] -> 53[1] via P2P/CUMEM
 8: nid006503:218413:221168 [3] NCCL INFO ncclCommSplit comm 0x4006c959d440 rank 35 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad50f1540 color 315732477 key 35 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 8: nid006503:218411:221167 [1] NCCL INFO ncclCommSplit comm 0x4006a559d440 rank 33 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab0c010470 color 315732477 key 33 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 8: nid006503:218413:221168 [3] NCCL INFO Init timings: rank 35 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226767:229506 [2] NCCL INFO ncclCommSplit comm 0x4006e959d2c0 rank 14 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf026f8b0 color 315732477 key 14 commId 0xd4eed6c1342b1de7 - Init COMPLETE
27: nid006566:216748:219524 [2] NCCL INFO Channel 01/0 : 110[2] -> 111[3] via P2P/CUMEM
 8: nid006503:218411:221167 [1] NCCL INFO Init timings: rank 33 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 5: nid006500:260083:262846 [0] NCCL INFO Init timings: rank 20 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226765:229505 [0] NCCL INFO ncclCommSplit comm 0x40063d59d400 rank 12 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf9754bd0 color 315732477 key 12 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 3: nid006498:226767:229506 [2] NCCL INFO Init timings: rank 14 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
31: nid007342:59767:63010 [0] NCCL INFO Channel 06/0 : 124[0] -> 127[3] via P2P/CUMEM
 3: nid006498:226765:229505 [0] NCCL INFO Init timings: rank 12 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
30: nid007318:20580:23385 [0] NCCL INFO Channel 02/0 : 120[0] -> 123[3] via P2P/CUMEM
11: nid006507:211335:214099 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[1] via P2P/CUMEM
 3: nid006498:226768:229503 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d320 rank 15 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaae06b1570 color 315732477 key 15 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 3: nid006498:226766:229504 [1] NCCL INFO ncclCommSplit comm 0x4006c559d3a0 rank 13 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafc6c1d20 color 315732477 key 13 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 3: nid006498:226768:229503 [3] NCCL INFO Init timings: rank 15 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 3: nid006498:226766:229504 [1] NCCL INFO Init timings: rank 13 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.32, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222465:225281 [0] NCCL INFO Channel 02/0 : 104[0] -> 107[3] via P2P/CUMEM
 9: nid006505:249080:251877 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[1] via P2P/CUMEM
19: nid006557:208417:211197 [0] NCCL INFO Channel 05/0 : 76[0] -> 77[1] via P2P/CUMEM
 2: nid006497:227575:230335 [0] NCCL INFO threadThresholds 8/8/64 | 1024/8/64 | 512 | 512
 2: nid006497:227575:230335 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
16: nid006554:221583:224394 [2] NCCL INFO Channel 00/0 : 66[2] -> 67[3] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 00/0 : 89[1] -> 90[2] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 05/0 : 93[1] -> 94[2] via P2P/CUMEM
28: nid007251:72169:75267 [0] NCCL INFO Channel 02/0 : 112[0] -> 115[3] via P2P/CUMEM
 7: nid006502:252584:255377 [0] NCCL INFO Channel 05/0 : 28[0] -> 29[1] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 00/0 : 97[1] -> 98[2] via P2P/CUMEM
29: nid007305:27222:29926 [0] NCCL INFO Channel 02/0 : 116[0] -> 119[3] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 05/0 : 94[2] -> 95[3] via P2P/CUMEM
30: nid007318:20580:23385 [0] NCCL INFO Channel 03/0 : 120[0] -> 123[3] via P2P/CUMEM
12: nid006508:205766:208660 [0] NCCL INFO Channel 04/0 : 48[0] -> 49[1] via P2P/CUMEM
 8: nid006503:218410:221180 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[1] via P2P/CUMEM
 6: nid006501:221941:224736 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 00/0 : 98[2] -> 99[3] via P2P/CUMEM
26: nid006565:222465:225281 [0] NCCL INFO Channel 03/0 : 104[0] -> 107[3] via P2P/CUMEM
21: nid006559:211123:213886 [0] NCCL INFO Channel 05/0 : 84[0] -> 85[1] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 05/0 : 109[1] -> 110[2] via P2P/CUMEM
25: nid006564:223021:225771 [0] NCCL INFO Channel 02/0 : 100[0] -> 103[3] via P2P/CUMEM
 5: nid006500:260083:262861 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[1] via P2P/CUMEM
 3: nid006498:226765:229515 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
27: nid006566:216748:219524 [2] NCCL INFO Channel 05/0 : 110[2] -> 111[3] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 02/0 : 121[1] -> 125[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72169:75267 [0] NCCL INFO Channel 03/0 : 112[0] -> 115[3] via P2P/CUMEM
31: nid007342:59767:63010 [0] NCCL INFO Channel 07/0 : 124[0] -> 127[3] via P2P/CUMEM
30: nid007318:20580:23385 [0] NCCL INFO Channel 06/0 : 120[0] -> 123[3] via P2P/CUMEM
17: nid006555:206593:209422 [0] NCCL INFO Channel 05/0 : 68[0] -> 69[1] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 06/0 : 121[1] -> 125[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210775:213647 [1] NCCL INFO Channel 00/0 : 73[1] -> 74[2] via P2P/CUMEM
26: nid006565:222465:225281 [0] NCCL INFO Channel 06/0 : 104[0] -> 107[3] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 03/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218271 [2] NCCL INFO Channel 00/0 : 82[2] -> 83[3] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 03/0 : 125[1] -> 1[1] [send] via NET/AWS Libfabric/1
18: nid006556:210776:213650 [2] NCCL INFO Channel 00/0 : 74[2] -> 75[3] via P2P/CUMEM
16: nid006554:221582:224397 [1] NCCL INFO Channel 00/0 : 65[1] -> 66[2] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 07/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:224394 [2] NCCL INFO Channel 04/0 : 66[2] -> 67[3] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 04/0 : 97[1] -> 98[2] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 00/0 : 90[2] -> 91[3] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 04/0 : 89[1] -> 90[2] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 01/0 : 69[1] -> 70[2] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 04/0 : 81[1] -> 82[2] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 07/0 : 125[1] -> 1[1] [send] via NET/AWS Libfabric/1
15: nid006553:223925:226707 [2] NCCL INFO Channel 01/0 : 62[2] -> 63[3] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 00/0 : 50[2] -> 51[3] via P2P/CUMEM
25: nid006564:223021:225771 [0] NCCL INFO Channel 03/0 : 100[0] -> 103[3] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 04/0 : 98[2] -> 99[3] via P2P/CUMEM
29: nid007305:27222:29926 [0] NCCL INFO Channel 03/0 : 116[0] -> 119[3] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 02/0 : 92[0] -> 95[3] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 01/0 : 77[1] -> 78[2] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 02/0 : 126[2] -> 2[2] [send] via NET/AWS Libfabric/2
13: nid006509:201789:204572 [1] NCCL INFO Channel 01/0 : 53[1] -> 54[2] via P2P/CUMEM
14: nid006510:229442:232235 [0] NCCL INFO Channel 04/0 : 56[0] -> 57[1] via P2P/CUMEM
 0: nid006495:241021:244119 [2] NCCL INFO ncclCommSplit comm 0x4006ad59d440 rank 2 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab287e25d0 color 315732477 key 2 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 0: nid006495:241022:244120 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d340 rank 3 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf2c322a0 color 315732477 key 3 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252585:255374 [1] NCCL INFO Channel 01/0 : 29[1] -> 30[2] via P2P/CUMEM
10: nid006506:263727:266505 [0] NCCL INFO Channel 04/0 : 40[0] -> 41[1] via P2P/CUMEM
 0: nid006495:241019:244118 [0] NCCL INFO ncclCommSplit comm 0x4006b00d5c40 rank 0 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaca364de0 color 315732477 key 0 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222465:225281 [0] NCCL INFO Channel 07/0 : 104[0] -> 107[3] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 06/0 : 126[2] -> 2[2] [send] via NET/AWS Libfabric/2
30: nid007318:20580:23385 [0] NCCL INFO Channel 07/0 : 120[0] -> 123[3] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 01/0 : 54[2] -> 55[3] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/CUMEM
 0: nid006495:241021:244119 [2] NCCL INFO Init timings: rank 2 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.02, topo 0.62, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241020:244121 [1] NCCL INFO ncclCommSplit comm 0x4006e559d380 rank 1 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad1d00bf0 color 315732477 key 1 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 0: nid006495:241022:244120 [3] NCCL INFO Init timings: rank 3 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.02, topo 0.62, graphs 0.02, connections 0.01, rest 0.00)
 0: nid006495:241019:244118 [0] NCCL INFO Init timings: rank 0 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.04, allgathers 0.02, topo 0.62, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223924:226708 [1] NCCL INFO Channel 01/0 : 61[1] -> 62[2] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 04/0 : 82[2] -> 83[3] via P2P/CUMEM
 0: nid006495:241020:244121 [1] NCCL INFO Init timings: rank 1 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.02, topo 0.62, graphs 0.02, connections 0.01, rest 0.00)
26: nid006565:222466:225278 [1] NCCL INFO Channel 03/0 : 101[1] -> 105[1] [receive] via NET/AWS Libfabric/1
21: nid006559:211124:213885 [1] NCCL INFO Channel 01/0 : 85[1] -> 86[2] via P2P/CUMEM
27: nid006566:216746:219525 [0] NCCL INFO Channel 02/0 : 108[0] -> 111[3] via P2P/CUMEM
 4: nid006499:254556:257329 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 00/0 : 49[1] -> 50[2] via P2P/CUMEM
18: nid006556:210775:213647 [1] NCCL INFO Channel 04/0 : 73[1] -> 74[2] via P2P/CUMEM
25: nid006564:223021:225771 [0] NCCL INFO Channel 06/0 : 100[0] -> 103[3] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 03/0 : 92[0] -> 95[3] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 01/0 : 78[2] -> 79[3] via P2P/CUMEM
 1: nid006496:242585:245389 [1] NCCL INFO ncclCommSplit comm 0x4006c559d4f0 rank 5 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab017914b0 color 315732477 key 5 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 1: nid006496:242587:245388 [3] NCCL INFO ncclCommSplit comm 0x4006ed59d380 rank 7 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaad33e0d00 color 315732477 key 7 commId 0xd4eed6c1342b1de7 - Init COMPLETE
18: nid006556:210776:213650 [2] NCCL INFO Channel 04/0 : 74[2] -> 75[3] via P2P/CUMEM
11: nid006507:211335:214099 [0] NCCL INFO Channel 05/0 : 44[0] -> 45[1] via P2P/CUMEM
 6: nid006501:221943:224735 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/CUMEM
 1: nid006496:242585:245389 [1] NCCL INFO Init timings: rank 5 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
16: nid006554:221582:224397 [1] NCCL INFO Channel 04/0 : 65[1] -> 66[2] via P2P/CUMEM
17: nid006555:206595:209421 [2] NCCL INFO Channel 01/0 : 70[2] -> 71[3] via P2P/CUMEM
 1: nid006496:242587:245388 [3] NCCL INFO Init timings: rank 7 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221140:223930 [0] NCCL INFO Channel 02/0 : 96[0] -> 99[3] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 07/0 : 101[1] -> 105[1] [receive] via NET/AWS Libfabric/1
21: nid006559:211125:213887 [2] NCCL INFO Channel 01/0 : 86[2] -> 87[3] via P2P/CUMEM
29: nid007305:27222:29926 [0] NCCL INFO Channel 06/0 : 116[0] -> 119[3] via P2P/CUMEM
 1: nid006496:242586:245386 [2] NCCL INFO ncclCommSplit comm 0x4006ed59d300 rank 6 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaae9db1a00 color 315732477 key 6 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 1: nid006496:242586:245386 [2] NCCL INFO Init timings: rank 6 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
20: nid006558:215467:218273 [0] NCCL INFO Channel 02/0 : 80[0] -> 83[3] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 01/0 : 30[2] -> 31[3] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 05/0 : 77[1] -> 78[2] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 03/0 : 117[1] -> 121[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223925:226707 [2] NCCL INFO Channel 05/0 : 62[2] -> 63[3] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 04/0 : 50[2] -> 51[3] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 05/0 : 69[1] -> 70[2] via P2P/CUMEM
 1: nid006496:242584:245387 [0] NCCL INFO ncclCommSplit comm 0x4006c00d5ce0 rank 4 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa2855c0 color 315732477 key 4 commId 0xd4eed6c1342b1de7 - Init COMPLETE
26: nid006565:222466:225278 [1] NCCL INFO Channel 02/0 : 105[1] -> 109[1] [send] via NET/AWS Libfabric/1
30: nid007318:20581:23381 [1] NCCL INFO Channel 07/0 : 117[1] -> 121[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249080:251877 [0] NCCL INFO Channel 05/0 : 36[0] -> 37[1] via P2P/CUMEM
13: nid006509:201789:204572 [1] NCCL INFO Channel 05/0 : 53[1] -> 54[2] via P2P/CUMEM
 1: nid006496:242584:245387 [0] NCCL INFO Init timings: rank 4 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
24: nid006563:221140:223930 [0] NCCL INFO Channel 03/0 : 96[0] -> 99[3] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 06/0 : 105[1] -> 109[1] [send] via NET/AWS Libfabric/1
29: nid007305:27224:29927 [2] NCCL INFO Channel 03/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241019:244132 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 06/0 : 92[0] -> 95[3] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 05/0 : 78[2] -> 79[3] via P2P/CUMEM
21: nid006559:211124:213885 [1] NCCL INFO Channel 05/0 : 85[1] -> 86[2] via P2P/CUMEM
27: nid006566:216746:219525 [0] NCCL INFO Channel 03/0 : 108[0] -> 111[3] via P2P/CUMEM
 2: nid006497:227578:230336 [3] NCCL INFO ncclCommSplit comm 0x4006cd59d3c0 rank 11 nranks 128 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf3bd2ec0 color 315732477 key 11 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 2: nid006497:227576:230337 [1] NCCL INFO ncclCommSplit comm 0x4006c959d080 rank 9 nranks 128 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaade5e19c0 color 315732477 key 9 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 7: nid006502:252585:255374 [1] NCCL INFO Channel 05/0 : 29[1] -> 30[2] via P2P/CUMEM
 5: nid006500:260083:262861 [0] NCCL INFO Channel 05/0 : 20[0] -> 21[1] via P2P/CUMEM
 2: nid006497:227578:230336 [3] NCCL INFO Init timings: rank 11 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
 2: nid006497:227576:230337 [1] NCCL INFO Init timings: rank 9 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
15: nid006553:223923:226710 [0] NCCL INFO Channel 05/0 : 60[0] -> 61[1] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 04/0 : 49[1] -> 50[2] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 05/0 : 54[2] -> 55[3] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Channel 03/0 : 80[0] -> 83[3] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 05/0 : 30[2] -> 31[3] via P2P/CUMEM
 8: nid006503:218410:221180 [0] NCCL INFO Channel 04/0 : 32[0] -> 33[1] via P2P/CUMEM
 2: nid006497:227577:230334 [2] NCCL INFO ncclCommSplit comm 0x4006cd59d470 rank 10 nranks 128 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab266a1d50 color 315732477 key 10 commId 0xd4eed6c1342b1de7 - Init COMPLETE
 2: nid006497:227577:230334 [2] NCCL INFO Init timings: rank 10 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
17: nid006555:206595:209421 [2] NCCL INFO Channel 05/0 : 70[2] -> 71[3] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 02/0 : 97[1] -> 101[1] [receive] via NET/AWS Libfabric/1
26: nid006565:222467:225279 [2] NCCL INFO Channel 02/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:29926 [0] NCCL INFO Channel 07/0 : 116[0] -> 119[3] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 02/0 : 121[1] -> 125[1] [send] via NET/AWS Libfabric/1
10: nid006506:263728:266502 [1] NCCL INFO Channel 00/0 : 41[1] -> 42[2] via P2P/CUMEM
 2: nid006497:227575:230335 [0] NCCL INFO ncclCommSplit comm 0x40061d59d270 rank 8 nranks 128 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab34a73550 color 315732477 key 8 commId 0xd4eed6c1342b1de7 - Init COMPLETE
15: nid006553:223924:226708 [1] NCCL INFO Channel 05/0 : 61[1] -> 62[2] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 02/0 : 64[0] -> 67[3] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 00/0 : 58[2] -> 59[3] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 03/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:225771 [0] NCCL INFO Channel 07/0 : 100[0] -> 103[3] via P2P/CUMEM
 3: nid006498:226765:229515 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
29: nid007305:27224:29927 [2] NCCL INFO Channel 07/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23383 [2] NCCL INFO Channel 02/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:266503 [2] NCCL INFO Channel 00/0 : 42[2] -> 43[3] via P2P/CUMEM
 2: nid006497:227575:230335 [0] NCCL INFO Init timings: rank 8 nranks 128 total 0.70 (kernels 0.00, bootstrap 0.03, allgathers 0.31, topo 0.32, graphs 0.02, connections 0.02, rest 0.00)
 6: nid006501:221942:224733 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Channel 06/0 : 80[0] -> 83[3] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 00/0 : 57[1] -> 58[2] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 06/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:213887 [2] NCCL INFO Channel 05/0 : 86[2] -> 87[3] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 02/0 : 113[1] -> 117[1] [receive] via NET/AWS Libfabric/1
30: nid007318:20581:23381 [1] NCCL INFO Channel 06/0 : 121[1] -> 125[1] [send] via NET/AWS Libfabric/1
18: nid006556:210774:213649 [0] NCCL INFO Channel 02/0 : 72[0] -> 75[3] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 07/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223022:225769 [1] NCCL INFO Channel 06/0 : 97[1] -> 101[1] [receive] via NET/AWS Libfabric/1
23: nid006561:220711:223500 [0] NCCL INFO Channel 07/0 : 92[0] -> 95[3] via P2P/CUMEM
19: nid006557:208417:211197 [0] NCCL INFO Channel 02/0 : 76[0] -> 79[3] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 03/0 : 109[1] -> 113[1] [receive] via NET/AWS Libfabric/1
26: nid006565:222467:225279 [2] NCCL INFO Channel 03/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
27: nid006566:216746:219525 [0] NCCL INFO Channel 06/0 : 108[0] -> 111[3] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 00/0 : 123[3] -> 127[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27224:29927 [2] NCCL INFO Channel 02/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:23383 [2] NCCL INFO Channel 06/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224735 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 02/0 : 89[1] -> 93[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72170:75268 [1] NCCL INFO Channel 07/0 : 109[1] -> 113[1] [receive] via NET/AWS Libfabric/1
31: nid007342:59767:63010 [0] NCCL INFO Channel 01/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27223:29929 [1] NCCL INFO Channel 06/0 : 113[1] -> 117[1] [receive] via NET/AWS Libfabric/1
30: nid007318:20582:23383 [2] NCCL INFO Channel 03/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:257328 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/CUMEM
17: nid006555:206593:209422 [0] NCCL INFO Channel 02/0 : 68[0] -> 71[3] via P2P/CUMEM
24: nid006563:221140:223930 [0] NCCL INFO Channel 06/0 : 96[0] -> 99[3] via P2P/CUMEM
28: nid007251:72169:75267 [0] NCCL INFO Channel 06/0 : 112[0] -> 115[3] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 07/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
21: nid006559:211123:213886 [0] NCCL INFO Channel 02/0 : 84[0] -> 87[3] via P2P/CUMEM
29: nid007305:27224:29927 [2] NCCL INFO Channel 06/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
18: nid006556:210774:213649 [0] NCCL INFO Channel 03/0 : 72[0] -> 75[3] via P2P/CUMEM
28: nid007251:72171:75269 [2] NCCL INFO Channel 02/0 : 110[2] -> 114[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59770:63007 [3] NCCL INFO Channel 04/0 : 123[3] -> 127[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27223:29929 [1] NCCL INFO Channel 03/0 : 117[1] -> 121[1] [send] via NET/AWS Libfabric/1
30: nid007318:20582:23383 [2] NCCL INFO Channel 07/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:208660 [0] NCCL INFO Channel 02/0 : 48[0] -> 51[3] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 02/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:225769 [1] NCCL INFO Channel 03/0 : 101[1] -> 105[1] [send] via NET/AWS Libfabric/1
23: nid006561:220712:223498 [1] NCCL INFO Channel 06/0 : 89[1] -> 93[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72170:75268 [1] NCCL INFO Channel 02/0 : 113[1] -> 117[1] [send] via NET/AWS Libfabric/1
31: nid007342:59767:63010 [0] NCCL INFO Channel 05/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:230349 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
11: nid006507:211337:214097 [2] NCCL INFO Channel 01/0 : 46[2] -> 47[3] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Channel 07/0 : 80[0] -> 83[3] via P2P/CUMEM
 7: nid006502:252584:255377 [0] NCCL INFO Channel 02/0 : 28[0] -> 31[3] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 03/0 : 64[0] -> 67[3] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 03/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:75269 [2] NCCL INFO Channel 06/0 : 110[2] -> 114[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27223:29929 [1] NCCL INFO Channel 07/0 : 117[1] -> 121[1] [send] via NET/AWS Libfabric/1
 4: nid006499:254557:257331 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/CUMEM
13: nid006509:201788:204571 [0] NCCL INFO Channel 02/0 : 52[0] -> 55[3] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 04/0 : 58[2] -> 59[3] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 06/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:225769 [1] NCCL INFO Channel 07/0 : 101[1] -> 105[1] [send] via NET/AWS Libfabric/1
23: nid006561:220712:223498 [1] NCCL INFO Channel 03/0 : 93[1] -> 97[1] [send] via NET/AWS Libfabric/1
19: nid006557:208417:211197 [0] NCCL INFO Channel 03/0 : 76[0] -> 79[3] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 06/0 : 113[1] -> 117[1] [send] via NET/AWS Libfabric/1
28: nid007251:72169:75267 [0] NCCL INFO Channel 07/0 : 112[0] -> 115[3] via P2P/CUMEM
27: nid006566:216746:219525 [0] NCCL INFO Channel 07/0 : 108[0] -> 111[3] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 01/0 : 127[3] -> 3[3] [send] via NET/AWS Libfabric/3
10: nid006506:263728:266502 [1] NCCL INFO Channel 04/0 : 41[1] -> 42[2] via P2P/CUMEM
17: nid006555:206593:209422 [0] NCCL INFO Channel 03/0 : 68[0] -> 71[3] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 04/0 : 57[1] -> 58[2] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 07/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:75269 [2] NCCL INFO Channel 03/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
21: nid006559:211123:213886 [0] NCCL INFO Channel 03/0 : 84[0] -> 87[3] via P2P/CUMEM
31: nid007342:59767:63010 [0] NCCL INFO Channel 00/0 : 124[0] -> 0[0] [send] via NET/AWS Libfabric/0
10: nid006506:263729:266503 [2] NCCL INFO Channel 04/0 : 42[2] -> 43[3] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 04/0 : 90[2] -> 91[3] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 01/0 : 38[2] -> 39[3] via P2P/CUMEM
15: nid006553:223923:226710 [0] NCCL INFO Channel 02/0 : 60[0] -> 63[3] via P2P/CUMEM
18: nid006556:210774:213649 [0] NCCL INFO Channel 06/0 : 72[0] -> 75[3] via P2P/CUMEM
 7: nid006502:252584:255377 [0] NCCL INFO Channel 03/0 : 28[0] -> 31[3] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 07/0 : 93[1] -> 97[1] [send] via NET/AWS Libfabric/1
27: nid006566:216748:219524 [2] NCCL INFO Channel 03/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59770:63007 [3] NCCL INFO Channel 05/0 : 127[3] -> 3[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249081:251878 [1] NCCL INFO Channel 01/0 : 37[1] -> 38[2] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[2] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 02/0 : 94[2] -> 98[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:75269 [2] NCCL INFO Channel 07/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
27: nid006566:216747:219523 [1] NCCL INFO Channel 02/0 : 105[1] -> 109[1] [receive] via NET/AWS Libfabric/1
31: nid007342:59767:63010 [0] NCCL INFO Channel 04/0 : 124[0] -> 0[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257328 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 01/0 : 45[1] -> 46[2] via P2P/CUMEM
12: nid006508:205766:208660 [0] NCCL INFO Channel 03/0 : 48[0] -> 51[3] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 00/0 : 34[2] -> 35[3] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 06/0 : 64[0] -> 67[3] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 01/0 : 22[2] -> 23[3] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 06/0 : 94[2] -> 98[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:219524 [2] NCCL INFO Channel 07/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254557:257331 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/CUMEM
11: nid006507:211337:214097 [2] NCCL INFO Channel 05/0 : 46[2] -> 47[3] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 03/0 : 77[1] -> 81[1] [receive] via NET/AWS Libfabric/1
 8: nid006503:218411:221181 [1] NCCL INFO Channel 00/0 : 33[1] -> 34[2] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 03/0 : 93[1] -> 97[1] [receive] via NET/AWS Libfabric/1
19: nid006557:208417:211197 [0] NCCL INFO Channel 06/0 : 76[0] -> 79[3] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 06/0 : 105[1] -> 109[1] [receive] via NET/AWS Libfabric/1
30: nid007318:20582:23383 [2] NCCL INFO Channel 02/0 : 122[2] -> 120[0] via P2P/CUMEM
13: nid006509:201788:204571 [0] NCCL INFO Channel 03/0 : 52[0] -> 55[3] via P2P/CUMEM
 6: nid006501:221941:224736 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/CUMEM
27: nid006566:216748:219524 [2] NCCL INFO Channel 02/0 : 110[2] -> 114[2] [send] via NET/AWS Libfabric/2
17: nid006555:206593:209422 [0] NCCL INFO Channel 06/0 : 68[0] -> 71[3] via P2P/CUMEM
 0: nid006495:241019:244132 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
24: nid006563:221140:223930 [0] NCCL INFO Channel 07/0 : 96[0] -> 99[3] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 02/0 : 94[2] -> 98[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211123:213886 [0] NCCL INFO Channel 06/0 : 84[0] -> 87[3] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 03/0 : 109[1] -> 113[1] [send] via NET/AWS Libfabric/1
20: nid006558:215468:218270 [1] NCCL INFO Channel 07/0 : 77[1] -> 81[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210774:213649 [0] NCCL INFO Channel 07/0 : 72[0] -> 75[3] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 07/0 : 93[1] -> 97[1] [receive] via NET/AWS Libfabric/1
27: nid006566:216748:219524 [2] NCCL INFO Channel 06/0 : 110[2] -> 114[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226710 [0] NCCL INFO Channel 03/0 : 60[0] -> 63[3] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 02/0 : 81[1] -> 85[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226767:229516 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 07/0 : 109[1] -> 113[1] [send] via NET/AWS Libfabric/1
 7: nid006502:252584:255377 [0] NCCL INFO Channel 06/0 : 28[0] -> 31[3] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 05/0 : 21[1] -> 22[2] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 06/0 : 94[2] -> 98[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221141:223928 [1] NCCL INFO Channel 02/0 : 97[1] -> 101[1] [send] via NET/AWS Libfabric/1
29: nid007305:27224:29927 [2] NCCL INFO Channel 03/0 : 118[2] -> 116[0] via P2P/CUMEM
10: nid006506:263727:266505 [0] NCCL INFO Channel 02/0 : 40[0] -> 43[3] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 02/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:251876 [2] NCCL INFO Channel 05/0 : 38[2] -> 39[3] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 05/0 : 45[1] -> 46[2] via P2P/CUMEM
12: nid006508:205766:208660 [0] NCCL INFO Channel 06/0 : 48[0] -> 51[3] via P2P/CUMEM
13: nid006509:201788:204571 [0] NCCL INFO Channel 06/0 : 52[0] -> 55[3] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 04/0 : 34[2] -> 35[3] via P2P/CUMEM
16: nid006554:221583:224394 [2] NCCL INFO Channel 02/0 : 62[2] -> 66[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226766:229518 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 05/0 : 37[1] -> 38[2] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 06/0 : 81[1] -> 85[1] [send] via NET/AWS Libfabric/1
 8: nid006503:218411:221181 [1] NCCL INFO Channel 04/0 : 33[1] -> 34[2] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 07/0 : 64[0] -> 67[3] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 03/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
24: nid006563:221141:223928 [1] NCCL INFO Channel 06/0 : 97[1] -> 101[1] [send] via NET/AWS Libfabric/1
30: nid007318:20582:23383 [2] NCCL INFO Channel 06/0 : 122[2] -> 120[0] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 02/0 : 78[2] -> 82[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229442:232235 [0] NCCL INFO Channel 02/0 : 56[0] -> 59[3] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 05/0 : 22[2] -> 23[3] via P2P/CUMEM
22: nid006560:222271:225068 [0] NCCL INFO Channel 04/0 : 88[0] -> 89[1] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 06/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218271 [2] NCCL INFO Channel 06/0 : 78[2] -> 82[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210775:213647 [1] NCCL INFO Channel 03/0 : 69[1] -> 73[1] [receive] via NET/AWS Libfabric/1
24: nid006563:221142:223931 [2] NCCL INFO Channel 07/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
19: nid006557:208417:211197 [0] NCCL INFO Channel 07/0 : 76[0] -> 79[3] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 02/0 : 106[2] -> 104[0] via P2P/CUMEM
21: nid006559:211123:213886 [0] NCCL INFO Channel 07/0 : 84[0] -> 87[3] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 03/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221941:224736 [0] NCCL INFO Channel 03/0 : 24[0] -> 27[3] via P2P/CUMEM
17: nid006555:206593:209422 [0] NCCL INFO Channel 07/0 : 68[0] -> 71[3] via P2P/CUMEM
18: nid006556:210776:213650 [2] NCCL INFO Channel 02/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:224394 [2] NCCL INFO Channel 06/0 : 62[2] -> 66[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208418:211195 [1] NCCL INFO Channel 02/0 : 73[1] -> 77[1] [receive] via NET/AWS Libfabric/1
27: nid006566:216748:219524 [2] NCCL INFO Channel 03/0 : 110[2] -> 108[0] via P2P/CUMEM
 4: nid006499:254556:257329 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 03/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
18: nid006556:210775:213647 [1] NCCL INFO Channel 07/0 : 69[1] -> 73[1] [receive] via NET/AWS Libfabric/1
19: nid006557:208418:211195 [1] NCCL INFO Channel 06/0 : 73[1] -> 77[1] [receive] via NET/AWS Libfabric/1
28: nid007251:72171:75269 [2] NCCL INFO Channel 02/0 : 114[2] -> 112[0] via P2P/CUMEM
26: nid006565:222465:225281 [0] NCCL INFO Channel 00/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211124:213885 [1] NCCL INFO Channel 02/0 : 81[1] -> 85[1] [receive] via NET/AWS Libfabric/1
30: nid007318:20580:23385 [0] NCCL INFO Channel 00/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222273:225067 [2] NCCL INFO Channel 07/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226710 [0] NCCL INFO Channel 06/0 : 60[0] -> 63[3] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 02/0 : 46[2] -> 50[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206594:209419 [1] NCCL INFO Channel 02/0 : 65[1] -> 69[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210776:213650 [2] NCCL INFO Channel 06/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221582:224397 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226767:229516 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 03/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:29926 [0] NCCL INFO Channel 01/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20583:23382 [3] NCCL INFO Channel 01/0 : 119[3] -> 123[3] [receive] via NET/AWS Libfabric/3
18: nid006556:210775:213647 [1] NCCL INFO Channel 02/0 : 73[1] -> 77[1] [send] via NET/AWS Libfabric/1
 7: nid006502:252584:255377 [0] NCCL INFO Channel 07/0 : 28[0] -> 31[3] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
16: nid006554:221583:224394 [2] NCCL INFO Channel 03/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:262861 [0] NCCL INFO Channel 02/0 : 20[0] -> 23[3] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 03/0 : 77[1] -> 81[1] [send] via NET/AWS Libfabric/1
26: nid006565:222465:225281 [0] NCCL INFO Channel 04/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211124:213885 [1] NCCL INFO Channel 06/0 : 81[1] -> 85[1] [receive] via NET/AWS Libfabric/1
29: nid007305:27225:29928 [3] NCCL INFO Channel 00/0 : 115[3] -> 119[3] [receive] via NET/AWS Libfabric/3
30: nid007318:20580:23385 [0] NCCL INFO Channel 04/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:266505 [0] NCCL INFO Channel 03/0 : 40[0] -> 43[3] via P2P/CUMEM
12: nid006508:205766:208660 [0] NCCL INFO Channel 07/0 : 48[0] -> 51[3] via P2P/CUMEM
13: nid006509:201788:204571 [0] NCCL INFO Channel 07/0 : 52[0] -> 55[3] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 03/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218271 [2] NCCL INFO Channel 07/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:213650 [2] NCCL INFO Channel 03/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:224397 [1] NCCL INFO Channel 07/0 : 61[1] -> 65[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226766:229518 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 07/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:29926 [0] NCCL INFO Channel 05/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20583:23382 [3] NCCL INFO Channel 05/0 : 119[3] -> 123[3] [receive] via NET/AWS Libfabric/3
 2: nid006497:227575:230349 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 06/0 : 46[2] -> 50[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201789:204572 [1] NCCL INFO Channel 02/0 : 49[1] -> 53[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210775:213647 [1] NCCL INFO Channel 06/0 : 73[1] -> 77[1] [send] via NET/AWS Libfabric/1
 7: nid006502:252585:255374 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
16: nid006554:221583:224394 [2] NCCL INFO Channel 07/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
25: nid006564:223021:225771 [0] NCCL INFO Channel 01/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208418:211195 [1] NCCL INFO Channel 07/0 : 77[1] -> 81[1] [send] via NET/AWS Libfabric/1
26: nid006565:222468:225280 [3] NCCL INFO Channel 01/0 : 103[3] -> 107[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211124:213885 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[1] [send] via NET/AWS Libfabric/1
29: nid007305:27225:29928 [3] NCCL INFO Channel 04/0 : 115[3] -> 119[3] [receive] via NET/AWS Libfabric/3
30: nid007318:20580:23385 [0] NCCL INFO Channel 01/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
12: nid006508:205767:208661 [1] NCCL INFO Channel 03/0 : 45[1] -> 49[1] [receive] via NET/AWS Libfabric/1
13: nid006509:201790:204570 [2] NCCL INFO Channel 07/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201789:204572 [1] NCCL INFO Channel 06/0 : 49[1] -> 53[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206594:209419 [1] NCCL INFO Channel 06/0 : 65[1] -> 69[1] [receive] via NET/AWS Libfabric/1
18: nid006556:210776:213650 [2] NCCL INFO Channel 07/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:255376 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229442:232235 [0] NCCL INFO Channel 03/0 : 56[0] -> 59[3] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 02/0 : 78[2] -> 82[2] [send] via NET/AWS Libfabric/2
26: nid006565:222465:225281 [0] NCCL INFO Channel 01/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
29: nid007305:27222:29926 [0] NCCL INFO Channel 00/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
30: nid007318:20583:23382 [3] NCCL INFO Channel 00/0 : 123[3] -> 127[3] [send] via NET/AWS Libfabric/3
15: nid006553:223925:226707 [2] NCCL INFO Channel 03/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:214099 [0] NCCL INFO Channel 02/0 : 44[0] -> 47[3] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 03/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209421 [2] NCCL INFO Channel 03/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206594:209419 [1] NCCL INFO Channel 03/0 : 69[1] -> 73[1] [send] via NET/AWS Libfabric/1
 0: nid006495:241021:244130 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 03/0 : 29[1] -> 33[1] [send] via NET/AWS Libfabric/1
 8: nid006503:218410:221180 [0] NCCL INFO Channel 02/0 : 32[0] -> 35[3] via P2P/CUMEM
16: nid006554:221582:224397 [1] NCCL INFO Channel 02/0 : 65[1] -> 69[1] [send] via NET/AWS Libfabric/1
25: nid006564:223024:225768 [3] NCCL INFO Channel 00/0 : 99[3] -> 103[3] [receive] via NET/AWS Libfabric/3
24: nid006563:221142:223931 [2] NCCL INFO Channel 02/0 : 98[2] -> 96[0] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 06/0 : 78[2] -> 82[2] [send] via NET/AWS Libfabric/2
28: nid007251:72169:75267 [0] NCCL INFO Channel 00/0 : 108[0] -> 112[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72172:75266 [3] NCCL INFO Channel 01/0 : 111[3] -> 115[3] [receive] via NET/AWS Libfabric/3
26: nid006565:222468:225280 [3] NCCL INFO Channel 05/0 : 103[3] -> 107[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211124:213885 [1] NCCL INFO Channel 07/0 : 85[1] -> 89[1] [send] via NET/AWS Libfabric/1
29: nid007305:27224:29927 [2] NCCL INFO Channel 07/0 : 118[2] -> 116[0] via P2P/CUMEM
30: nid007318:20580:23385 [0] NCCL INFO Channel 05/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251877 [0] NCCL INFO Channel 02/0 : 36[0] -> 39[3] via P2P/CUMEM
15: nid006553:223923:226710 [0] NCCL INFO Channel 07/0 : 60[0] -> 63[3] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 02/0 : 57[1] -> 61[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205767:208661 [1] NCCL INFO Channel 07/0 : 45[1] -> 49[1] [receive] via NET/AWS Libfabric/1
13: nid006509:201790:204570 [2] NCCL INFO Channel 02/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221941:224736 [0] NCCL INFO Channel 06/0 : 24[0] -> 27[3] via P2P/CUMEM
 1: nid006496:242585:245399 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221582:224397 [1] NCCL INFO Channel 06/0 : 65[1] -> 69[1] [send] via NET/AWS Libfabric/1
25: nid006564:223021:225771 [0] NCCL INFO Channel 05/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:262861 [0] NCCL INFO Channel 03/0 : 20[0] -> 23[3] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 06/0 : 106[2] -> 104[0] via P2P/CUMEM
26: nid006565:222465:225281 [0] NCCL INFO Channel 05/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
21: nid006559:211125:213887 [2] NCCL INFO Channel 03/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27225:29928 [3] NCCL INFO Channel 01/0 : 119[3] -> 123[3] [send] via NET/AWS Libfabric/3
30: nid007318:20583:23382 [3] NCCL INFO Channel 04/0 : 123[3] -> 127[3] [send] via NET/AWS Libfabric/3
 4: nid006499:254556:257329 [0] NCCL INFO Channel 03/0 : 16[0] -> 19[3] via P2P/CUMEM
15: nid006553:223925:226707 [2] NCCL INFO Channel 07/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:208659 [2] NCCL INFO Channel 07/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
13: nid006509:201789:204572 [1] NCCL INFO Channel 03/0 : 53[1] -> 57[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221942:224733 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206594:209419 [1] NCCL INFO Channel 07/0 : 69[1] -> 73[1] [send] via NET/AWS Libfabric/1
17: nid006555:206595:209421 [2] NCCL INFO Channel 07/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241020:244133 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 07/0 : 29[1] -> 33[1] [send] via NET/AWS Libfabric/1
25: nid006564:223024:225768 [3] NCCL INFO Channel 04/0 : 99[3] -> 103[3] [receive] via NET/AWS Libfabric/3
23: nid006561:220714:223497 [3] NCCL INFO Channel 00/0 : 91[3] -> 95[3] [receive] via NET/AWS Libfabric/3
23: nid006561:220713:223499 [2] NCCL INFO Channel 03/0 : 94[2] -> 92[0] via P2P/CUMEM
28: nid007251:72171:75269 [2] NCCL INFO Channel 06/0 : 114[2] -> 112[0] via P2P/CUMEM
26: nid006565:222468:225280 [3] NCCL INFO Channel 00/0 : 107[3] -> 111[3] [send] via NET/AWS Libfabric/3
21: nid006559:211125:213887 [2] NCCL INFO Channel 07/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:219524 [2] NCCL INFO Channel 07/0 : 110[2] -> 108[0] via P2P/CUMEM
27: nid006566:216746:219525 [0] NCCL INFO Channel 01/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29926 [0] NCCL INFO Channel 04/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:266505 [0] NCCL INFO Channel 06/0 : 40[0] -> 43[3] via P2P/CUMEM
22: nid006560:222271:225068 [0] NCCL INFO Channel 02/0 : 88[0] -> 91[3] via P2P/CUMEM
15: nid006553:223925:226707 [2] NCCL INFO Channel 02/0 : 62[2] -> 66[2] [send] via NET/AWS Libfabric/2
12: nid006508:205767:208661 [1] NCCL INFO Channel 02/0 : 49[1] -> 53[1] [send] via NET/AWS Libfabric/1
13: nid006509:201790:204570 [2] NCCL INFO Channel 06/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:224735 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:209421 [2] NCCL INFO Channel 02/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:245400 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 02/0 : 30[2] -> 34[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:225770 [2] NCCL INFO Channel 03/0 : 102[2] -> 100[0] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 01/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75267 [0] NCCL INFO Channel 04/0 : 108[0] -> 112[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222468:225280 [3] NCCL INFO Channel 04/0 : 107[3] -> 111[3] [send] via NET/AWS Libfabric/3
21: nid006559:211125:213887 [2] NCCL INFO Channel 02/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
27: nid006566:216749:219522 [3] NCCL INFO Channel 00/0 : 107[3] -> 111[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27225:29928 [3] NCCL INFO Channel 05/0 : 119[3] -> 123[3] [send] via NET/AWS Libfabric/3
22: nid006560:222271:225068 [0] NCCL INFO Channel 03/0 : 88[0] -> 91[3] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 06/0 : 57[1] -> 61[1] [receive] via NET/AWS Libfabric/1
11: nid006507:211335:214099 [0] NCCL INFO Channel 03/0 : 44[0] -> 47[3] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 06/0 : 49[1] -> 53[1] [send] via NET/AWS Libfabric/1
13: nid006509:201789:204572 [1] NCCL INFO Channel 07/0 : 53[1] -> 57[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221942:224733 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
17: nid006555:206595:209421 [2] NCCL INFO Channel 06/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:255376 [2] NCCL INFO Channel 06/0 : 30[2] -> 34[2] [send] via NET/AWS Libfabric/2
14: nid006510:229442:232235 [0] NCCL INFO Channel 06/0 : 56[0] -> 59[3] via P2P/CUMEM
25: nid006564:223021:225771 [0] NCCL INFO Channel 00/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
25: nid006564:223024:225768 [3] NCCL INFO Channel 01/0 : 103[3] -> 107[3] [send] via NET/AWS Libfabric/3
24: nid006563:221142:223931 [2] NCCL INFO Channel 06/0 : 98[2] -> 96[0] via P2P/CUMEM
23: nid006561:220711:223500 [0] NCCL INFO Channel 05/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229515 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
28: nid007251:72172:75266 [3] NCCL INFO Channel 05/0 : 111[3] -> 115[3] [receive] via NET/AWS Libfabric/3
27: nid006566:216746:219525 [0] NCCL INFO Channel 05/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263728:266502 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223925:226707 [2] NCCL INFO Channel 06/0 : 62[2] -> 66[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:224735 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218410:221180 [0] NCCL INFO Channel 03/0 : 32[0] -> 35[3] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 02/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:225771 [0] NCCL INFO Channel 04/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
25: nid006564:223024:225768 [3] NCCL INFO Channel 05/0 : 103[3] -> 107[3] [send] via NET/AWS Libfabric/3
 5: nid006500:260083:262861 [0] NCCL INFO Channel 06/0 : 20[0] -> 23[3] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 04/0 : 91[3] -> 95[3] [receive] via NET/AWS Libfabric/3
19: nid006557:208419:211196 [2] NCCL INFO Channel 03/0 : 78[2] -> 76[0] via P2P/CUMEM
28: nid007251:72169:75267 [0] NCCL INFO Channel 01/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
21: nid006559:211125:213887 [2] NCCL INFO Channel 06/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
27: nid006566:216746:219525 [0] NCCL INFO Channel 00/0 : 108[0] -> 112[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:257329 [0] NCCL INFO Channel 06/0 : 16[0] -> 19[3] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 07/0 : 37[1] -> 41[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249080:251877 [0] NCCL INFO Channel 03/0 : 36[0] -> 39[3] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221942:224733 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
 1: nid006496:242585:245399 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Channel 00/0 : 76[0] -> 80[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229443:232232 [1] NCCL INFO Channel 03/0 : 53[1] -> 57[1] [receive] via NET/AWS Libfabric/1
25: nid006564:223023:225770 [2] NCCL INFO Channel 07/0 : 102[2] -> 100[0] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 07/0 : 94[2] -> 92[0] via P2P/CUMEM
28: nid007251:72172:75266 [3] NCCL INFO Channel 00/0 : 115[3] -> 119[3] [send] via NET/AWS Libfabric/3
27: nid006566:216749:219522 [3] NCCL INFO Channel 04/0 : 107[3] -> 111[3] [receive] via NET/AWS Libfabric/3
30: nid007318:20583:23382 [3] NCCL INFO Channel 02/0 : 123[3] -> 121[1] via P2P/CUMEM
10: nid006506:263729:266503 [2] NCCL INFO Channel 02/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222271:225068 [0] NCCL INFO Channel 06/0 : 88[0] -> 91[3] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 07/0 : 61[1] -> 65[1] [send] via NET/AWS Libfabric/1
 6: nid006501:221943:224735 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221941:224736 [0] NCCL INFO Channel 07/0 : 24[0] -> 27[3] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 02/0 : 82[2] -> 80[0] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 06/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:223500 [0] NCCL INFO Channel 00/0 : 92[0] -> 96[0] [send] via NET/AWS Libfabric/0
23: nid006561:220714:223497 [3] NCCL INFO Channel 01/0 : 95[3] -> 99[3] [send] via NET/AWS Libfabric/3
28: nid007251:72169:75267 [0] NCCL INFO Channel 05/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:219525 [0] NCCL INFO Channel 04/0 : 108[0] -> 112[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254557:257331 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
10: nid006506:263727:266505 [0] NCCL INFO Channel 07/0 : 40[0] -> 43[3] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 02/0 : 41[1] -> 45[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227576:230348 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
20: nid006558:215467:218273 [0] NCCL INFO Channel 04/0 : 76[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241021:244130 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
16: nid006554:221584:224395 [3] NCCL INFO Channel 01/0 : 63[3] -> 67[3] [receive] via NET/AWS Libfabric/3
14: nid006510:229443:232232 [1] NCCL INFO Channel 07/0 : 53[1] -> 57[1] [receive] via NET/AWS Libfabric/1
23: nid006561:220711:223500 [0] NCCL INFO Channel 04/0 : 92[0] -> 96[0] [send] via NET/AWS Libfabric/0
28: nid007251:72172:75266 [3] NCCL INFO Channel 04/0 : 115[3] -> 119[3] [send] via NET/AWS Libfabric/3
27: nid006566:216749:219522 [3] NCCL INFO Channel 01/0 : 111[3] -> 115[3] [send] via NET/AWS Libfabric/3
 4: nid006499:254558:257328 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:266503 [2] NCCL INFO Channel 06/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:230347 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
 6: nid006501:221943:224735 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209421 [2] NCCL INFO Channel 03/0 : 70[2] -> 68[0] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 01/0 : 79[3] -> 83[3] [receive] via NET/AWS Libfabric/3
 0: nid006495:241020:244133 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 03/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
14: nid006510:229442:232235 [0] NCCL INFO Channel 07/0 : 56[0] -> 59[3] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 05/0 : 95[3] -> 99[3] [send] via NET/AWS Libfabric/3
21: nid006559:211125:213887 [2] NCCL INFO Channel 03/0 : 86[2] -> 84[0] via P2P/CUMEM
27: nid006566:216749:219522 [3] NCCL INFO Channel 05/0 : 111[3] -> 115[3] [send] via NET/AWS Libfabric/3
29: nid007305:27225:29928 [3] NCCL INFO Channel 03/0 : 119[3] -> 117[1] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
10: nid006506:263728:266502 [1] NCCL INFO Channel 06/0 : 41[1] -> 45[1] [send] via NET/AWS Libfabric/1
22: nid006560:222271:225068 [0] NCCL INFO Channel 07/0 : 88[0] -> 91[3] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Channel 01/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
18: nid006556:210776:213650 [2] NCCL INFO Channel 02/0 : 74[2] -> 72[0] via P2P/CUMEM
16: nid006554:221583:224394 [2] NCCL INFO Channel 02/0 : 66[2] -> 64[0] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 02/0 : 57[1] -> 61[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260084:262860 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
24: nid006563:221140:223930 [0] NCCL INFO Channel 00/0 : 92[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229515 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
30: nid007318:20583:23382 [3] NCCL INFO Channel 06/0 : 123[3] -> 121[1] via P2P/CUMEM
 4: nid006499:254558:257328 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:266503 [2] NCCL INFO Channel 03/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225067 [2] NCCL INFO Channel 02/0 : 90[2] -> 88[0] via P2P/CUMEM
11: nid006507:211335:214099 [0] NCCL INFO Channel 06/0 : 44[0] -> 47[3] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 05/0 : 79[3] -> 83[3] [receive] via NET/AWS Libfabric/3
20: nid006558:215467:218273 [0] NCCL INFO Channel 05/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:213649 [0] NCCL INFO Channel 00/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221180 [0] NCCL INFO Channel 06/0 : 32[0] -> 35[3] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 00/0 : 60[0] -> 64[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229444:232233 [2] NCCL INFO Channel 07/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:262861 [0] NCCL INFO Channel 07/0 : 20[0] -> 23[3] via P2P/CUMEM
24: nid006563:221143:223929 [3] NCCL INFO Channel 01/0 : 95[3] -> 99[3] [receive] via NET/AWS Libfabric/3
19: nid006557:208419:211196 [2] NCCL INFO Channel 07/0 : 78[2] -> 76[0] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
10: nid006506:263729:266503 [2] NCCL INFO Channel 07/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249080:251877 [0] NCCL INFO Channel 06/0 : 36[0] -> 39[3] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 01/0 : 71[3] -> 75[3] [receive] via NET/AWS Libfabric/3
 8: nid006503:218412:221178 [2] NCCL INFO Channel 02/0 : 30[2] -> 34[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221584:224395 [3] NCCL INFO Channel 05/0 : 63[3] -> 67[3] [receive] via NET/AWS Libfabric/3
14: nid006510:229443:232232 [1] NCCL INFO Channel 06/0 : 57[1] -> 61[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260084:262860 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
24: nid006563:221140:223930 [0] NCCL INFO Channel 04/0 : 92[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257329 [0] NCCL INFO Channel 07/0 : 16[0] -> 19[3] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 03/0 : 54[2] -> 52[0] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 00/0 : 83[3] -> 87[3] [send] via NET/AWS Libfabric/3
18: nid006556:210774:213649 [0] NCCL INFO Channel 04/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:224396 [0] NCCL INFO Channel 04/0 : 60[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260085:262859 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221140:223930 [0] NCCL INFO Channel 01/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
26: nid006565:222468:225280 [3] NCCL INFO Channel 02/0 : 107[3] -> 105[1] via P2P/CUMEM
 4: nid006499:254558:257328 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
22: nid006560:222272:225066 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[1] [receive] via NET/AWS Libfabric/1
11: nid006507:211337:214097 [2] NCCL INFO Channel 03/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210777:213648 [3] NCCL INFO Channel 05/0 : 71[3] -> 75[3] [receive] via NET/AWS Libfabric/3
 8: nid006503:218412:221178 [2] NCCL INFO Channel 06/0 : 30[2] -> 34[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221584:224395 [3] NCCL INFO Channel 00/0 : 67[3] -> 71[3] [send] via NET/AWS Libfabric/3
 5: nid006500:260084:262860 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
24: nid006563:221143:223929 [3] NCCL INFO Channel 05/0 : 95[3] -> 99[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27225:29928 [3] NCCL INFO Channel 07/0 : 119[3] -> 117[1] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
 2: nid006497:227576:230348 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 03/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215470:218272 [3] NCCL INFO Channel 04/0 : 83[3] -> 87[3] [send] via NET/AWS Libfabric/3
16: nid006554:221583:224394 [2] NCCL INFO Channel 06/0 : 66[2] -> 64[0] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 02/0 : 58[2] -> 56[0] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221140:223930 [0] NCCL INFO Channel 05/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229515 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
21: nid006559:211125:213887 [2] NCCL INFO Channel 07/0 : 86[2] -> 84[0] via P2P/CUMEM
 4: nid006499:254558:257328 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
22: nid006560:222272:225066 [1] NCCL INFO Channel 07/0 : 85[1] -> 89[1] [receive] via NET/AWS Libfabric/1
 2: nid006497:227577:230347 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 02/0 : 33[1] -> 37[1] [receive] via NET/AWS Libfabric/1
15: nid006553:223925:226707 [2] NCCL INFO Channel 03/0 : 62[2] -> 60[0] via P2P/CUMEM
11: nid006507:211337:214097 [2] NCCL INFO Channel 07/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218271 [2] NCCL INFO Channel 06/0 : 82[2] -> 80[0] via P2P/CUMEM
16: nid006554:221581:224396 [0] NCCL INFO Channel 01/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
16: nid006554:221584:224395 [3] NCCL INFO Channel 04/0 : 67[3] -> 71[3] [send] via NET/AWS Libfabric/3
 5: nid006500:260084:262860 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
24: nid006563:221143:223929 [3] NCCL INFO Channel 00/0 : 99[3] -> 103[3] [send] via NET/AWS Libfabric/3
21: nid006559:211123:213886 [0] NCCL INFO Channel 01/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216749:219522 [3] NCCL INFO Channel 03/0 : 111[3] -> 109[1] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 06/0 : 90[2] -> 88[0] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 07/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211336:214100 [1] NCCL INFO Channel 02/0 : 41[1] -> 45[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205769:208662 [3] NCCL INFO Channel 01/0 : 47[3] -> 51[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201791:204569 [3] NCCL INFO Channel 00/0 : 51[3] -> 55[3] [receive] via NET/AWS Libfabric/3
 1: nid006496:242584:245401 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 00/0 : 75[3] -> 79[3] [send] via NET/AWS Libfabric/3
18: nid006556:210776:213650 [2] NCCL INFO Channel 06/0 : 74[2] -> 72[0] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 03/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
16: nid006554:221581:224396 [0] NCCL INFO Channel 05/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260085:262859 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
19: nid006557:208420:211194 [3] NCCL INFO Channel 00/0 : 75[3] -> 79[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211126:213884 [3] NCCL INFO Channel 00/0 : 83[3] -> 87[3] [receive] via NET/AWS Libfabric/3
22: nid006560:222272:225066 [1] NCCL INFO Channel 02/0 : 89[1] -> 93[1] [send] via NET/AWS Libfabric/1
 9: nid006505:249081:251878 [1] NCCL INFO Channel 06/0 : 33[1] -> 37[1] [receive] via NET/AWS Libfabric/1
11: nid006507:211337:214097 [2] NCCL INFO Channel 02/0 : 46[2] -> 50[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:208660 [0] NCCL INFO Channel 00/0 : 44[0] -> 48[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:204571 [0] NCCL INFO Channel 01/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206595:209421 [2] NCCL INFO Channel 07/0 : 70[2] -> 68[0] via P2P/CUMEM
 0: nid006495:241019:244132 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
18: nid006556:210774:213649 [0] NCCL INFO Channel 01/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252587:255375 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
 5: nid006500:260085:262859 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
24: nid006563:221143:223929 [3] NCCL INFO Channel 04/0 : 99[3] -> 103[3] [send] via NET/AWS Libfabric/3
28: nid007251:72172:75266 [3] NCCL INFO Channel 02/0 : 115[3] -> 113[1] via P2P/CUMEM
21: nid006559:211123:213886 [0] NCCL INFO Channel 05/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20583:23382 [3] NCCL INFO Channel 01/0 : 123[3] -> 122[2] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 06/0 : 89[1] -> 93[1] [send] via NET/AWS Libfabric/1
 9: nid006505:249080:251877 [0] NCCL INFO Channel 07/0 : 36[0] -> 39[3] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 06/0 : 41[1] -> 45[1] [receive] via NET/AWS Libfabric/1
12: nid006508:205769:208662 [3] NCCL INFO Channel 05/0 : 47[3] -> 51[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201791:204569 [3] NCCL INFO Channel 04/0 : 51[3] -> 55[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201788:204571 [0] NCCL INFO Channel 05/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206593:209422 [0] NCCL INFO Channel 01/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210777:213648 [3] NCCL INFO Channel 04/0 : 75[3] -> 79[3] [send] via NET/AWS Libfabric/3
 7: nid006502:252584:255377 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218411:221181 [1] NCCL INFO Channel 03/0 : 29[1] -> 33[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226767:229516 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208420:211194 [3] NCCL INFO Channel 04/0 : 75[3] -> 79[3] [receive] via NET/AWS Libfabric/3
21: nid006559:211126:213884 [3] NCCL INFO Channel 04/0 : 83[3] -> 87[3] [receive] via NET/AWS Libfabric/3
 9: nid006505:249082:251876 [2] NCCL INFO Channel 02/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:214097 [2] NCCL INFO Channel 06/0 : 46[2] -> 50[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:208660 [0] NCCL INFO Channel 04/0 : 44[0] -> 48[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:209420 [3] NCCL INFO Channel 00/0 : 67[3] -> 71[3] [receive] via NET/AWS Libfabric/3
18: nid006556:210774:213649 [0] NCCL INFO Channel 05/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252587:255375 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
 7: nid006502:252584:255377 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221180 [0] NCCL INFO Channel 07/0 : 32[0] -> 35[3] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 07/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226766:229518 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
21: nid006559:211123:213886 [0] NCCL INFO Channel 00/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211126:213884 [3] NCCL INFO Channel 01/0 : 87[3] -> 91[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249081:251878 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[1] [send] via NET/AWS Libfabric/1
11: nid006507:211336:214100 [1] NCCL INFO Channel 03/0 : 45[1] -> 49[1] [send] via NET/AWS Libfabric/1
11: nid006507:211335:214099 [0] NCCL INFO Channel 07/0 : 44[0] -> 47[3] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 00/0 : 51[3] -> 55[3] [send] via NET/AWS Libfabric/3
17: nid006555:206593:209422 [0] NCCL INFO Channel 05/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218411:221181 [1] NCCL INFO Channel 07/0 : 29[1] -> 33[1] [receive] via NET/AWS Libfabric/1
 3: nid006498:226767:229516 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208420:211194 [3] NCCL INFO Channel 01/0 : 79[3] -> 83[3] [send] via NET/AWS Libfabric/3
26: nid006565:222468:225280 [3] NCCL INFO Channel 06/0 : 107[3] -> 105[1] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 06/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
11: nid006507:211336:214100 [1] NCCL INFO Channel 07/0 : 45[1] -> 49[1] [send] via NET/AWS Libfabric/1
12: nid006508:205766:208660 [0] NCCL INFO Channel 01/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
13: nid006509:201791:204569 [3] NCCL INFO Channel 01/0 : 55[3] -> 59[3] [send] via NET/AWS Libfabric/3
13: nid006509:201788:204571 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
17: nid006555:206596:209420 [3] NCCL INFO Channel 04/0 : 67[3] -> 71[3] [receive] via NET/AWS Libfabric/3
 1: nid006496:242584:245401 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
 8: nid006503:218411:221181 [1] NCCL INFO Channel 02/0 : 33[1] -> 37[1] [send] via NET/AWS Libfabric/1
 3: nid006498:226765:229515 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
 2: nid006497:227575:230349 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 07/0 : 37[1] -> 41[1] [send] via NET/AWS Libfabric/1
12: nid006508:205766:208660 [0] NCCL INFO Channel 05/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209422 [0] NCCL INFO Channel 00/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:255376 [2] NCCL INFO Channel 03/0 : 30[2] -> 28[0] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 01/0 : 31[3] -> 35[3] [send] via NET/AWS Libfabric/3
 8: nid006503:218411:221181 [1] NCCL INFO Channel 06/0 : 33[1] -> 37[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260085:262859 [2] NCCL INFO Channel 03/0 : 22[2] -> 20[0] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
27: nid006566:216749:219522 [3] NCCL INFO Channel 07/0 : 111[3] -> 109[1] via P2P/CUMEM
29: nid007305:27225:29928 [3] NCCL INFO Channel 00/0 : 119[3] -> 118[2] via P2P/CUMEM
15: nid006553:223925:226707 [2] NCCL INFO Channel 07/0 : 62[2] -> 60[0] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 00/0 : 59[3] -> 63[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211337:214097 [2] NCCL INFO Channel 03/0 : 46[2] -> 44[0] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 07/0 : 54[2] -> 52[0] via P2P/CUMEM
 6: nid006501:221943:224735 [2] NCCL INFO Channel 02/0 : 26[2] -> 24[0] via P2P/CUMEM
17: nid006555:206596:209420 [3] NCCL INFO Channel 01/0 : 71[3] -> 75[3] [send] via NET/AWS Libfabric/3
17: nid006555:206593:209422 [0] NCCL INFO Channel 04/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255377 [0] NCCL INFO Channel 00/0 : 28[0] -> 32[0] [send] via NET/AWS Libfabric/0
14: nid006510:229444:232233 [2] NCCL INFO Channel 06/0 : 58[2] -> 56[0] via P2P/CUMEM
25: nid006564:223024:225768 [3] NCCL INFO Channel 03/0 : 103[3] -> 101[1] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
19: nid006557:208420:211194 [3] NCCL INFO Channel 05/0 : 79[3] -> 83[3] [send] via NET/AWS Libfabric/3
19: nid006557:208417:211197 [0] NCCL INFO Channel 01/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72172:75266 [3] NCCL INFO Channel 06/0 : 115[3] -> 113[1] via P2P/CUMEM
15: nid006553:223923:226710 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205768:208659 [2] NCCL INFO Channel 02/0 : 50[2] -> 48[0] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 04/0 : 51[3] -> 55[3] [send] via NET/AWS Libfabric/3
13: nid006509:201791:204569 [3] NCCL INFO Channel 05/0 : 55[3] -> 59[3] [send] via NET/AWS Libfabric/3
17: nid006555:206596:209420 [3] NCCL INFO Channel 05/0 : 71[3] -> 75[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241019:244132 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 05/0 : 31[3] -> 35[3] [send] via NET/AWS Libfabric/3
 7: nid006502:252584:255377 [0] NCCL INFO Channel 04/0 : 28[0] -> 32[0] [send] via NET/AWS Libfabric/0
24: nid006563:221143:223929 [3] NCCL INFO Channel 02/0 : 99[3] -> 97[1] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
19: nid006557:208417:211197 [0] NCCL INFO Channel 05/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211123:213886 [0] NCCL INFO Channel 04/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211126:213884 [3] NCCL INFO Channel 05/0 : 87[3] -> 91[3] [send] via NET/AWS Libfabric/3
 2: nid006497:227575:230349 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 04/0 : 59[3] -> 63[3] [receive] via NET/AWS Libfabric/3
13: nid006509:201788:204571 [0] NCCL INFO Channel 04/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:255376 [2] NCCL INFO Channel 07/0 : 30[2] -> 28[0] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 02/0 : 34[2] -> 32[0] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
15: nid006553:223923:226710 [0] NCCL INFO Channel 05/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222468:225280 [3] NCCL INFO Channel 01/0 : 107[3] -> 106[2] via P2P/CUMEM
22: nid006560:222271:225068 [0] NCCL INFO Channel 00/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249082:251876 [2] NCCL INFO Channel 03/0 : 38[2] -> 36[0] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 02/0 : 83[3] -> 81[1] via P2P/CUMEM
19: nid006557:208420:211194 [3] NCCL INFO Channel 03/0 : 79[3] -> 77[1] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 01/0 : 87[3] -> 91[3] [receive] via NET/AWS Libfabric/3
 6: nid006501:221941:224736 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221944:224734 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254558:257328 [2] NCCL INFO Channel 02/0 : 18[2] -> 16[0] via P2P/CUMEM
10: nid006506:263729:266503 [2] NCCL INFO Channel 02/0 : 42[2] -> 40[0] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 01/0 : 63[3] -> 67[3] [send] via NET/AWS Libfabric/3
 6: nid006501:221943:224735 [2] NCCL INFO Channel 06/0 : 26[2] -> 24[0] via P2P/CUMEM
19: nid006557:208417:211197 [0] NCCL INFO Channel 00/0 : 76[0] -> 80[0] [send] via NET/AWS Libfabric/0
15: nid006553:223923:226710 [0] NCCL INFO Channel 00/0 : 60[0] -> 64[0] [send] via NET/AWS Libfabric/0
28: nid007251:72172:75266 [3] NCCL INFO Channel 01/0 : 115[3] -> 114[2] via P2P/CUMEM
22: nid006560:222271:225068 [0] NCCL INFO Channel 04/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:245401 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 07/0 : 22[2] -> 20[0] via P2P/CUMEM
10: nid006506:263727:266505 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222274:225065 [3] NCCL INFO Channel 05/0 : 87[3] -> 91[3] [receive] via NET/AWS Libfabric/3
15: nid006553:223926:226709 [3] NCCL INFO Channel 05/0 : 63[3] -> 67[3] [send] via NET/AWS Libfabric/3
 1: nid006496:242585:245399 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
25: nid006564:223024:225768 [3] NCCL INFO Channel 07/0 : 103[3] -> 101[1] via P2P/CUMEM
30: nid007318:20583:23382 [3] NCCL INFO Channel 03/0 : 123[3] -> 122[2] via P2P/CUMEM
 2: nid006497:227575:230349 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
15: nid006553:223923:226710 [0] NCCL INFO Channel 04/0 : 60[0] -> 64[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:224736 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210777:213648 [3] NCCL INFO Channel 02/0 : 75[3] -> 73[1] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 06/0 : 34[2] -> 32[0] via P2P/CUMEM
19: nid006557:208417:211197 [0] NCCL INFO Channel 04/0 : 76[0] -> 80[0] [send] via NET/AWS Libfabric/0
11: nid006507:211337:214097 [2] NCCL INFO Channel 07/0 : 46[2] -> 44[0] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
17: nid006555:206596:209420 [3] NCCL INFO Channel 03/0 : 71[3] -> 69[1] via P2P/CUMEM
 0: nid006495:241019:244132 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
 4: nid006499:254556:257329 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263730:266504 [3] NCCL INFO Channel 01/0 : 39[3] -> 43[3] [receive] via NET/AWS Libfabric/3
10: nid006506:263727:266505 [0] NCCL INFO Channel 04/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225068 [0] NCCL INFO Channel 01/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:208659 [2] NCCL INFO Channel 06/0 : 50[2] -> 48[0] via P2P/CUMEM
 6: nid006501:221941:224736 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242586:245400 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221143:223929 [3] NCCL INFO Channel 06/0 : 99[3] -> 97[1] via P2P/CUMEM
19: nid006557:208420:211194 [3] NCCL INFO Channel 07/0 : 79[3] -> 77[1] via P2P/CUMEM
27: nid006566:216749:219522 [3] NCCL INFO Channel 00/0 : 111[3] -> 110[2] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
10: nid006506:263727:266505 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
22: nid006560:222274:225065 [3] NCCL INFO Channel 00/0 : 91[3] -> 95[3] [send] via NET/AWS Libfabric/3
 1: nid006496:242585:245399 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
20: nid006558:215470:218272 [3] NCCL INFO Channel 06/0 : 83[3] -> 81[1] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 01/0 : 55[3] -> 59[3] [receive] via NET/AWS Libfabric/3
 5: nid006500:260086:262858 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254558:257328 [2] NCCL INFO Channel 06/0 : 18[2] -> 16[0] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 05/0 : 39[3] -> 43[3] [receive] via NET/AWS Libfabric/3
 1: nid006496:242586:245400 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242585:245399 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
 0: nid006495:241021:244130 [2] NCCL INFO Channel 02/0 : 126[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254556:257329 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:266505 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
22: nid006560:222274:225065 [3] NCCL INFO Channel 04/0 : 91[3] -> 95[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249082:251876 [2] NCCL INFO Channel 07/0 : 38[2] -> 36[0] via P2P/CUMEM
 6: nid006501:221941:224736 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:244133 [1] NCCL INFO Channel 03/0 : 125[1] -> 1[1] [receive] via NET/AWS Libfabric/1
 4: nid006499:254559:257330 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
22: nid006560:222271:225068 [0] NCCL INFO Channel 05/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230349 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241021:244130 [2] NCCL INFO Channel 06/0 : 126[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260086:262858 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
 4: nid006499:254556:257329 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
10: nid006506:263730:266504 [3] NCCL INFO Channel 00/0 : 43[3] -> 47[3] [send] via NET/AWS Libfabric/3
 6: nid006501:221944:224734 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
 1: nid006496:242586:245400 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:245399 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
 0: nid006495:241020:244133 [1] NCCL INFO Channel 07/0 : 125[1] -> 1[1] [receive] via NET/AWS Libfabric/1
16: nid006554:221584:224395 [3] NCCL INFO Channel 02/0 : 67[3] -> 65[1] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 05/0 : 55[3] -> 59[3] [receive] via NET/AWS Libfabric/3
29: nid007305:27225:29928 [3] NCCL INFO Channel 02/0 : 119[3] -> 118[2] via P2P/CUMEM
10: nid006506:263729:266503 [2] NCCL INFO Channel 06/0 : 42[2] -> 40[0] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 06/0 : 75[3] -> 73[1] via P2P/CUMEM
25: nid006564:223024:225768 [3] NCCL INFO Channel 00/0 : 103[3] -> 102[2] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
 4: nid006499:254559:257330 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
10: nid006506:263730:266504 [3] NCCL INFO Channel 04/0 : 43[3] -> 47[3] [send] via NET/AWS Libfabric/3
17: nid006555:206596:209420 [3] NCCL INFO Channel 07/0 : 71[3] -> 69[1] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:244130 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241019:244132 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
14: nid006510:229442:232235 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211126:213884 [3] NCCL INFO Channel 03/0 : 87[3] -> 85[1] via P2P/CUMEM
 4: nid006499:254556:257329 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:244133 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
14: nid006510:229445:232234 [3] NCCL INFO Channel 00/0 : 59[3] -> 63[3] [send] via NET/AWS Libfabric/3
19: nid006557:208420:211194 [3] NCCL INFO Channel 00/0 : 79[3] -> 78[2] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
20: nid006558:215470:218272 [3] NCCL INFO Channel 01/0 : 83[3] -> 82[2] via P2P/CUMEM
 0: nid006495:241021:244130 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
14: nid006510:229445:232234 [3] NCCL INFO Channel 04/0 : 59[3] -> 63[3] [send] via NET/AWS Libfabric/3
 5: nid006500:260083:262861 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220714:223497 [3] NCCL INFO Channel 03/0 : 95[3] -> 93[1] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
 2: nid006497:227576:230348 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
 9: nid006505:249080:251877 [0] NCCL INFO Channel 01/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242584:245401 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
 0: nid006495:241020:244133 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
14: nid006510:229442:232235 [0] NCCL INFO Channel 04/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260086:262858 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
24: nid006563:221143:223929 [3] NCCL INFO Channel 01/0 : 99[3] -> 98[2] via P2P/CUMEM
 2: nid006497:227577:230347 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211338:214098 [3] NCCL INFO Channel 00/0 : 43[3] -> 47[3] [receive] via NET/AWS Libfabric/3
22: nid006560:222274:225065 [3] NCCL INFO Channel 02/0 : 91[3] -> 89[1] via P2P/CUMEM
11: nid006507:211335:214099 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249083:251879 [3] NCCL INFO Channel 00/0 : 35[3] -> 39[3] [receive] via NET/AWS Libfabric/3
 9: nid006505:249080:251877 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72172:75266 [3] NCCL INFO Channel 03/0 : 115[3] -> 114[2] via P2P/CUMEM
13: nid006509:201791:204569 [3] NCCL INFO Channel 03/0 : 55[3] -> 53[1] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
 5: nid006500:260083:262861 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227577:230347 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218413:221179 [3] NCCL INFO Channel 01/0 : 31[3] -> 35[3] [receive] via NET/AWS Libfabric/3
 8: nid006503:218410:221180 [0] NCCL INFO Channel 00/0 : 28[0] -> 32[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221584:224395 [3] NCCL INFO Channel 06/0 : 67[3] -> 65[1] via P2P/CUMEM
 5: nid006500:260083:262861 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227576:230348 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
 8: nid006503:218410:221180 [0] NCCL INFO Channel 04/0 : 28[0] -> 32[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222468:225280 [3] NCCL INFO Channel 03/0 : 107[3] -> 106[2] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 04/0 : 35[3] -> 39[3] [receive] via NET/AWS Libfabric/3
30: nid007318:20581:23381 [1] NCCL INFO Channel 01/0 : 121[1] -> 120[0] via P2P/CUMEM
 9: nid006505:249080:251877 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
30: nid007318:20583:23382 [3] NCCL INFO Channel 05/0 : 123[3] -> 122[2] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 03/0 : 63[3] -> 61[1] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 04/0 : 43[3] -> 47[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211335:214099 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210777:213648 [3] NCCL INFO Channel 01/0 : 75[3] -> 74[2] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 03/0 : 126[2] -> 124[0] via P2P/CUMEM
 0: nid006495:241021:244130 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
 2: nid006497:227577:230347 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:230347 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:251879 [3] NCCL INFO Channel 01/0 : 39[3] -> 43[3] [send] via NET/AWS Libfabric/3
 9: nid006505:249080:251877 [0] NCCL INFO Channel 04/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:262861 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
27: nid006566:216749:219522 [3] NCCL INFO Channel 02/0 : 111[3] -> 110[2] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 02/0 : 59[3] -> 57[1] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 03/0 : 23[3] -> 21[1] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 02/0 : 27[3] -> 25[1] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 05/0 : 31[3] -> 35[3] [receive] via NET/AWS Libfabric/3
14: nid006510:229442:232235 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226768:229517 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
17: nid006555:206596:209420 [3] NCCL INFO Channel 00/0 : 71[3] -> 70[2] via P2P/CUMEM
 8: nid006503:218410:221180 [0] NCCL INFO Channel 01/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229515 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211126:213884 [3] NCCL INFO Channel 07/0 : 87[3] -> 85[1] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 06/0 : 91[3] -> 89[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 05/0 : 39[3] -> 43[3] [send] via NET/AWS Libfabric/3
11: nid006507:211338:214098 [3] NCCL INFO Channel 01/0 : 47[3] -> 51[3] [send] via NET/AWS Libfabric/3
11: nid006507:211335:214099 [0] NCCL INFO Channel 00/0 : 44[0] -> 48[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:221180 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218413:221179 [3] NCCL INFO Channel 00/0 : 35[3] -> 39[3] [send] via NET/AWS Libfabric/3
23: nid006561:220714:223497 [3] NCCL INFO Channel 07/0 : 95[3] -> 93[1] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
11: nid006507:211338:214098 [3] NCCL INFO Channel 05/0 : 47[3] -> 51[3] [send] via NET/AWS Libfabric/3
11: nid006507:211335:214099 [0] NCCL INFO Channel 04/0 : 44[0] -> 48[0] [send] via NET/AWS Libfabric/0
20: nid006558:215470:218272 [3] NCCL INFO Channel 03/0 : 83[3] -> 82[2] via P2P/CUMEM
 3: nid006498:226765:229515 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
25: nid006564:223024:225768 [3] NCCL INFO Channel 02/0 : 103[3] -> 102[2] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
13: nid006509:201791:204569 [3] NCCL INFO Channel 07/0 : 55[3] -> 53[1] via P2P/CUMEM
14: nid006510:229442:232235 [0] NCCL INFO Channel 05/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229515 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
29: nid007305:27223:29929 [1] NCCL INFO Channel 00/0 : 117[1] -> 116[0] via P2P/CUMEM
 0: nid006495:241021:244130 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
29: nid007305:27225:29928 [3] NCCL INFO Channel 04/0 : 119[3] -> 118[2] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 07/0 : 63[3] -> 61[1] via P2P/CUMEM
19: nid006557:208420:211194 [3] NCCL INFO Channel 02/0 : 79[3] -> 78[2] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 04/0 : 35[3] -> 39[3] [send] via NET/AWS Libfabric/3
14: nid006510:229445:232234 [3] NCCL INFO Channel 06/0 : 59[3] -> 57[1] via P2P/CUMEM
 3: nid006498:226765:229515 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221944:224734 [3] NCCL INFO Channel 06/0 : 27[3] -> 25[1] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 03/0 : 75[3] -> 74[2] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 01/0 : 113[1] -> 112[0] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 07/0 : 126[2] -> 124[0] via P2P/CUMEM
16: nid006554:221584:224395 [3] NCCL INFO Channel 01/0 : 67[3] -> 66[2] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 03/0 : 121[1] -> 120[0] via P2P/CUMEM
 2: nid006497:227577:230347 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 07/0 : 23[3] -> 21[1] via P2P/CUMEM
13: nid006509:201791:204569 [3] NCCL INFO Channel 00/0 : 55[3] -> 54[2] via P2P/CUMEM
24: nid006563:221143:223929 [3] NCCL INFO Channel 03/0 : 99[3] -> 98[2] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 01/0 : 105[1] -> 104[0] via P2P/CUMEM
21: nid006559:211126:213884 [3] NCCL INFO Channel 00/0 : 87[3] -> 86[2] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 03/0 : 47[3] -> 45[1] via P2P/CUMEM
28: nid007251:72172:75266 [3] NCCL INFO Channel 05/0 : 115[3] -> 114[2] via P2P/CUMEM
30: nid007318:20583:23382 [3] NCCL INFO Channel 07/0 : 123[3] -> 122[2] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 02/0 : 51[3] -> 49[1] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 03/0 : 31[3] -> 29[1] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
26: nid006565:222468:225280 [3] NCCL INFO Channel 05/0 : 107[3] -> 106[2] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 00/0 : 63[3] -> 62[2] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 02/0 : 19[3] -> 17[1] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 02/0 : 43[3] -> 41[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 03/0 : 39[3] -> 37[1] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 00/0 : 95[3] -> 94[2] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 01/0 : 81[1] -> 80[0] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 02/0 : 35[3] -> 33[1] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 05/0 : 83[3] -> 82[2] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 01/0 : 59[3] -> 58[2] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 01/0 : 91[3] -> 90[2] via P2P/CUMEM
17: nid006555:206596:209420 [3] NCCL INFO Channel 02/0 : 71[3] -> 70[2] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
27: nid006566:216747:219523 [1] NCCL INFO Channel 00/0 : 109[1] -> 108[0] via P2P/CUMEM
19: nid006557:208420:211194 [3] NCCL INFO Channel 04/0 : 79[3] -> 78[2] via P2P/CUMEM
25: nid006564:223024:225768 [3] NCCL INFO Channel 04/0 : 103[3] -> 102[2] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 01/0 : 127[3] -> 3[3] [receive] via NET/AWS Libfabric/3
19: nid006557:208418:211195 [1] NCCL INFO Channel 00/0 : 77[1] -> 76[0] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 02/0 : 117[1] -> 116[0] via P2P/CUMEM
27: nid006566:216749:219522 [3] NCCL INFO Channel 04/0 : 111[3] -> 110[2] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 00/0 : 101[1] -> 100[0] via P2P/CUMEM
29: nid007305:27225:29928 [3] NCCL INFO Channel 06/0 : 119[3] -> 118[2] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 06/0 : 51[3] -> 49[1] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 05/0 : 127[3] -> 3[3] [receive] via NET/AWS Libfabric/3
 2: nid006497:227577:230347 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
 7: nid006502:252587:255375 [3] NCCL INFO Channel 07/0 : 31[3] -> 29[1] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 00/0 : 23[3] -> 22[2] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 07/0 : 47[3] -> 45[1] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20582:23383 [2] NCCL INFO Channel 01/0 : 122[2] -> 121[1] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241022:244131 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241019:244132 [0] NCCL INFO Channel 00/0 : 124[0] -> 0[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263730:266504 [3] NCCL INFO Channel 06/0 : 43[3] -> 41[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 07/0 : 39[3] -> 37[1] via P2P/CUMEM
13: nid006509:201791:204569 [3] NCCL INFO Channel 02/0 : 55[3] -> 54[2] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254559:257330 [3] NCCL INFO Channel 06/0 : 19[3] -> 17[1] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
 0: nid006495:241022:244131 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
 8: nid006503:218413:221179 [3] NCCL INFO Channel 06/0 : 35[3] -> 33[1] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
 0: nid006495:241019:244132 [0] NCCL INFO Channel 04/0 : 124[0] -> 0[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20581:23381 [1] NCCL INFO Channel 05/0 : 121[1] -> 120[0] via P2P/CUMEM
18: nid006556:210775:213647 [1] NCCL INFO Channel 01/0 : 73[1] -> 72[0] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 03/0 : 113[1] -> 112[0] via P2P/CUMEM
28: nid007251:72172:75266 [3] NCCL INFO Channel 07/0 : 115[3] -> 114[2] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 02/0 : 63[3] -> 62[2] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 00/0 : 69[1] -> 68[0] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241019:244132 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
16: nid006554:221584:224395 [3] NCCL INFO Channel 03/0 : 67[3] -> 66[2] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 05/0 : 75[3] -> 74[2] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 01/0 : 97[1] -> 96[0] via P2P/CUMEM
19: nid006557:208420:211194 [3] NCCL INFO Channel 06/0 : 79[3] -> 78[2] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 03/0 : 105[1] -> 104[0] via P2P/CUMEM
 0: nid006495:241019:244132 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
24: nid006563:221143:223929 [3] NCCL INFO Channel 05/0 : 99[3] -> 98[2] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 02/0 : 77[1] -> 76[0] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 01/0 : 106[2] -> 105[1] via P2P/CUMEM
 1: nid006496:242584:245401 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
20: nid006558:215468:218270 [1] NCCL INFO Channel 03/0 : 81[1] -> 80[0] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 00/0 : 31[3] -> 30[2] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 03/0 : 59[3] -> 58[2] via P2P/CUMEM
26: nid006565:222468:225280 [3] NCCL INFO Channel 07/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211126:213884 [3] NCCL INFO Channel 02/0 : 87[3] -> 86[2] via P2P/CUMEM
27: nid006566:216748:219524 [2] NCCL INFO Channel 00/0 : 110[2] -> 109[1] via P2P/CUMEM
30: nid007318:20582:23383 [2] NCCL INFO Channel 05/0 : 122[2] -> 121[1] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 01/0 : 51[3] -> 50[2] via P2P/CUMEM
13: nid006509:201789:204572 [1] NCCL INFO Channel 00/0 : 53[1] -> 52[0] via P2P/CUMEM
29: nid007305:27224:29927 [2] NCCL INFO Channel 00/0 : 118[2] -> 117[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 00/0 : 39[3] -> 38[2] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 02/0 : 109[1] -> 108[0] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 03/0 : 127[3] -> 125[1] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 01/0 : 43[3] -> 42[2] via P2P/CUMEM
20: nid006558:215470:218272 [3] NCCL INFO Channel 07/0 : 83[3] -> 82[2] via P2P/CUMEM
30: nid007318:20581:23381 [1] NCCL INFO Channel 07/0 : 121[1] -> 120[0] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
27: nid006566:216749:219522 [3] NCCL INFO Channel 06/0 : 111[3] -> 110[2] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 00/0 : 61[1] -> 60[0] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 00/0 : 47[3] -> 46[2] via P2P/CUMEM
13: nid006509:201791:204569 [3] NCCL INFO Channel 04/0 : 55[3] -> 54[2] via P2P/CUMEM
25: nid006564:223024:225768 [3] NCCL INFO Channel 06/0 : 103[3] -> 102[2] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 02/0 : 95[3] -> 94[2] via P2P/CUMEM
28: nid007251:72171:75269 [2] NCCL INFO Channel 01/0 : 114[2] -> 113[1] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
25: nid006564:223022:225769 [1] NCCL INFO Channel 02/0 : 101[1] -> 100[0] via P2P/CUMEM
 2: nid006497:227575:230349 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206596:209420 [3] NCCL INFO Channel 04/0 : 71[3] -> 70[2] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 04/0 : 117[1] -> 116[0] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
26: nid006565:222466:225278 [1] NCCL INFO Channel 05/0 : 105[1] -> 104[0] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 07/0 : 127[3] -> 125[1] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 03/0 : 91[3] -> 90[2] via P2P/CUMEM
 2: nid006497:227575:230349 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260086:262858 [3] NCCL INFO Channel 02/0 : 23[3] -> 22[2] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
 2: nid006497:227575:230349 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218413:221179 [3] NCCL INFO Channel 01/0 : 35[3] -> 34[2] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 05/0 : 113[1] -> 112[0] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 02/0 : 69[1] -> 68[0] via P2P/CUMEM
26: nid006565:222467:225279 [2] NCCL INFO Channel 05/0 : 106[2] -> 105[1] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
15: nid006553:223926:226709 [3] NCCL INFO Channel 04/0 : 63[3] -> 62[2] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 03/0 : 27[3] -> 26[2] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
29: nid007305:27224:29927 [2] NCCL INFO Channel 04/0 : 118[2] -> 117[1] via P2P/CUMEM
18: nid006556:210775:213647 [1] NCCL INFO Channel 03/0 : 73[1] -> 72[0] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 03/0 : 97[1] -> 96[0] via P2P/CUMEM
27: nid006566:216748:219524 [2] NCCL INFO Channel 04/0 : 110[2] -> 109[1] via P2P/CUMEM
29: nid007305:27223:29929 [1] NCCL INFO Channel 06/0 : 117[1] -> 116[0] via P2P/CUMEM
18: nid006556:210777:213648 [3] NCCL INFO Channel 07/0 : 75[3] -> 74[2] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 01/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 05/0 : 59[3] -> 58[2] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 04/0 : 77[1] -> 76[0] via P2P/CUMEM
 2: nid006497:227575:230349 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
16: nid006554:221582:224397 [1] NCCL INFO Channel 01/0 : 65[1] -> 64[0] via P2P/CUMEM
16: nid006554:221584:224395 [3] NCCL INFO Channel 05/0 : 67[3] -> 66[2] via P2P/CUMEM
24: nid006563:221143:223929 [3] NCCL INFO Channel 07/0 : 99[3] -> 98[2] via P2P/CUMEM
26: nid006565:222466:225278 [1] NCCL INFO Channel 07/0 : 105[1] -> 104[0] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 04/0 : 109[1] -> 108[0] via P2P/CUMEM
13: nid006509:201789:204572 [1] NCCL INFO Channel 02/0 : 53[1] -> 52[0] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 05/0 : 81[1] -> 80[0] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 01/0 : 98[2] -> 97[1] via P2P/CUMEM
21: nid006559:211124:213885 [1] NCCL INFO Channel 00/0 : 85[1] -> 84[0] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 02/0 : 31[3] -> 30[2] via P2P/CUMEM
28: nid007251:72171:75269 [2] NCCL INFO Channel 05/0 : 114[2] -> 113[1] via P2P/CUMEM
21: nid006559:211126:213884 [3] NCCL INFO Channel 04/0 : 87[3] -> 86[2] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 00/0 : 102[2] -> 101[1] via P2P/CUMEM
28: nid007251:72170:75268 [1] NCCL INFO Channel 07/0 : 113[1] -> 112[0] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 03/0 : 51[3] -> 50[2] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 02/0 : 39[3] -> 38[2] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 00/0 : 93[1] -> 92[0] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 03/0 : 43[3] -> 42[2] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 02/0 : 61[1] -> 60[0] via P2P/CUMEM
13: nid006509:201791:204569 [3] NCCL INFO Channel 06/0 : 55[3] -> 54[2] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 00/0 : 127[3] -> 126[2] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 01/0 : 89[1] -> 88[0] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 04/0 : 101[1] -> 100[0] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 04/0 : 95[3] -> 94[2] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
17: nid006555:206596:209420 [3] NCCL INFO Channel 06/0 : 71[3] -> 70[2] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 05/0 : 97[1] -> 96[0] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 05/0 : 91[3] -> 90[2] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 02/0 : 47[3] -> 46[2] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/CUMEM
17: nid006555:206595:209421 [2] NCCL INFO Channel 00/0 : 70[2] -> 69[1] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 00/0 : 21[1] -> 20[0] via P2P/CUMEM
27: nid006566:216747:219523 [1] NCCL INFO Channel 06/0 : 109[1] -> 108[0] via P2P/CUMEM
25: nid006564:223023:225770 [2] NCCL INFO Channel 04/0 : 102[2] -> 101[1] via P2P/CUMEM
19: nid006557:208418:211195 [1] NCCL INFO Channel 06/0 : 77[1] -> 76[0] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 03/0 : 19[3] -> 18[2] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 04/0 : 23[3] -> 22[2] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 04/0 : 69[1] -> 68[0] via P2P/CUMEM
25: nid006564:223022:225769 [1] NCCL INFO Channel 06/0 : 101[1] -> 100[0] via P2P/CUMEM
24: nid006563:221142:223931 [2] NCCL INFO Channel 05/0 : 98[2] -> 97[1] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 03/0 : 35[3] -> 34[2] via P2P/CUMEM
15: nid006553:223926:226709 [3] NCCL INFO Channel 06/0 : 63[3] -> 62[2] via P2P/CUMEM
20: nid006558:215468:218270 [1] NCCL INFO Channel 07/0 : 81[1] -> 80[0] via P2P/CUMEM
18: nid006556:210775:213647 [1] NCCL INFO Channel 05/0 : 73[1] -> 72[0] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 03/0 : 57[1] -> 56[0] via P2P/CUMEM
24: nid006563:221141:223928 [1] NCCL INFO Channel 07/0 : 97[1] -> 96[0] via P2P/CUMEM
14: nid006510:229445:232234 [3] NCCL INFO Channel 07/0 : 59[3] -> 58[2] via P2P/CUMEM
13: nid006509:201789:204572 [1] NCCL INFO Channel 04/0 : 53[1] -> 52[0] via P2P/CUMEM
21: nid006559:211124:213885 [1] NCCL INFO Channel 02/0 : 85[1] -> 84[0] via P2P/CUMEM
16: nid006554:221582:224397 [1] NCCL INFO Channel 03/0 : 65[1] -> 64[0] via P2P/CUMEM
16: nid006554:221584:224395 [3] NCCL INFO Channel 07/0 : 67[3] -> 66[2] via P2P/CUMEM
21: nid006559:211126:213884 [3] NCCL INFO Channel 06/0 : 87[3] -> 86[2] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 05/0 : 51[3] -> 50[2] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 04/0 : 31[3] -> 30[2] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 01/0 : 41[1] -> 40[0] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 03/0 : 89[1] -> 88[0] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 02/0 : 127[3] -> 126[2] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 02/0 : 93[1] -> 92[0] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 00/0 : 29[1] -> 28[0] via P2P/CUMEM
23: nid006561:220714:223497 [3] NCCL INFO Channel 06/0 : 95[3] -> 94[2] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 04/0 : 61[1] -> 60[0] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 01/0 : 49[1] -> 48[0] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 05/0 : 43[3] -> 42[2] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 04/0 : 39[3] -> 38[2] via P2P/CUMEM
17: nid006555:206595:209421 [2] NCCL INFO Channel 04/0 : 70[2] -> 69[1] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 00/0 : 37[1] -> 36[0] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
17: nid006555:206594:209419 [1] NCCL INFO Channel 06/0 : 69[1] -> 68[0] via P2P/CUMEM
22: nid006560:222274:225065 [3] NCCL INFO Channel 07/0 : 91[3] -> 90[2] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/CUMEM
13: nid006509:201789:204572 [1] NCCL INFO Channel 06/0 : 53[1] -> 52[0] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 02/0 : 21[1] -> 20[0] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 01/0 : 17[1] -> 16[0] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 00/0 : 45[1] -> 44[0] via P2P/CUMEM
18: nid006556:210776:213650 [2] NCCL INFO Channel 01/0 : 74[2] -> 73[1] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 00/0 : 78[2] -> 77[1] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 04/0 : 47[3] -> 46[2] via P2P/CUMEM
18: nid006556:210775:213647 [1] NCCL INFO Channel 07/0 : 73[1] -> 72[0] via P2P/CUMEM
 5: nid006500:260086:262858 [3] NCCL INFO Channel 06/0 : 23[3] -> 22[2] via P2P/CUMEM
 6: nid006501:221944:224734 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/CUMEM
16: nid006554:221583:224394 [2] NCCL INFO Channel 01/0 : 66[2] -> 65[1] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 01/0 : 82[2] -> 81[1] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/CUMEM
 8: nid006503:218411:221181 [1] NCCL INFO Channel 01/0 : 33[1] -> 32[0] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 05/0 : 57[1] -> 56[0] via P2P/CUMEM
19: nid006557:208419:211196 [2] NCCL INFO Channel 04/0 : 78[2] -> 77[1] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 05/0 : 35[3] -> 34[2] via P2P/CUMEM
21: nid006559:211125:213887 [2] NCCL INFO Channel 00/0 : 86[2] -> 85[1] via P2P/CUMEM
20: nid006558:215469:218271 [2] NCCL INFO Channel 05/0 : 82[2] -> 81[1] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 03/0 : 41[1] -> 40[0] via P2P/CUMEM
21: nid006559:211124:213885 [1] NCCL INFO Channel 04/0 : 85[1] -> 84[0] via P2P/CUMEM
12: nid006508:205769:208662 [3] NCCL INFO Channel 07/0 : 51[3] -> 50[2] via P2P/CUMEM
18: nid006556:210776:213650 [2] NCCL INFO Channel 05/0 : 74[2] -> 73[1] via P2P/CUMEM
15: nid006553:223924:226708 [1] NCCL INFO Channel 06/0 : 61[1] -> 60[0] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 00/0 : 94[2] -> 93[1] via P2P/CUMEM
 7: nid006502:252587:255375 [3] NCCL INFO Channel 06/0 : 31[3] -> 30[2] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 04/0 : 93[1] -> 92[0] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 00/0 : 125[1] -> 124[0] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 04/0 : 127[3] -> 126[2] via P2P/CUMEM
10: nid006506:263730:266504 [3] NCCL INFO Channel 07/0 : 43[3] -> 42[2] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 05/0 : 89[1] -> 88[0] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 03/0 : 49[1] -> 48[0] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 02/0 : 29[1] -> 28[0] via P2P/CUMEM
16: nid006554:221582:224397 [1] NCCL INFO Channel 05/0 : 65[1] -> 64[0] via P2P/CUMEM
16: nid006554:221583:224394 [2] NCCL INFO Channel 05/0 : 66[2] -> 65[1] via P2P/CUMEM
14: nid006510:229443:232232 [1] NCCL INFO Channel 07/0 : 57[1] -> 56[0] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 01/0 : 34[2] -> 33[1] via P2P/CUMEM
 9: nid006505:249083:251879 [3] NCCL INFO Channel 06/0 : 39[3] -> 38[2] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 01/0 : 90[2] -> 89[1] via P2P/CUMEM
 0: nid006495:241020:244133 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 04/0 : 21[1] -> 20[0] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 02/0 : 37[1] -> 36[0] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 02/0 : 45[1] -> 44[0] via P2P/CUMEM
21: nid006559:211125:213887 [2] NCCL INFO Channel 04/0 : 86[2] -> 85[1] via P2P/CUMEM
 6: nid006501:221943:224735 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 00/0 : 30[2] -> 29[1] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 00/0 : 38[2] -> 37[1] via P2P/CUMEM
15: nid006553:223925:226707 [2] NCCL INFO Channel 00/0 : 62[2] -> 61[1] via P2P/CUMEM
11: nid006507:211338:214098 [3] NCCL INFO Channel 06/0 : 47[3] -> 46[2] via P2P/CUMEM
21: nid006559:211124:213885 [1] NCCL INFO Channel 06/0 : 85[1] -> 84[0] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 00/0 : 54[2] -> 53[1] via P2P/CUMEM
 8: nid006503:218411:221181 [1] NCCL INFO Channel 03/0 : 33[1] -> 32[0] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 00/0 : 22[2] -> 21[1] via P2P/CUMEM
23: nid006561:220713:223499 [2] NCCL INFO Channel 04/0 : 94[2] -> 93[1] via P2P/CUMEM
23: nid006561:220712:223498 [1] NCCL INFO Channel 06/0 : 93[1] -> 92[0] via P2P/CUMEM
 1: nid006496:242585:245399 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
 8: nid006503:218413:221179 [3] NCCL INFO Channel 07/0 : 35[3] -> 34[2] via P2P/CUMEM
 4: nid006499:254559:257330 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/CUMEM
22: nid006560:222272:225066 [1] NCCL INFO Channel 07/0 : 89[1] -> 88[0] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 01/0 : 58[2] -> 57[1] via P2P/CUMEM
22: nid006560:222273:225067 [2] NCCL INFO Channel 05/0 : 90[2] -> 89[1] via P2P/CUMEM
13: nid006509:201790:204570 [2] NCCL INFO Channel 04/0 : 54[2] -> 53[1] via P2P/CUMEM
10: nid006506:263729:266503 [2] NCCL INFO Channel 01/0 : 42[2] -> 41[1] via P2P/CUMEM
15: nid006553:223925:226707 [2] NCCL INFO Channel 04/0 : 62[2] -> 61[1] via P2P/CUMEM
 6: nid006501:221942:224733 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/CUMEM
16: nid006554:221582:224397 [1] NCCL INFO Channel 07/0 : 65[1] -> 64[0] via P2P/CUMEM
 5: nid006500:260084:262860 [1] NCCL INFO Channel 06/0 : 21[1] -> 20[0] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
 6: nid006501:221943:224735 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 05/0 : 41[1] -> 40[0] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 04/0 : 29[1] -> 28[0] via P2P/CUMEM
 5: nid006500:260085:262859 [2] NCCL INFO Channel 04/0 : 22[2] -> 21[1] via P2P/CUMEM
14: nid006510:229444:232233 [2] NCCL INFO Channel 05/0 : 58[2] -> 57[1] via P2P/CUMEM
 8: nid006503:218412:221178 [2] NCCL INFO Channel 05/0 : 34[2] -> 33[1] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 02/0 : 125[1] -> 124[0] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 05/0 : 49[1] -> 48[0] via P2P/CUMEM
 7: nid006502:252586:255376 [2] NCCL INFO Channel 04/0 : 30[2] -> 29[1] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 01/0 : 50[2] -> 49[1] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 04/0 : 37[1] -> 36[0] via P2P/CUMEM
 0: nid006495:241022:244131 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/CUMEM
 9: nid006505:249082:251876 [2] NCCL INFO Channel 04/0 : 38[2] -> 37[1] via P2P/CUMEM
 8: nid006503:218411:221181 [1] NCCL INFO Channel 05/0 : 33[1] -> 32[0] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
31: nid007342:59770:63007 [3] NCCL INFO Channel 06/0 : 127[3] -> 126[2] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
11: nid006507:211337:214097 [2] NCCL INFO Channel 00/0 : 46[2] -> 45[1] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
 0: nid006495:241020:244133 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 04/0 : 45[1] -> 44[0] via P2P/CUMEM
10: nid006506:263729:266503 [2] NCCL INFO Channel 05/0 : 42[2] -> 41[1] via P2P/CUMEM
 7: nid006502:252585:255374 [1] NCCL INFO Channel 06/0 : 29[1] -> 28[0] via P2P/CUMEM
10: nid006506:263728:266502 [1] NCCL INFO Channel 07/0 : 41[1] -> 40[0] via P2P/CUMEM
 4: nid006499:254558:257328 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/CUMEM
12: nid006508:205767:208661 [1] NCCL INFO Channel 07/0 : 49[1] -> 48[0] via P2P/CUMEM
11: nid006507:211337:214097 [2] NCCL INFO Channel 04/0 : 46[2] -> 45[1] via P2P/CUMEM
 1: nid006496:242585:245399 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
12: nid006508:205768:208659 [2] NCCL INFO Channel 05/0 : 50[2] -> 49[1] via P2P/CUMEM
 8: nid006503:218411:221181 [1] NCCL INFO Channel 07/0 : 33[1] -> 32[0] via P2P/CUMEM
 9: nid006505:249081:251878 [1] NCCL INFO Channel 06/0 : 37[1] -> 36[0] via P2P/CUMEM
11: nid006507:211336:214100 [1] NCCL INFO Channel 06/0 : 45[1] -> 44[0] via P2P/CUMEM
 1: nid006496:242587:245398 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
 4: nid006499:254557:257331 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 04/0 : 125[1] -> 124[0] via P2P/CUMEM
 4: nid006499:254558:257328 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 00/0 : 126[2] -> 125[1] via P2P/CUMEM
 0: nid006495:241020:244133 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
 3: nid006498:226768:229517 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
 0: nid006495:241021:244130 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
31: nid007342:59768:63008 [1] NCCL INFO Channel 06/0 : 125[1] -> 124[0] via P2P/CUMEM
 2: nid006497:227578:230346 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
 1: nid006496:242585:245399 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
 0: nid006495:241020:244133 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
31: nid007342:59769:63009 [2] NCCL INFO Channel 04/0 : 126[2] -> 125[1] via P2P/CUMEM
 0: nid006495:241021:244130 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
 1: nid006496:242585:245399 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
 2: nid006497:227577:230347 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
 3: nid006498:226766:229518 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
 3: nid006498:226767:229516 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
 2: nid006497:227576:230348 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
 1: nid006496:242586:245400 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
 2: nid006497:227577:230347 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
20: nid006558:215467:218273 [0] NCCL INFO Connected all rings
20: nid006558:215470:218272 [3] NCCL INFO Connected all rings
20: nid006558:215469:218271 [2] NCCL INFO Connected all rings
20: nid006558:215468:218270 [1] NCCL INFO Connected all rings
19: nid006557:208417:211197 [0] NCCL INFO Connected all rings
19: nid006557:208418:211195 [1] NCCL INFO Connected all rings
19: nid006557:208420:211194 [3] NCCL INFO Connected all rings
19: nid006557:208419:211196 [2] NCCL INFO Connected all rings
13: nid006509:201788:204571 [0] NCCL INFO Connected all rings
13: nid006509:201791:204569 [3] NCCL INFO Connected all rings
13: nid006509:201789:204572 [1] NCCL INFO Connected all rings
13: nid006509:201790:204570 [2] NCCL INFO Connected all rings
21: nid006559:211123:213886 [0] NCCL INFO Connected all rings
21: nid006559:211126:213884 [3] NCCL INFO Connected all rings
21: nid006559:211124:213885 [1] NCCL INFO Connected all rings
21: nid006559:211125:213887 [2] NCCL INFO Connected all rings
18: nid006556:210777:213648 [3] NCCL INFO Connected all rings
18: nid006556:210774:213649 [0] NCCL INFO Connected all rings
18: nid006556:210776:213650 [2] NCCL INFO Connected all rings
18: nid006556:210775:213647 [1] NCCL INFO Connected all rings
27: nid006566:216746:219525 [0] NCCL INFO Connected all rings
27: nid006566:216749:219522 [3] NCCL INFO Connected all rings
27: nid006566:216748:219524 [2] NCCL INFO Connected all rings
27: nid006566:216747:219523 [1] NCCL INFO Connected all rings
17: nid006555:206593:209422 [0] NCCL INFO Connected all rings
17: nid006555:206596:209420 [3] NCCL INFO Connected all rings
17: nid006555:206595:209421 [2] NCCL INFO Connected all rings
17: nid006555:206594:209419 [1] NCCL INFO Connected all rings
28: nid007251:72169:75267 [0] NCCL INFO Connected all rings
29: nid007305:27222:29926 [0] NCCL INFO Connected all rings
28: nid007251:72172:75266 [3] NCCL INFO Connected all rings
29: nid007305:27225:29928 [3] NCCL INFO Connected all rings
28: nid007251:72171:75269 [2] NCCL INFO Connected all rings
29: nid007305:27224:29927 [2] NCCL INFO Connected all rings
28: nid007251:72170:75268 [1] NCCL INFO Connected all rings
29: nid007305:27223:29929 [1] NCCL INFO Connected all rings
12: nid006508:205766:208660 [0] NCCL INFO Connected all rings
11: nid006507:211335:214099 [0] NCCL INFO Connected all rings
12: nid006508:205767:208661 [1] NCCL INFO Connected all rings
11: nid006507:211338:214098 [3] NCCL INFO Connected all rings
12: nid006508:205769:208662 [3] NCCL INFO Connected all rings
11: nid006507:211337:214097 [2] NCCL INFO Connected all rings
11: nid006507:211336:214100 [1] NCCL INFO Connected all rings
12: nid006508:205768:208659 [2] NCCL INFO Connected all rings
16: nid006554:221581:224396 [0] NCCL INFO Connected all rings
16: nid006554:221584:224395 [3] NCCL INFO Connected all rings
15: nid006553:223926:226709 [3] NCCL INFO Connected all rings
15: nid006553:223923:226710 [0] NCCL INFO Connected all rings
15: nid006553:223924:226708 [1] NCCL INFO Connected all rings
16: nid006554:221583:224394 [2] NCCL INFO Connected all rings
14: nid006510:229442:232235 [0] NCCL INFO Connected all rings
15: nid006553:223925:226707 [2] NCCL INFO Connected all rings
16: nid006554:221582:224397 [1] NCCL INFO Connected all rings
14: nid006510:229445:232234 [3] NCCL INFO Connected all rings
14: nid006510:229443:232232 [1] NCCL INFO Connected all rings
14: nid006510:229444:232233 [2] NCCL INFO Connected all rings
22: nid006560:222271:225068 [0] NCCL INFO Connected all rings
22: nid006560:222274:225065 [3] NCCL INFO Connected all rings
22: nid006560:222273:225067 [2] NCCL INFO Connected all rings
22: nid006560:222272:225066 [1] NCCL INFO Connected all rings
10: nid006506:263727:266505 [0] NCCL INFO Connected all rings
10: nid006506:263730:266504 [3] NCCL INFO Connected all rings
10: nid006506:263729:266503 [2] NCCL INFO Connected all rings
10: nid006506:263728:266502 [1] NCCL INFO Connected all rings
26: nid006565:222468:225280 [3] NCCL INFO Connected all rings
26: nid006565:222465:225281 [0] NCCL INFO Connected all rings
26: nid006565:222467:225279 [2] NCCL INFO Connected all rings
26: nid006565:222466:225278 [1] NCCL INFO Connected all rings
24: nid006563:221140:223930 [0] NCCL INFO Connected all rings
25: nid006564:223021:225771 [0] NCCL INFO Connected all rings
25: nid006564:223024:225768 [3] NCCL INFO Connected all rings
24: nid006563:221141:223928 [1] NCCL INFO Connected all rings
24: nid006563:221143:223929 [3] NCCL INFO Connected all rings
23: nid006561:220714:223497 [3] NCCL INFO Connected all rings
25: nid006564:223022:225769 [1] NCCL INFO Connected all rings
23: nid006561:220711:223500 [0] NCCL INFO Connected all rings
25: nid006564:223023:225770 [2] NCCL INFO Connected all rings
24: nid006563:221142:223931 [2] NCCL INFO Connected all rings
23: nid006561:220713:223499 [2] NCCL INFO Connected all rings
23: nid006561:220712:223498 [1] NCCL INFO Connected all rings
 6: nid006501:221944:224734 [3] NCCL INFO Connected all rings
30: nid007318:20580:23385 [0] NCCL INFO Connected all rings
30: nid007318:20583:23382 [3] NCCL INFO Connected all rings
30: nid007318:20582:23383 [2] NCCL INFO Connected all rings
30: nid007318:20581:23381 [1] NCCL INFO Connected all rings
 5: nid006500:260086:262858 [3] NCCL INFO Connected all rings
 5: nid006500:260083:262861 [0] NCCL INFO Connected all rings
 3: nid006498:226768:229517 [3] NCCL INFO Connected all rings
 3: nid006498:226765:229515 [0] NCCL INFO Connected all rings
 4: nid006499:254556:257329 [0] NCCL INFO Connected all rings
 5: nid006500:260084:262860 [1] NCCL INFO Connected all rings
 4: nid006499:254559:257330 [3] NCCL INFO Connected all rings
 5: nid006500:260085:262859 [2] NCCL INFO Connected all rings
 4: nid006499:254557:257331 [1] NCCL INFO Connected all rings
 4: nid006499:254558:257328 [2] NCCL INFO Connected all rings
 3: nid006498:226766:229518 [1] NCCL INFO Connected all rings
 3: nid006498:226767:229516 [2] NCCL INFO Connected all rings
 6: nid006501:221941:224736 [0] NCCL INFO Connected all rings
 2: nid006497:227575:230349 [0] NCCL INFO Connected all rings
 6: nid006501:221943:224735 [2] NCCL INFO Connected all rings
 6: nid006501:221942:224733 [1] NCCL INFO Connected all rings
 2: nid006497:227576:230348 [1] NCCL INFO Connected all rings
 2: nid006497:227578:230346 [3] NCCL INFO Connected all rings
 2: nid006497:227577:230347 [2] NCCL INFO Connected all rings
 9: nid006505:249080:251877 [0] NCCL INFO Connected all rings
 9: nid006505:249083:251879 [3] NCCL INFO Connected all rings
 8: nid006503:218410:221180 [0] NCCL INFO Connected all rings
 8: nid006503:218413:221179 [3] NCCL INFO Connected all rings
 9: nid006505:249082:251876 [2] NCCL INFO Connected all rings
20: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
20:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
20: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
20:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
20: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
20:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
20: /usr/local/lib/python3.10/dist-packages/torch/utils/checkp
 8: nid006503:218412:221178 [2] NCCL INFO Connected all rings
 9: nid006505:249081:251878 [1] NCCL INFO Connected all rings
20: oint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
20:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 8: nid006503:218411:221181 [1] NCCL INFO Connected all rings
13: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
13:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
19: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
19:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
19: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
19:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
21: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
21:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
21: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
21:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
18: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
18:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
18: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
18:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
13: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
13:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
27: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
27:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
27: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
27:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
28: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
28:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
28: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
28:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
13: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
13:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
13: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
13:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
17: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
17:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
17: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
17:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
17: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
17:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
18: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
18:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
29: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
29:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
29: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
29:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
21: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
21:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
12: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
12:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
19: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
19:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
19: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
19:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
18: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
18:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
21: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
21:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
28: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
28:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
12: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
12:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
15: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
15:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
15: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
15:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
11: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
11:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
11: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
11:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
27: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
27:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
28: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
28:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
27: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
27:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
17: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
17:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
16: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
16:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
16: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
16:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
22: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
22:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
29: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
29:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
15: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
15:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
12: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
12:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
12: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
12:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
29: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
29:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
26: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
26:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
26: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
26:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
10: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
10:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
10: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
10:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
10: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
10:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
11: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
11:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
11: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
11:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
14: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
14:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
14: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
14:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
25: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
25:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
25: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
25:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
25: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
25:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
26: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
26:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
16: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
16:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
16: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
16:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
10: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
10:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
15: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
15:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
24: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
24:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
24: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
24:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
22: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
22:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
22: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
22:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
22: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
22:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
24: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
24:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
24: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
24:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
26: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
26:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
23: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
23:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
23: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
23:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 5: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 5:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 5: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 5:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 6: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 6:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
14: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
14:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
14: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
14:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
30: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
30:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
25: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
25:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
30: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
30:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 4: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 4:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 4: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 4:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 5: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 5:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
23: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
23:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
23: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
23:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 5: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 5:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 4: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 4:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 4: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 4:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
30: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
30:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
30: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
30:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 6: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 6:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 6: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 6:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 6: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 6:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 9: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 9:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 9: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 9:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 8: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 8:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 8: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 8:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 8: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 8:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 9: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 9:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 9: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 9:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 8: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 8:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 7: nid006502:252584:255377 [0] NCCL INFO Connected all rings
 7: nid006502:252587:255375 [3] NCCL INFO Connected all rings
 7: nid006502:252586:255376 [2] NCCL INFO Connected all rings
 7: nid006502:252585:255374 [1] NCCL INFO Connected all rings
 7: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 7:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 7: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 7:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 7: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 7:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 7: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 7:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 1: nid006496:242587:245398 [3] NCCL INFO Connected all rings
 1: nid006496:242584:245401 [0] NCCL INFO Connected all rings
 1: nid006496:242585:245399 [1] NCCL INFO Connected all rings
 1: nid006496:242586:245400 [2] NCCL INFO Connected all rings
 0: nid006495:241019:244132 [0] NCCL INFO Connected all rings
31: nid007342:59770:63007 [3] NCCL INFO Connected all rings
 0: nid006495:241022:244131 [3] NCCL INFO Connected all rings
31: nid007342:59767:63010 [0] NCCL INFO Connected all rings
 0: nid006495:241021:244130 [2] NCCL INFO Connected all rings
 0: nid006495:241020:244133 [1] NCCL INFO Connected all rings
31: nid007342:59769:63009 [2] NCCL INFO Connected all rings
31: nid007342:59768:63008 [1] NCCL INFO Connected all rings
 1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
31: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
31:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
31: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
31:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
31: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
31:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
31: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
31:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 72[0] -> 73[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 00/0 : 54[2] -> 55[3] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[1] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 02/0 : 72[0] -> 73[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 02/0 : 54[2] -> 55[3] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[1] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[1] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[1] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 03/0 : 72[0] -> 73[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 03/0 : 54[2] -> 55[3] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[1] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[1] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 112[0] -> 113[1] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 00/0 : 84[0] -> 85[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 04/0 : 54[2] -> 55[3] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 00/0 : 86[2] -> 87[3] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 80[0] -> 81[1] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[1] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[1] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 02/0 : 86[2] -> 87[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 05/0 : 72[0] -> 73[1] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 05/0 : 32[0] -> 33[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[1] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 02/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 03/0 : 86[2] -> 87[3] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 02/0 : 112[0] -> 113[1] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[1] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 54[2] -> 55[3] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 03/0 : 112[0] -> 113[1] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 01/0 : 106[2] -> 107[3] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 03/0 : 84[0] -> 85[1] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 04/0 : 86[2] -> 87[3] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 64[0] -> 65[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 06/0 : 72[0] -> 73[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[1] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 106[2] -> 107[3] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 104[0] -> 105[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[1] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 84[0] -> 85[1] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 06/0 : 32[0] -> 33[1] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 86[2] -> 87[3] via P2P/CUMEM
10: nid006506:263728:266528 [1] NCCL INFO Channel 01/0 : 41[1] -> 42[2] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
 8: nid006503:218411:221186 [1] NCCL INFO Channel 01/0 : 33[1] -> 34[2] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 02/0 : 80[0] -> 81[1] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 00/0 : 100[0] -> 101[1] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 01/0 : 34[2] -> 35[3] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 05/0 : 112[0] -> 113[1] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 01/0 : 82[2] -> 83[3] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 00/0 : 102[2] -> 103[3] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[1] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 54[2] -> 55[3] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 06/0 : 84[0] -> 85[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 86[2] -> 87[3] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[1] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 106[2] -> 107[3] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 88[0] -> 89[1] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 01/0 : 74[2] -> 75[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 07/0 : 72[0] -> 73[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
10: nid006506:263728:266528 [1] NCCL INFO Channel 05/0 : 41[1] -> 42[2] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 05/0 : 48[0] -> 49[1] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 00/0 : 62[2] -> 63[3] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[1] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 60[0] -> 61[1] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 07/0 : 32[0] -> 33[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 36[0] -> 37[1] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 00/0 : 38[2] -> 39[3] via P2P/CUMEM
 8: nid006503:218411:221186 [1] NCCL INFO Channel 05/0 : 33[1] -> 34[2] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 34[2] -> 35[3] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 02/0 : 102[2] -> 103[3] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[1] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 03/0 : 80[0] -> 81[1] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 74[2] -> 75[3] via P2P/CUMEM
20: nid006558:215468:218296 [1] NCCL INFO Channel 01/0 : 81[1] -> 82[2] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 96[0] -> 97[1] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 82[2] -> 83[3] via P2P/CUMEM
18: nid006556:210775:213696 [1] NCCL INFO Channel 01/0 : 73[1] -> 74[2] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 06/0 : 112[0] -> 113[1] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 02/0 : 104[0] -> 105[1] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 02/0 : 55[3] -> 52[0] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[1] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[1] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 01/0 : 50[2] -> 51[3] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 74[2] -> 75[3] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 07/0 : 84[0] -> 85[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 18[2] -> 19[3] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 00/0 : 46[2] -> 47[3] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 02/0 : 100[0] -> 101[1] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 06/0 : 48[0] -> 49[1] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 52[0] -> 53[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 06/0 : 36[0] -> 37[1] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 02/0 : 62[2] -> 63[3] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[1] via P2P/CUMEM
18: nid006556:210775:213696 [1] NCCL INFO Channel 05/0 : 73[1] -> 74[2] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 05/0 : 106[2] -> 107[3] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 06/0 : 60[0] -> 61[1] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 03/0 : 55[3] -> 52[0] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 02/0 : 38[2] -> 39[3] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 05/0 : 74[2] -> 75[3] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 34[2] -> 35[3] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 05/0 : 80[0] -> 81[1] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
20: nid006558:215468:218296 [1] NCCL INFO Channel 05/0 : 81[1] -> 82[2] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 01/0 : 66[2] -> 67[3] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[1] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 00/0 : 76[0] -> 77[1] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 07/0 : 112[0] -> 113[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 01/0 : 42[2] -> 43[3] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 82[2] -> 83[3] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 03/0 : 104[0] -> 105[1] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 06/0 : 52[0] -> 53[1] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 66[2] -> 67[3] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 06/0 : 74[2] -> 75[3] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 04/0 : 32[0] -> 36[0] [send] via NET/AWS Libfabric/0
31: nid007342:59769:63053 [2] NCCL INFO Channel 00/0 : 126[2] -> 127[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 01/0 : 114[2] -> 115[3] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 03/0 : 102[2] -> 103[3] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 06/0 : 55[3] -> 52[0] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 50[2] -> 51[3] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 05/0 : 34[2] -> 35[3] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 44[0] -> 45[1] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 00/0 : 116[0] -> 117[1] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 02/0 : 88[0] -> 89[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 07/0 : 74[2] -> 75[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 05/0 : 40[0] -> 41[1] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 01/0 : 90[2] -> 91[3] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[1] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 07/0 : 48[0] -> 49[1] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 66[2] -> 67[3] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 03/0 : 62[2] -> 63[3] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 02/0 : 96[0] -> 97[1] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 07/0 : 36[0] -> 37[1] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 03/0 : 100[0] -> 101[1] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 02/0 : 46[2] -> 47[3] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 26[2] -> 27[3] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 03/0 : 38[2] -> 39[3] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 06/0 : 106[2] -> 107[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[1] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 07/0 : 60[0] -> 61[1] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 06/0 : 80[0] -> 81[1] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 00/0 : 30[2] -> 31[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 07/0 : 52[0] -> 53[1] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 05/0 : 68[0] -> 72[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 00/0 : 124[0] -> 125[1] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 02/0 : 64[0] -> 65[1] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 00/0 : 22[2] -> 23[3] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 01/0 : 58[2] -> 59[3] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 06/0 : 34[2] -> 35[3] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 05/0 : 66[2] -> 67[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 00/0 : 72[0] -> 76[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 28[0] -> 29[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 42[2] -> 43[3] via P2P/CUMEM
13: nid006509:201789:204594 [1] NCCL INFO Channel 00/0 : 53[1] -> 54[2] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 07/0 : 55[3] -> 52[0] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 02/0 : 76[0] -> 77[1] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 06/0 : 40[0] -> 41[1] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 05/0 : 104[0] -> 105[1] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 05/0 : 82[2] -> 83[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 07/0 : 34[2] -> 35[3] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 02/0 : 75[3] -> 72[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 04/0 : 102[2] -> 103[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 114[2] -> 115[3] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
 4: nid006499:254557:257360 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 50[2] -> 51[3] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 01/0 : 98[2] -> 99[3] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 03/0 : 96[0] -> 97[1] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 04/0 : 62[2] -> 63[3] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 06/0 : 44[0] -> 45[1] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 07/0 : 80[0] -> 81[1] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205767:208688 [1] NCCL INFO Channel 01/0 : 49[1] -> 50[2] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 03/0 : 46[2] -> 47[3] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 07/0 : 70[2] -> 74[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201789:204594 [1] NCCL INFO Channel 04/0 : 53[1] -> 54[2] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[1] via P2P/CUMEM
 9: nid006505:249081:251902 [1] NCCL INFO Channel 00/0 : 37[1] -> 38[2] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 04/0 : 38[2] -> 39[3] via P2P/CUMEM
 4: nid006499:254557:257360 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 42[2] -> 43[3] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 100[0] -> 101[1] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 07/0 : 106[2] -> 107[3] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 02/0 : 116[0] -> 117[1] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 02/0 : 30[2] -> 31[3] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 03/0 : 64[0] -> 65[1] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 06/0 : 66[2] -> 67[3] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 02/0 : 22[2] -> 23[3] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 74[2] -> 78[2] [send] via NET/AWS Libfabric/2
18: nid006556:210777:213694 [3] NCCL INFO Channel 03/0 : 75[3] -> 72[0] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 06/0 : 82[2] -> 83[3] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 06/0 : 104[0] -> 105[1] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 07/0 : 40[0] -> 41[1] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 05/0 : 50[2] -> 51[3] via P2P/CUMEM
 6: nid006501:221942:224743 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/CUMEM
15: nid006553:223924:226752 [1] NCCL INFO Channel 00/0 : 61[1] -> 62[2] via P2P/CUMEM
18: nid006556:210775:213696 [1] NCCL INFO Channel 00/0 : 73[1] -> 72[0] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 03/0 : 76[0] -> 77[1] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[1] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 02/0 : 19[3] -> 16[0] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 58[2] -> 59[3] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 02/0 : 35[3] -> 32[0] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 62[2] -> 63[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 114[2] -> 115[3] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 06/0 : 28[0] -> 29[1] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 02/0 : 126[2] -> 127[3] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 03/0 : 126[2] -> 127[3] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 102[2] -> 103[3] via P2P/CUMEM
28: nid007251:72170:75297 [1] NCCL INFO Channel 01/0 : 113[1] -> 114[2] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
12: nid006508:205767:208688 [1] NCCL INFO Channel 05/0 : 49[1] -> 50[2] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 06/0 : 34[2] -> 38[2] [send] via NET/AWS Libfabric/2
24: nid006563:221140:223979 [0] NCCL INFO Channel 05/0 : 96[0] -> 97[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 05/0 : 42[2] -> 43[3] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 07/0 : 82[2] -> 83[3] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 03/0 : 19[3] -> 16[0] via P2P/CUMEM
 9: nid006505:249081:251902 [1] NCCL INFO Channel 04/0 : 37[1] -> 38[2] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 06/0 : 75[3] -> 72[0] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
18: nid006556:210775:213696 [1] NCCL INFO Channel 02/0 : 73[1] -> 72[0] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 07/0 : 44[0] -> 45[1] via P2P/CUMEM
 6: nid006501:221942:224743 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 03/0 : 35[3] -> 32[0] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 07/0 : 66[2] -> 67[3] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 03/0 : 30[2] -> 31[3] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 98[2] -> 99[3] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 38[2] -> 39[3] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 06/0 : 50[2] -> 51[3] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 48[0] -> 52[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 05/0 : 64[0] -> 65[1] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 05/0 : 56[0] -> 57[1] via P2P/CUMEM
15: nid006553:223924:226752 [1] NCCL INFO Channel 04/0 : 61[1] -> 62[2] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 04/0 : 46[2] -> 47[3] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 03/0 : 116[0] -> 117[1] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 07/0 : 104[0] -> 105[1] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 06/0 : 19[3] -> 16[0] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 04/0 : 80[0] -> 84[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:225817 [0] NCCL INFO Channel 06/0 : 100[0] -> 101[1] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 07/0 : 28[0] -> 29[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 50[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 62[2] -> 63[3] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 02/0 : 107[3] -> 104[0] via P2P/CUMEM
21: nid006559:211124:213935 [1] NCCL INFO Channel 00/0 : 85[1] -> 86[2] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 06/0 : 42[2] -> 43[3] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 07/0 : 50[2] -> 51[3] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 05/0 : 52[0] -> 56[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218411:221186 [1] NCCL INFO Channel 00/0 : 33[1] -> 32[0] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 07/0 : 75[3] -> 72[0] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 05/0 : 114[2] -> 115[3] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 03/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
18: nid006556:210775:213696 [1] NCCL INFO Channel 04/0 : 73[1] -> 72[0] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 06/0 : 35[3] -> 32[0] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 03/0 : 22[2] -> 23[3] via P2P/CUMEM
21: nid006559:211124:213935 [1] NCCL INFO Channel 04/0 : 85[1] -> 86[2] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 20[0] -> 21[1] via P2P/CUMEM
23: nid006561:220711:223529 [0] NCCL INFO Channel 00/0 : 92[0] -> 93[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 54[2] -> 58[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 58[2] -> 59[3] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 04/0 : 30[2] -> 31[3] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 00/0 : 78[2] -> 79[3] via P2P/CUMEM
28: nid007251:72170:75297 [1] NCCL INFO Channel 05/0 : 113[1] -> 114[2] via P2P/CUMEM
13: nid006509:201789:204594 [1] NCCL INFO Channel 01/0 : 53[1] -> 52[0] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 06/0 : 64[0] -> 65[1] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 02/0 : 87[3] -> 84[0] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 07/0 : 19[3] -> 16[0] via P2P/CUMEM
 7: nid006502:252585:255424 [1] NCCL INFO Channel 00/0 : 29[1] -> 30[2] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
20: nid006558:215470:218299 [3] NCCL INFO Channel 02/0 : 83[3] -> 80[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 102[2] -> 103[3] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 38[2] -> 39[3] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 46[2] -> 47[3] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
26: nid006565:222466:225311 [1] NCCL INFO Channel 01/0 : 105[1] -> 106[2] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 06/0 : 96[0] -> 97[1] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 03/0 : 87[3] -> 84[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 120[0] -> 121[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 07/0 : 42[2] -> 43[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 05/0 : 36[0] -> 40[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201789:204594 [1] NCCL INFO Channel 03/0 : 53[1] -> 52[0] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 02/0 : 67[3] -> 64[0] via P2P/CUMEM
 3: nid006498:226766:229564 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218411:221186 [1] NCCL INFO Channel 02/0 : 33[1] -> 32[0] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 02/0 : 63[3] -> 60[0] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 02/0 : 27[3] -> 24[0] via P2P/CUMEM
18: nid006556:210775:213696 [1] NCCL INFO Channel 06/0 : 73[1] -> 72[0] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 98[2] -> 99[3] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 03/0 : 107[3] -> 104[0] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 06/0 : 82[2] -> 86[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:244159 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 07/0 : 100[0] -> 101[1] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 00/0 : 94[2] -> 95[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 06/0 : 114[2] -> 115[3] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 32[0] -> 36[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 04/0 : 48[0] -> 52[0] [send] via NET/AWS Libfabric/0
13: nid006509:201789:204594 [1] NCCL INFO Channel 05/0 : 53[1] -> 52[0] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 06/0 : 56[0] -> 57[1] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 116[0] -> 117[1] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 02/0 : 51[3] -> 48[0] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 30[2] -> 31[3] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 04/0 : 22[2] -> 23[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [send] via NET/AWS Libfabric/0
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 46[2] -> 47[3] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252585:255424 [1] NCCL INFO Channel 04/0 : 29[1] -> 30[2] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 07/0 : 35[3] -> 32[0] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
20: nid006558:215470:218299 [3] NCCL INFO Channel 03/0 : 83[3] -> 80[0] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 06/0 : 87[3] -> 84[0] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221581:224445 [0] NCCL INFO Channel 07/0 : 64[0] -> 65[1] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 06/0 : 20[0] -> 21[1] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 80[0] -> 84[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254557:257360 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 05/0 : 36[0] -> 40[0] [send] via NET/AWS Libfabric/0
11: nid006507:211336:214150 [1] NCCL INFO Channel 00/0 : 45[1] -> 46[2] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 06/0 : 50[2] -> 54[2] [send] via NET/AWS Libfabric/2
20: nid006558:215468:218296 [1] NCCL INFO Channel 00/0 : 81[1] -> 80[0] via P2P/CUMEM
26: nid006565:222466:225311 [1] NCCL INFO Channel 05/0 : 105[1] -> 106[2] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 07/0 : 87[3] -> 84[0] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 07/0 : 96[0] -> 97[1] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 06/0 : 107[3] -> 104[0] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 01/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 82[2] -> 86[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 02/0 : 58[2] -> 62[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201789:204594 [1] NCCL INFO Channel 07/0 : 53[1] -> 52[0] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 05/0 : 58[2] -> 59[3] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 02/0 : 78[2] -> 79[3] via P2P/CUMEM
 4: nid006499:254557:257360 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 03/0 : 27[3] -> 24[0] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 05/0 : 98[2] -> 99[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 07/0 : 114[2] -> 115[3] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 05/0 : 84[0] -> 88[0] [send] via NET/AWS Libfabric/0
15: nid006553:223926:226754 [3] NCCL INFO Channel 03/0 : 63[3] -> 60[0] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249083:251905 [3] NCCL INFO Channel 02/0 : 39[3] -> 36[0] via P2P/CUMEM
24: nid006563:221141:223977 [1] NCCL INFO Channel 01/0 : 97[1] -> 98[2] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 03/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
15: nid006553:223924:226752 [1] NCCL INFO Channel 01/0 : 61[1] -> 60[0] via P2P/CUMEM
11: nid006507:211336:214150 [1] NCCL INFO Channel 04/0 : 45[1] -> 46[2] via P2P/CUMEM
 8: nid006503:218411:221186 [1] NCCL INFO Channel 04/0 : 33[1] -> 32[0] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 03/0 : 67[3] -> 64[0] via P2P/CUMEM
 3: nid006498:226766:229564 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 03/0 : 51[3] -> 48[0] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 30[2] -> 31[3] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
 4: nid006499:254557:257360 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 02/0 : 43[3] -> 40[0] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 22[2] -> 23[3] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 42[2] -> 50[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222468:225309 [3] NCCL INFO Channel 07/0 : 107[3] -> 104[0] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 86[2] -> 90[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 36[0] -> 44[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 04/0 : 112[0] -> 116[0] [send] via NET/AWS Libfabric/0
20: nid006558:215470:218299 [3] NCCL INFO Channel 06/0 : 83[3] -> 80[0] via P2P/CUMEM
 4: nid006499:254557:257360 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/CUMEM
20: nid006558:215468:218296 [1] NCCL INFO Channel 02/0 : 81[1] -> 80[0] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 07/0 : 20[0] -> 21[1] via P2P/CUMEM
23: nid006561:220711:223529 [0] NCCL INFO Channel 02/0 : 92[0] -> 93[1] via P2P/CUMEM
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 07/0 : 56[0] -> 57[1] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 02/0 : 47[3] -> 44[0] via P2P/CUMEM
 1: nid006496:242585:245430 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249083:251905 [3] NCCL INFO Channel 03/0 : 39[3] -> 36[0] via P2P/CUMEM
 9: nid006505:249080:251903 [0] NCCL INFO Channel 05/0 : 36[0] -> 44[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 72[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218411:221186 [1] NCCL INFO Channel 06/0 : 33[1] -> 32[0] via P2P/CUMEM
 2: nid006497:227576:230385 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 06/0 : 98[2] -> 99[3] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 34[2] -> 38[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 42[2] -> 50[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221141:223977 [1] NCCL INFO Channel 05/0 : 97[1] -> 98[2] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 06/0 : 63[3] -> 60[0] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 50[2] -> 58[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221944:224741 [3] NCCL INFO Channel 06/0 : 27[3] -> 24[0] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 72[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221942:224743 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
25: nid006564:223022:225818 [1] NCCL INFO Channel 00/0 : 101[1] -> 102[2] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 03/0 : 78[2] -> 79[3] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 03/0 : 43[3] -> 40[0] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 03/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
16: nid006554:221584:224444 [3] NCCL INFO Channel 06/0 : 67[3] -> 64[0] via P2P/CUMEM
21: nid006559:211124:213935 [1] NCCL INFO Channel 01/0 : 85[1] -> 84[0] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 06/0 : 116[0] -> 117[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 07/0 : 38[2] -> 42[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223924:226752 [1] NCCL INFO Channel 03/0 : 61[1] -> 60[0] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 03/0 : 47[3] -> 44[0] via P2P/CUMEM
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 76[0] -> 77[1] via P2P/CUMEM
 0: nid006495:241020:244162 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 38[2] -> 42[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249081:251902 [1] NCCL INFO Channel 01/0 : 37[1] -> 36[0] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 50[2] -> 58[2] [send] via NET/AWS Libfabric/2
12: nid006508:205769:208690 [3] NCCL INFO Channel 06/0 : 51[3] -> 48[0] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 80[0] -> 88[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211124:213935 [1] NCCL INFO Channel 03/0 : 85[1] -> 84[0] via P2P/CUMEM
12: nid006508:205767:208688 [1] NCCL INFO Channel 00/0 : 49[1] -> 48[0] via P2P/CUMEM
 1: nid006496:242585:245430 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 74[2] -> 82[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260084:262913 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[2] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 42[2] -> 46[2] [send] via NET/AWS Libfabric/2
17: nid006555:206593:209444 [0] NCCL INFO Channel 00/0 : 68[0] -> 69[1] via P2P/CUMEM
20: nid006558:215470:218299 [3] NCCL INFO Channel 07/0 : 83[3] -> 80[0] via P2P/CUMEM
25: nid006564:223022:225818 [1] NCCL INFO Channel 04/0 : 101[1] -> 102[2] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 22[2] -> 23[3] via P2P/CUMEM
28: nid007251:72172:75299 [3] NCCL INFO Channel 02/0 : 115[3] -> 112[0] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 06/0 : 39[3] -> 36[0] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 06/0 : 58[2] -> 59[3] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 07/0 : 98[2] -> 99[3] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 00/0 : 118[2] -> 119[3] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
20: nid006558:215468:218296 [1] NCCL INFO Channel 04/0 : 81[1] -> 80[0] via P2P/CUMEM
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 80[0] -> 88[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 74[2] -> 82[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 02/0 : 94[2] -> 95[3] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 06/0 : 47[3] -> 44[0] via P2P/CUMEM
26: nid006565:222465:225310 [0] NCCL INFO Channel 05/0 : 100[0] -> 104[0] [receive] via NET/AWS Libfabric/0
21: nid006559:211124:213935 [1] NCCL INFO Channel 05/0 : 85[1] -> 84[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 02/0 : 120[0] -> 121[1] via P2P/CUMEM
 2: nid006497:227576:230385 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
16: nid006554:221582:224447 [1] NCCL INFO Channel 01/0 : 65[1] -> 66[2] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263728:266528 [1] NCCL INFO Channel 00/0 : 41[1] -> 40[0] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 07/0 : 63[3] -> 60[0] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 07/0 : 67[3] -> 64[0] via P2P/CUMEM
25: nid006564:223024:225819 [3] NCCL INFO Channel 02/0 : 103[3] -> 100[0] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 06/0 : 114[2] -> 118[2] [send] via NET/AWS Libfabric/2
10: nid006506:263730:266531 [3] NCCL INFO Channel 06/0 : 43[3] -> 40[0] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 07/0 : 27[3] -> 24[0] via P2P/CUMEM
 0: nid006495:241020:244162 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
 7: nid006502:252587:255422 [3] NCCL INFO Channel 02/0 : 31[3] -> 28[0] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 90[2] -> 91[3] via P2P/CUMEM
15: nid006553:223924:226752 [1] NCCL INFO Channel 05/0 : 61[1] -> 60[0] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 02/0 : 42[2] -> 46[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221942:224743 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 82[2] -> 90[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 90[2] -> 91[3] via P2P/CUMEM
 9: nid006505:249081:251902 [1] NCCL INFO Channel 03/0 : 37[1] -> 36[0] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260084:262913 [1] NCCL INFO Channel 04/0 : 21[1] -> 22[2] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 07/0 : 102[2] -> 106[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222465:225310 [0] NCCL INFO Channel 00/0 : 104[0] -> 108[0] [send] via NET/AWS Libfabric/0
21: nid006559:211124:213935 [1] NCCL INFO Channel 07/0 : 85[1] -> 84[0] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [send] via NET/AWS Libfabric/0
22: nid006560:222273:225096 [2] NCCL INFO Channel 05/0 : 90[2] -> 91[3] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 36[0] -> 44[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205769:208690 [3] NCCL INFO Channel 07/0 : 51[3] -> 48[0] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 82[2] -> 90[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223024:225819 [3] NCCL INFO Channel 03/0 : 103[3] -> 100[0] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222272:225097 [1] NCCL INFO Channel 01/0 : 89[1] -> 90[2] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 06/0 : 90[2] -> 91[3] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 38[2] -> 46[2] [send] via NET/AWS Libfabric/2
12: nid006508:205767:208688 [1] NCCL INFO Channel 02/0 : 49[1] -> 48[0] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 28[0] -> 44[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 04/0 : 96[0] -> 100[0] [send] via NET/AWS Libfabric/0
26: nid006565:222466:225311 [1] NCCL INFO Channel 00/0 : 105[1] -> 104[0] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [send] via NET/AWS Libfabric/0
22: nid006560:222272:225097 [1] NCCL INFO Channel 05/0 : 89[1] -> 90[2] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 07/0 : 39[3] -> 36[0] via P2P/CUMEM
20: nid006558:215468:218296 [1] NCCL INFO Channel 06/0 : 81[1] -> 80[0] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 18[2] -> 34[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72172:75299 [3] NCCL INFO Channel 03/0 : 115[3] -> 112[0] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 07/0 : 90[2] -> 91[3] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 38[2] -> 46[2] [send] via NET/AWS Libfabric/2
28: nid007251:72170:75297 [1] NCCL INFO Channel 00/0 : 113[1] -> 112[0] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 07/0 : 116[0] -> 117[1] via P2P/CUMEM
22: nid006560:222274:225095 [3] NCCL INFO Channel 02/0 : 91[3] -> 88[0] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 02/0 : 99[3] -> 96[0] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 106[2] -> 110[2] [send] via NET/AWS Libfabric/2
22: nid006560:222274:225095 [3] NCCL INFO Channel 03/0 : 91[3] -> 88[0] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 07/0 : 47[3] -> 44[0] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242587:245427 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 34[2] -> 50[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 42[2] -> 50[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211336:214150 [1] NCCL INFO Channel 01/0 : 45[1] -> 44[0] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 28[0] -> 44[0] [send] via NET/AWS Libfabric/0
25: nid006564:223024:225819 [3] NCCL INFO Channel 06/0 : 103[3] -> 100[0] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 04/0 : 78[2] -> 79[3] via P2P/CUMEM
26: nid006565:222466:225311 [1] NCCL INFO Channel 02/0 : 105[1] -> 104[0] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 07/0 : 86[2] -> 90[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 38[2] -> 46[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 36[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252587:255422 [3] NCCL INFO Channel 03/0 : 31[3] -> 28[0] via P2P/CUMEM
 7: nid006502:252585:255424 [1] NCCL INFO Channel 01/0 : 29[1] -> 28[0] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 07/0 : 58[2] -> 59[3] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 96[0] -> 100[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260086:262910 [3] NCCL INFO Channel 02/0 : 23[3] -> 20[0] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222274:225095 [3] NCCL INFO Channel 06/0 : 91[3] -> 88[0] via P2P/CUMEM
15: nid006553:223924:226752 [1] NCCL INFO Channel 07/0 : 61[1] -> 60[0] via P2P/CUMEM
 6: nid006501:221942:224743 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 34[2] -> 50[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:224447 [1] NCCL INFO Channel 05/0 : 65[1] -> 66[2] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 01/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 06/0 : 76[0] -> 77[1] via P2P/CUMEM
10: nid006506:263728:266528 [1] NCCL INFO Channel 02/0 : 41[1] -> 40[0] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 42[2] -> 50[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 90[2] -> 94[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249081:251902 [1] NCCL INFO Channel 05/0 : 37[1] -> 36[0] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241022:244161 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
14: nid006510:229443:232281 [1] NCCL INFO Channel 01/0 : 57[1] -> 58[2] via P2P/CUMEM
25: nid006564:223024:225819 [3] NCCL INFO Channel 07/0 : 103[3] -> 100[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 98[2] -> 102[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222466:225311 [1] NCCL INFO Channel 04/0 : 105[1] -> 104[0] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 07/0 : 43[3] -> 40[0] via P2P/CUMEM
22: nid006560:222274:225095 [3] NCCL INFO Channel 07/0 : 91[3] -> 88[0] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 38[2] -> 46[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 44[0] -> 52[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221942:224743 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 05/0 : 100[0] -> 104[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226768:229565 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 78[2] -> 86[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 02/0 : 118[2] -> 119[3] via P2P/CUMEM
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205767:208688 [1] NCCL INFO Channel 04/0 : 49[1] -> 48[0] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 03/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:223978 [2] NCCL INFO Channel 06/0 : 98[2] -> 102[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 03/0 : 94[2] -> 95[3] via P2P/CUMEM
28: nid007251:72172:75299 [3] NCCL INFO Channel 06/0 : 115[3] -> 112[0] via P2P/CUMEM
26: nid006565:222466:225311 [1] NCCL INFO Channel 06/0 : 105[1] -> 104[0] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 78[2] -> 86[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 01/0 : 122[2] -> 123[3] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 46[2] -> 54[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242584:245429 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260086:262910 [3] NCCL INFO Channel 03/0 : 23[3] -> 20[0] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 50[2] -> 42[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222271:225098 [0] NCCL INFO Channel 03/0 : 88[0] -> 89[1] via P2P/CUMEM
 9: nid006505:249081:251902 [1] NCCL INFO Channel 07/0 : 37[1] -> 36[0] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 44[0] -> 52[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241019:244160 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 102[2] -> 106[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72170:75297 [1] NCCL INFO Channel 02/0 : 113[1] -> 112[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 03/0 : 120[0] -> 121[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 50[2] -> 42[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211336:214150 [1] NCCL INFO Channel 03/0 : 45[1] -> 44[0] via P2P/CUMEM
12: nid006508:205767:208688 [1] NCCL INFO Channel 06/0 : 49[1] -> 48[0] via P2P/CUMEM
 7: nid006502:252585:255424 [1] NCCL INFO Channel 03/0 : 29[1] -> 28[0] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 80[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 44[0] -> 36[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 46[2] -> 54[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242585:245430 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
 7: nid006502:252587:255422 [3] NCCL INFO Channel 06/0 : 31[3] -> 28[0] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 03/0 : 99[3] -> 96[0] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 80[0] -> 96[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227575:230384 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 02/0 : 68[0] -> 69[1] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 30[2] -> 46[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260086:262910 [3] NCCL INFO Channel 06/0 : 23[3] -> 20[0] via P2P/CUMEM
10: nid006506:263728:266528 [1] NCCL INFO Channel 04/0 : 41[1] -> 40[0] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 46[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 30[2] -> 46[2] [send] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 100[0] -> 108[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 05/0 : 44[0] -> 36[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211336:214150 [1] NCCL INFO Channel 05/0 : 45[1] -> 44[0] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226768:229565 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
29: nid007305:27223:29970 [1] NCCL INFO Channel 00/0 : 117[1] -> 118[2] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 46[2] -> 38[2] [receive] via NET/AWS Libfabric/2
 0: nid006495:241021:244159 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
24: nid006563:221141:223977 [1] NCCL INFO Channel 00/0 : 97[1] -> 96[0] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 05/0 : 88[0] -> 89[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260083:262912 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 96[0] -> 112[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260085:262911 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 96[0] -> 112[0] [send] via NET/AWS Libfabric/0
28: nid007251:72172:75299 [3] NCCL INFO Channel 07/0 : 115[3] -> 112[0] via P2P/CUMEM
28: nid007251:72170:75297 [1] NCCL INFO Channel 04/0 : 113[1] -> 112[0] via P2P/CUMEM
14: nid006510:229443:232281 [1] NCCL INFO Channel 05/0 : 57[1] -> 58[2] via P2P/CUMEM
25: nid006564:223022:225818 [1] NCCL INFO Channel 01/0 : 101[1] -> 100[0] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 05/0 : 100[0] -> 108[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242585:245430 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 04/0 : 64[0] -> 68[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226766:229564 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
11: nid006507:211336:214150 [1] NCCL INFO Channel 07/0 : 45[1] -> 44[0] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 00/0 : 64[0] -> 0[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 06/0 : 66[2] -> 70[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226768:229565 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 03/0 : 118[2] -> 119[3] via P2P/CUMEM
 2: nid006497:227576:230385 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252585:255424 [1] NCCL INFO Channel 05/0 : 29[1] -> 28[0] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 82[2] -> 98[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263728:266528 [1] NCCL INFO Channel 06/0 : 41[1] -> 40[0] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 06/0 : 88[0] -> 89[1] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252587:255422 [3] NCCL INFO Channel 07/0 : 31[3] -> 28[0] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260086:262910 [3] NCCL INFO Channel 07/0 : 23[3] -> 20[0] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 78[2] -> 79[3] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 04/0 : 94[2] -> 95[3] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 07/0 : 76[0] -> 77[1] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 04/0 : 126[2] -> 127[3] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 102[2] -> 110[2] [send] via NET/AWS Libfabric/2
25: nid006564:223022:225818 [1] NCCL INFO Channel 03/0 : 101[1] -> 100[0] via P2P/CUMEM
23: nid006561:220711:223529 [0] NCCL INFO Channel 03/0 : 92[0] -> 93[1] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 06/0 : 126[2] -> 127[3] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 02/0 : 59[3] -> 56[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 102[2] -> 110[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:63053 [2] NCCL INFO Channel 07/0 : 126[2] -> 127[3] via P2P/CUMEM
 6: nid006501:221941:224742 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242585:245430 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 02/0 : 66[2] -> 2[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226766:229564 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
31: nid007342:59768:63054 [1] NCCL INFO Channel 00/0 : 125[1] -> 126[2] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 01/0 : 64[0] -> 0[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 82[2] -> 98[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59768:63054 [1] NCCL INFO Channel 04/0 : 125[1] -> 126[2] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 122[2] -> 123[3] via P2P/CUMEM
 2: nid006497:227576:230385 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 98[2] -> 114[2] [send] via NET/AWS Libfabric/2
31: nid007342:59770:63055 [3] NCCL INFO Channel 02/0 : 127[3] -> 124[0] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 07/0 : 88[0] -> 89[1] via P2P/CUMEM
 0: nid006495:241020:244162 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
 5: nid006500:260084:262913 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[0] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 06/0 : 99[3] -> 96[0] via P2P/CUMEM
31: nid007342:59770:63055 [3] NCCL INFO Channel 03/0 : 127[3] -> 124[0] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242585:245430 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
16: nid006554:221582:224447 [1] NCCL INFO Channel 00/0 : 65[1] -> 64[0] via P2P/CUMEM
25: nid006564:223022:225818 [1] NCCL INFO Channel 05/0 : 101[1] -> 100[0] via P2P/CUMEM
24: nid006563:221141:223977 [1] NCCL INFO Channel 02/0 : 97[1] -> 96[0] via P2P/CUMEM
31: nid007342:59770:63055 [3] NCCL INFO Channel 06/0 : 127[3] -> 124[0] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 03/0 : 68[0] -> 69[1] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 98[2] -> 114[2] [send] via NET/AWS Libfabric/2
31: nid007342:59769:63053 [2] NCCL INFO Channel 02/0 : 122[2] -> 126[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27223:29970 [1] NCCL INFO Channel 04/0 : 117[1] -> 118[2] via P2P/CUMEM
17: nid006555:206595:209445 [2] NCCL INFO Channel 00/0 : 70[2] -> 71[3] via P2P/CUMEM
 0: nid006495:241019:244160 [0] NCCL INFO Channel 00/0 : 0[0] -> 64[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241021:244159 [2] NCCL INFO Channel 03/0 : 66[2] -> 2[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 78[2] -> 79[3] via P2P/CUMEM
31: nid007342:59770:63055 [3] NCCL INFO Channel 07/0 : 127[3] -> 124[0] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 04/0 : 118[2] -> 119[3] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 05/0 : 120[0] -> 121[1] via P2P/CUMEM
 2: nid006497:227576:230385 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
 7: nid006502:252585:255424 [1] NCCL INFO Channel 07/0 : 29[1] -> 28[0] via P2P/CUMEM
 5: nid006500:260083:262912 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72170:75297 [1] NCCL INFO Channel 06/0 : 113[1] -> 112[0] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241020:244162 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 03/0 : 59[3] -> 56[0] via P2P/CUMEM
25: nid006564:223022:225818 [1] NCCL INFO Channel 07/0 : 101[1] -> 100[0] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226768:229565 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260084:262913 [1] NCCL INFO Channel 03/0 : 21[1] -> 20[0] via P2P/CUMEM
 3: nid006498:226766:229564 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227576:230385 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
16: nid006554:221582:224447 [1] NCCL INFO Channel 02/0 : 65[1] -> 64[0] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 05/0 : 52[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:244159 [2] NCCL INFO Channel 02/0 : 2[2] -> 66[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241019:244160 [0] NCCL INFO Channel 01/0 : 0[0] -> 64[0] [send] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 94[2] -> 95[3] via P2P/CUMEM
31: nid007342:59767:63056 [0] NCCL INFO Channel 02/0 : 124[0] -> 125[1] via P2P/CUMEM
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:245429 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241020:244162 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 06/0 : 59[3] -> 56[0] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:244159 [2] NCCL INFO Channel 03/0 : 2[2] -> 66[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260084:262913 [1] NCCL INFO Channel 05/0 : 21[1] -> 20[0] via P2P/CUMEM
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 92[0] -> 93[1] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 118[2] -> 119[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
24: nid006563:221143:223976 [3] NCCL INFO Channel 07/0 : 99[3] -> 96[0] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [send] via NET/AWS Libfabric/0
24: nid006563:221141:223977 [1] NCCL INFO Channel 04/0 : 97[1] -> 96[0] via P2P/CUMEM
16: nid006554:221582:224447 [1] NCCL INFO Channel 04/0 : 65[1] -> 64[0] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 07/0 : 54[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 68[0] -> 69[1] via P2P/CUMEM
 5: nid006500:260084:262913 [1] NCCL INFO Channel 07/0 : 21[1] -> 20[0] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 02/0 : 70[2] -> 71[3] via P2P/CUMEM
 0: nid006495:241020:244162 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
19: nid006557:208418:211248 [1] NCCL INFO Channel 00/0 : 77[1] -> 78[2] via P2P/CUMEM
31: nid007342:59767:63056 [0] NCCL INFO Channel 03/0 : 124[0] -> 125[1] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 58[2] -> 62[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221941:224742 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
16: nid006554:221582:224447 [1] NCCL INFO Channel 06/0 : 65[1] -> 64[0] via P2P/CUMEM
 3: nid006498:226766:229564 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227575:230384 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 118[2] -> 119[3] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 04/0 : 124[0] -> 125[1] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 122[2] -> 123[3] via P2P/CUMEM
27: nid006566:216746:219574 [0] NCCL INFO Channel 00/0 : 108[0] -> 109[1] via P2P/CUMEM
22: nid006560:222271:225098 [0] NCCL INFO Channel 05/0 : 84[0] -> 88[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 44[0] -> 52[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
14: nid006510:229445:232282 [3] NCCL INFO Channel 07/0 : 59[3] -> 56[0] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [send] via NET/AWS Libfabric/0
19: nid006557:208418:211248 [1] NCCL INFO Channel 04/0 : 77[1] -> 78[2] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 06/0 : 120[0] -> 121[1] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 05/0 : 44[0] -> 52[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 28[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 112[0] -> 116[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222272:225097 [1] NCCL INFO Channel 00/0 : 89[1] -> 88[0] via P2P/CUMEM
24: nid006563:221141:223977 [1] NCCL INFO Channel 06/0 : 97[1] -> 96[0] via P2P/CUMEM
19: nid006557:208420:211246 [3] NCCL INFO Channel 02/0 : 79[3] -> 76[0] via P2P/CUMEM
29: nid007305:27222:29972 [0] NCCL INFO Channel 01/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 00/0 : 88[0] -> 92[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 94[2] -> 95[3] via P2P/CUMEM
31: nid007342:59767:63056 [0] NCCL INFO Channel 06/0 : 124[0] -> 125[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 46[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 06/0 : 92[0] -> 93[1] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 28[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
17: nid006555:206593:209444 [0] NCCL INFO Channel 06/0 : 68[0] -> 69[1] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 05/0 : 116[0] -> 120[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 18[2] -> 34[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59767:63056 [0] NCCL INFO Channel 07/0 : 124[0] -> 125[1] via P2P/CUMEM
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222272:225097 [1] NCCL INFO Channel 02/0 : 89[1] -> 88[0] via P2P/CUMEM
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 52[0] -> 44[0] [send] via NET/AWS Libfabric/0
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 50[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 28[0] -> 44[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 46[2] -> 54[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 03/0 : 70[2] -> 71[3] via P2P/CUMEM
 1: nid006496:242584:245429 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 28[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
19: nid006557:208420:211246 [3] NCCL INFO Channel 03/0 : 79[3] -> 76[0] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 30[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 60[0] -> 92[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201788:204595 [0] NCCL INFO Channel 05/0 : 52[0] -> 44[0] [send] via NET/AWS Libfabric/0
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 50[2] -> 58[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229443:232281 [1] NCCL INFO Channel 00/0 : 57[1] -> 56[0] via P2P/CUMEM
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 76[0] -> 84[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222272:225097 [1] NCCL INFO Channel 04/0 : 89[1] -> 88[0] via P2P/CUMEM
23: nid006561:220712:223530 [1] NCCL INFO Channel 00/0 : 93[1] -> 94[2] via P2P/CUMEM
19: nid006557:208420:211246 [3] NCCL INFO Channel 06/0 : 79[3] -> 76[0] via P2P/CUMEM
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 104[0] -> 112[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 30[2] -> 62[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [receive] via NET/AWS Libfabric/0
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 54[2] -> 46[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260083:262912 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
21: nid006559:211123:213933 [0] NCCL INFO Channel 05/0 : 76[0] -> 84[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27225:29971 [3] NCCL INFO Channel 02/0 : 119[3] -> 116[0] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 60[0] -> 92[0] [send] via NET/AWS Libfabric/0
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 54[2] -> 46[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 104[0] -> 112[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20582:23432 [2] NCCL INFO Channel 05/0 : 122[2] -> 123[3] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 34[2] -> 18[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 62[2] -> 94[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 30[2] -> 46[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222272:225097 [1] NCCL INFO Channel 06/0 : 89[1] -> 88[0] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 44[0] -> 28[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 34[2] -> 50[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 58[2] -> 50[2] [send] via NET/AWS Libfabric/2
14: nid006510:229443:232281 [1] NCCL INFO Channel 02/0 : 57[1] -> 56[0] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 00/0 : 72[0] -> 76[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 112[0] -> 120[0] [send] via NET/AWS Libfabric/0
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 30[2] -> 46[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 58[2] -> 50[2] [send] via NET/AWS Libfabric/2
19: nid006557:208420:211246 [3] NCCL INFO Channel 07/0 : 79[3] -> 76[0] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 114[2] -> 118[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20581:23430 [1] NCCL INFO Channel 01/0 : 121[1] -> 122[2] via P2P/CUMEM
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 44[0] -> 28[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 07/0 : 92[0] -> 93[1] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 02/0 : 74[2] -> 78[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 112[0] -> 120[0] [send] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 62[2] -> 94[2] [send] via NET/AWS Libfabric/2
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 34[2] -> 50[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 02/0 : 108[0] -> 109[1] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 03/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 32[0] -> 64[0] [send] via NET/AWS Libfabric/0
23: nid006561:220712:223530 [1] NCCL INFO Channel 04/0 : 93[1] -> 94[2] via P2P/CUMEM
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 28[0] -> 60[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 32[0] -> 64[0] [send] via NET/AWS Libfabric/0
14: nid006510:229443:232281 [1] NCCL INFO Channel 04/0 : 57[1] -> 56[0] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 46[2] -> 30[2] [send] via NET/AWS Libfabric/2
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 28[0] -> 60[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 118[2] -> 122[2] [send] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 07/0 : 120[0] -> 121[1] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 68[0] -> 76[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 00/0 : 120[0] -> 124[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 07/0 : 68[0] -> 69[1] via P2P/CUMEM
17: nid006555:206595:209445 [2] NCCL INFO Channel 04/0 : 70[2] -> 71[3] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 03/0 : 119[3] -> 116[0] via P2P/CUMEM
23: nid006561:220714:223527 [3] NCCL INFO Channel 02/0 : 95[3] -> 92[0] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 30[2] -> 62[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 46[2] -> 30[2] [send] via NET/AWS Libfabric/2
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 50[2] -> 34[2] [send] via NET/AWS Libfabric/2
14: nid006510:229443:232281 [1] NCCL INFO Channel 06/0 : 57[1] -> 56[0] via P2P/CUMEM
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 68[0] -> 76[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27223:29970 [1] NCCL INFO Channel 01/0 : 117[1] -> 116[0] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 50[2] -> 34[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 30[2] -> 62[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 70[2] -> 78[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 76[0] -> 84[0] [send] via NET/AWS Libfabric/0
31: nid007342:59768:63054 [1] NCCL INFO Channel 01/0 : 125[1] -> 124[0] via P2P/CUMEM
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 60[0] -> 28[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 34[2] -> 66[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 70[2] -> 78[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 60[0] -> 28[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 76[0] -> 84[0] [send] via NET/AWS Libfabric/0
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 78[2] -> 86[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 106[2] -> 114[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 06/0 : 122[2] -> 123[3] via P2P/CUMEM
30: nid007318:20581:23430 [1] NCCL INFO Channel 05/0 : 121[1] -> 122[2] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 34[2] -> 66[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 106[2] -> 114[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59768:63054 [1] NCCL INFO Channel 03/0 : 125[1] -> 124[0] via P2P/CUMEM
19: nid006557:208418:211248 [1] NCCL INFO Channel 01/0 : 77[1] -> 76[0] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 62[2] -> 30[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 78[2] -> 86[2] [send] via NET/AWS Libfabric/2
23: nid006561:220714:223527 [3] NCCL INFO Channel 03/0 : 95[3] -> 92[0] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 114[2] -> 122[2] [send] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 00/0 : 88[0] -> 92[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 62[2] -> 30[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59768:63054 [1] NCCL INFO Channel 05/0 : 125[1] -> 124[0] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 06/0 : 119[3] -> 116[0] via P2P/CUMEM
29: nid007305:27223:29970 [1] NCCL INFO Channel 03/0 : 117[1] -> 116[0] via P2P/CUMEM
19: nid006557:208418:211248 [1] NCCL INFO Channel 03/0 : 77[1] -> 76[0] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 114[2] -> 122[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 02/0 : 90[2] -> 94[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59768:63054 [1] NCCL INFO Channel 07/0 : 125[1] -> 124[0] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 84[0] -> 76[0] [send] via NET/AWS Libfabric/0
30: nid007318:20582:23432 [2] NCCL INFO Channel 07/0 : 122[2] -> 123[3] via P2P/CUMEM
19: nid006557:208418:211248 [1] NCCL INFO Channel 05/0 : 77[1] -> 76[0] via P2P/CUMEM
21: nid006559:211123:213933 [0] NCCL INFO Channel 05/0 : 84[0] -> 76[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 76[0] -> 92[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 70[2] -> 71[3] via P2P/CUMEM
27: nid006566:216746:219574 [0] NCCL INFO Channel 03/0 : 108[0] -> 109[1] via P2P/CUMEM
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 76[0] -> 92[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 00/0 : 80[0] -> 88[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208418:211248 [1] NCCL INFO Channel 07/0 : 77[1] -> 76[0] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 07/0 : 119[3] -> 116[0] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 86[2] -> 78[2] [send] via NET/AWS Libfabric/2
29: nid007305:27223:29970 [1] NCCL INFO Channel 05/0 : 117[1] -> 116[0] via P2P/CUMEM
23: nid006561:220714:223527 [3] NCCL INFO Channel 06/0 : 95[3] -> 92[0] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 78[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 92[0] -> 108[0] [send] via NET/AWS Libfabric/0
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 86[2] -> 78[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 82[2] -> 90[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 80[0] -> 88[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 78[2] -> 94[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 92[0] -> 108[0] [send] via NET/AWS Libfabric/0
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 82[2] -> 90[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 94[2] -> 110[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 70[2] -> 71[3] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 02/0 : 123[3] -> 120[0] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 94[2] -> 110[2] [send] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 05/0 : 116[0] -> 120[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 00/0 : 88[0] -> 80[0] [send] via NET/AWS Libfabric/0
29: nid007305:27223:29970 [1] NCCL INFO Channel 07/0 : 117[1] -> 116[0] via P2P/CUMEM
23: nid006561:220714:223527 [3] NCCL INFO Channel 07/0 : 95[3] -> 92[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 00/0 : 120[0] -> 124[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 88[0] -> 80[0] [send] via NET/AWS Libfabric/0
23: nid006561:220712:223530 [1] NCCL INFO Channel 01/0 : 93[1] -> 92[0] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 90[2] -> 82[2] [send] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 108[0] -> 109[1] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 90[2] -> 82[2] [send] via NET/AWS Libfabric/2
30: nid007318:20583:23431 [3] NCCL INFO Channel 03/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 07/0 : 118[2] -> 122[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 108[0] -> 116[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 04/0 : 60[0] -> 124[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 05/0 : 108[0] -> 116[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220712:223530 [1] NCCL INFO Channel 03/0 : 93[1] -> 92[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 00/0 : 112[0] -> 120[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 05/0 : 60[0] -> 124[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 122[2] -> 126[2] [send] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 112[0] -> 120[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59767:63056 [0] NCCL INFO Channel 04/0 : 124[0] -> 60[0] [send] via NET/AWS Libfabric/0
23: nid006561:220712:223530 [1] NCCL INFO Channel 05/0 : 93[1] -> 92[0] via P2P/CUMEM
17: nid006555:206596:209446 [3] NCCL INFO Channel 02/0 : 71[3] -> 68[0] via P2P/CUMEM
31: nid007342:59767:63056 [0] NCCL INFO Channel 05/0 : 124[0] -> 60[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 00/0 : 110[2] -> 111[3] via P2P/CUMEM
30: nid007318:20581:23430 [1] NCCL INFO Channel 00/0 : 121[1] -> 120[0] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 06/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20580:23433 [0] NCCL INFO Channel 00/0 : 120[0] -> 112[0] [send] via NET/AWS Libfabric/0
23: nid006561:220712:223530 [1] NCCL INFO Channel 07/0 : 93[1] -> 92[0] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 110[2] -> 118[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 114[2] -> 122[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 110[2] -> 118[2] [receive] via NET/AWS Libfabric/2
31: nid007342:59769:63053 [2] NCCL INFO Channel 06/0 : 62[2] -> 126[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 120[0] -> 112[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 06/0 : 108[0] -> 109[1] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 114[2] -> 122[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206596:209446 [3] NCCL INFO Channel 03/0 : 71[3] -> 68[0] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 07/0 : 62[2] -> 126[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206594:209447 [1] NCCL INFO Channel 00/0 : 69[1] -> 70[2] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 06/0 : 126[2] -> 62[2] [send] via NET/AWS Libfabric/2
30: nid007318:20581:23430 [1] NCCL INFO Channel 02/0 : 121[1] -> 120[0] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 07/0 : 126[2] -> 62[2] [send] via NET/AWS Libfabric/2
30: nid007318:20583:23431 [3] NCCL INFO Channel 07/0 : 123[3] -> 120[0] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 122[2] -> 114[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 122[2] -> 114[2] [send] via NET/AWS Libfabric/2
17: nid006555:206596:209446 [3] NCCL INFO Channel 06/0 : 71[3] -> 68[0] via P2P/CUMEM
17: nid006555:206594:209447 [1] NCCL INFO Channel 04/0 : 69[1] -> 70[2] via P2P/CUMEM
27: nid006566:216748:219576 [2] NCCL INFO Channel 02/0 : 110[2] -> 111[3] via P2P/CUMEM
30: nid007318:20581:23430 [1] NCCL INFO Channel 04/0 : 121[1] -> 120[0] via P2P/CUMEM
27: nid006566:216746:219574 [0] NCCL INFO Channel 07/0 : 108[0] -> 109[1] via P2P/CUMEM
17: nid006555:206596:209446 [3] NCCL INFO Channel 07/0 : 71[3] -> 68[0] via P2P/CUMEM
30: nid007318:20581:23430 [1] NCCL INFO Channel 06/0 : 121[1] -> 120[0] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 64[0] -> 68[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 03/0 : 110[2] -> 111[3] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 01/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 66[2] -> 70[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216747:219573 [1] NCCL INFO Channel 00/0 : 109[1] -> 110[2] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 05/0 : 68[0] -> 72[0] [send] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 03/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 70[2] -> 74[2] [send] via NET/AWS Libfabric/2
17: nid006555:206594:209447 [1] NCCL INFO Channel 01/0 : 69[1] -> 68[0] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 32[0] -> 64[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 00/0 : 72[0] -> 80[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 68[0] -> 76[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 32[0] -> 64[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 04/0 : 110[2] -> 111[3] via P2P/CUMEM
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 72[0] -> 80[0] [send] via NET/AWS Libfabric/0
17: nid006555:206594:209447 [1] NCCL INFO Channel 03/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 05/0 : 68[0] -> 76[0] [send] via NET/AWS Libfabric/0
27: nid006566:216747:219573 [1] NCCL INFO Channel 04/0 : 109[1] -> 110[2] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 64[0] -> 96[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 34[2] -> 66[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 70[2] -> 78[2] [send] via NET/AWS Libfabric/2
17: nid006555:206594:209447 [1] NCCL INFO Channel 05/0 : 69[1] -> 68[0] via P2P/CUMEM
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 76[0] -> 68[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 80[0] -> 96[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 76[0] -> 92[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 00/0 : 80[0] -> 72[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 70[2] -> 78[2] [send] via NET/AWS Libfabric/2
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 80[0] -> 96[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 64[0] -> 96[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 34[2] -> 66[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206593:209444 [0] NCCL INFO Channel 05/0 : 76[0] -> 68[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206594:209447 [1] NCCL INFO Channel 07/0 : 69[1] -> 68[0] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 74[2] -> 82[2] [send] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 76[0] -> 92[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 66[2] -> 98[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 110[2] -> 111[3] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 74[2] -> 82[2] [send] via NET/AWS Libfabric/2
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 80[0] -> 72[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 66[2] -> 98[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 78[2] -> 70[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 78[2] -> 94[2] [send] via NET/AWS Libfabric/2
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 96[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 64[0] -> 32[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 92[0] -> 76[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 78[2] -> 70[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 78[2] -> 94[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 64[0] -> 32[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 96[0] -> 80[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 92[0] -> 76[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 66[2] -> 34[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 82[2] -> 74[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 110[2] -> 111[3] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 82[2] -> 98[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 66[2] -> 34[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 82[2] -> 74[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 94[2] -> 78[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 82[2] -> 98[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 94[2] -> 78[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 00/0 : 104[0] -> 108[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 98[2] -> 82[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 98[2] -> 82[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222465:225310 [0] NCCL INFO Channel 00/0 : 104[0] -> 112[0] [send] via NET/AWS Libfabric/0
27: nid006566:216749:219575 [3] NCCL INFO Channel 02/0 : 111[3] -> 108[0] via P2P/CUMEM
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 100[0] -> 108[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 104[0] -> 112[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 100[0] -> 108[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 108[0] -> 116[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 02/0 : 106[2] -> 110[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 108[0] -> 116[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 00/0 : 112[0] -> 104[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 96[0] -> 112[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 112[0] -> 104[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 96[0] -> 112[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216749:219575 [3] NCCL INFO Channel 03/0 : 111[3] -> 108[0] via P2P/CUMEM
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 108[0] -> 100[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216747:219573 [1] NCCL INFO Channel 01/0 : 109[1] -> 108[0] via P2P/CUMEM
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 102[2] -> 110[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 112[0] -> 96[0] [send] via NET/AWS Libfabric/0
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 106[2] -> 114[2] [send] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 05/0 : 108[0] -> 100[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 112[0] -> 96[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 92[0] -> 108[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 102[2] -> 110[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 64[0] -> 96[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 106[2] -> 114[2] [send] via NET/AWS Libfabric/2
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 116[0] -> 108[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 64[0] -> 96[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 92[0] -> 108[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 05/0 : 116[0] -> 108[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 110[2] -> 118[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 114[2] -> 106[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 98[2] -> 114[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216749:219575 [3] NCCL INFO Channel 06/0 : 111[3] -> 108[0] via P2P/CUMEM
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 96[0] -> 64[0] [send] via NET/AWS Libfabric/0
27: nid006566:216747:219573 [1] NCCL INFO Channel 03/0 : 109[1] -> 108[0] via P2P/CUMEM
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 0[0] -> 64[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 96[0] -> 64[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 60[0] -> 92[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 98[2] -> 114[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 108[0] -> 92[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 110[2] -> 118[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 114[2] -> 106[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 0[0] -> 64[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 60[0] -> 92[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 108[0] -> 92[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 64[0] -> 0[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 64[0] -> 0[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 92[0] -> 60[0] [send] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 114[2] -> 98[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 124[0] -> 60[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 66[2] -> 98[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216749:219575 [3] NCCL INFO Channel 07/0 : 111[3] -> 108[0] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 124[0] -> 60[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216747:219573 [1] NCCL INFO Channel 05/0 : 109[1] -> 108[0] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 110[2] -> 102[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 92[0] -> 60[0] [send] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 114[2] -> 98[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 110[2] -> 102[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 66[2] -> 98[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 118[2] -> 110[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 60[0] -> 124[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 96[0] -> 64[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 94[2] -> 110[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 118[2] -> 110[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 60[0] -> 124[0] [send] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 96[0] -> 64[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241019:244160 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 94[2] -> 110[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221581:224445 [0] NCCL INFO Channel 00/0 : 64[0] -> 32[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 2[2] -> 66[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 92[0] -> 60[0] [receive] via NET/AWS Libfabric/0
 0: nid006495:241019:244160 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 01/0 : 64[0] -> 32[0] [send] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 98[2] -> 66[2] [send] via NET/AWS Libfabric/2
31: nid007342:59767:63056 [0] NCCL INFO Channel 00/0 : 124[0] -> 120[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 2[2] -> 66[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216747:219573 [1] NCCL INFO Channel 07/0 : 109[1] -> 108[0] via P2P/CUMEM
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 92[0] -> 60[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223923:226751 [0] NCCL INFO Channel 04/0 : 60[0] -> 28[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 110[2] -> 94[2] [send] via NET/AWS Libfabric/2
31: nid007342:59767:63056 [0] NCCL INFO Channel 01/0 : 124[0] -> 120[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 66[2] -> 2[2] [send] via NET/AWS Libfabric/2
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 98[2] -> 66[2] [send] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 62[2] -> 94[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 110[2] -> 94[2] [send] via NET/AWS Libfabric/2
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 66[2] -> 2[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 05/0 : 60[0] -> 28[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 112[0] -> 96[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 62[2] -> 94[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 04/0 : 68[0] -> 64[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 112[0] -> 96[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 00/0 : 96[0] -> 80[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221581:224445 [0] NCCL INFO Channel 05/0 : 68[0] -> 64[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 01/0 : 96[0] -> 80[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 108[0] -> 92[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 98[2] -> 66[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 94[2] -> 62[2] [send] via NET/AWS Libfabric/2
 0: nid006495:241021:244159 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 126[2] -> 62[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218410:221187 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 98[2] -> 66[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 108[0] -> 92[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 94[2] -> 62[2] [send] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [send] via NET/AWS Libfabric/0
 0: nid006495:241021:244159 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:224446 [2] NCCL INFO Channel 02/0 : 66[2] -> 34[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 44[0] -> 28[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 04/0 : 92[0] -> 76[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 120[0] -> 112[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 126[2] -> 62[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223923:226751 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [send] via NET/AWS Libfabric/0
24: nid006563:221140:223979 [0] NCCL INFO Channel 04/0 : 100[0] -> 96[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 88[0] -> 80[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 05/0 : 92[0] -> 76[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 120[0] -> 112[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 44[0] -> 28[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 88[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 04/0 : 36[0] -> 32[0] [receive] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 03/0 : 66[2] -> 34[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 62[2] -> 126[2] [send] via NET/AWS Libfabric/2
24: nid006563:221140:223979 [0] NCCL INFO Channel 05/0 : 100[0] -> 96[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 00/0 : 112[0] -> 104[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [receive] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 01/0 : 112[0] -> 104[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 00/0 : 80[0] -> 72[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252584:255423 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218410:221187 [0] NCCL INFO Channel 05/0 : 36[0] -> 32[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 62[2] -> 126[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 116[0] -> 108[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 00/0 : 92[0] -> 88[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [send] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 01/0 : 80[0] -> 72[0] [send] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 84[0] -> 76[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 116[0] -> 108[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
23: nid006561:220711:223529 [0] NCCL INFO Channel 01/0 : 92[0] -> 88[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [send] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 114[2] -> 98[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 84[0] -> 76[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 04/0 : 108[0] -> 100[0] [send] via NET/AWS Libfabric/0
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 50[2] -> 34[2] [receive] via NET/AWS Libfabric/2
16: nid006554:221583:224446 [2] NCCL INFO Channel 06/0 : 70[2] -> 66[2] [receive] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 94[2] -> 62[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 52[0] -> 44[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
16: nid006554:221583:224446 [2] NCCL INFO Channel 07/0 : 70[2] -> 66[2] [receive] via NET/AWS Libfabric/2
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 114[2] -> 98[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216746:219574 [0] NCCL INFO Channel 05/0 : 108[0] -> 100[0] [send] via NET/AWS Libfabric/0
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 52[0] -> 44[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:23433 [0] NCCL INFO Channel 00/0 : 124[0] -> 120[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 04/0 : 76[0] -> 68[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 04/0 : 116[0] -> 112[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59769:63053 [2] NCCL INFO Channel 02/0 : 126[2] -> 122[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 02/0 : 98[2] -> 82[2] [send] via NET/AWS Libfabric/2
26: nid006565:222465:225310 [0] NCCL INFO Channel 00/0 : 108[0] -> 104[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 04/0 : 84[0] -> 80[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 50[2] -> 34[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 124[0] -> 120[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 00/0 : 92[0] -> 88[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 94[2] -> 62[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [send] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 05/0 : 76[0] -> 68[0] [send] via NET/AWS Libfabric/0
28: nid007251:72169:75296 [0] NCCL INFO Channel 05/0 : 116[0] -> 112[0] [receive] via NET/AWS Libfabric/0
31: nid007342:59769:63053 [2] NCCL INFO Channel 03/0 : 126[2] -> 122[2] [send] via NET/AWS Libfabric/2
15: nid006553:223925:226753 [2] NCCL INFO Channel 06/0 : 62[2] -> 30[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252584:255423 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 108[0] -> 104[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215467:218298 [0] NCCL INFO Channel 05/0 : 84[0] -> 80[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 03/0 : 98[2] -> 82[2] [send] via NET/AWS Libfabric/2
26: nid006565:222465:225310 [0] NCCL INFO Channel 00/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:23433 [0] NCCL INFO Channel 00/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 92[0] -> 88[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 04/0 : 52[0] -> 48[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
 8: nid006503:218412:221188 [2] NCCL INFO Channel 03/0 : 34[2] -> 18[2] [send] via NET/AWS Libfabric/2
14: nid006510:229442:232280 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 07/0 : 62[2] -> 30[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 04/0 : 44[0] -> 36[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 00/0 : 76[0] -> 72[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 00/0 : 108[0] -> 104[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211335:214148 [0] NCCL INFO Channel 05/0 : 44[0] -> 36[0] [send] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 76[0] -> 72[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 00/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:266529 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [receive] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 01/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:213933 [0] NCCL INFO Channel 00/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 01/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:23433 [0] NCCL INFO Channel 01/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254556:257359 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [receive] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 00/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
12: nid006508:205766:208689 [0] NCCL INFO Channel 05/0 : 52[0] -> 48[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 04/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:213933 [0] NCCL INFO Channel 01/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216746:219574 [0] NCCL INFO Channel 01/0 : 108[0] -> 104[0] [send] via NET/AWS Libfabric/0
30: nid007318:20580:23433 [0] NCCL INFO Channel 04/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 01/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 00/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:225817 [0] NCCL INFO Channel 00/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
19: nid006557:208417:211245 [0] NCCL INFO Channel 00/0 : 76[0] -> 72[0] [send] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 122[2] -> 114[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263727:266529 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 00/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
18: nid006556:210774:213695 [0] NCCL INFO Channel 01/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:225817 [0] NCCL INFO Channel 01/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 06/0 : 102[2] -> 98[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208417:211245 [0] NCCL INFO Channel 01/0 : 76[0] -> 72[0] [send] via NET/AWS Libfabric/0
26: nid006565:222465:225310 [0] NCCL INFO Channel 05/0 : 104[0] -> 100[0] [send] via NET/AWS Libfabric/0
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20580:23433 [0] NCCL INFO Channel 05/0 : 120[0] -> 116[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 04/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
11: nid006507:211335:214148 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 58[2] -> 50[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201788:204595 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 01/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 90[2] -> 82[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210774:213695 [0] NCCL INFO Channel 04/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
14: nid006510:229442:232280 [0] NCCL INFO Channel 04/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 110[2] -> 94[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226765:229566 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 122[2] -> 114[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27222:29972 [0] NCCL INFO Channel 05/0 : 120[0] -> 116[0] [receive] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 02/0 : 62[2] -> 58[2] [send] via NET/AWS Libfabric/2
11: nid006507:211335:214148 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 58[2] -> 50[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201788:204595 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 90[2] -> 82[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229442:232280 [0] NCCL INFO Channel 05/0 : 56[0] -> 52[0] [send] via NET/AWS Libfabric/0
24: nid006563:221142:223978 [2] NCCL INFO Channel 07/0 : 102[2] -> 98[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 110[2] -> 94[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211123:213933 [0] NCCL INFO Channel 05/0 : 88[0] -> 84[0] [receive] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 04/0 : 116[0] -> 112[0] [send] via NET/AWS Libfabric/0
10: nid006506:263727:266529 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
22: nid006560:222271:225098 [0] NCCL INFO Channel 05/0 : 88[0] -> 84[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
15: nid006553:223925:226753 [2] NCCL INFO Channel 03/0 : 62[2] -> 58[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221941:224742 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 05/0 : 72[0] -> 68[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 02/0 : 82[2] -> 74[2] [send] via NET/AWS Libfabric/2
18: nid006556:210774:213695 [0] NCCL INFO Channel 05/0 : 72[0] -> 68[0] [send] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 46[2] -> 30[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 05/0 : 104[0] -> 100[0] [receive] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 06/0 : 94[2] -> 78[2] [send] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 02/0 : 114[2] -> 106[2] [send] via NET/AWS Libfabric/2
21: nid006559:211123:213933 [0] NCCL INFO Channel 04/0 : 84[0] -> 80[0] [send] via NET/AWS Libfabric/0
29: nid007305:27222:29972 [0] NCCL INFO Channel 05/0 : 116[0] -> 112[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263727:266529 [0] NCCL INFO Channel 04/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
 2: nid006497:227575:230384 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 05/0 : 40[0] -> 36[0] [receive] via NET/AWS Libfabric/0
12: nid006508:205768:208691 [2] NCCL INFO Channel 02/0 : 50[2] -> 42[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
 6: nid006501:221941:224742 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 04/0 : 68[0] -> 64[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:245429 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 03/0 : 82[2] -> 74[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 46[2] -> 30[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 06/0 : 38[2] -> 34[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 04/0 : 100[0] -> 96[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 07/0 : 94[2] -> 78[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226765:229566 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
28: nid007251:72171:75298 [2] NCCL INFO Channel 03/0 : 114[2] -> 106[2] [send] via NET/AWS Libfabric/2
21: nid006559:211123:213933 [0] NCCL INFO Channel 05/0 : 84[0] -> 80[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263727:266529 [0] NCCL INFO Channel 05/0 : 40[0] -> 36[0] [send] via NET/AWS Libfabric/0
 9: nid006505:249080:251903 [0] NCCL INFO Channel 04/0 : 36[0] -> 32[0] [send] via NET/AWS Libfabric/0
12: nid006508:205768:208691 [2] NCCL INFO Channel 03/0 : 50[2] -> 42[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:204595 [0] NCCL INFO Channel 05/0 : 56[0] -> 52[0] [receive] via NET/AWS Libfabric/0
17: nid006555:206593:209444 [0] NCCL INFO Channel 05/0 : 68[0] -> 64[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:245429 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
 8: nid006503:218412:221188 [2] NCCL INFO Channel 07/0 : 38[2] -> 34[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223021:225817 [0] NCCL INFO Channel 05/0 : 100[0] -> 96[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
 3: nid006498:226765:229566 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
 4: nid006499:254558:257358 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249080:251903 [0] NCCL INFO Channel 05/0 : 36[0] -> 32[0] [send] via NET/AWS Libfabric/0
13: nid006509:201788:204595 [0] NCCL INFO Channel 04/0 : 52[0] -> 48[0] [send] via NET/AWS Libfabric/0
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
 7: nid006502:252586:255425 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 126[2] -> 122[2] [receive] via NET/AWS Libfabric/2
 4: nid006499:254558:257358 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
13: nid006509:201788:204595 [0] NCCL INFO Channel 05/0 : 52[0] -> 48[0] [send] via NET/AWS Libfabric/0
 5: nid006500:260083:262912 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 118[2] -> 110[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:245429 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 126[2] -> 122[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260083:262912 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
23: nid006561:220713:223528 [2] NCCL INFO Channel 02/0 : 94[2] -> 90[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 118[2] -> 110[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:245429 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 110[2] -> 106[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 86[2] -> 78[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 02/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 94[2] -> 90[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:208691 [2] NCCL INFO Channel 06/0 : 54[2] -> 50[2] [receive] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 62[2] -> 58[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 06/0 : 118[2] -> 114[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 54[2] -> 46[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242584:245429 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
20: nid006558:215469:218297 [2] NCCL INFO Channel 06/0 : 86[2] -> 82[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260083:262912 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
27: nid006566:216748:219576 [2] NCCL INFO Channel 06/0 : 110[2] -> 102[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 03/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 46[2] -> 42[2] [receive] via NET/AWS Libfabric/2
12: nid006508:205768:208691 [2] NCCL INFO Channel 07/0 : 54[2] -> 50[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215469:218297 [2] NCCL INFO Channel 07/0 : 86[2] -> 82[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 78[2] -> 74[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220713:223528 [2] NCCL INFO Channel 03/0 : 94[2] -> 90[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 86[2] -> 78[2] [receive] via NET/AWS Libfabric/2
28: nid007251:72171:75298 [2] NCCL INFO Channel 07/0 : 118[2] -> 114[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 07/0 : 110[2] -> 102[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 94[2] -> 90[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 54[2] -> 46[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 78[2] -> 74[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252586:255425 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 62[2] -> 58[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 110[2] -> 106[2] [receive] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 06/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:257358 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 46[2] -> 42[2] [receive] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 02/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 06/0 : 46[2] -> 38[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
 7: nid006502:252586:255425 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 02/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 06/0 : 78[2] -> 70[2] [send] via NET/AWS Libfabric/2
 4: nid006499:254558:257358 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 02/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 02/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 07/0 : 78[2] -> 70[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 02/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
30: nid007318:20582:23432 [2] NCCL INFO Channel 07/0 : 122[2] -> 118[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 03/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 03/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 06/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 03/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 07/0 : 46[2] -> 38[2] [send] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 06/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
27: nid006566:216749:219575 [3] NCCL INFO Channel 01/0 : 111[3] -> 110[2] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 06/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
27: nid006566:216748:219576 [2] NCCL INFO Channel 02/0 : 110[2] -> 106[2] [send] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 02/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 03/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
22: nid006560:222273:225096 [2] NCCL INFO Channel 07/0 : 90[2] -> 86[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 03/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
31: nid007342:59770:63055 [3] NCCL INFO Channel 01/0 : 127[3] -> 126[2] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 06/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 06/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 02/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
23: nid006561:220714:223527 [3] NCCL INFO Channel 01/0 : 95[3] -> 94[2] via P2P/CUMEM
27: nid006566:216748:219576 [2] NCCL INFO Channel 03/0 : 110[2] -> 106[2] [send] via NET/AWS Libfabric/2
10: nid006506:263729:266530 [2] NCCL INFO Channel 07/0 : 42[2] -> 38[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 02/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 6: nid006501:221943:224744 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
18: nid006556:210776:213697 [2] NCCL INFO Channel 07/0 : 74[2] -> 70[2] [send] via NET/AWS Libfabric/2
14: nid006510:229444:232279 [2] NCCL INFO Channel 07/0 : 58[2] -> 54[2] [send] via NET/AWS Libfabric/2
26: nid006565:222467:225308 [2] NCCL INFO Channel 07/0 : 106[2] -> 102[2] [send] via NET/AWS Libfabric/2
21: nid006559:211125:213932 [2] NCCL INFO Channel 02/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 03/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 02/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 02/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 03/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 02/0 : 78[2] -> 74[2] [send] via NET/AWS Libfabric/2
21: nid006559:211125:213932 [2] NCCL INFO Channel 03/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
 2: nid006497:227577:230375 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
15: nid006553:223926:226754 [3] NCCL INFO Channel 01/0 : 63[3] -> 62[2] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 02/0 : 46[2] -> 42[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 03/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 122[2] -> 118[2] [receive] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 03/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208420:211246 [3] NCCL INFO Channel 01/0 : 79[3] -> 78[2] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 01/0 : 119[3] -> 118[2] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 00/0 : 123[3] -> 122[2] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 01/0 : 47[3] -> 46[2] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 7: nid006502:252587:255422 [3] NCCL INFO Channel 01/0 : 31[3] -> 30[2] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
 3: nid006498:226767:229567 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
19: nid006557:208419:211247 [2] NCCL INFO Channel 03/0 : 78[2] -> 74[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
11: nid006507:211337:214149 [2] NCCL INFO Channel 03/0 : 46[2] -> 42[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 03/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 106[2] -> 102[2] [receive] via NET/AWS Libfabric/2
 5: nid006500:260085:262911 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
27: nid006566:216749:219575 [3] NCCL INFO Channel 03/0 : 111[3] -> 110[2] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 06/0 : 118[2] -> 114[2] [send] via NET/AWS Libfabric/2
22: nid006560:222274:225095 [3] NCCL INFO Channel 00/0 : 91[3] -> 90[2] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 42[2] -> 38[2] [receive] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 58[2] -> 54[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
17: nid006555:206596:209446 [3] NCCL INFO Channel 01/0 : 71[3] -> 70[2] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 00/0 : 67[3] -> 66[2] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 00/0 : 99[3] -> 98[2] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
26: nid006565:222468:225309 [3] NCCL INFO Channel 00/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 90[2] -> 86[2] [receive] via NET/AWS Libfabric/2
29: nid007305:27224:29973 [2] NCCL INFO Channel 07/0 : 118[2] -> 114[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 06/0 : 54[2] -> 50[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
20: nid006558:215470:218299 [3] NCCL INFO Channel 00/0 : 83[3] -> 82[2] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 00/0 : 75[3] -> 74[2] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 00/0 : 59[3] -> 58[2] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 06/0 : 102[2] -> 98[2] [send] via NET/AWS Libfabric/2
25: nid006564:223024:225819 [3] NCCL INFO Channel 01/0 : 103[3] -> 102[2] via P2P/CUMEM
 3: nid006498:226768:229565 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
28: nid007251:72172:75299 [3] NCCL INFO Channel 00/0 : 115[3] -> 114[2] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 01/0 : 87[3] -> 86[2] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 06/0 : 86[2] -> 82[2] [send] via NET/AWS Libfabric/2
 9: nid006505:249082:251904 [2] NCCL INFO Channel 06/0 : 38[2] -> 34[2] [send] via NET/AWS Libfabric/2
13: nid006509:201790:204592 [2] NCCL INFO Channel 07/0 : 54[2] -> 50[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 74[2] -> 70[2] [receive] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
25: nid006564:223023:225816 [2] NCCL INFO Channel 07/0 : 102[2] -> 98[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
21: nid006559:211125:213932 [2] NCCL INFO Channel 07/0 : 86[2] -> 82[2] [send] via NET/AWS Libfabric/2
31: nid007342:59770:63055 [3] NCCL INFO Channel 03/0 : 127[3] -> 126[2] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 00/0 : 43[3] -> 42[2] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 07/0 : 38[2] -> 34[2] [send] via NET/AWS Libfabric/2
17: nid006555:206595:209445 [2] NCCL INFO Channel 06/0 : 70[2] -> 66[2] [send] via NET/AWS Libfabric/2
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
19: nid006557:208420:211246 [3] NCCL INFO Channel 03/0 : 79[3] -> 78[2] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 03/0 : 119[3] -> 118[2] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 02/0 : 123[3] -> 122[2] via P2P/CUMEM
17: nid006555:206595:209445 [2] NCCL INFO Channel 07/0 : 70[2] -> 66[2] [send] via NET/AWS Libfabric/2
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
 8: nid006503:218413:221189 [3] NCCL INFO Channel 00/0 : 35[3] -> 34[2] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
23: nid006561:220714:223527 [3] NCCL INFO Channel 03/0 : 95[3] -> 94[2] via P2P/CUMEM
22: nid006560:222274:225095 [3] NCCL INFO Channel 02/0 : 91[3] -> 90[2] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 01/0 : 39[3] -> 38[2] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 00/0 : 51[3] -> 50[2] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 01/0 : 55[3] -> 54[2] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
 2: nid006497:227578:230383 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
 7: nid006502:252587:255422 [3] NCCL INFO Channel 03/0 : 31[3] -> 30[2] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
28: nid007251:72172:75299 [3] NCCL INFO Channel 02/0 : 115[3] -> 114[2] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/CUMEM
17: nid006555:206596:209446 [3] NCCL INFO Channel 03/0 : 71[3] -> 70[2] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 02/0 : 59[3] -> 58[2] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
16: nid006554:221584:224444 [3] NCCL INFO Channel 02/0 : 67[3] -> 66[2] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 03/0 : 63[3] -> 62[2] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 03/0 : 47[3] -> 46[2] via P2P/CUMEM
20: nid006558:215470:218299 [3] NCCL INFO Channel 02/0 : 83[3] -> 82[2] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 02/0 : 75[3] -> 74[2] via P2P/CUMEM
25: nid006564:223024:225819 [3] NCCL INFO Channel 03/0 : 103[3] -> 102[2] via P2P/CUMEM
 5: nid006500:260086:262910 [3] NCCL INFO Channel 01/0 : 23[3] -> 22[2] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 02/0 : 99[3] -> 98[2] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 02/0 : 107[3] -> 106[2] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 03/0 : 87[3] -> 86[2] via P2P/CUMEM
 3: nid006498:226768:229565 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
27: nid006566:216749:219575 [3] NCCL INFO Channel 05/0 : 111[3] -> 110[2] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 02/0 : 43[3] -> 42[2] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 02/0 : 35[3] -> 34[2] via P2P/CUMEM
31: nid007342:59770:63055 [3] NCCL INFO Channel 05/0 : 127[3] -> 126[2] via P2P/CUMEM
17: nid006555:206596:209446 [3] NCCL INFO Channel 05/0 : 71[3] -> 70[2] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 05/0 : 119[3] -> 118[2] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 03/0 : 39[3] -> 38[2] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 02/0 : 51[3] -> 50[2] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 03/0 : 55[3] -> 54[2] via P2P/CUMEM
23: nid006561:220714:223527 [3] NCCL INFO Channel 05/0 : 95[3] -> 94[2] via P2P/CUMEM
19: nid006557:208420:211246 [3] NCCL INFO Channel 05/0 : 79[3] -> 78[2] via P2P/CUMEM
28: nid007251:72172:75299 [3] NCCL INFO Channel 04/0 : 115[3] -> 114[2] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 04/0 : 67[3] -> 66[2] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 04/0 : 123[3] -> 122[2] via P2P/CUMEM
22: nid006560:222274:225095 [3] NCCL INFO Channel 04/0 : 91[3] -> 90[2] via P2P/CUMEM
 7: nid006502:252587:255422 [3] NCCL INFO Channel 05/0 : 31[3] -> 30[2] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 05/0 : 63[3] -> 62[2] via P2P/CUMEM
 5: nid006500:260086:262910 [3] NCCL INFO Channel 03/0 : 23[3] -> 22[2] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 04/0 : 75[3] -> 74[2] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 04/0 : 107[3] -> 106[2] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 04/0 : 59[3] -> 58[2] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 05/0 : 47[3] -> 46[2] via P2P/CUMEM
20: nid006558:215470:218299 [3] NCCL INFO Channel 04/0 : 83[3] -> 82[2] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 04/0 : 99[3] -> 98[2] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 05/0 : 87[3] -> 86[2] via P2P/CUMEM
25: nid006564:223024:225819 [3] NCCL INFO Channel 05/0 : 103[3] -> 102[2] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 04/0 : 51[3] -> 50[2] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
 3: nid006498:226768:229565 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
27: nid006566:216749:219575 [3] NCCL INFO Channel 07/0 : 111[3] -> 110[2] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 04/0 : 43[3] -> 42[2] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 05/0 : 39[3] -> 38[2] via P2P/CUMEM
29: nid007305:27225:29971 [3] NCCL INFO Channel 07/0 : 119[3] -> 118[2] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 04/0 : 35[3] -> 34[2] via P2P/CUMEM
16: nid006554:221584:224444 [3] NCCL INFO Channel 06/0 : 67[3] -> 66[2] via P2P/CUMEM
31: nid007342:59770:63055 [3] NCCL INFO Channel 07/0 : 127[3] -> 126[2] via P2P/CUMEM
17: nid006555:206596:209446 [3] NCCL INFO Channel 07/0 : 71[3] -> 70[2] via P2P/CUMEM
19: nid006557:208420:211246 [3] NCCL INFO Channel 07/0 : 79[3] -> 78[2] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 05/0 : 55[3] -> 54[2] via P2P/CUMEM
23: nid006561:220714:223527 [3] NCCL INFO Channel 07/0 : 95[3] -> 94[2] via P2P/CUMEM
28: nid007251:72172:75299 [3] NCCL INFO Channel 06/0 : 115[3] -> 114[2] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
30: nid007318:20583:23431 [3] NCCL INFO Channel 06/0 : 123[3] -> 122[2] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/CUMEM
22: nid006560:222274:225095 [3] NCCL INFO Channel 06/0 : 91[3] -> 90[2] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/CUMEM
 7: nid006502:252587:255422 [3] NCCL INFO Channel 07/0 : 31[3] -> 30[2] via P2P/CUMEM
15: nid006553:223926:226754 [3] NCCL INFO Channel 07/0 : 63[3] -> 62[2] via P2P/CUMEM
18: nid006556:210777:213694 [3] NCCL INFO Channel 06/0 : 75[3] -> 74[2] via P2P/CUMEM
 5: nid006500:260086:262910 [3] NCCL INFO Channel 05/0 : 23[3] -> 22[2] via P2P/CUMEM
11: nid006507:211338:214147 [3] NCCL INFO Channel 07/0 : 47[3] -> 46[2] via P2P/CUMEM
14: nid006510:229445:232282 [3] NCCL INFO Channel 06/0 : 59[3] -> 58[2] via P2P/CUMEM
26: nid006565:222468:225309 [3] NCCL INFO Channel 06/0 : 107[3] -> 106[2] via P2P/CUMEM
20: nid006558:215470:218299 [3] NCCL INFO Channel 06/0 : 83[3] -> 82[2] via P2P/CUMEM
10: nid006506:263730:266531 [3] NCCL INFO Channel 06/0 : 43[3] -> 42[2] via P2P/CUMEM
24: nid006563:221143:223976 [3] NCCL INFO Channel 06/0 : 99[3] -> 98[2] via P2P/CUMEM
21: nid006559:211126:213934 [3] NCCL INFO Channel 07/0 : 87[3] -> 86[2] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
 9: nid006505:249083:251905 [3] NCCL INFO Channel 07/0 : 39[3] -> 38[2] via P2P/CUMEM
12: nid006508:205769:208690 [3] NCCL INFO Channel 06/0 : 51[3] -> 50[2] via P2P/CUMEM
 3: nid006498:226768:229565 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 01/0 : 126[2] -> 125[1] via P2P/CUMEM
 6: nid006501:221944:224741 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 01/0 : 30[2] -> 29[1] via P2P/CUMEM
 8: nid006503:218413:221189 [3] NCCL INFO Channel 06/0 : 35[3] -> 34[2] via P2P/CUMEM
25: nid006564:223024:225819 [3] NCCL INFO Channel 07/0 : 103[3] -> 102[2] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 01/0 : 94[2] -> 93[1] via P2P/CUMEM
13: nid006509:201791:204593 [3] NCCL INFO Channel 07/0 : 55[3] -> 54[2] via P2P/CUMEM
 4: nid006499:254559:257357 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/CUMEM
 2: nid006497:227578:230383 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
31: nid007342:59769:63053 [2] NCCL INFO Channel 05/0 : 126[2] -> 125[1] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 01/0 : 78[2] -> 77[1] via P2P/CUMEM
27: nid006566:216748:219576 [2] NCCL INFO Channel 01/0 : 110[2] -> 109[1] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 01/0 : 62[2] -> 61[1] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 00/0 : 122[2] -> 121[1] via P2P/CUMEM
 7: nid006502:252586:255425 [2] NCCL INFO Channel 05/0 : 30[2] -> 29[1] via P2P/CUMEM
 5: nid006500:260086:262910 [3] NCCL INFO Channel 07/0 : 23[3] -> 22[2] via P2P/CUMEM
19: nid006557:208419:211247 [2] NCCL INFO Channel 05/0 : 78[2] -> 77[1] via P2P/CUMEM
23: nid006561:220713:223528 [2] NCCL INFO Channel 05/0 : 94[2] -> 93[1] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 00/0 : 66[2] -> 65[1] via P2P/CUMEM
 1: nid006496:242587:245427 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 01/0 : 118[2] -> 117[1] via P2P/CUMEM
15: nid006553:223925:226753 [2] NCCL INFO Channel 05/0 : 62[2] -> 61[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 01/0 : 86[2] -> 85[1] via P2P/CUMEM
27: nid006566:216748:219576 [2] NCCL INFO Channel 05/0 : 110[2] -> 109[1] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 00/0 : 90[2] -> 89[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 00/0 : 74[2] -> 73[1] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 00/0 : 106[2] -> 105[1] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 01/0 : 38[2] -> 37[1] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 01/0 : 46[2] -> 45[1] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 00/0 : 50[2] -> 49[1] via P2P/CUMEM
30: nid007318:20582:23432 [2] NCCL INFO Channel 04/0 : 122[2] -> 121[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 00/0 : 42[2] -> 41[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 01/0 : 54[2] -> 53[1] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 00/0 : 82[2] -> 81[1] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 00/0 : 58[2] -> 57[1] via P2P/CUMEM
 0: nid006495:241022:244161 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 00/0 : 98[2] -> 97[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/CUMEM
 9: nid006505:249082:251904 [2] NCCL INFO Channel 05/0 : 38[2] -> 37[1] via P2P/CUMEM
 3: nid006498:226767:229567 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/CUMEM
26: nid006565:222467:225308 [2] NCCL INFO Channel 04/0 : 106[2] -> 105[1] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 00/0 : 34[2] -> 33[1] via P2P/CUMEM
11: nid006507:211337:214149 [2] NCCL INFO Channel 05/0 : 46[2] -> 45[1] via P2P/CUMEM
13: nid006509:201790:204592 [2] NCCL INFO Channel 05/0 : 54[2] -> 53[1] via P2P/CUMEM
10: nid006506:263729:266530 [2] NCCL INFO Channel 04/0 : 42[2] -> 41[1] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 01/0 : 102[2] -> 101[1] via P2P/CUMEM
12: nid006508:205768:208691 [2] NCCL INFO Channel 04/0 : 50[2] -> 49[1] via P2P/CUMEM
 4: nid006499:254558:257358 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 01/0 : 22[2] -> 21[1] via P2P/CUMEM
14: nid006510:229444:232279 [2] NCCL INFO Channel 04/0 : 58[2] -> 57[1] via P2P/CUMEM
 6: nid006501:221943:224744 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/CUMEM
20: nid006558:215469:218297 [2] NCCL INFO Channel 04/0 : 82[2] -> 81[1] via P2P/CUMEM
24: nid006563:221142:223978 [2] NCCL INFO Channel 04/0 : 98[2] -> 97[1] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
 5: nid006500:260085:262911 [2] NCCL INFO Channel 05/0 : 22[2] -> 21[1] via P2P/CUMEM
25: nid006564:223023:225816 [2] NCCL INFO Channel 05/0 : 102[2] -> 101[1] via P2P/CUMEM
 8: nid006503:218412:221188 [2] NCCL INFO Channel 04/0 : 34[2] -> 33[1] via P2P/CUMEM
 1: nid006496:242586:245428 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
 0: nid006495:241021:244159 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
16: nid006554:221583:224446 [2] NCCL INFO Channel 04/0 : 66[2] -> 65[1] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 00/0 : 114[2] -> 113[1] via P2P/CUMEM
21: nid006559:211125:213932 [2] NCCL INFO Channel 05/0 : 86[2] -> 85[1] via P2P/CUMEM
17: nid006555:206595:209445 [2] NCCL INFO Channel 01/0 : 70[2] -> 69[1] via P2P/CUMEM
18: nid006556:210776:213697 [2] NCCL INFO Channel 04/0 : 74[2] -> 73[1] via P2P/CUMEM
29: nid007305:27224:29973 [2] NCCL INFO Channel 05/0 : 118[2] -> 117[1] via P2P/CUMEM
22: nid006560:222273:225096 [2] NCCL INFO Channel 04/0 : 90[2] -> 89[1] via P2P/CUMEM
 2: nid006497:227577:230375 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
28: nid007251:72171:75298 [2] NCCL INFO Channel 04/0 : 114[2] -> 113[1] via P2P/CUMEM
17: nid006555:206595:209445 [2] NCCL INFO Channel 05/0 : 70[2] -> 69[1] via P2P/CUMEM
31: nid007342:59767:63056 [0] NCCL INFO Connected all trees
15: nid006553:223923:226751 [0] NCCL INFO Connected all trees
 7: nid006502:252584:255423 [0] NCCL INFO Connected all trees
24: nid006563:221140:223979 [0] NCCL INFO Connected all trees
 8: nid006503:218410:221187 [0] NCCL INFO Connected all trees
23: nid006561:220711:223529 [0] NCCL INFO Connected all trees
 9: nid006505:249080:251903 [0] NCCL INFO Connected all trees
 1: nid006496:242584:245429 [0] NCCL INFO Connected all trees
20: nid006558:215467:218298 [0] NCCL INFO Connected all trees
 3: nid006498:226765:229566 [0] NCCL INFO Connected all trees
17: nid006555:206593:209444 [0] NCCL INFO Connected all trees
22: nid006560:222271:225098 [0] NCCL INFO Connected all trees
 4: nid006499:254556:257359 [0] NCCL INFO Connected all trees
19: nid006557:208417:211245 [0] NCCL INFO Connected all trees
21: nid006559:211123:213933 [0] NCCL INFO Connected all trees
 2: nid006497:227575:230384 [0] NCCL INFO Connected all trees
 5: nid006500:260083:262912 [0] NCCL INFO Connected all trees
27: nid006566:216746:219574 [0] NCCL INFO Connected all trees
 6: nid006501:221941:224742 [0] NCCL INFO Connected all trees
18: nid006556:210774:213695 [0] NCCL INFO Connected all trees
28: nid007251:72169:75296 [0] NCCL INFO Connected all trees
25: nid006564:223021:225817 [0] NCCL INFO Connected all trees
26: nid006565:222465:225310 [0] NCCL INFO Connected all trees
30: nid007318:20580:23433 [0] NCCL INFO Connected all trees
29: nid007305:27222:29972 [0] NCCL INFO Connected all trees
16: nid006554:221581:224445 [0] NCCL INFO Connected all trees
11: nid006507:211335:214148 [0] NCCL INFO Connected all trees
10: nid006506:263727:266529 [0] NCCL INFO Connected all trees
12: nid006508:205766:208689 [0] NCCL INFO Connected all trees
13: nid006509:201788:204595 [0] NCCL INFO Connected all trees
14: nid006510:229442:232280 [0] NCCL INFO Connected all trees
 0: nid006495:241019:244160 [0] NCCL INFO Connected all trees
 0: nid006495:241020:244162 [1] NCCL INFO Connected all trees
 7: nid006502:252585:255424 [1] NCCL INFO Connected all trees
15: nid006553:223924:226752 [1] NCCL INFO Connected all trees
 0: nid006495:241022:244161 [3] NCCL INFO Connected all trees
 7: nid006502:252587:255422 [3] NCCL INFO Connected all trees
 4: nid006499:254557:257360 [1] NCCL INFO Connected all trees
 0: nid006495:241021:244159 [2] NCCL INFO Connected all trees
15: nid006553:223926:226754 [3] NCCL INFO Connected all trees
 7: nid006502:252586:255425 [2] NCCL INFO Connected all trees
 8: nid006503:218411:221186 [1] NCCL INFO Connected all trees
15: nid006553:223925:226753 [2] NCCL INFO Connected all trees
11: nid006507:211336:214150 [1] NCCL INFO Connected all trees
16: nid006554:221582:224447 [1] NCCL INFO Connected all trees
 4: nid006499:254559:257357 [3] NCCL INFO Connected all trees
 6: nid006501:221942:224743 [1] NCCL INFO Connected all trees
 9: nid006505:249081:251902 [1] NCCL INFO Connected all trees
16: nid006554:221584:224444 [3] NCCL INFO Connected all trees
 5: nid006500:260084:262913 [1] NCCL INFO Connected all trees
12: nid006508:205767:208688 [1] NCCL INFO Connected all trees
 8: nid006503:218413:221189 [3] NCCL INFO Connected all trees
16: nid006554:221583:224446 [2] NCCL INFO Connected all trees
11: nid006507:211338:214147 [3] NCCL INFO Connected all trees
11: nid006507:211337:214149 [2] NCCL INFO Connected all trees
 8: nid006503:218412:221188 [2] NCCL INFO Connected all trees
18: nid006556:210775:213696 [1] NCCL INFO Connected all trees
19: nid006557:208418:211248 [1] NCCL INFO Connected all trees
10: nid006506:263728:266528 [1] NCCL INFO Connected all trees
 9: nid006505:249083:251905 [3] NCCL INFO Connected all trees
 9: nid006505:249082:251904 [2] NCCL INFO Connected all trees
20: nid006558:215468:218296 [1] NCCL INFO Connected all trees
 5: nid006500:260086:262910 [3] NCCL INFO Connected all trees
 4: nid006499:254558:257358 [2] NCCL INFO Connected all trees
12: nid006508:205769:208690 [3] NCCL INFO Connected all trees
13: nid006509:201789:204594 [1] NCCL INFO Connected all trees
27: nid006566:216747:219573 [1] NCCL INFO Connected all trees
17: nid006555:206594:209447 [1] NCCL INFO Connected all trees
14: nid006510:229443:232281 [1] NCCL INFO Connected all trees
24: nid006563:221141:223977 [1] NCCL INFO Connected all trees
26: nid006565:222466:225311 [1] NCCL INFO Connected all trees
 6: nid006501:221944:224741 [3] NCCL INFO Connected all trees
 6: nid006501:221943:224744 [2] NCCL INFO Connected all trees
20: nid006558:215470:218299 [3] NCCL INFO Connected all trees
18: nid006556:210777:213694 [3] NCCL INFO Connected all trees
19: nid006557:208420:211246 [3] NCCL INFO Connected all trees
18: nid006556:210776:213697 [2] NCCL INFO Connected all trees
23: nid006561:220712:223530 [1] NCCL INFO Connected all trees
19: nid006557:208419:211247 [2] NCCL INFO Connected all trees
28: nid007251:72170:75297 [1] NCCL INFO Connected all trees
27: nid006566:216749:219575 [3] NCCL INFO Connected all trees
31: nid007342:59768:63054 [1] NCCL INFO Connected all trees
20: nid006558:215469:218297 [2] NCCL INFO Connected all trees
25: nid006564:223022:225818 [1] NCCL INFO Connected all trees
27: nid006566:216748:219576 [2] NCCL INFO Connected all trees
10: nid006506:263730:266531 [3] NCCL INFO Connected all trees
12: nid006508:205768:208691 [2] NCCL INFO Connected all trees
13: nid006509:201791:204593 [3] NCCL INFO Connected all trees
 5: nid006500:260085:262911 [2] NCCL INFO Connected all trees
10: nid006506:263729:266530 [2] NCCL INFO Connected all trees
17: nid006555:206596:209446 [3] NCCL INFO Connected all trees
14: nid006510:229445:232282 [3] NCCL INFO Connected all trees
29: nid007305:27223:29970 [1] NCCL INFO Connected all trees
17: nid006555:206595:209445 [2] NCCL INFO Connected all trees
14: nid006510:229444:232279 [2] NCCL INFO Connected all trees
24: nid006563:221143:223976 [3] NCCL INFO Connected all trees
26: nid006565:222468:225309 [3] NCCL INFO Connected all trees
21: nid006559:211124:213935 [1] NCCL INFO Connected all trees
24: nid006563:221142:223978 [2] NCCL INFO Connected all trees
30: nid007318:20582:23432 [2] NCCL INFO Connected all trees
22: nid006560:222272:225097 [1] NCCL INFO Connected all trees
25: nid006564:223023:225816 [2] NCCL INFO Connected all trees
23: nid006561:220714:223527 [3] NCCL INFO Connected all trees
31: nid007342:59770:63055 [3] NCCL INFO Connected all trees
23: nid006561:220713:223528 [2] NCCL INFO Connected all trees
31: nid007342:59769:63053 [2] NCCL INFO Connected all trees
29: nid007305:27225:29971 [3] NCCL INFO Connected all trees
26: nid006565:222467:225308 [2] NCCL INFO Connected all trees
21: nid006559:211126:213934 [3] NCCL INFO Connected all trees
29: nid007305:27224:29973 [2] NCCL INFO Connected all trees
22: nid006560:222274:225095 [3] NCCL INFO Connected all trees
21: nid006559:211125:213932 [2] NCCL INFO Connected all trees
22: nid006560:222273:225096 [2] NCCL INFO Connected all trees
30: nid007318:20583:23431 [3] NCCL INFO Connected all trees
25: nid006564:223024:225819 [3] NCCL INFO Connected all trees
30: nid007318:20581:23430 [1] NCCL INFO Connected all trees
 1: nid006496:242585:245430 [1] NCCL INFO Connected all trees
 1: nid006496:242587:245427 [3] NCCL INFO Connected all trees
 1: nid006496:242586:245428 [2] NCCL INFO Connected all trees
 3: nid006498:226766:229564 [1] NCCL INFO Connected all trees
 2: nid006497:227578:230383 [3] NCCL INFO Connected all trees
 3: nid006498:226768:229565 [3] NCCL INFO Connected all trees
 2: nid006497:227577:230375 [2] NCCL INFO Connected all trees
 3: nid006498:226767:229567 [2] NCCL INFO Connected all trees
28: nid007251:72171:75298 [2] NCCL INFO Connected all trees
28: nid007251:72172:75299 [3] NCCL INFO Connected all trees
 2: nid006497:227576:230385 [1] NCCL INFO Connected all trees
13: nid006509:201790:204592 [2] NCCL INFO Connected all trees
 0:  68%|   | 3001/4398 [00:21<00:09, 140.19it/s]
 0:                                                     
 0: 
 0: {'loss': 0.4289, 'grad_norm': 0.9704819262986172, 'learning_rate': 2.420707745193076e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3001/4398 [00:21<00:09, 140.19it/s]
 0:                                                     
 0: 
 0: {'loss': 0.4261, 'grad_norm': 1.3946958917240202, 'learning_rate': 2.4175540611221405e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3002/4398 [00:27<00:09, 140.19it/s]
 0:                                                     
 0: 
 0: {'loss': 0.426, 'grad_norm': 1.208478524036004, 'learning_rate': 2.414401777571035e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3003/4398 [00:32<00:09, 140.19it/s]
 0:                                                     
 0: 
 0: {'loss': 0.4591, 'grad_norm': 1.187867406951277, 'learning_rate': 2.4112508962493147e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3004/4398 [00:38<00:09, 140.19it/s]
 0:  68%|   | 3004/4398 [00:39<00:09, 140.19it/s]
 0:  68%|   | 3005/4398 [00:43<00:24, 56.25it/s] 
 0:                                                    
 0: 
 0: {'loss': 0.4699, 'grad_norm': 1.944642706962604, 'learning_rate': 2.408101418865776e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3005/4398 [00:43<00:24, 56.25it/s]
 0:  68%|   | 3006/4398 [00:49<00:30, 46.37it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4518, 'grad_norm': 1.0622065463081622, 'learning_rate': 2.4049533471284514e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3006/4398 [00:49<00:30, 46.37it/s]
 0:  68%|   | 3007/4398 [00:55<00:37, 36.97it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4679, 'grad_norm': 1.194303780683716, 'learning_rate': 2.4018066827446118e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3007/4398 [00:55<00:37, 36.97it/s]
 0:  68%|   | 3008/4398 [01:00<00:48, 28.88it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4172, 'grad_norm': 1.1822025264293103, 'learning_rate': 2.398661427420769e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3008/4398 [01:00<00:48, 28.88it/s]
 0:  68%|   | 3009/4398 [01:06<01:03, 21.92it/s]
 0:                                                    
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.062576910140534, 'learning_rate': 2.395517582862666e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3009/4398 [01:06<01:03, 21.92it/s]
 0:  68%|   | 3010/4398 [01:11<01:24, 16.39it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4099, 'grad_norm': 1.0191276431292133, 'learning_rate': 2.392375150775283e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3010/4398 [01:11<01:24, 16.39it/s]
 0:  68%|   | 3011/4398 [01:17<01:54, 12.09it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4371, 'grad_norm': 0.9910508019509823, 'learning_rate': 2.3892341328628306e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3011/4398 [01:17<01:54, 12.09it/s]
 0:  68%|   | 3012/4398 [01:22<02:38,  8.76it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4557, 'grad_norm': 1.2246623712712006, 'learning_rate': 2.3860945308287554e-06, 'epoch': 0.68}
 0: 
 0:  68%|   | 3012/4398 [01:22<02:38,  8.76it/s]
 0:  69%|   | 3013/4398 [01:28<03:40,  6.29it/s]
 0:                                                    
 0: 
 0: {'loss': 0.458, 'grad_norm': 1.4516057365883466, 'learning_rate': 2.382956346375737e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3013/4398 [01:28<03:40,  6.29it/s]
 0:  69%|   | 3014/4398 [01:34<05:05,  4.52it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4395, 'grad_norm': 1.0758105016369595, 'learning_rate': 2.3798195812056853e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3014/4398 [01:34<05:05,  4.52it/s]
 0:  69%|   | 3015/4398 [01:39<07:02,  3.27it/s]
 0:                                                    
 0: 
 0: {'loss': 0.446, 'grad_norm': 1.205259794186182, 'learning_rate': 2.3766842370197367e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3015/4398 [01:39<07:02,  3.27it/s]
 0:  69%|   | 3016/4398 [01:45<09:41,  2.38it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4249, 'grad_norm': 0.9972427986551022, 'learning_rate': 2.3735503155182655e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3016/4398 [01:45<09:41,  2.38it/s]
 0:  69%|   | 3017/4398 [01:50<13:16,  1.73it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4221, 'grad_norm': 0.964292006036897, 'learning_rate': 2.3704178184008675e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3017/4398 [01:50<13:16,  1.73it/s]
 0:  69%|   | 3018/4398 [01:56<18:00,  1.28it/s]
 0:                                                    
 0: 
 0: {'loss': 0.4283, 'grad_norm': 1.1871690388866842, 'learning_rate': 2.3672867473663672e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3018/4398 [01:56<18:00,  1.28it/s]
 0:  69%|   | 3019/4398 [02:01<24:26,  1.06s/it]
 0:                                                    
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.2332796511145885, 'learning_rate': 2.3641571041128176e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3019/4398 [02:01<24:26,  1.06s/it]
 0:  69%|   | 3020/4398 [02:07<32:08,  1.40s/it]
 0:                                                    
 0: 
 0: {'loss': 0.4136, 'grad_norm': 1.1537401978558914, 'learning_rate': 2.361028890337495e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3020/4398 [02:07<32:08,  1.40s/it]
 0:  69%|   | 3021/4398 [02:13<41:28,  1.81s/it]
 0:                                                    
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.185839394672139, 'learning_rate': 2.3579021077369047e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3021/4398 [02:13<41:28,  1.81s/it]
 0:  69%|   | 3022/4398 [02:18<51:44,  2.26s/it]
 0:                                                    
 0: 
 0: {'loss': 0.4387, 'grad_norm': 1.0962107661299816, 'learning_rate': 2.3547767580067674e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3022/4398 [02:18<51:44,  2.26s/it]
 0:  69%|   | 3023/4398 [02:24<1:02:39,  2.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4261, 'grad_norm': 1.0425780046944406, 'learning_rate': 2.3516528428420343e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3023/4398 [02:24<1:02:39,  2.73s/it]
 0:  69%|   | 3024/4398 [02:29<1:13:28,  3.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4327, 'grad_norm': 1.2959070710941483, 'learning_rate': 2.3485303639368783e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3024/4398 [02:29<1:13:28,  3.21s/it]
 0:  69%|   | 3025/4398 [02:35<1:24:02,  3.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4627, 'grad_norm': 1.0410418081248274, 'learning_rate': 2.345409322984691e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3025/4398 [02:35<1:24:02,  3.67s/it]
 0:  69%|   | 3026/4398 [02:40<1:34:19,  4.13s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4665, 'grad_norm': 1.4231041761976522, 'learning_rate': 2.3422897216780836e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3026/4398 [02:40<1:34:19,  4.13s/it]
 0:  69%|   | 3027/4398 [02:46<1:45:22,  4.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4119, 'grad_norm': 0.9306735227548533, 'learning_rate': 2.339171561708889e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3027/4398 [02:46<1:45:22,  4.61s/it]
 0:  69%|   | 3028/4398 [02:52<1:51:18,  4.88s/it]
 0:                                                      
 0: 
 0: {'loss': 0.408, 'grad_norm': 0.9925144944022553, 'learning_rate': 2.3360548447681556e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3028/4398 [02:52<1:51:18,  4.88s/it]
 0:  69%|   | 3029/4398 [03:09<3:02:14,  7.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4045, 'grad_norm': 0.9877606765972344, 'learning_rate': 2.3329395725461513e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3029/4398 [03:09<3:02:14,  7.99s/it]
 0:  69%|   | 3030/4398 [03:17<3:07:42,  8.23s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4809, 'grad_norm': 1.2708566633338458, 'learning_rate': 2.3298257467323605e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3030/4398 [03:17<3:07:42,  8.23s/it]
 0:  69%|   | 3031/4398 [03:23<2:52:11,  7.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4616, 'grad_norm': 0.9853568294700141, 'learning_rate': 2.3267133690154814e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3031/4398 [03:23<2:52:11,  7.56s/it]
 0:  69%|   | 3032/4398 [03:37<3:35:20,  9.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.451, 'grad_norm': 1.0569761704279008, 'learning_rate': 2.3236024410834284e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3032/4398 [03:37<3:35:20,  9.46s/it]
 0:  69%|   | 3033/4398 [03:43<3:09:03,  8.31s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4054, 'grad_norm': 0.9958923603670025, 'learning_rate': 2.32049296462333e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3033/4398 [03:43<3:09:03,  8.31s/it]
 0:  69%|   | 3034/4398 [03:48<2:49:22,  7.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4519, 'grad_norm': 1.085524372599815, 'learning_rate': 2.317384941321526e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3034/4398 [03:48<2:49:22,  7.45s/it]
 0:  69%|   | 3035/4398 [03:54<2:35:49,  6.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4099, 'grad_norm': 1.096839910898233, 'learning_rate': 2.314278372863569e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3035/4398 [03:54<2:35:49,  6.86s/it]
 0:  69%|   | 3036/4398 [03:59<2:26:27,  6.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4436, 'grad_norm': 1.2120186100343815, 'learning_rate': 2.311173260934223e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3036/4398 [03:59<2:26:27,  6.45s/it]
 0:  69%|   | 3037/4398 [04:05<2:19:47,  6.16s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4391, 'grad_norm': 2.542480184182764, 'learning_rate': 2.3080696072174613e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3037/4398 [04:05<2:19:47,  6.16s/it]
 0:  69%|   | 3038/4398 [04:10<2:15:05,  5.96s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3939, 'grad_norm': 0.9933738196763098, 'learning_rate': 2.3049674133964667e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3038/4398 [04:10<2:15:05,  5.96s/it]
 0:  69%|   | 3039/4398 [04:16<2:11:53,  5.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4538, 'grad_norm': 1.0877171457018198, 'learning_rate': 2.3018666811536307e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3039/4398 [04:16<2:11:53,  5.82s/it]
 0:  69%|   | 3040/4398 [04:21<2:08:58,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4579, 'grad_norm': 1.163044919252036, 'learning_rate': 2.2987674121705504e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3040/4398 [04:21<2:08:58,  5.70s/it]
 0:  69%|   | 3041/4398 [04:27<2:06:53,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4658, 'grad_norm': 1.1724329197361751, 'learning_rate': 2.2956696081280305e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3041/4398 [04:27<2:06:53,  5.61s/it]
 0:  69%|   | 3042/4398 [04:32<2:06:04,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4234, 'grad_norm': 1.1019145482481105, 'learning_rate': 2.292573270706084e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3042/4398 [04:32<2:06:04,  5.58s/it]
 0:  69%|   | 3043/4398 [04:38<2:05:33,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4203, 'grad_norm': 1.2541462581393854, 'learning_rate': 2.289478401583924e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3043/4398 [04:38<2:05:33,  5.56s/it]
 0:  69%|   | 3044/4398 [04:46<2:25:56,  6.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4354, 'grad_norm': 1.0739279222065075, 'learning_rate': 2.28638500243997e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3044/4398 [04:46<2:25:56,  6.47s/it]
 0:  69%|   | 3045/4398 [04:52<2:18:36,  6.15s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4529, 'grad_norm': 1.0421277247228242, 'learning_rate': 2.2832930749518447e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3045/4398 [04:52<2:18:36,  6.15s/it]
 0:  69%|   | 3046/4398 [04:57<2:13:24,  5.92s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4424, 'grad_norm': 1.1453132087923994, 'learning_rate': 2.280202620796368e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3046/4398 [04:57<2:13:24,  5.92s/it]
 0:  69%|   | 3047/4398 [05:02<2:09:40,  5.76s/it]
 0:                                                      
 0: 
 0: {'loss': 0.462, 'grad_norm': 0.9628477748251215, 'learning_rate': 2.277113641649566e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3047/4398 [05:02<2:09:40,  5.76s/it]
 0:  69%|   | 3048/4398 [05:08<2:06:56,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4638, 'grad_norm': 0.9572302148487697, 'learning_rate': 2.2740261391866634e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3048/4398 [05:08<2:06:56,  5.64s/it]
 0:  69%|   | 3049/4398 [05:13<2:05:52,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4422, 'grad_norm': 1.0129545617156661, 'learning_rate': 2.270940115082082e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3049/4398 [05:13<2:05:52,  5.60s/it]
 0:  69%|   | 3050/4398 [05:19<2:05:08,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4228, 'grad_norm': 1.0158652338445144, 'learning_rate': 2.2678555710094468e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3050/4398 [05:19<2:05:08,  5.57s/it]
 0:  69%|   | 3051/4398 [05:24<2:03:54,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4703, 'grad_norm': 1.2873065065121032, 'learning_rate': 2.2647725086415755e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3051/4398 [05:24<2:03:54,  5.52s/it]
 0:  69%|   | 3052/4398 [05:29<2:02:41,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4524, 'grad_norm': 1.1449047336610543, 'learning_rate': 2.261690929650484e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3052/4398 [05:29<2:02:41,  5.47s/it]
 0:  69%|   | 3053/4398 [05:35<2:02:07,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.455, 'grad_norm': 1.086311046874735, 'learning_rate': 2.2586108357073834e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3053/4398 [05:35<2:02:07,  5.45s/it]
 0:  69%|   | 3054/4398 [05:40<2:01:36,  5.43s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4933, 'grad_norm': 1.0846407569823036, 'learning_rate': 2.2555322284826786e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3054/4398 [05:40<2:01:36,  5.43s/it]
 0:  69%|   | 3055/4398 [05:46<2:01:50,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4796, 'grad_norm': 1.1064309226040714, 'learning_rate': 2.2524551096459703e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3055/4398 [05:46<2:01:50,  5.44s/it]
 0:  69%|   | 3056/4398 [05:51<2:01:34,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4339, 'grad_norm': 1.2009305968362736, 'learning_rate': 2.24937948086605e-06, 'epoch': 0.69}
 0: 
 0:  69%|   | 3056/4398 [05:51<2:01:34,  5.44s/it]
 0:  70%|   | 3057/4398 [05:57<2:02:01,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4834, 'grad_norm': 1.1037797484031078, 'learning_rate': 2.2463053438109023e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3057/4398 [05:57<2:02:01,  5.46s/it]
 0:  70%|   | 3058/4398 [06:02<2:01:27,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4053, 'grad_norm': 1.0528870360088454, 'learning_rate': 2.2432327001477022e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3058/4398 [06:02<2:01:27,  5.44s/it]
 0:  70%|   | 3059/4398 [06:07<2:01:18,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 1.322833294628964, 'learning_rate': 2.2401615515428153e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3059/4398 [06:07<2:01:18,  5.44s/it]
 0:  70%|   | 3060/4398 [06:13<2:00:57,  5.42s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4102, 'grad_norm': 1.0162999272480806, 'learning_rate': 2.2370918996617962e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3060/4398 [06:13<2:00:57,  5.42s/it]
 0:  70%|   | 3061/4398 [06:18<2:01:28,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4599, 'grad_norm': 1.2916449765007014, 'learning_rate': 2.2340237461693876e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3061/4398 [06:18<2:01:28,  5.45s/it]
 0:  70%|   | 3062/4398 [06:24<2:01:12,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.5027, 'grad_norm': 1.2462292624588056, 'learning_rate': 2.2309570927295195e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3062/4398 [06:24<2:01:12,  5.44s/it]
 0:  70%|   | 3063/4398 [06:29<2:01:34,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4514, 'grad_norm': 1.1961129749806332, 'learning_rate': 2.2278919410053085e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3063/4398 [06:29<2:01:34,  5.46s/it]
 0:  70%|   | 3064/4398 [06:35<2:02:44,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4412, 'grad_norm': 1.0640243914645116, 'learning_rate': 2.2248282926590577e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3064/4398 [06:35<2:02:44,  5.52s/it]
 0:  70%|   | 3065/4398 [06:40<2:01:53,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4385, 'grad_norm': 1.0395210781029098, 'learning_rate': 2.221766149352253e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3065/4398 [06:40<2:01:53,  5.49s/it]
 0:  70%|   | 3066/4398 [06:46<2:01:52,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4343, 'grad_norm': 1.2066198862631834, 'learning_rate': 2.2187055127455653e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3066/4398 [06:46<2:01:52,  5.49s/it]
 0:  70%|   | 3067/4398 [06:51<2:01:00,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4127, 'grad_norm': 1.1060903259856467, 'learning_rate': 2.2156463844988475e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3067/4398 [06:51<2:01:00,  5.45s/it]
 0:  70%|   | 3068/4398 [06:57<2:01:48,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 1.0818895955068044, 'learning_rate': 2.21258876627114e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3068/4398 [06:57<2:01:48,  5.49s/it]
 0:  70%|   | 3069/4398 [07:02<2:00:57,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4753, 'grad_norm': 1.2154845346713812, 'learning_rate': 2.209532659720653e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3069/4398 [07:02<2:00:57,  5.46s/it]
 0:  70%|   | 3070/4398 [07:08<2:00:47,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4501, 'grad_norm': 1.026374875280189, 'learning_rate': 2.206478066504787e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3070/4398 [07:08<2:00:47,  5.46s/it]
 0:  70%|   | 3071/4398 [07:13<2:00:34,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.407, 'grad_norm': 0.9674170160984595, 'learning_rate': 2.2034249882801167e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3071/4398 [07:13<2:00:34,  5.45s/it]
 0:  70%|   | 3072/4398 [07:19<2:00:48,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4176, 'grad_norm': 1.1176969761236555, 'learning_rate': 2.2003734267023975e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3072/4398 [07:19<2:00:48,  5.47s/it]
 0:  70%|   | 3073/4398 [07:24<2:00:42,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4278, 'grad_norm': 1.1958000538981133, 'learning_rate': 2.197323383426561e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3073/4398 [07:24<2:00:42,  5.47s/it]
 0:  70%|   | 3074/4398 [07:30<2:00:49,  5.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.0551605323528048, 'learning_rate': 2.194274860106716e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3074/4398 [07:30<2:00:49,  5.48s/it]
 0:  70%|   | 3075/4398 [07:35<2:00:42,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4263, 'grad_norm': 1.3009242498898392, 'learning_rate': 2.1912278583961454e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3075/4398 [07:35<2:00:42,  5.47s/it]
 0:  70%|   | 3076/4398 [07:40<2:00:46,  5.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4495, 'grad_norm': 1.025125689025867, 'learning_rate': 2.1881823799473113e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3076/4398 [07:41<2:00:46,  5.48s/it]
 0:  70%|   | 3077/4398 [07:46<2:00:39,  5.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4586, 'grad_norm': 1.1166898355428336, 'learning_rate': 2.1851384264118447e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3077/4398 [07:46<2:00:39,  5.48s/it]
 0:  70%|   | 3078/4398 [07:51<2:00:21,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4755, 'grad_norm': 1.0837926110336706, 'learning_rate': 2.1820959994405522e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3078/4398 [07:51<2:00:21,  5.47s/it]
 0:  70%|   | 3079/4398 [07:57<1:59:52,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4118, 'grad_norm': 1.0925087289311082, 'learning_rate': 2.1790551006834122e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3079/4398 [07:57<1:59:52,  5.45s/it]
 0:  70%|   | 3080/4398 [08:02<1:59:29,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4173, 'grad_norm': 1.9857989524022066, 'learning_rate': 2.176015731789575e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3080/4398 [08:02<1:59:29,  5.44s/it]
 0:  70%|   | 3081/4398 [08:08<1:59:31,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4629, 'grad_norm': 1.1025020676950017, 'learning_rate': 2.172977894407357e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3081/4398 [08:08<1:59:31,  5.45s/it]
 0:  70%|   | 3082/4398 [08:13<1:59:14,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4292, 'grad_norm': 1.2396407037098671, 'learning_rate': 2.1699415901842492e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3082/4398 [08:13<1:59:14,  5.44s/it]
 0:  70%|   | 3083/4398 [08:19<1:59:35,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4719, 'grad_norm': 0.9788117191929536, 'learning_rate': 2.1669068207669076e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3083/4398 [08:19<1:59:35,  5.46s/it]
 0:  70%|   | 3084/4398 [08:24<1:59:08,  5.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4421, 'grad_norm': 1.1962883195004586, 'learning_rate': 2.1638735878011603e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3084/4398 [08:24<1:59:08,  5.44s/it]
 0:  70%|   | 3085/4398 [08:29<1:58:51,  5.43s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4296, 'grad_norm': 1.2096618490226252, 'learning_rate': 2.1608418929319985e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3085/4398 [08:29<1:58:51,  5.43s/it]
 0:  70%|   | 3086/4398 [08:35<1:58:27,  5.42s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4759, 'grad_norm': 1.1013017847647828, 'learning_rate': 2.157811737803579e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3086/4398 [08:35<1:58:27,  5.42s/it]
 0:  70%|   | 3087/4398 [08:40<1:58:25,  5.42s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4633, 'grad_norm': 1.3244945224018585, 'learning_rate': 2.1547831240592253e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3087/4398 [08:40<1:58:25,  5.42s/it]
 0:  70%|   | 3088/4398 [08:46<1:59:05,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4688, 'grad_norm': 1.1457993660471455, 'learning_rate': 2.1517560533414246e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3088/4398 [08:46<1:59:05,  5.45s/it]
 0:  70%|   | 3089/4398 [08:51<1:59:29,  5.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.451, 'grad_norm': 1.4720990106620804, 'learning_rate': 2.1487305272918263e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3089/4398 [08:51<1:59:29,  5.48s/it]
 0:  70%|   | 3090/4398 [08:57<1:59:36,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4315, 'grad_norm': 1.007924389332382, 'learning_rate': 2.1457065475512433e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3090/4398 [08:57<1:59:36,  5.49s/it]
 0:  70%|   | 3091/4398 [09:02<1:59:00,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4374, 'grad_norm': 0.9902922815134393, 'learning_rate': 2.1426841157596483e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3091/4398 [09:02<1:59:00,  5.46s/it]
 0:  70%|   | 3092/4398 [09:08<1:58:37,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4239, 'grad_norm': 0.9889146368020919, 'learning_rate': 2.1396632335561775e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3092/4398 [09:08<1:58:37,  5.45s/it]
 0:  70%|   | 3093/4398 [09:13<1:58:27,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4141, 'grad_norm': 1.1306261625879495, 'learning_rate': 2.1366439025791226e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3093/4398 [09:13<1:58:27,  5.45s/it]
 0:  70%|   | 3094/4398 [09:19<1:58:26,  5.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.457, 'grad_norm': 1.5025533691638364, 'learning_rate': 2.1336261244659377e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3094/4398 [09:19<1:58:26,  5.45s/it]
 0:  70%|   | 3095/4398 [09:24<1:58:45,  5.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4454, 'grad_norm': 1.0958073749343946, 'learning_rate': 2.1306099008532327e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3095/4398 [09:24<1:58:45,  5.47s/it]
 0:  70%|   | 3096/4398 [09:29<1:58:25,  5.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 1.1292469574694735, 'learning_rate': 2.1275952333767756e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3096/4398 [09:29<1:58:25,  5.46s/it]
 0:  70%|   | 3097/4398 [09:56<4:17:13, 11.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4046, 'grad_norm': 1.1048857869768287, 'learning_rate': 2.124582123671489e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3097/4398 [09:56<4:17:13, 11.86s/it]
 0:  70%|   | 3098/4398 [10:17<5:15:51, 14.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4263, 'grad_norm': 1.135780843556044, 'learning_rate': 2.1215705733714523e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3098/4398 [10:17<5:15:51, 14.58s/it]
 0:  70%|   | 3099/4398 [10:23<4:17:09, 11.88s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4328, 'grad_norm': 1.0731723209637485, 'learning_rate': 2.118560584109899e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3099/4398 [10:23<4:17:09, 11.88s/it]
 0:  70%|   | 3100/4398 [10:28<3:36:06,  9.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4653, 'grad_norm': 1.3766939869005956, 'learning_rate': 2.115552157519214e-06, 'epoch': 0.7}
 0: 
 0:  70%|   | 3100/4398 [10:28<3:36:06,  9.99s/it]
 0:  71%|   | 3101/4398 [10:34<3:06:43,  8.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4548, 'grad_norm': 1.0414819837015203, 'learning_rate': 2.1125452952309357e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3101/4398 [10:34<3:06:43,  8.64s/it]
 0:  71%|   | 3102/4398 [10:39<2:46:14,  7.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4229, 'grad_norm': 1.1322649013570274, 'learning_rate': 2.1095399988757574e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3102/4398 [10:39<2:46:14,  7.70s/it]
 0:  71%|   | 3103/4398 [10:45<2:31:43,  7.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4586, 'grad_norm': 1.0979831932610766, 'learning_rate': 2.1065362700835214e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3103/4398 [10:45<2:31:43,  7.03s/it]
 0:  71%|   | 3104/4398 [10:50<2:21:37,  6.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4827, 'grad_norm': 1.1039182960815823, 'learning_rate': 2.1035341104832142e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3104/4398 [10:50<2:21:37,  6.57s/it]
 0:  71%|   | 3105/4398 [11:06<3:20:25,  9.30s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4554, 'grad_norm': 1.2636382780053483, 'learning_rate': 2.100533521702978e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3105/4398 [11:06<3:20:25,  9.30s/it]
 0:  71%|   | 3106/4398 [11:11<2:55:28,  8.15s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4487, 'grad_norm': 1.1406381874166722, 'learning_rate': 2.0975345053701025e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3106/4398 [11:11<2:55:28,  8.15s/it]
 0:  71%|   | 3107/4398 [11:17<2:37:53,  7.34s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4108, 'grad_norm': 0.9994828745818043, 'learning_rate': 2.0945370631110218e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3107/4398 [11:17<2:37:53,  7.34s/it]
 0:  71%|   | 3108/4398 [11:22<2:25:33,  6.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4461, 'grad_norm': 1.0923870548009953, 'learning_rate': 2.0915411965513182e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3108/4398 [11:22<2:25:33,  6.77s/it]
 0:  71%|   | 3109/4398 [11:28<2:17:02,  6.38s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4506, 'grad_norm': 1.1283131445722439, 'learning_rate': 2.088546907315717e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3109/4398 [11:28<2:17:02,  6.38s/it]
 0:  71%|   | 3110/4398 [11:33<2:10:50,  6.10s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4447, 'grad_norm': 1.0936671898569466, 'learning_rate': 2.0855541970280946e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3110/4398 [11:33<2:10:50,  6.10s/it]
 0:  71%|   | 3111/4398 [11:39<2:06:33,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4352, 'grad_norm': 1.1878103623646696, 'learning_rate': 2.0825630673114635e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3111/4398 [11:39<2:06:33,  5.90s/it]
 0:  71%|   | 3112/4398 [11:44<2:03:32,  5.76s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4611, 'grad_norm': 1.1446707048517153, 'learning_rate': 2.079573519787983e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3112/4398 [11:44<2:03:32,  5.76s/it]
 0:  71%|   | 3113/4398 [11:52<2:19:44,  6.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4659, 'grad_norm': 1.0848411390130681, 'learning_rate': 2.0765855560789535e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3113/4398 [11:52<2:19:44,  6.53s/it]
 0:  71%|   | 3114/4398 [11:58<2:12:42,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4216, 'grad_norm': 1.0259977757474252, 'learning_rate': 2.0735991778048158e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3114/4398 [11:58<2:12:42,  6.20s/it]
 0:  71%|   | 3115/4398 [12:03<2:07:59,  5.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4243, 'grad_norm': 1.1244561088523322, 'learning_rate': 2.0706143865851545e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3115/4398 [12:03<2:07:59,  5.99s/it]
 0:  71%|   | 3116/4398 [12:09<2:05:02,  5.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.116303967555462, 'learning_rate': 2.067631184038686e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3116/4398 [12:09<2:05:02,  5.85s/it]
 0:  71%|   | 3117/4398 [12:14<2:02:16,  5.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.217633542456924, 'learning_rate': 2.064649571783269e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3117/4398 [12:14<2:02:16,  5.73s/it]
 0:  71%|   | 3118/4398 [12:20<2:00:23,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4621, 'grad_norm': 1.1632613592310839, 'learning_rate': 2.061669551435906e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3118/4398 [12:20<2:00:23,  5.64s/it]
 0:  71%|   | 3119/4398 [12:25<1:59:22,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4356, 'grad_norm': 1.1096344790720267, 'learning_rate': 2.0586911246127274e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3119/4398 [12:25<1:59:22,  5.60s/it]
 0:  71%|   | 3120/4398 [12:31<1:58:29,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4417, 'grad_norm': 1.2554548470467777, 'learning_rate': 2.0557142929290027e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3120/4398 [12:31<1:58:29,  5.56s/it]
 0:  71%|   | 3121/4398 [12:36<1:58:08,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4777, 'grad_norm': 1.253345079469469, 'learning_rate': 2.0527390579991363e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3121/4398 [12:36<1:58:08,  5.55s/it]
 0:  71%|   | 3122/4398 [12:42<1:57:37,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4381, 'grad_norm': 1.2103630760419817, 'learning_rate': 2.0497654214366663e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3122/4398 [12:42<1:57:37,  5.53s/it]
 0:  71%|   | 3123/4398 [12:47<1:57:03,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4283, 'grad_norm': 1.1015636022836934, 'learning_rate': 2.0467933848542642e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3123/4398 [12:47<1:57:03,  5.51s/it]
 0:  71%|   | 3124/4398 [12:53<1:56:52,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4119, 'grad_norm': 1.0421738397330982, 'learning_rate': 2.0438229498637347e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3124/4398 [12:53<1:56:52,  5.50s/it]
 0:  71%|   | 3125/4398 [12:58<1:56:39,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4469, 'grad_norm': 1.3442989320801129, 'learning_rate': 2.0408541180760123e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3125/4398 [12:58<1:56:39,  5.50s/it]
 0:  71%|   | 3126/4398 [13:04<1:56:28,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4356, 'grad_norm': 1.3543778528600932, 'learning_rate': 2.0378868911011624e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3126/4398 [13:04<1:56:28,  5.49s/it]
 0:  71%|   | 3127/4398 [13:09<1:56:31,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4238, 'grad_norm': 1.0030152311053204, 'learning_rate': 2.034921270548381e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3127/4398 [13:09<1:56:31,  5.50s/it]
 0:  71%|   | 3128/4398 [13:15<1:56:11,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4883, 'grad_norm': 1.1774517774036535, 'learning_rate': 2.031957258025992e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3128/4398 [13:15<1:56:11,  5.49s/it]
 0:  71%|   | 3129/4398 [13:20<1:56:38,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4327, 'grad_norm': 1.0456977286735685, 'learning_rate': 2.0289948551414486e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3129/4398 [13:20<1:56:38,  5.51s/it]
 0:  71%|   | 3130/4398 [13:26<1:56:12,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4535, 'grad_norm': 1.3945427103382153, 'learning_rate': 2.026034063501328e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3130/4398 [13:26<1:56:12,  5.50s/it]
 0:  71%|   | 3131/4398 [13:31<1:56:14,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4148, 'grad_norm': 1.023274760523591, 'learning_rate': 2.0230748847113375e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3131/4398 [13:31<1:56:14,  5.51s/it]
 0:  71%|   | 3132/4398 [13:37<1:56:03,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4448, 'grad_norm': 1.2499490578661618, 'learning_rate': 2.0201173203763068e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3132/4398 [13:37<1:56:03,  5.50s/it]
 0:  71%|   | 3133/4398 [13:42<1:56:26,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.1211116214411414, 'learning_rate': 2.0171613721001913e-06, 'epoch': 0.71}
 0: 
 0:  71%|   | 3133/4398 [13:42<1:56:26,  5.52s/it]
 0:  71%|  | 3134/4398 [13:48<1:56:12,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3986, 'grad_norm': 1.0566472184344955, 'learning_rate': 2.0142070414860704e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3134/4398 [13:48<1:56:12,  5.52s/it]
 0:  71%|  | 3135/4398 [13:53<1:56:03,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3927, 'grad_norm': 1.1664157750807025, 'learning_rate': 2.0112543301361426e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3135/4398 [13:53<1:56:03,  5.51s/it]
 0:  71%|  | 3136/4398 [13:59<1:56:20,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4345, 'grad_norm': 1.1176951004623987, 'learning_rate': 2.0083032396517354e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3136/4398 [13:59<1:56:20,  5.53s/it]
 0:  71%|  | 3137/4398 [14:04<1:56:04,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4227, 'grad_norm': 1.3945022279052384, 'learning_rate': 2.005353771633291e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3137/4398 [14:04<1:56:04,  5.52s/it]
 0:  71%|  | 3138/4398 [14:10<1:55:47,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.422, 'grad_norm': 1.1010820457125812, 'learning_rate': 2.0024059276803742e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3138/4398 [14:10<1:55:47,  5.51s/it]
 0:  71%|  | 3139/4398 [14:15<1:55:21,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4365, 'grad_norm': 0.9910686523174013, 'learning_rate': 1.99945970939167e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3139/4398 [14:15<1:55:21,  5.50s/it]
 0:  71%|  | 3140/4398 [14:21<1:55:08,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4275, 'grad_norm': 1.013262160264789, 'learning_rate': 1.9965151183649766e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3140/4398 [14:21<1:55:08,  5.49s/it]
 0:  71%|  | 3141/4398 [14:26<1:55:02,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4443, 'grad_norm': 1.1324421053177947, 'learning_rate': 1.993572156197216e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3141/4398 [14:26<1:55:02,  5.49s/it]
 0:  71%|  | 3142/4398 [14:32<1:54:56,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4229, 'grad_norm': 1.2452984124952473, 'learning_rate': 1.990630824484423e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3142/4398 [14:32<1:54:56,  5.49s/it]
 0:  71%|  | 3143/4398 [14:37<1:54:42,  5.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4061, 'grad_norm': 1.9207892373906486, 'learning_rate': 1.987691124821749e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3143/4398 [14:37<1:54:42,  5.48s/it]
 0:  71%|  | 3144/4398 [14:43<1:54:39,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4317, 'grad_norm': 1.311804171760536, 'learning_rate': 1.9847530588034635e-06, 'epoch': 0.71}
 0: 
 0:  71%|  | 3144/4398 [14:43<1:54:39,  5.49s/it]
 0:  72%|  | 3145/4398 [14:48<1:54:38,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4676, 'grad_norm': 1.0483165411187099, 'learning_rate': 1.9818166280229457e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3145/4398 [14:48<1:54:38,  5.49s/it]
 0:  72%|  | 3146/4398 [14:54<1:54:51,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4362, 'grad_norm': 1.09407987674935, 'learning_rate': 1.9788818340726896e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3146/4398 [14:54<1:54:51,  5.50s/it]
 0:  72%|  | 3147/4398 [14:59<1:54:40,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4342, 'grad_norm': 1.0748372204150705, 'learning_rate': 1.975948678544301e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3147/4398 [14:59<1:54:40,  5.50s/it]
 0:  72%|  | 3148/4398 [15:05<1:54:29,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4262, 'grad_norm': 1.0689569868027187, 'learning_rate': 1.9730171630284985e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3148/4398 [15:05<1:54:29,  5.50s/it]
 0:  72%|  | 3149/4398 [15:10<1:54:21,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.413, 'grad_norm': 1.065443425821959, 'learning_rate': 1.9700872891151096e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3149/4398 [15:10<1:54:21,  5.49s/it]
 0:  72%|  | 3150/4398 [15:16<1:54:09,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4571, 'grad_norm': 1.0322803973626105, 'learning_rate': 1.9671590583930744e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3150/4398 [15:16<1:54:09,  5.49s/it]
 0:  72%|  | 3151/4398 [15:21<1:54:03,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4563, 'grad_norm': 1.0674964877553772, 'learning_rate': 1.9642324724504346e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3151/4398 [15:21<1:54:03,  5.49s/it]
 0:  72%|  | 3152/4398 [15:27<1:54:04,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4508, 'grad_norm': 1.1831356817392038, 'learning_rate': 1.96130753287435e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3152/4398 [15:27<1:54:04,  5.49s/it]
 0:  72%|  | 3153/4398 [15:32<1:54:01,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.495, 'grad_norm': 0.99544487333303, 'learning_rate': 1.958384241251082e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3153/4398 [15:32<1:54:01,  5.50s/it]
 0:  72%|  | 3154/4398 [15:38<1:53:59,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.478, 'grad_norm': 1.0325230536007768, 'learning_rate': 1.955462599165997e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3154/4398 [15:38<1:53:59,  5.50s/it]
 0:  72%|  | 3155/4398 [15:43<1:53:47,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4451, 'grad_norm': 1.1278482740159779, 'learning_rate': 1.95254260820357e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3155/4398 [15:43<1:53:47,  5.49s/it]
 0:  72%|  | 3156/4398 [15:49<1:53:47,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4588, 'grad_norm': 1.138076504896387, 'learning_rate': 1.949624269947378e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3156/4398 [15:49<1:53:47,  5.50s/it]
 0:  72%|  | 3157/4398 [15:54<1:53:35,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.436, 'grad_norm': 1.0824135868412295, 'learning_rate': 1.946707585980105e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3157/4398 [15:54<1:53:35,  5.49s/it]
 0:  72%|  | 3158/4398 [16:00<1:53:22,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3992, 'grad_norm': 1.1889636682713907, 'learning_rate': 1.943792557883534e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3158/4398 [16:00<1:53:22,  5.49s/it]
 0:  72%|  | 3159/4398 [16:05<1:53:21,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4031, 'grad_norm': 0.8904153646041775, 'learning_rate': 1.9408791872385528e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3159/4398 [16:05<1:53:21,  5.49s/it]
 0:  72%|  | 3160/4398 [16:11<1:53:19,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4289, 'grad_norm': 1.115923649591991, 'learning_rate': 1.9379674756251495e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3160/4398 [16:11<1:53:19,  5.49s/it]
 0:  72%|  | 3161/4398 [16:16<1:53:23,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4655, 'grad_norm': 1.1389440218445754, 'learning_rate': 1.935057424622411e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3161/4398 [16:16<1:53:23,  5.50s/it]
 0:  72%|  | 3162/4398 [16:22<1:53:05,  5.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4574, 'grad_norm': 1.0130641500990254, 'learning_rate': 1.9321490358085253e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3162/4398 [16:22<1:53:05,  5.49s/it]
 0:  72%|  | 3163/4398 [16:27<1:53:31,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4257, 'grad_norm': 0.9918680448016161, 'learning_rate': 1.929242310760779e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3163/4398 [16:27<1:53:31,  5.52s/it]
 0:  72%|  | 3164/4398 [16:33<1:53:18,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4273, 'grad_norm': 1.146497521586519, 'learning_rate': 1.9263372510555567e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3164/4398 [16:33<1:53:18,  5.51s/it]
 0:  72%|  | 3165/4398 [17:01<4:14:38, 12.39s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4448, 'grad_norm': 1.0807453903587887, 'learning_rate': 1.923433858268338e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3165/4398 [17:01<4:14:38, 12.39s/it]
 0:  72%|  | 3166/4398 [17:52<8:08:17, 23.78s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4483, 'grad_norm': 1.2423975672444354, 'learning_rate': 1.920532133973699e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3166/4398 [17:52<8:08:17, 23.78s/it]
 0:  72%|  | 3167/4398 [17:57<6:15:19, 18.29s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4361, 'grad_norm': 1.0694927950704247, 'learning_rate': 1.917632079745313e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3167/4398 [17:57<6:15:19, 18.29s/it]
 0:  72%|  | 3168/4398 [18:02<4:56:14, 14.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4728, 'grad_norm': 1.18117304485692, 'learning_rate': 1.914733697155945e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3168/4398 [18:02<4:56:14, 14.45s/it]
 0:  72%|  | 3169/4398 [18:08<4:01:01, 11.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4774, 'grad_norm': 1.0425462250491864, 'learning_rate': 1.9118369877774535e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3169/4398 [18:08<4:01:01, 11.77s/it]
 0:  72%|  | 3170/4398 [18:13<3:22:18,  9.88s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4348, 'grad_norm': 0.971943195813937, 'learning_rate': 1.908941953180794e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3170/4398 [18:13<3:22:18,  9.88s/it]
 0:  72%|  | 3171/4398 [18:19<2:55:23,  8.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.056253929746302, 'learning_rate': 1.906048594936008e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3171/4398 [18:19<2:55:23,  8.58s/it]
 0:  72%|  | 3172/4398 [18:37<3:51:51, 11.35s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4621, 'grad_norm': 1.1370755668862675, 'learning_rate': 1.9031569146122308e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3172/4398 [18:37<3:51:51, 11.35s/it]
 0:  72%|  | 3173/4398 [18:42<3:16:06,  9.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4215, 'grad_norm': 1.0538238215322044, 'learning_rate': 1.9002669137776874e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3173/4398 [18:42<3:16:06,  9.61s/it]
 0:  72%|  | 3174/4398 [18:48<2:51:15,  8.40s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4291, 'grad_norm': 1.0543833764041841, 'learning_rate': 1.897378593999693e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3174/4398 [18:48<2:51:15,  8.40s/it]
 0:  72%|  | 3175/4398 [18:53<2:33:28,  7.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4504, 'grad_norm': 1.1660558750024743, 'learning_rate': 1.8944919568446463e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3175/4398 [18:53<2:33:28,  7.53s/it]
 0:  72%|  | 3176/4398 [18:59<2:20:40,  6.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4009, 'grad_norm': 1.1045279963830794, 'learning_rate': 1.8916070038780387e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3176/4398 [18:59<2:20:40,  6.91s/it]
 0:  72%|  | 3177/4398 [19:04<2:12:27,  6.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4337, 'grad_norm': 1.0187679976342363, 'learning_rate': 1.8887237366644452e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3177/4398 [19:04<2:12:27,  6.51s/it]
 0:  72%|  | 3178/4398 [19:10<2:06:14,  6.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4272, 'grad_norm': 1.0273774286141373, 'learning_rate': 1.8858421567675311e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3178/4398 [19:10<2:06:14,  6.21s/it]
 0:  72%|  | 3179/4398 [19:19<2:20:45,  6.93s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4174, 'grad_norm': 0.9198963379188441, 'learning_rate': 1.8829622657500424e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3179/4398 [19:19<2:20:45,  6.93s/it]
 0:  72%|  | 3180/4398 [19:24<2:11:48,  6.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3995, 'grad_norm': 1.0695207521824264, 'learning_rate': 1.8800840651738096e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3180/4398 [19:24<2:11:48,  6.49s/it]
 0:  72%|  | 3181/4398 [19:30<2:05:44,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4106, 'grad_norm': 1.1338455565516588, 'learning_rate': 1.8772075565997466e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3181/4398 [19:30<2:05:44,  6.20s/it]
 0:  72%|  | 3182/4398 [19:35<2:01:22,  5.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3922, 'grad_norm': 1.4643249152122377, 'learning_rate': 1.8743327415878509e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3182/4398 [19:35<2:01:22,  5.99s/it]
 0:  72%|  | 3183/4398 [19:41<1:58:23,  5.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4336, 'grad_norm': 1.084485571499036, 'learning_rate': 1.8714596216972008e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3183/4398 [19:41<1:58:23,  5.85s/it]
 0:  72%|  | 3184/4398 [19:46<1:56:04,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4597, 'grad_norm': 1.3151542866518153, 'learning_rate': 1.868588198485955e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3184/4398 [19:46<1:56:04,  5.74s/it]
 0:  72%|  | 3185/4398 [19:52<1:54:38,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.444, 'grad_norm': 1.0602056751116617, 'learning_rate': 1.865718473511352e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3185/4398 [19:52<1:54:38,  5.67s/it]
 0:  72%|  | 3186/4398 [19:57<1:53:31,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4478, 'grad_norm': 1.0386309343178282, 'learning_rate': 1.862850448329711e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3186/4398 [19:57<1:53:31,  5.62s/it]
 0:  72%|  | 3187/4398 [20:03<1:52:46,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4682, 'grad_norm': 1.0853853906306545, 'learning_rate': 1.8599841244964267e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3187/4398 [20:03<1:52:46,  5.59s/it]
 0:  72%|  | 3188/4398 [20:08<1:51:58,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.0668244576246595, 'learning_rate': 1.857119503565973e-06, 'epoch': 0.72}
 0: 
 0:  72%|  | 3188/4398 [20:08<1:51:58,  5.55s/it]
 0:  73%|  | 3189/4398 [20:14<1:51:35,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4529, 'grad_norm': 0.9789267816621822, 'learning_rate': 1.8542565870918994e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3189/4398 [20:14<1:51:35,  5.54s/it]
 0:  73%|  | 3190/4398 [20:19<1:51:14,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.443, 'grad_norm': 1.078954697512653, 'learning_rate': 1.8513953766268316e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3190/4398 [20:19<1:51:14,  5.52s/it]
 0:  73%|  | 3191/4398 [20:25<1:50:56,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4076, 'grad_norm': 0.9526003603212719, 'learning_rate': 1.8485358737224702e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3191/4398 [20:25<1:50:56,  5.52s/it]
 0:  73%|  | 3192/4398 [20:30<1:51:09,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4479, 'grad_norm': 1.0598534520064675, 'learning_rate': 1.8456780799295888e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3192/4398 [20:30<1:51:09,  5.53s/it]
 0:  73%|  | 3193/4398 [20:36<1:50:58,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4458, 'grad_norm': 1.0508120675575936, 'learning_rate': 1.8428219967980343e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3193/4398 [20:36<1:50:58,  5.53s/it]
 0:  73%|  | 3194/4398 [20:41<1:50:40,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4376, 'grad_norm': 1.077462471177061, 'learning_rate': 1.8399676258767273e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3194/4398 [20:41<1:50:40,  5.52s/it]
 0:  73%|  | 3195/4398 [20:47<1:50:31,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4215, 'grad_norm': 1.1397190025340969, 'learning_rate': 1.8371149687136585e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3195/4398 [20:47<1:50:31,  5.51s/it]
 0:  73%|  | 3196/4398 [20:52<1:50:19,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4471, 'grad_norm': 0.9831114967352825, 'learning_rate': 1.8342640268558882e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3196/4398 [20:52<1:50:19,  5.51s/it]
 0:  73%|  | 3197/4398 [20:58<1:50:17,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4146, 'grad_norm': 1.0312915694751963, 'learning_rate': 1.8314148018495513e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3197/4398 [20:58<1:50:17,  5.51s/it]
 0:  73%|  | 3198/4398 [21:03<1:50:09,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3976, 'grad_norm': 1.1260628003921271, 'learning_rate': 1.8285672952398448e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3198/4398 [21:03<1:50:09,  5.51s/it]
 0:  73%|  | 3199/4398 [21:09<1:50:09,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4214, 'grad_norm': 1.0883875787733626, 'learning_rate': 1.8257215085710373e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3199/4398 [21:09<1:50:09,  5.51s/it]
 0:  73%|  | 3200/4398 [21:14<1:49:56,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4873, 'grad_norm': 1.1914052996581443, 'learning_rate': 1.8228774433864649e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3200/4398 [21:14<1:49:56,  5.51s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  73%|  | 3201/4398 [22:26<8:27:14, 25.43s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3837, 'grad_norm': 1.113462138982791, 'learning_rate': 1.82003510122853e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3201/4398 [22:26<8:27:14, 25.43s/it]
 0:  73%|  | 3202/4398 [22:32<6:27:33, 19.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4387, 'grad_norm': 1.1220866757115953, 'learning_rate': 1.8171944836386996e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3202/4398 [22:32<6:27:33, 19.44s/it]
 0:  73%|  | 3203/4398 [22:37<5:03:51, 15.26s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4694, 'grad_norm': 1.1280848547412963, 'learning_rate': 1.8143555921575062e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3203/4398 [22:37<5:03:51, 15.26s/it]
 0:  73%|  | 3204/4398 [22:43<4:05:50, 12.35s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4311, 'grad_norm': 1.183738306727406, 'learning_rate': 1.8115184283245441e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3204/4398 [22:43<4:05:50, 12.35s/it]
 0:  73%|  | 3205/4398 [22:48<3:25:22, 10.33s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4403, 'grad_norm': 1.4298450132399585, 'learning_rate': 1.8086829936784755e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3205/4398 [22:48<3:25:22, 10.33s/it]
 0:  73%|  | 3206/4398 [22:54<2:56:21,  8.88s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4545, 'grad_norm': 1.0488382512635988, 'learning_rate': 1.8058492897570212e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3206/4398 [22:54<2:56:21,  8.88s/it]
 0:  73%|  | 3207/4398 [22:59<2:36:02,  7.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.2127882136908767, 'learning_rate': 1.803017318096963e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3207/4398 [22:59<2:36:02,  7.86s/it]
 0:  73%|  | 3208/4398 [23:05<2:21:47,  7.15s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4342, 'grad_norm': 1.198608195563754, 'learning_rate': 1.8001870802341448e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3208/4398 [23:05<2:21:47,  7.15s/it]
 0:  73%|  | 3209/4398 [23:10<2:12:19,  6.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4527, 'grad_norm': 1.595373495493679, 'learning_rate': 1.7973585777034714e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3209/4398 [23:10<2:12:19,  6.68s/it]
 0:  73%|  | 3210/4398 [23:16<2:05:26,  6.34s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4723, 'grad_norm': 1.2457227773357356, 'learning_rate': 1.794531812038901e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3210/4398 [23:16<2:05:26,  6.34s/it]
 0:  73%|  | 3211/4398 [23:21<2:00:34,  6.09s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4529, 'grad_norm': 1.112508458537924, 'learning_rate': 1.7917067847734554e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3211/4398 [23:21<2:00:34,  6.09s/it]
 0:  73%|  | 3212/4398 [23:27<1:56:53,  5.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.0026840401168766, 'learning_rate': 1.7888834974392106e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3212/4398 [23:27<1:56:53,  5.91s/it]
 0:  73%|  | 3213/4398 [23:32<1:54:33,  5.80s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4521, 'grad_norm': 1.114462126564105, 'learning_rate': 1.7860619515673034e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3213/4398 [23:32<1:54:33,  5.80s/it]
 0:  73%|  | 3214/4398 [23:38<1:52:35,  5.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.2220777279847728, 'learning_rate': 1.7832421486879215e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3214/4398 [23:38<1:52:35,  5.71s/it]
 0:  73%|  | 3215/4398 [23:43<1:51:19,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4633, 'grad_norm': 1.279911020207214, 'learning_rate': 1.7804240903303082e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3215/4398 [23:43<1:51:19,  5.65s/it]
 0:  73%|  | 3216/4398 [23:49<1:50:28,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4265, 'grad_norm': 1.0586619398082202, 'learning_rate': 1.7776077780227619e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3216/4398 [23:49<1:50:28,  5.61s/it]
 0:  73%|  | 3217/4398 [23:54<1:49:48,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4339, 'grad_norm': 1.5426232694562612, 'learning_rate': 1.7747932132926338e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3217/4398 [23:54<1:49:48,  5.58s/it]
 0:  73%|  | 3218/4398 [24:00<1:49:19,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4455, 'grad_norm': 1.0435120668877889, 'learning_rate': 1.7719803976663264e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3218/4398 [24:00<1:49:19,  5.56s/it]
 0:  73%|  | 3219/4398 [24:05<1:48:56,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4395, 'grad_norm': 1.0929906268152516, 'learning_rate': 1.7691693326692944e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3219/4398 [24:05<1:48:56,  5.54s/it]
 0:  73%|  | 3220/4398 [24:11<1:48:55,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.0268325207032498, 'learning_rate': 1.766360019826044e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3220/4398 [24:11<1:48:55,  5.55s/it]
 0:  73%|  | 3221/4398 [24:17<1:48:41,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4442, 'grad_norm': 0.9669893299109038, 'learning_rate': 1.763552460660129e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3221/4398 [24:17<1:48:41,  5.54s/it]
 0:  73%|  | 3222/4398 [24:22<1:48:22,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.2737411225936743, 'learning_rate': 1.7607466566941534e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3222/4398 [24:22<1:48:22,  5.53s/it]
 0:  73%|  | 3223/4398 [24:28<1:48:21,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4202, 'grad_norm': 0.9871357294818428, 'learning_rate': 1.7579426094497703e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3223/4398 [24:28<1:48:21,  5.53s/it]
 0:  73%|  | 3224/4398 [24:33<1:48:05,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.437, 'grad_norm': 1.089603079235489, 'learning_rate': 1.755140320447678e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3224/4398 [24:33<1:48:05,  5.52s/it]
 0:  73%|  | 3225/4398 [24:39<1:48:02,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4005, 'grad_norm': 1.0310104596927792, 'learning_rate': 1.7523397912076223e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3225/4398 [24:39<1:48:02,  5.53s/it]
 0:  73%|  | 3226/4398 [24:44<1:47:59,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4647, 'grad_norm': 1.0491871996966506, 'learning_rate': 1.749541023248395e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3226/4398 [24:44<1:47:59,  5.53s/it]
 0:  73%|  | 3227/4398 [24:50<1:47:51,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4328, 'grad_norm': 1.065967939201258, 'learning_rate': 1.7467440180878326e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3227/4398 [24:50<1:47:51,  5.53s/it]
 0:  73%|  | 3228/4398 [24:55<1:47:48,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4853, 'grad_norm': 1.0397582974251744, 'learning_rate': 1.7439487772428142e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3228/4398 [24:55<1:47:48,  5.53s/it]
 0:  73%|  | 3229/4398 [25:01<1:48:14,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.446, 'grad_norm': 1.0768956819154782, 'learning_rate': 1.7411553022292643e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3229/4398 [25:01<1:48:14,  5.56s/it]
 0:  73%|  | 3230/4398 [25:06<1:47:46,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4829, 'grad_norm': 1.1109449589931526, 'learning_rate': 1.7383635945621468e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3230/4398 [25:06<1:47:46,  5.54s/it]
 0:  73%|  | 3231/4398 [25:12<1:47:44,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4204, 'grad_norm': 1.3384423481657988, 'learning_rate': 1.735573655755472e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3231/4398 [25:12<1:47:44,  5.54s/it]
 0:  73%|  | 3232/4398 [25:17<1:47:23,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.433, 'grad_norm': 1.0707716278530126, 'learning_rate': 1.7327854873222883e-06, 'epoch': 0.73}
 0: 
 0:  73%|  | 3232/4398 [25:17<1:47:23,  5.53s/it]
 0:  74%|  | 3233/4398 [25:44<3:49:36, 11.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4028, 'grad_norm': 1.0284425922545841, 'learning_rate': 1.7299990907746794e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3233/4398 [25:44<3:49:36, 11.83s/it]
 0:  74%|  | 3234/4398 [27:27<12:43:48, 39.37s/it]
 0:                                                       
 0: 
 0: {'loss': 0.4464, 'grad_norm': 1.0148229806730285, 'learning_rate': 1.727214467623775e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3234/4398 [27:28<12:43:48, 39.37s/it]
 0:  74%|  | 3235/4398 [27:35<9:38:40, 29.85s/it] 
 0:                                                      
 0: 
 0: {'loss': 0.4191, 'grad_norm': 1.0918793045203725, 'learning_rate': 1.7244316193797395e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3235/4398 [27:35<9:38:40, 29.85s/it]
 0:  74%|  | 3236/4398 [27:41<7:16:31, 22.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4258, 'grad_norm': 1.0302803060221737, 'learning_rate': 1.721650547551776e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3236/4398 [27:41<7:16:31, 22.54s/it]
 0:  74%|  | 3237/4398 [27:46<5:37:23, 17.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.0216811320214563, 'learning_rate': 1.7188712536481233e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3237/4398 [27:46<5:37:23, 17.44s/it]
 0:  74%|  | 3238/4398 [27:52<4:27:57, 13.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4757, 'grad_norm': 1.0069783616073713, 'learning_rate': 1.7160937391760552e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3238/4398 [27:52<4:27:57, 13.86s/it]
 0:  74%|  | 3239/4398 [27:57<3:39:12, 11.35s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4458, 'grad_norm': 1.0523271510420682, 'learning_rate': 1.713318005641884e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3239/4398 [27:57<3:39:12, 11.35s/it]
 0:  74%|  | 3240/4398 [28:03<3:05:16,  9.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4058, 'grad_norm': 1.0014015955329034, 'learning_rate': 1.7105440545509533e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3240/4398 [28:03<3:05:16,  9.60s/it]
 0:  74%|  | 3241/4398 [28:08<2:41:27,  8.37s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4302, 'grad_norm': 1.0453907283608055, 'learning_rate': 1.7077718874076405e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3241/4398 [28:08<2:41:27,  8.37s/it]
 0:  74%|  | 3242/4398 [28:14<2:24:35,  7.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4309, 'grad_norm': 1.010942983390894, 'learning_rate': 1.7050015057153552e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3242/4398 [28:14<2:24:35,  7.51s/it]
 0:  74%|  | 3243/4398 [28:19<2:12:57,  6.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4477, 'grad_norm': 1.0148039529660486, 'learning_rate': 1.702232910976539e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3243/4398 [28:19<2:12:57,  6.91s/it]
 0:  74%|  | 3244/4398 [28:25<2:04:40,  6.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4581, 'grad_norm': 1.047383441638413, 'learning_rate': 1.6994661046926663e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3244/4398 [28:25<2:04:40,  6.48s/it]
 0:  74%|  | 3245/4398 [28:30<1:58:56,  6.19s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4406, 'grad_norm': 1.043602365214965, 'learning_rate': 1.6967010883642366e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3245/4398 [28:30<1:58:56,  6.19s/it]
 0:  74%|  | 3246/4398 [28:36<1:54:49,  5.98s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4133, 'grad_norm': 0.9910752600986361, 'learning_rate': 1.6939378634907815e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3246/4398 [28:36<1:54:49,  5.98s/it]
 0:  74%|  | 3247/4398 [28:41<1:52:00,  5.84s/it]
 0:                                                      
 0: 
 0: {'loss': 0.442, 'grad_norm': 1.051070745297151, 'learning_rate': 1.691176431570865e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3247/4398 [28:41<1:52:00,  5.84s/it]
 0:  74%|  | 3248/4398 [28:47<1:50:00,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.426, 'grad_norm': 1.189067822040123, 'learning_rate': 1.688416794102074e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3248/4398 [28:47<1:50:00,  5.74s/it]
 0:  74%|  | 3249/4398 [28:52<1:48:33,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4581, 'grad_norm': 0.9924117374822505, 'learning_rate': 1.6856589525810229e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3249/4398 [28:52<1:48:33,  5.67s/it]
 0:  74%|  | 3250/4398 [28:58<1:47:31,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4662, 'grad_norm': 1.0303414254773837, 'learning_rate': 1.682902908503352e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3250/4398 [28:58<1:47:31,  5.62s/it]
 0:  74%|  | 3251/4398 [29:03<1:46:49,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4707, 'grad_norm': 1.1775688667274897, 'learning_rate': 1.6801486633637283e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3251/4398 [29:03<1:46:49,  5.59s/it]
 0:  74%|  | 3252/4398 [29:09<1:46:22,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4271, 'grad_norm': 0.9676207653980006, 'learning_rate': 1.6773962186558418e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3252/4398 [29:09<1:46:22,  5.57s/it]
 0:  74%|  | 3253/4398 [29:14<1:45:55,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4122, 'grad_norm': 1.076361710081614, 'learning_rate': 1.6746455758724067e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3253/4398 [29:14<1:45:55,  5.55s/it]
 0:  74%|  | 3254/4398 [29:20<1:45:32,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4036, 'grad_norm': 1.0180894113593772, 'learning_rate': 1.6718967365051597e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3254/4398 [29:20<1:45:32,  5.54s/it]
 0:  74%|  | 3255/4398 [29:25<1:45:24,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4532, 'grad_norm': 1.0050534273998333, 'learning_rate': 1.6691497020448598e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3255/4398 [29:25<1:45:24,  5.53s/it]
 0:  74%|  | 3256/4398 [29:31<1:45:13,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.395, 'grad_norm': 0.9403978991614919, 'learning_rate': 1.6664044739812873e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3256/4398 [29:31<1:45:13,  5.53s/it]
 0:  74%|  | 3257/4398 [29:36<1:45:33,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4597, 'grad_norm': 1.1014092078483797, 'learning_rate': 1.663661053803242e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3257/4398 [29:36<1:45:33,  5.55s/it]
 0:  74%|  | 3258/4398 [29:42<1:45:07,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4619, 'grad_norm': 1.0393440478171398, 'learning_rate': 1.6609194429985437e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3258/4398 [29:42<1:45:07,  5.53s/it]
 0:  74%|  | 3259/4398 [29:47<1:44:58,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4578, 'grad_norm': 1.0539306495319818, 'learning_rate': 1.6581796430540314e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3259/4398 [29:47<1:44:58,  5.53s/it]
 0:  74%|  | 3260/4398 [29:53<1:44:41,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4194, 'grad_norm': 0.9904144002215235, 'learning_rate': 1.6554416554555613e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3260/4398 [29:53<1:44:41,  5.52s/it]
 0:  74%|  | 3261/4398 [29:58<1:44:32,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4411, 'grad_norm': 1.1886297342076564, 'learning_rate': 1.6527054816880073e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3261/4398 [29:58<1:44:32,  5.52s/it]
 0:  74%|  | 3262/4398 [30:04<1:44:23,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4607, 'grad_norm': 1.1090804005452668, 'learning_rate': 1.6499711232352588e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3262/4398 [30:04<1:44:23,  5.51s/it]
 0:  74%|  | 3263/4398 [30:09<1:44:12,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3538, 'grad_norm': 1.1490177727768864, 'learning_rate': 1.6472385815802222e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3263/4398 [30:09<1:44:12,  5.51s/it]
 0:  74%|  | 3264/4398 [30:15<1:44:05,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4125, 'grad_norm': 0.9883012734811156, 'learning_rate': 1.6445078582048158e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3264/4398 [30:15<1:44:05,  5.51s/it]
 0:  74%|  | 3265/4398 [30:20<1:44:06,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4476, 'grad_norm': 1.1400153724215851, 'learning_rate': 1.6417789545899764e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3265/4398 [30:20<1:44:06,  5.51s/it]
 0:  74%|  | 3266/4398 [30:26<1:43:55,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4383, 'grad_norm': 1.3381987865441025, 'learning_rate': 1.6390518722156495e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3266/4398 [30:26<1:43:55,  5.51s/it]
 0:  74%|  | 3267/4398 [30:31<1:43:50,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4236, 'grad_norm': 0.9956814700572694, 'learning_rate': 1.6363266125607958e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3267/4398 [30:31<1:43:50,  5.51s/it]
 0:  74%|  | 3268/4398 [30:37<1:43:37,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4375, 'grad_norm': 0.9743788889472752, 'learning_rate': 1.6336031771033866e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3268/4398 [30:37<1:43:37,  5.50s/it]
 0:  74%|  | 3269/4398 [30:42<1:43:28,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4326, 'grad_norm': 0.9149177665783352, 'learning_rate': 1.6308815673204009e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3269/4398 [30:42<1:43:28,  5.50s/it]
 0:  74%|  | 3270/4398 [30:48<1:43:28,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4609, 'grad_norm': 1.081844591642023, 'learning_rate': 1.6281617846878323e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3270/4398 [30:48<1:43:28,  5.50s/it]
 0:  74%|  | 3271/4398 [30:53<1:43:25,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4596, 'grad_norm': 1.0403873151558012, 'learning_rate': 1.6254438306806809e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3271/4398 [30:53<1:43:25,  5.51s/it]
 0:  74%|  | 3272/4398 [30:59<1:43:11,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4503, 'grad_norm': 1.0416251260300204, 'learning_rate': 1.6227277067729536e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3272/4398 [30:59<1:43:11,  5.50s/it]
 0:  74%|  | 3273/4398 [31:04<1:43:10,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4092, 'grad_norm': 1.0114966575679065, 'learning_rate': 1.6200134144376711e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3273/4398 [31:04<1:43:10,  5.50s/it]
 0:  74%|  | 3274/4398 [31:10<1:43:12,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4396, 'grad_norm': 1.0541740565347841, 'learning_rate': 1.6173009551468543e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3274/4398 [31:10<1:43:12,  5.51s/it]
 0:  74%|  | 3275/4398 [31:15<1:43:07,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4729, 'grad_norm': 1.1731892547038008, 'learning_rate': 1.6145903303715333e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3275/4398 [31:15<1:43:07,  5.51s/it]
 0:  74%|  | 3276/4398 [31:21<1:43:07,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4652, 'grad_norm': 1.0027000299703934, 'learning_rate': 1.6118815415817408e-06, 'epoch': 0.74}
 0: 
 0:  74%|  | 3276/4398 [31:21<1:43:07,  5.52s/it]
 0:  75%|  | 3277/4398 [31:27<1:43:02,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3857, 'grad_norm': 0.941280836754942, 'learning_rate': 1.6091745902465162e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3277/4398 [31:27<1:43:02,  5.52s/it]
 0:  75%|  | 3278/4398 [31:32<1:43:18,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4459, 'grad_norm': 1.0024973978443676, 'learning_rate': 1.6064694778339007e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3278/4398 [31:32<1:43:18,  5.53s/it]
 0:  75%|  | 3279/4398 [31:38<1:43:14,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4448, 'grad_norm': 1.1602294951867387, 'learning_rate': 1.6037662058109416e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3279/4398 [31:38<1:43:14,  5.54s/it]
 0:  75%|  | 3280/4398 [31:43<1:42:57,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4605, 'grad_norm': 1.1140413675530187, 'learning_rate': 1.6010647756436798e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3280/4398 [31:43<1:42:57,  5.53s/it]
 0:  75%|  | 3281/4398 [31:49<1:42:47,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3941, 'grad_norm': 1.0615459989602003, 'learning_rate': 1.5983651887971685e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3281/4398 [31:49<1:42:47,  5.52s/it]
 0:  75%|  | 3282/4398 [31:54<1:42:37,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4557, 'grad_norm': 1.1018229604781182, 'learning_rate': 1.5956674467354538e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3282/4398 [31:54<1:42:37,  5.52s/it]
 0:  75%|  | 3283/4398 [32:00<1:42:32,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4318, 'grad_norm': 0.986912131180862, 'learning_rate': 1.5929715509215832e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3283/4398 [32:00<1:42:32,  5.52s/it]
 0:  75%|  | 3284/4398 [32:05<1:42:30,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4454, 'grad_norm': 1.1172010662537406, 'learning_rate': 1.5902775028176032e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3284/4398 [32:05<1:42:30,  5.52s/it]
 0:  75%|  | 3285/4398 [32:11<1:42:29,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4919, 'grad_norm': 1.101231837221866, 'learning_rate': 1.587585303884558e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3285/4398 [32:11<1:42:29,  5.53s/it]
 0:  75%|  | 3286/4398 [32:16<1:42:47,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4757, 'grad_norm': 0.9960291735145382, 'learning_rate': 1.5848949555824888e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3286/4398 [32:16<1:42:47,  5.55s/it]
 0:  75%|  | 3287/4398 [32:22<1:42:30,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4345, 'grad_norm': 1.0985086490408862, 'learning_rate': 1.5822064593704334e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3287/4398 [32:22<1:42:30,  5.54s/it]
 0:  75%|  | 3288/4398 [32:27<1:42:11,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3893, 'grad_norm': 1.1228325292457475, 'learning_rate': 1.5795198167064252e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3288/4398 [32:27<1:42:11,  5.52s/it]
 0:  75%|  | 3289/4398 [32:33<1:42:05,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4476, 'grad_norm': 1.0939841026052404, 'learning_rate': 1.5768350290474927e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3289/4398 [32:33<1:42:05,  5.52s/it]
 0:  75%|  | 3290/4398 [32:38<1:41:57,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4423, 'grad_norm': 1.1854891592999097, 'learning_rate': 1.574152097849656e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3290/4398 [32:38<1:41:57,  5.52s/it]
 0:  75%|  | 3291/4398 [32:44<1:41:48,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4122, 'grad_norm': 1.011695796240243, 'learning_rate': 1.5714710245679348e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3291/4398 [32:44<1:41:48,  5.52s/it]
 0:  75%|  | 3292/4398 [32:49<1:41:44,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4081, 'grad_norm': 0.9655802884805573, 'learning_rate': 1.5687918106563326e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3292/4398 [32:49<1:41:44,  5.52s/it]
 0:  75%|  | 3293/4398 [32:55<1:41:39,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4189, 'grad_norm': 1.0597330935187306, 'learning_rate': 1.5661144575678504e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3293/4398 [32:55<1:41:39,  5.52s/it]
 0:  75%|  | 3294/4398 [33:00<1:41:41,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4531, 'grad_norm': 0.9546258137481022, 'learning_rate': 1.5634389667544785e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3294/4398 [33:00<1:41:41,  5.53s/it]
 0:  75%|  | 3295/4398 [33:06<1:41:41,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4847, 'grad_norm': 1.0214718650353594, 'learning_rate': 1.5607653396671963e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3295/4398 [33:06<1:41:41,  5.53s/it]
 0:  75%|  | 3296/4398 [33:27<3:04:14, 10.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4073, 'grad_norm': 0.9886146860059624, 'learning_rate': 1.5580935777559736e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3296/4398 [33:27<3:04:14, 10.03s/it]
 0:  75%|  | 3297/4398 [33:32<2:39:20,  8.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4186, 'grad_norm': 1.0888160819775567, 'learning_rate': 1.5554236824697688e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3297/4398 [33:32<2:39:20,  8.68s/it]
 0:  75%|  | 3298/4398 [33:38<2:21:56,  7.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4173, 'grad_norm': 1.0038531047017347, 'learning_rate': 1.5527556552565254e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3298/4398 [33:38<2:21:56,  7.74s/it]
 0:  75%|  | 3299/4398 [33:43<2:09:50,  7.09s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4241, 'grad_norm': 1.0727887989873184, 'learning_rate': 1.5500894975631792e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3299/4398 [33:43<2:09:50,  7.09s/it]
 0:  75%|  | 3300/4398 [33:49<2:01:14,  6.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4326, 'grad_norm': 1.0558183523673768, 'learning_rate': 1.5474252108356475e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3300/4398 [33:49<2:01:14,  6.63s/it]
 0:  75%|  | 3301/4398 [34:06<3:01:22,  9.92s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.102025551035722, 'learning_rate': 1.5447627965188345e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3301/4398 [34:06<3:01:22,  9.92s/it]
 0:  75%|  | 3302/4398 [35:45<11:05:28, 36.43s/it]
 0:                                                       
 0: 
 0: {'loss': 0.4496, 'grad_norm': 1.0768789431987795, 'learning_rate': 1.5421022560566273e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3302/4398 [35:45<11:05:28, 36.43s/it]
 0:  75%|  | 3303/4398 [35:54<8:37:23, 28.35s/it] 
 0:                                                      
 0: 
 0: {'loss': 0.4208, 'grad_norm': 0.9626992690855924, 'learning_rate': 1.5394435908919015e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3303/4398 [35:54<8:37:23, 28.35s/it]
 0:  75%|  | 3304/4398 [36:00<6:32:07, 21.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4619, 'grad_norm': 1.0000221630646116, 'learning_rate': 1.5367868024665083e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3304/4398 [36:00<6:32:07, 21.51s/it]
 0:  75%|  | 3305/4398 [36:05<5:04:15, 16.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.445, 'grad_norm': 1.0934984279498052, 'learning_rate': 1.5341318922212873e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3305/4398 [36:05<5:04:15, 16.70s/it]
 0:  75%|  | 3306/4398 [36:11<4:02:50, 13.34s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4161, 'grad_norm': 1.004406276038915, 'learning_rate': 1.5314788615960551e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3306/4398 [36:11<4:02:50, 13.34s/it]
 0:  75%|  | 3307/4398 [36:16<3:19:50, 10.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4184, 'grad_norm': 1.1920972733231365, 'learning_rate': 1.5288277120296146e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3307/4398 [36:16<3:19:50, 10.99s/it]
 0:  75%|  | 3308/4398 [36:22<2:49:37,  9.34s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4032, 'grad_norm': 1.0218499701532606, 'learning_rate': 1.5261784449597445e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3308/4398 [36:22<2:49:37,  9.34s/it]
 0:  75%|  | 3309/4398 [36:27<2:29:02,  8.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4178, 'grad_norm': 1.0337038185594487, 'learning_rate': 1.523531061823202e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3309/4398 [36:27<2:29:02,  8.21s/it]
 0:  75%|  | 3310/4398 [36:33<2:14:11,  7.40s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4486, 'grad_norm': 1.064588893971227, 'learning_rate': 1.520885564055724e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3310/4398 [36:33<2:14:11,  7.40s/it]
 0:  75%|  | 3311/4398 [36:38<2:04:11,  6.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4471, 'grad_norm': 1.0254245115806362, 'learning_rate': 1.5182419530920256e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3311/4398 [36:38<2:04:11,  6.86s/it]
 0:  75%|  | 3312/4398 [36:44<1:56:41,  6.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4193, 'grad_norm': 1.0086890999009044, 'learning_rate': 1.5156002303657975e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3312/4398 [36:44<1:56:41,  6.45s/it]
 0:  75%|  | 3313/4398 [36:49<1:51:31,  6.17s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4594, 'grad_norm': 3.3889266821247213, 'learning_rate': 1.5129603973097061e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3313/4398 [36:49<1:51:31,  6.17s/it]
 0:  75%|  | 3314/4398 [36:55<1:47:35,  5.96s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4489, 'grad_norm': 1.097894044850704, 'learning_rate': 1.5103224553553947e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3314/4398 [36:55<1:47:35,  5.96s/it]
 0:  75%|  | 3315/4398 [37:00<1:45:08,  5.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4655, 'grad_norm': 1.0569629357146908, 'learning_rate': 1.507686405933479e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3315/4398 [37:00<1:45:08,  5.83s/it]
 0:  75%|  | 3316/4398 [37:06<1:43:09,  5.72s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4237, 'grad_norm': 0.9267346235033852, 'learning_rate': 1.5050522504735492e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3316/4398 [37:06<1:43:09,  5.72s/it]
 0:  75%|  | 3317/4398 [37:11<1:41:50,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4636, 'grad_norm': 1.0753972814360728, 'learning_rate': 1.5024199904041693e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3317/4398 [37:11<1:41:50,  5.65s/it]
 0:  75%|  | 3318/4398 [37:17<1:40:44,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4515, 'grad_norm': 1.0301553818109879, 'learning_rate': 1.499789627152874e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3318/4398 [37:17<1:40:44,  5.60s/it]
 0:  75%|  | 3319/4398 [37:22<1:40:10,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4231, 'grad_norm': 1.1339109734753048, 'learning_rate': 1.497161162146169e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3319/4398 [37:22<1:40:10,  5.57s/it]
 0:  75%|  | 3320/4398 [37:28<1:39:33,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.0847965614034458, 'learning_rate': 1.494534596809532e-06, 'epoch': 0.75}
 0: 
 0:  75%|  | 3320/4398 [37:28<1:39:33,  5.54s/it]
 0:  76%|  | 3321/4398 [37:33<1:39:13,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4715, 'grad_norm': 1.1091625751187904, 'learning_rate': 1.4919099325674103e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3321/4398 [37:33<1:39:13,  5.53s/it]
 0:  76%|  | 3322/4398 [37:39<1:38:49,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4458, 'grad_norm': 1.038037626381999, 'learning_rate': 1.4892871708432189e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3322/4398 [37:39<1:38:49,  5.51s/it]
 0:  76%|  | 3323/4398 [37:44<1:38:44,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4077, 'grad_norm': 1.1490974649994397, 'learning_rate': 1.4866663130593423e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3323/4398 [37:44<1:38:44,  5.51s/it]
 0:  76%|  | 3324/4398 [37:50<1:39:08,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4301, 'grad_norm': 0.9790231207791389, 'learning_rate': 1.4840473606371297e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3324/4398 [37:50<1:39:08,  5.54s/it]
 0:  76%|  | 3325/4398 [37:55<1:38:48,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.401, 'grad_norm': 0.9777020102733589, 'learning_rate': 1.4814303149969033e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3325/4398 [37:55<1:38:48,  5.52s/it]
 0:  76%|  | 3326/4398 [38:01<1:39:01,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4355, 'grad_norm': 1.0743119676482926, 'learning_rate': 1.4788151775579461e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3326/4398 [38:01<1:39:01,  5.54s/it]
 0:  76%|  | 3327/4398 [38:06<1:38:47,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4387, 'grad_norm': 0.9928020620617487, 'learning_rate': 1.4762019497385055e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3327/4398 [38:06<1:38:47,  5.53s/it]
 0:  76%|  | 3328/4398 [38:12<1:38:29,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3995, 'grad_norm': 1.044024629618659, 'learning_rate': 1.4735906329557959e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3328/4398 [38:12<1:38:29,  5.52s/it]
 0:  76%|  | 3329/4398 [38:17<1:38:25,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4607, 'grad_norm': 1.1237176307190873, 'learning_rate': 1.4709812286259938e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3329/4398 [38:17<1:38:25,  5.52s/it]
 0:  76%|  | 3330/4398 [38:23<1:38:13,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.395, 'grad_norm': 1.0157220782176686, 'learning_rate': 1.46837373816424e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3330/4398 [38:23<1:38:13,  5.52s/it]
 0:  76%|  | 3331/4398 [38:28<1:38:11,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4021, 'grad_norm': 0.9186930885228577, 'learning_rate': 1.465768162984636e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3331/4398 [38:28<1:38:11,  5.52s/it]
 0:  76%|  | 3332/4398 [38:34<1:37:55,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.459, 'grad_norm': 1.5158868707187314, 'learning_rate': 1.4631645045002445e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3332/4398 [38:34<1:37:55,  5.51s/it]
 0:  76%|  | 3333/4398 [38:39<1:37:44,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4131, 'grad_norm': 1.0743591423474075, 'learning_rate': 1.4605627641230917e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3333/4398 [38:39<1:37:44,  5.51s/it]
 0:  76%|  | 3334/4398 [38:45<1:37:30,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4095, 'grad_norm': 1.7778763489622886, 'learning_rate': 1.4579629432641601e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3334/4398 [38:45<1:37:30,  5.50s/it]
 0:  76%|  | 3335/4398 [38:50<1:37:26,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4146, 'grad_norm': 1.0854888463415193, 'learning_rate': 1.4553650433333915e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3335/4398 [38:50<1:37:26,  5.50s/it]
 0:  76%|  | 3336/4398 [38:56<1:37:17,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4658, 'grad_norm': 1.0595302663362196, 'learning_rate': 1.452769065739688e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3336/4398 [38:56<1:37:17,  5.50s/it]
 0:  76%|  | 3337/4398 [39:01<1:37:18,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4429, 'grad_norm': 1.0316694435628817, 'learning_rate': 1.4501750118909075e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3337/4398 [39:01<1:37:18,  5.50s/it]
 0:  76%|  | 3338/4398 [39:07<1:37:13,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4489, 'grad_norm': 1.1772014088714742, 'learning_rate': 1.4475828831938665e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3338/4398 [39:07<1:37:13,  5.50s/it]
 0:  76%|  | 3339/4398 [39:12<1:37:18,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4349, 'grad_norm': 1.107013982536907, 'learning_rate': 1.444992681054333e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3339/4398 [39:12<1:37:18,  5.51s/it]
 0:  76%|  | 3340/4398 [39:18<1:37:17,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4466, 'grad_norm': 1.0455284417904003, 'learning_rate': 1.4424044068770326e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3340/4398 [39:18<1:37:17,  5.52s/it]
 0:  76%|  | 3341/4398 [39:24<1:37:08,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.441, 'grad_norm': 1.3760708986130594, 'learning_rate': 1.43981806206565e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3341/4398 [39:24<1:37:08,  5.51s/it]
 0:  76%|  | 3342/4398 [39:29<1:37:02,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4497, 'grad_norm': 1.0764432376169155, 'learning_rate': 1.4372336480228172e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3342/4398 [39:29<1:37:02,  5.51s/it]
 0:  76%|  | 3343/4398 [39:35<1:37:03,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.423, 'grad_norm': 1.0270244591020947, 'learning_rate': 1.4346511661501223e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3343/4398 [39:35<1:37:03,  5.52s/it]
 0:  76%|  | 3344/4398 [39:40<1:36:56,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4577, 'grad_norm': 1.1864694170627557, 'learning_rate': 1.432070617848103e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3344/4398 [39:40<1:36:56,  5.52s/it]
 0:  76%|  | 3345/4398 [39:46<1:36:56,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4156, 'grad_norm': 1.0669793244983543, 'learning_rate': 1.4294920045162514e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3345/4398 [39:46<1:36:56,  5.52s/it]
 0:  76%|  | 3346/4398 [39:51<1:36:34,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4254, 'grad_norm': 1.0753688014034948, 'learning_rate': 1.4269153275530084e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3346/4398 [39:51<1:36:34,  5.51s/it]
 0:  76%|  | 3347/4398 [39:57<1:36:28,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.423, 'grad_norm': 0.9986387329139047, 'learning_rate': 1.4243405883557654e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3347/4398 [39:57<1:36:28,  5.51s/it]
 0:  76%|  | 3348/4398 [40:02<1:36:19,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4356, 'grad_norm': 1.1238137577360947, 'learning_rate': 1.4217677883208626e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3348/4398 [40:02<1:36:19,  5.50s/it]
 0:  76%|  | 3349/4398 [40:08<1:36:19,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4318, 'grad_norm': 2.28256283652781, 'learning_rate': 1.419196928843588e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3349/4398 [40:08<1:36:19,  5.51s/it]
 0:  76%|  | 3350/4398 [40:13<1:35:59,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4303, 'grad_norm': 1.0974233987368285, 'learning_rate': 1.4166280113181786e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3350/4398 [40:13<1:35:59,  5.50s/it]
 0:  76%|  | 3351/4398 [40:19<1:35:58,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.455, 'grad_norm': 1.0890375000598882, 'learning_rate': 1.4140610371378167e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3351/4398 [40:19<1:35:58,  5.50s/it]
 0:  76%|  | 3352/4398 [40:24<1:35:58,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4662, 'grad_norm': 1.4087014258218413, 'learning_rate': 1.4114960076946326e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3352/4398 [40:24<1:35:58,  5.51s/it]
 0:  76%|  | 3353/4398 [40:30<1:35:50,  5.50s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4492, 'grad_norm': 1.2810675787412324, 'learning_rate': 1.4089329243796994e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3353/4398 [40:30<1:35:50,  5.50s/it]
 0:  76%|  | 3354/4398 [40:44<2:24:42,  8.32s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4293, 'grad_norm': 1.2451188778421218, 'learning_rate': 1.4063717885830375e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3354/4398 [40:44<2:24:42,  8.32s/it]
 0:  76%|  | 3355/4398 [40:50<2:09:56,  7.48s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4737, 'grad_norm': 1.0790005486494403, 'learning_rate': 1.4038126016936087e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3355/4398 [40:50<2:09:56,  7.48s/it]
 0:  76%|  | 3356/4398 [40:56<2:00:03,  6.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4254, 'grad_norm': 1.0877983761793941, 'learning_rate': 1.4012553650993193e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3356/4398 [40:56<2:00:03,  6.91s/it]
 0:  76%|  | 3357/4398 [41:01<1:52:39,  6.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.0281423287827582, 'learning_rate': 1.3987000801870172e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3357/4398 [41:01<1:52:39,  6.49s/it]
 0:  76%|  | 3358/4398 [41:07<1:47:19,  6.19s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4765, 'grad_norm': 1.0270624022465469, 'learning_rate': 1.3961467483424912e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3358/4398 [41:07<1:47:19,  6.19s/it]
 0:  76%|  | 3359/4398 [41:12<1:43:49,  6.00s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4611, 'grad_norm': 1.0601139620364142, 'learning_rate': 1.3935953709504745e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3359/4398 [41:12<1:43:49,  6.00s/it]
 0:  76%|  | 3360/4398 [41:18<1:41:04,  5.84s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4239, 'grad_norm': 1.5314639009344118, 'learning_rate': 1.3910459493946371e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3360/4398 [41:18<1:41:04,  5.84s/it]
 0:  76%|  | 3361/4398 [41:23<1:39:19,  5.75s/it]
 0:                                                      
 0: 
 0: {'loss': 0.438, 'grad_norm': 1.0207053904080776, 'learning_rate': 1.388498485057589e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3361/4398 [41:23<1:39:19,  5.75s/it]
 0:  76%|  | 3362/4398 [41:31<1:48:01,  6.26s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4063, 'grad_norm': 1.0059013095196583, 'learning_rate': 1.385952979320877e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3362/4398 [41:31<1:48:01,  6.26s/it]
 0:  76%|  | 3363/4398 [41:36<1:44:04,  6.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4365, 'grad_norm': 1.142453861321444, 'learning_rate': 1.383409433564988e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3363/4398 [41:36<1:44:04,  6.03s/it]
 0:  76%|  | 3364/4398 [41:42<1:41:09,  5.87s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4111, 'grad_norm': 1.0220431104608876, 'learning_rate': 1.3808678491693455e-06, 'epoch': 0.76}
 0: 
 0:  76%|  | 3364/4398 [41:42<1:41:09,  5.87s/it]
 0:  77%|  | 3365/4398 [41:47<1:39:10,  5.76s/it]
 0:                                                      
 0: 
 0: {'loss': 0.418, 'grad_norm': 1.84558553982201, 'learning_rate': 1.3783282275123099e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3365/4398 [41:47<1:39:10,  5.76s/it]
 0:  77%|  | 3366/4398 [41:53<1:37:41,  5.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4102, 'grad_norm': 1.2120071511343227, 'learning_rate': 1.3757905699711743e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3366/4398 [41:53<1:37:41,  5.68s/it]
 0:  77%|  | 3367/4398 [41:58<1:36:42,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4109, 'grad_norm': 1.3244963190040906, 'learning_rate': 1.3732548779221717e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3367/4398 [41:58<1:36:42,  5.63s/it]
 0:  77%|  | 3368/4398 [42:04<1:36:03,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4189, 'grad_norm': 1.1340567404561623, 'learning_rate': 1.3707211527404651e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3368/4398 [42:04<1:36:03,  5.60s/it]
 0:  77%|  | 3369/4398 [42:17<2:16:48,  7.98s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4553, 'grad_norm': 1.1696246912662194, 'learning_rate': 1.3681893958001518e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3369/4398 [42:17<2:16:48,  7.98s/it]
 0:  77%|  | 3370/4398 [43:54<9:52:00, 34.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4693, 'grad_norm': 0.9975397262098957, 'learning_rate': 1.365659608474262e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3370/4398 [43:54<9:52:00, 34.55s/it]
 0:  77%|  | 3371/4398 [44:14<8:39:07, 30.33s/it]
 0:                                                      
 0: 
 0: {'loss': 0.423, 'grad_norm': 1.1181054416271763, 'learning_rate': 1.3631317921347564e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3371/4398 [44:14<8:39:07, 30.33s/it]
 0:  77%|  | 3372/4398 [44:20<6:31:10, 22.88s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4284, 'grad_norm': 1.047215817365352, 'learning_rate': 1.3606059481525296e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3372/4398 [44:20<6:31:10, 22.88s/it]
 0:  77%|  | 3373/4398 [44:25<5:01:41, 17.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4285, 'grad_norm': 1.0437182593906253, 'learning_rate': 1.3580820778974059e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3373/4398 [44:25<5:01:41, 17.66s/it]
 0:  77%|  | 3374/4398 [44:31<3:59:04, 14.01s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4358, 'grad_norm': 1.2248800456197486, 'learning_rate': 1.3555601827381326e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3374/4398 [44:31<3:59:04, 14.01s/it]
 0:  77%|  | 3375/4398 [44:36<3:15:22, 11.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4458, 'grad_norm': 1.103722021147449, 'learning_rate': 1.3530402640423973e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3375/4398 [44:36<3:15:22, 11.46s/it]
 0:  77%|  | 3376/4398 [44:42<2:44:42,  9.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4193, 'grad_norm': 1.3208887776845377, 'learning_rate': 1.3505223231768078e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3376/4398 [44:42<2:44:42,  9.67s/it]
 0:  77%|  | 3377/4398 [44:47<2:23:51,  8.45s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4893, 'grad_norm': 1.150337339800759, 'learning_rate': 1.348006361506901e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3377/4398 [44:47<2:23:51,  8.45s/it]
 0:  77%|  | 3378/4398 [44:53<2:08:43,  7.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4121, 'grad_norm': 0.9797793974724474, 'learning_rate': 1.345492380397142e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3378/4398 [44:53<2:08:43,  7.57s/it]
 0:  77%|  | 3379/4398 [44:58<1:58:06,  6.95s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4399, 'grad_norm': 1.0512563463356959, 'learning_rate': 1.3429803812109194e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3379/4398 [44:58<1:58:06,  6.95s/it]
 0:  77%|  | 3380/4398 [45:04<1:50:35,  6.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.429, 'grad_norm': 1.1005466859468838, 'learning_rate': 1.3404703653105482e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3380/4398 [45:04<1:50:35,  6.52s/it]
 0:  77%|  | 3381/4398 [45:09<1:45:22,  6.22s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4211, 'grad_norm': 1.155585045878921, 'learning_rate': 1.3379623340572684e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3381/4398 [45:09<1:45:22,  6.22s/it]
 0:  77%|  | 3382/4398 [45:15<1:41:39,  6.00s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4208, 'grad_norm': 1.1347077073713678, 'learning_rate': 1.3354562888112426e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3382/4398 [45:15<1:41:39,  6.00s/it]
 0:  77%|  | 3383/4398 [45:20<1:38:59,  5.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.449, 'grad_norm': 1.0990366982268454, 'learning_rate': 1.3329522309315568e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3383/4398 [45:20<1:38:59,  5.85s/it]
 0:  77%|  | 3384/4398 [45:26<1:37:35,  5.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4482, 'grad_norm': 1.2657425590711535, 'learning_rate': 1.3304501617762178e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3384/4398 [45:26<1:37:35,  5.77s/it]
 0:  77%|  | 3385/4398 [45:31<1:36:10,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4195, 'grad_norm': 1.1092562312144238, 'learning_rate': 1.3279500827021596e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3385/4398 [45:31<1:36:10,  5.70s/it]
 0:  77%|  | 3386/4398 [45:37<1:35:03,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3964, 'grad_norm': 1.041567014550281, 'learning_rate': 1.3254519950652284e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3386/4398 [45:37<1:35:03,  5.64s/it]
 0:  77%|  | 3387/4398 [45:42<1:34:13,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4099, 'grad_norm': 1.2519329166015811, 'learning_rate': 1.3229559002201957e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3387/4398 [45:42<1:34:13,  5.59s/it]
 0:  77%|  | 3388/4398 [45:48<1:33:46,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4222, 'grad_norm': 1.255174231736357, 'learning_rate': 1.3204617995207513e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3388/4398 [45:48<1:33:46,  5.57s/it]
 0:  77%|  | 3389/4398 [45:53<1:33:27,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3944, 'grad_norm': 1.2771976309221276, 'learning_rate': 1.317969694319503e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3389/4398 [45:53<1:33:27,  5.56s/it]
 0:  77%|  | 3390/4398 [45:59<1:33:00,  5.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4233, 'grad_norm': 0.9843880970164762, 'learning_rate': 1.3154795859679781e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3390/4398 [45:59<1:33:00,  5.54s/it]
 0:  77%|  | 3391/4398 [46:04<1:32:51,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4311, 'grad_norm': 1.298838804033221, 'learning_rate': 1.312991475816618e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3391/4398 [46:04<1:32:51,  5.53s/it]
 0:  77%|  | 3392/4398 [46:10<1:32:35,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4595, 'grad_norm': 1.897204166693982, 'learning_rate': 1.3105053652147814e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3392/4398 [46:10<1:32:35,  5.52s/it]
 0:  77%|  | 3393/4398 [46:16<1:32:37,  5.53s/it]
 0:                                                      
 0: 
 0: {'loss': 0.436, 'grad_norm': 1.0483294905796876, 'learning_rate': 1.3080212555107458e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3393/4398 [46:16<1:32:37,  5.53s/it]
 0:  77%|  | 3394/4398 [46:21<1:32:26,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3926, 'grad_norm': 0.9991584712245328, 'learning_rate': 1.3055391480517005e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3394/4398 [46:21<1:32:26,  5.52s/it]
 0:  77%|  | 3395/4398 [46:27<1:32:13,  5.52s/it]
 0:                                                      
 0: 
 0: {'loss': 0.449, 'grad_norm': 1.0398184782306625, 'learning_rate': 1.3030590441837483e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3395/4398 [46:27<1:32:13,  5.52s/it]
 0:  77%|  | 3396/4398 [46:32<1:32:01,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4708, 'grad_norm': 1.5068698238484497, 'learning_rate': 1.3005809452519086e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3396/4398 [46:32<1:32:01,  5.51s/it]
 0:  77%|  | 3397/4398 [46:38<1:31:51,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4305, 'grad_norm': 0.9898180045226378, 'learning_rate': 1.2981048526001084e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3397/4398 [46:38<1:31:51,  5.51s/it]
 0:  77%|  | 3398/4398 [46:43<1:31:45,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4279, 'grad_norm': 1.0567822657820132, 'learning_rate': 1.2956307675711916e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3398/4398 [46:43<1:31:45,  5.51s/it]
 0:  77%|  | 3399/4398 [46:49<1:31:40,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4339, 'grad_norm': 1.6647787024973333, 'learning_rate': 1.2931586915069106e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3399/4398 [46:49<1:31:40,  5.51s/it]
 0:  77%|  | 3400/4398 [46:54<1:31:35,  5.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3997, 'grad_norm': 0.9960489944182175, 'learning_rate': 1.2906886257479279e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3400/4398 [46:54<1:31:35,  5.51s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  77%|  | 3401/4398 [47:59<6:28:40, 23.39s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4518, 'grad_norm': 1.0758023000751837, 'learning_rate': 1.2882205716338203e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3401/4398 [47:59<6:28:40, 23.39s/it]
 0:  77%|  | 3402/4398 [48:15<5:52:01, 21.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4482, 'grad_norm': 1.0345526146334327, 'learning_rate': 1.2857545305030688e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3402/4398 [48:15<5:52:01, 21.21s/it]
 0:  77%|  | 3403/4398 [48:21<4:33:49, 16.51s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4174, 'grad_norm': 0.9755983280539965, 'learning_rate': 1.2832905036930642e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3403/4398 [48:21<4:33:49, 16.51s/it]
 0:  77%|  | 3404/4398 [48:26<3:39:09, 13.23s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4285, 'grad_norm': 1.113944289673613, 'learning_rate': 1.2808284925401043e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3404/4398 [48:26<3:39:09, 13.23s/it]
 0:  77%|  | 3405/4398 [48:32<3:00:54, 10.93s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4213, 'grad_norm': 1.1040336967595221, 'learning_rate': 1.2783684983793954e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3405/4398 [48:32<3:00:54, 10.93s/it]
 0:  77%|  | 3406/4398 [48:38<2:34:07,  9.32s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4265, 'grad_norm': 1.2534885455262916, 'learning_rate': 1.2759105225450475e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3406/4398 [48:38<2:34:07,  9.32s/it]
 0:  77%|  | 3407/4398 [48:45<2:23:30,  8.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4664, 'grad_norm': 1.2043803597197422, 'learning_rate': 1.2734545663700792e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3407/4398 [48:45<2:23:30,  8.69s/it]
 0:  77%|  | 3408/4398 [48:50<2:07:57,  7.75s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4669, 'grad_norm': 1.2752576520960528, 'learning_rate': 1.2710006311864104e-06, 'epoch': 0.77}
 0: 
 0:  77%|  | 3408/4398 [48:50<2:07:57,  7.75s/it]
 0:  78%|  | 3409/4398 [48:56<1:57:11,  7.11s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4279, 'grad_norm': 1.0204855060864686, 'learning_rate': 1.2685487183248673e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3409/4398 [48:56<1:57:11,  7.11s/it]
 0:  78%|  | 3410/4398 [49:01<1:49:22,  6.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4167, 'grad_norm': 0.913189184658172, 'learning_rate': 1.2660988291151793e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3410/4398 [49:01<1:49:22,  6.64s/it]
 0:  78%|  | 3411/4398 [49:07<1:44:22,  6.35s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4597, 'grad_norm': 1.0426486089988354, 'learning_rate': 1.2636509648859762e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3411/4398 [49:07<1:44:22,  6.35s/it]
 0:  78%|  | 3412/4398 [49:13<1:40:23,  6.11s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4366, 'grad_norm': 1.0126987002435275, 'learning_rate': 1.2612051269647924e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3412/4398 [49:13<1:40:23,  6.11s/it]
 0:  78%|  | 3413/4398 [49:18<1:37:43,  5.95s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3993, 'grad_norm': 1.049779554629943, 'learning_rate': 1.2587613166780615e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3413/4398 [49:18<1:37:43,  5.95s/it]
 0:  78%|  | 3414/4398 [49:24<1:35:43,  5.84s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3997, 'grad_norm': 0.9714365501243238, 'learning_rate': 1.2563195353511176e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3414/4398 [49:24<1:35:43,  5.84s/it]
 0:  78%|  | 3415/4398 [49:29<1:34:29,  5.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4586, 'grad_norm': 1.1439863192681348, 'learning_rate': 1.2538797843081956e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3415/4398 [49:29<1:34:29,  5.77s/it]
 0:  78%|  | 3416/4398 [49:35<1:33:22,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4787, 'grad_norm': 1.1281519427728997, 'learning_rate': 1.2514420648724284e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3416/4398 [49:35<1:33:22,  5.70s/it]
 0:  78%|  | 3417/4398 [49:41<1:32:47,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4408, 'grad_norm': 1.5134565065925054, 'learning_rate': 1.2490063783658474e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3417/4398 [49:41<1:32:47,  5.67s/it]
 0:  78%|  | 3418/4398 [49:46<1:32:02,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4447, 'grad_norm': 1.2900816909207582, 'learning_rate': 1.2465727261093797e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3418/4398 [49:46<1:32:02,  5.63s/it]
 0:  78%|  | 3419/4398 [49:52<1:31:39,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4227, 'grad_norm': 1.1241496711575039, 'learning_rate': 1.244141109422854e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3419/4398 [49:52<1:31:39,  5.62s/it]
 0:  78%|  | 3420/4398 [49:57<1:31:15,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4304, 'grad_norm': 1.2067174279875164, 'learning_rate': 1.2417115296249916e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3420/4398 [49:57<1:31:15,  5.60s/it]
 0:  78%|  | 3421/4398 [50:03<1:30:57,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4378, 'grad_norm': 1.658848937657544, 'learning_rate': 1.2392839880334069e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3421/4398 [50:03<1:30:57,  5.59s/it]
 0:  78%|  | 3422/4398 [50:08<1:30:48,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4398, 'grad_norm': 1.2336256425964782, 'learning_rate': 1.2368584859646132e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3422/4398 [50:08<1:30:48,  5.58s/it]
 0:  78%|  | 3423/4398 [50:14<1:30:42,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4377, 'grad_norm': 1.105747589021595, 'learning_rate': 1.2344350247340159e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3423/4398 [50:14<1:30:42,  5.58s/it]
 0:  78%|  | 3424/4398 [50:20<1:30:32,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.422, 'grad_norm': 1.3162101923322724, 'learning_rate': 1.2320136056559134e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3424/4398 [50:20<1:30:32,  5.58s/it]
 0:  78%|  | 3425/4398 [50:25<1:30:28,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4529, 'grad_norm': 1.020244564690174, 'learning_rate': 1.2295942300434971e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3425/4398 [50:25<1:30:28,  5.58s/it]
 0:  78%|  | 3426/4398 [50:31<1:30:18,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4495, 'grad_norm': 1.2136888769486047, 'learning_rate': 1.227176899208849e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3426/4398 [50:31<1:30:18,  5.57s/it]
 0:  78%|  | 3427/4398 [50:36<1:30:14,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4129, 'grad_norm': 1.0009586723735349, 'learning_rate': 1.2247616144629459e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3427/4398 [50:36<1:30:14,  5.58s/it]
 0:  78%|  | 3428/4398 [50:42<1:30:01,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4127, 'grad_norm': 1.0970393374827418, 'learning_rate': 1.2223483771156509e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3428/4398 [50:42<1:30:01,  5.57s/it]
 0:  78%|  | 3429/4398 [50:47<1:30:03,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4155, 'grad_norm': 1.0305879809120413, 'learning_rate': 1.2199371884757172e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3429/4398 [50:47<1:30:03,  5.58s/it]
 0:  78%|  | 3430/4398 [50:53<1:29:48,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4477, 'grad_norm': 1.0491827966255105, 'learning_rate': 1.2175280498507897e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3430/4398 [50:53<1:29:48,  5.57s/it]
 0:  78%|  | 3431/4398 [50:59<1:29:56,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4233, 'grad_norm': 1.0888365587058788, 'learning_rate': 1.215120962547398e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3431/4398 [50:59<1:29:56,  5.58s/it]
 0:  78%|  | 3432/4398 [51:04<1:29:42,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4094, 'grad_norm': 1.049819590993458, 'learning_rate': 1.2127159278709628e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3432/4398 [51:04<1:29:42,  5.57s/it]
 0:  78%|  | 3433/4398 [51:10<1:29:39,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.512, 'grad_norm': 1.1539964087898622, 'learning_rate': 1.2103129471257873e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3433/4398 [51:10<1:29:39,  5.57s/it]
 0:  78%|  | 3434/4398 [51:15<1:29:32,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.451, 'grad_norm': 1.1564821617392795, 'learning_rate': 1.207912021615063e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3434/4398 [51:15<1:29:32,  5.57s/it]
 0:  78%|  | 3435/4398 [51:21<1:29:35,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4473, 'grad_norm': 1.1136070720975642, 'learning_rate': 1.2055131526408698e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3435/4398 [51:21<1:29:35,  5.58s/it]
 0:  78%|  | 3436/4398 [51:26<1:29:26,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4349, 'grad_norm': 1.0859351258694274, 'learning_rate': 1.2031163415041685e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3436/4398 [51:26<1:29:26,  5.58s/it]
 0:  78%|  | 3437/4398 [51:38<1:58:36,  7.41s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4355, 'grad_norm': 1.0138069563703411, 'learning_rate': 1.2007215895048042e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3437/4398 [51:38<1:58:36,  7.41s/it]
 0:  78%|  | 3438/4398 [53:14<9:03:10, 33.95s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4544, 'grad_norm': 1.1445588539500582, 'learning_rate': 1.1983288979415065e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3438/4398 [53:14<9:03:10, 33.95s/it]
 0:  78%|  | 3439/4398 [53:47<8:56:17, 33.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.471, 'grad_norm': 1.108990118574667, 'learning_rate': 1.1959382681118865e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3439/4398 [53:47<8:56:17, 33.55s/it]
 0:  78%|  | 3440/4398 [53:52<6:41:32, 25.15s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4414, 'grad_norm': 2.1583457286876753, 'learning_rate': 1.1935497013124381e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3440/4398 [53:52<6:41:32, 25.15s/it]
 0:  78%|  | 3441/4398 [53:58<5:07:29, 19.28s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4097, 'grad_norm': 1.0229838582885582, 'learning_rate': 1.1911631988385364e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3441/4398 [53:58<5:07:29, 19.28s/it]
 0:  78%|  | 3442/4398 [54:03<4:01:29, 15.16s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4268, 'grad_norm': 1.032724254023625, 'learning_rate': 1.188778761984436e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3442/4398 [54:03<4:01:29, 15.16s/it]
 0:  78%|  | 3443/4398 [54:09<3:15:28, 12.28s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4343, 'grad_norm': 1.012253278357997, 'learning_rate': 1.1863963920432726e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3443/4398 [54:09<3:15:28, 12.28s/it]
 0:  78%|  | 3444/4398 [54:15<2:43:58, 10.31s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4434, 'grad_norm': 1.2276719582902487, 'learning_rate': 1.1840160903070591e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3444/4398 [54:15<2:43:58, 10.31s/it]
 0:  78%|  | 3445/4398 [54:20<2:21:29,  8.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.46, 'grad_norm': 1.068617116111136, 'learning_rate': 1.1816378580666888e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3445/4398 [54:20<2:21:29,  8.91s/it]
 0:  78%|  | 3446/4398 [54:26<2:05:21,  7.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4306, 'grad_norm': 1.1340795420381566, 'learning_rate': 1.1792616966119314e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3446/4398 [54:26<2:05:21,  7.90s/it]
 0:  78%|  | 3447/4398 [54:31<1:54:08,  7.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4447, 'grad_norm': 1.2468103937812518, 'learning_rate': 1.176887607231434e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3447/4398 [54:31<1:54:08,  7.20s/it]
 0:  78%|  | 3448/4398 [54:37<1:46:14,  6.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4202, 'grad_norm': 1.0096230829854655, 'learning_rate': 1.1745155912127193e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3448/4398 [54:37<1:46:14,  6.71s/it]
 0:  78%|  | 3449/4398 [54:42<1:40:43,  6.37s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4629, 'grad_norm': 1.3694357723067665, 'learning_rate': 1.1721456498421867e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3449/4398 [54:42<1:40:43,  6.37s/it]
 0:  78%|  | 3450/4398 [54:48<1:36:44,  6.12s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4236, 'grad_norm': 1.1029402038493084, 'learning_rate': 1.1697777844051105e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3450/4398 [54:48<1:36:44,  6.12s/it]
 0:  78%|  | 3451/4398 [54:54<1:34:07,  5.96s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4563, 'grad_norm': 1.0535073630438248, 'learning_rate': 1.1674119961856378e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3451/4398 [54:54<1:34:07,  5.96s/it]
 0:  78%|  | 3452/4398 [54:59<1:32:10,  5.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4126, 'grad_norm': 1.0372257785197478, 'learning_rate': 1.1650482864667883e-06, 'epoch': 0.78}
 0: 
 0:  78%|  | 3452/4398 [54:59<1:32:10,  5.85s/it]
 0:  79%|  | 3453/4398 [55:05<1:30:52,  5.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3856, 'grad_norm': 1.2040340387131165, 'learning_rate': 1.1626866565304594e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3453/4398 [55:05<1:30:52,  5.77s/it]
 0:  79%|  | 3454/4398 [55:10<1:29:49,  5.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4142, 'grad_norm': 1.2607258811732105, 'learning_rate': 1.1603271076574163e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3454/4398 [55:10<1:29:49,  5.71s/it]
 0:  79%|  | 3455/4398 [55:16<1:29:08,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4509, 'grad_norm': 1.1829443227622163, 'learning_rate': 1.1579696411272973e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3455/4398 [55:16<1:29:08,  5.67s/it]
 0:  79%|  | 3456/4398 [55:21<1:28:29,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.0726046752585956, 'learning_rate': 1.1556142582186087e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3456/4398 [55:22<1:28:29,  5.64s/it]
 0:  79%|  | 3457/4398 [55:37<2:15:01,  8.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4019, 'grad_norm': 1.0152450591418303, 'learning_rate': 1.1532609602087291e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3457/4398 [55:37<2:15:01,  8.61s/it]
 0:  79%|  | 3458/4398 [55:43<2:00:36,  7.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4455, 'grad_norm': 1.0958231844742747, 'learning_rate': 1.1509097483739073e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3458/4398 [55:43<2:00:36,  7.70s/it]
 0:  79%|  | 3459/4398 [55:48<1:50:30,  7.06s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4447, 'grad_norm': 1.0968197207586943, 'learning_rate': 1.1485606239892588e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3459/4398 [55:48<1:50:30,  7.06s/it]
 0:  79%|  | 3460/4398 [55:54<1:43:15,  6.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4318, 'grad_norm': 0.9883099700884591, 'learning_rate': 1.1462135883287668e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3460/4398 [55:54<1:43:15,  6.61s/it]
 0:  79%|  | 3461/4398 [55:59<1:38:19,  6.30s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4073, 'grad_norm': 1.05504385947692, 'learning_rate': 1.143868642665285e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3461/4398 [55:59<1:38:19,  6.30s/it]
 0:  79%|  | 3462/4398 [56:05<1:34:49,  6.08s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4418, 'grad_norm': 1.115169268015269, 'learning_rate': 1.141525788270531e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3462/4398 [56:05<1:34:49,  6.08s/it]
 0:  79%|  | 3463/4398 [56:10<1:32:23,  5.93s/it]
 0:                                                      
 0: 
 0: {'loss': 0.417, 'grad_norm': 0.9853263400039199, 'learning_rate': 1.1391850264150872e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3463/4398 [56:10<1:32:23,  5.93s/it]
 0:  79%|  | 3464/4398 [56:16<1:30:30,  5.81s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.272308700315129, 'learning_rate': 1.1368463583684042e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3464/4398 [56:16<1:30:30,  5.81s/it]
 0:  79%|  | 3465/4398 [56:22<1:29:24,  5.75s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4275, 'grad_norm': 1.1496699424346182, 'learning_rate': 1.134509785398794e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3465/4398 [56:22<1:29:24,  5.75s/it]
 0:  79%|  | 3466/4398 [56:27<1:28:21,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4226, 'grad_norm': 1.0224082115735476, 'learning_rate': 1.1321753087734344e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3466/4398 [56:27<1:28:21,  5.69s/it]
 0:  79%|  | 3467/4398 [56:33<1:27:43,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4578, 'grad_norm': 1.16295624084323, 'learning_rate': 1.1298429297583674e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3467/4398 [56:33<1:27:43,  5.65s/it]
 0:  79%|  | 3468/4398 [56:40<1:36:08,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.3219586935461574, 'learning_rate': 1.1275126496184919e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3468/4398 [56:40<1:36:08,  6.20s/it]
 0:  79%|  | 3469/4398 [56:46<1:33:12,  6.02s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4306, 'grad_norm': 1.0136140337751054, 'learning_rate': 1.1251844696175757e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3469/4398 [56:46<1:33:12,  6.02s/it]
 0:  79%|  | 3470/4398 [56:51<1:31:02,  5.89s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4414, 'grad_norm': 1.037969603568572, 'learning_rate': 1.1228583910182434e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3470/4398 [56:51<1:31:02,  5.89s/it]
 0:  79%|  | 3471/4398 [56:57<1:29:40,  5.80s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4535, 'grad_norm': 1.098287490253671, 'learning_rate': 1.1205344150819808e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3471/4398 [56:57<1:29:40,  5.80s/it]
 0:  79%|  | 3472/4398 [57:03<1:28:34,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4456, 'grad_norm': 1.0668515919379864, 'learning_rate': 1.1182125430691342e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3472/4398 [57:03<1:28:34,  5.74s/it]
 0:  79%|  | 3473/4398 [57:08<1:27:53,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4411, 'grad_norm': 1.0422240585168192, 'learning_rate': 1.1158927762389072e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3473/4398 [57:08<1:27:53,  5.70s/it]
 0:  79%|  | 3474/4398 [57:14<1:27:18,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4239, 'grad_norm': 1.0830686154305422, 'learning_rate': 1.1135751158493634e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3474/4398 [57:14<1:27:18,  5.67s/it]
 0:  79%|  | 3475/4398 [57:19<1:26:47,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.1184970759204638, 'learning_rate': 1.111259563157423e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3475/4398 [57:19<1:26:47,  5.64s/it]
 0:  79%|  | 3476/4398 [57:25<1:26:23,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4655, 'grad_norm': 1.2136079478306143, 'learning_rate': 1.1089461194188633e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3476/4398 [57:25<1:26:23,  5.62s/it]
 0:  79%|  | 3477/4398 [57:30<1:26:04,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4414, 'grad_norm': 1.2269313930967192, 'learning_rate': 1.1066347858883186e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3477/4398 [57:31<1:26:04,  5.61s/it]
 0:  79%|  | 3478/4398 [57:36<1:25:46,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3947, 'grad_norm': 1.0254357231572635, 'learning_rate': 1.1043255638192778e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3478/4398 [57:36<1:25:46,  5.59s/it]
 0:  79%|  | 3479/4398 [57:42<1:25:40,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4671, 'grad_norm': 1.0981609725325963, 'learning_rate': 1.1020184544640856e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3479/4398 [57:42<1:25:40,  5.59s/it]
 0:  79%|  | 3480/4398 [57:47<1:25:24,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4231, 'grad_norm': 1.062520044642502, 'learning_rate': 1.09971345907394e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3480/4398 [57:47<1:25:24,  5.58s/it]
 0:  79%|  | 3481/4398 [57:53<1:25:25,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4449, 'grad_norm': 1.039104744447393, 'learning_rate': 1.097410578898893e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3481/4398 [57:53<1:25:25,  5.59s/it]
 0:  79%|  | 3482/4398 [57:58<1:25:14,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4779, 'grad_norm': 1.3047384898111163, 'learning_rate': 1.0951098151878497e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3482/4398 [57:58<1:25:14,  5.58s/it]
 0:  79%|  | 3483/4398 [58:04<1:25:09,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4311, 'grad_norm': 1.6979120077285261, 'learning_rate': 1.0928111691885674e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3483/4398 [58:04<1:25:09,  5.58s/it]
 0:  79%|  | 3484/4398 [58:10<1:24:56,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4209, 'grad_norm': 2.046579368140776, 'learning_rate': 1.0905146421476543e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3484/4398 [58:10<1:24:56,  5.58s/it]
 0:  79%|  | 3485/4398 [58:15<1:24:54,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4216, 'grad_norm': 1.2463280940442474, 'learning_rate': 1.0882202353105698e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3485/4398 [58:15<1:24:54,  5.58s/it]
 0:  79%|  | 3486/4398 [58:21<1:24:50,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4191, 'grad_norm': 1.218632084456872, 'learning_rate': 1.085927949921624e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3486/4398 [58:21<1:24:50,  5.58s/it]
 0:  79%|  | 3487/4398 [58:26<1:24:43,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4515, 'grad_norm': 1.0616430706558806, 'learning_rate': 1.083637787223974e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3487/4398 [58:26<1:24:43,  5.58s/it]
 0:  79%|  | 3488/4398 [58:32<1:24:37,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4088, 'grad_norm': 1.0239355864752373, 'learning_rate': 1.0813497484596307e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3488/4398 [58:32<1:24:37,  5.58s/it]
 0:  79%|  | 3489/4398 [58:37<1:24:32,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4612, 'grad_norm': 1.146487873634217, 'learning_rate': 1.0790638348694487e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3489/4398 [58:37<1:24:32,  5.58s/it]
 0:  79%|  | 3490/4398 [58:43<1:24:25,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4713, 'grad_norm': 1.036130912336942, 'learning_rate': 1.0767800476931334e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3490/4398 [58:43<1:24:25,  5.58s/it]
 0:  79%|  | 3491/4398 [58:49<1:24:18,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4539, 'grad_norm': 1.0687976144566402, 'learning_rate': 1.0744983881692322e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3491/4398 [58:49<1:24:18,  5.58s/it]
 0:  79%|  | 3492/4398 [58:54<1:24:11,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4044, 'grad_norm': 1.1200869356915404, 'learning_rate': 1.0722188575351423e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3492/4398 [58:54<1:24:11,  5.58s/it]
 0:  79%|  | 3493/4398 [59:00<1:24:18,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.447, 'grad_norm': 1.117992379936494, 'learning_rate': 1.0699414570271055e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3493/4398 [59:00<1:24:18,  5.59s/it]
 0:  79%|  | 3494/4398 [59:05<1:24:06,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4388, 'grad_norm': 1.28333566478247, 'learning_rate': 1.0676661878802098e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3494/4398 [59:05<1:24:06,  5.58s/it]
 0:  79%|  | 3495/4398 [59:11<1:24:01,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4704, 'grad_norm': 1.0826810472483561, 'learning_rate': 1.0653930513283833e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3495/4398 [59:11<1:24:01,  5.58s/it]
 0:  79%|  | 3496/4398 [59:16<1:23:43,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4222, 'grad_norm': 1.216238768088199, 'learning_rate': 1.0631220486044035e-06, 'epoch': 0.79}
 0: 
 0:  79%|  | 3496/4398 [59:16<1:23:43,  5.57s/it]
 0:  80%|  | 3497/4398 [59:22<1:23:31,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4437, 'grad_norm': 1.1772206202716102, 'learning_rate': 1.0608531809398858e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3497/4398 [59:22<1:23:31,  5.56s/it]
 0:  80%|  | 3498/4398 [59:28<1:23:17,  5.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4199, 'grad_norm': 1.1104994791887606, 'learning_rate': 1.0585864495652899e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3498/4398 [59:28<1:23:17,  5.55s/it]
 0:  80%|  | 3499/4398 [59:33<1:23:18,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4389, 'grad_norm': 1.0510830445486887, 'learning_rate': 1.0563218557099158e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3499/4398 [59:33<1:23:18,  5.56s/it]
 0:  80%|  | 3500/4398 [59:39<1:23:14,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4549, 'grad_norm': 1.094682334865372, 'learning_rate': 1.054059400601905e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3500/4398 [59:39<1:23:14,  5.56s/it]
 0:  80%|  | 3501/4398 [59:44<1:23:13,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.0473336515299942, 'learning_rate': 1.0517990854682392e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3501/4398 [59:44<1:23:13,  5.57s/it]
 0:  80%|  | 3502/4398 [59:50<1:23:13,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.432, 'grad_norm': 1.142383728717596, 'learning_rate': 1.0495409115347382e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3502/4398 [59:50<1:23:13,  5.57s/it]
 0:  80%|  | 3503/4398 [59:55<1:23:13,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4201, 'grad_norm': 1.0643766938425863, 'learning_rate': 1.0472848800260631e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3503/4398 [59:55<1:23:13,  5.58s/it]
 0:  80%|  | 3504/4398 [1:00:01<1:22:49,  5.56s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4406, 'grad_norm': 1.163202522891138, 'learning_rate': 1.0450309921657115e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3504/4398 [1:00:01<1:22:49,  5.56s/it]
 0:  80%|  | 3505/4398 [1:00:13<1:49:38,  7.37s/it]
 0:                                                        
 0: 
 0: {'loss': 0.442, 'grad_norm': 1.0573861288475244, 'learning_rate': 1.0427792491760175e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3505/4398 [1:00:13<1:49:38,  7.37s/it]
 0:  80%|  | 3506/4398 [1:01:38<7:38:57, 30.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4312, 'grad_norm': 1.0826340324648218, 'learning_rate': 1.0405296522781538e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3506/4398 [1:01:38<7:38:57, 30.87s/it]
 0:  80%|  | 3507/4398 [1:02:27<8:57:08, 36.17s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.2822620538073806, 'learning_rate': 1.0382822026921291e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3507/4398 [1:02:27<8:57:08, 36.17s/it]
 0:  80%|  | 3508/4398 [1:02:32<6:40:15, 26.98s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4238, 'grad_norm': 1.2327546358690897, 'learning_rate': 1.036036901636786e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3508/4398 [1:02:32<6:40:15, 26.98s/it]
 0:  80%|  | 3509/4398 [1:02:38<5:04:41, 20.56s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4478, 'grad_norm': 1.2724203768145523, 'learning_rate': 1.0337937503298034e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3509/4398 [1:02:38<5:04:41, 20.56s/it]
 0:  80%|  | 3510/4398 [1:02:43<3:57:43, 16.06s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.0323762273411532, 'learning_rate': 1.0315527499876938e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3510/4398 [1:02:43<3:57:43, 16.06s/it]
 0:  80%|  | 3511/4398 [1:02:49<3:10:58, 12.92s/it]
 0:                                                        
 0: 
 0: {'loss': 0.426, 'grad_norm': 1.0445017544391881, 'learning_rate': 1.029313901825803e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3511/4398 [1:02:49<3:10:58, 12.92s/it]
 0:  80%|  | 3512/4398 [1:03:09<3:39:50, 14.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4136, 'grad_norm': 1.2119315462568516, 'learning_rate': 1.0270772070583107e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3512/4398 [1:03:09<3:39:50, 14.89s/it]
 0:  80%|  | 3513/4398 [1:03:14<2:58:23, 12.09s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4316, 'grad_norm': 1.050313879338056, 'learning_rate': 1.0248426668982254e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3513/4398 [1:03:14<2:58:23, 12.09s/it]
 0:  80%|  | 3514/4398 [1:03:20<2:29:18, 10.13s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4084, 'grad_norm': 0.9989614913085135, 'learning_rate': 1.0226102825573947e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3514/4398 [1:03:20<2:29:18, 10.13s/it]
 0:  80%|  | 3515/4398 [1:03:25<2:09:05,  8.77s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.2234747977869802, 'learning_rate': 1.0203800552464872e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3515/4398 [1:03:25<2:09:05,  8.77s/it]
 0:  80%|  | 3516/4398 [1:03:31<1:54:48,  7.81s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4408, 'grad_norm': 1.0593278166648155, 'learning_rate': 1.0181519861750078e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3516/4398 [1:03:31<1:54:48,  7.81s/it]
 0:  80%|  | 3517/4398 [1:03:36<1:44:57,  7.15s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4364, 'grad_norm': 1.024106998051462, 'learning_rate': 1.0159260765512901e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3517/4398 [1:03:36<1:44:57,  7.15s/it]
 0:  80%|  | 3518/4398 [1:03:42<1:37:55,  6.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.438, 'grad_norm': 1.179664159503801, 'learning_rate': 1.0137023275824959e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3518/4398 [1:03:42<1:37:55,  6.68s/it]
 0:  80%|  | 3519/4398 [1:03:48<1:32:56,  6.34s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4199, 'grad_norm': 1.351373982303684, 'learning_rate': 1.0114807404746152e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3519/4398 [1:03:48<1:32:56,  6.34s/it]
 0:  80%|  | 3520/4398 [1:03:53<1:29:27,  6.11s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4627, 'grad_norm': 1.007064618321833, 'learning_rate': 1.009261316432466e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3520/4398 [1:03:53<1:29:27,  6.11s/it]
 0:  80%|  | 3521/4398 [1:04:02<1:40:53,  6.90s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4325, 'grad_norm': 0.9730767404627031, 'learning_rate': 1.0070440566596917e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3521/4398 [1:04:02<1:40:53,  6.90s/it]
 0:  80%|  | 3522/4398 [1:04:07<1:34:58,  6.50s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4343, 'grad_norm': 0.9707649237484098, 'learning_rate': 1.004828962358766e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3522/4398 [1:04:08<1:34:58,  6.50s/it]
 0:  80%|  | 3523/4398 [1:04:13<1:30:50,  6.23s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4683, 'grad_norm': 1.9106072694679825, 'learning_rate': 1.0026160347309838e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3523/4398 [1:04:13<1:30:50,  6.23s/it]
 0:  80%|  | 3524/4398 [1:04:19<1:27:40,  6.02s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4498, 'grad_norm': 1.3150543931600978, 'learning_rate': 1.000405274976467e-06, 'epoch': 0.8}
 0: 
 0:  80%|  | 3524/4398 [1:04:19<1:27:40,  6.02s/it]
 0:  80%|  | 3525/4398 [1:04:24<1:25:42,  5.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4012, 'grad_norm': 1.4016409488221238, 'learning_rate': 9.981966842941625e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3525/4398 [1:04:24<1:25:42,  5.89s/it]
 0:  80%|  | 3526/4398 [1:04:30<1:24:11,  5.79s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4505, 'grad_norm': 1.8831569672482582, 'learning_rate': 9.959902638818375e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3526/4398 [1:04:30<1:24:11,  5.79s/it]
 0:  80%|  | 3527/4398 [1:04:35<1:23:11,  5.73s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4167, 'grad_norm': 1.0791689841770027, 'learning_rate': 9.937860149360851e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3527/4398 [1:04:35<1:23:11,  5.73s/it]
 0:  80%|  | 3528/4398 [1:04:41<1:22:23,  5.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4391, 'grad_norm': 1.1282022799653064, 'learning_rate': 9.915839386523212e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3528/4398 [1:04:41<1:22:23,  5.68s/it]
 0:  80%|  | 3529/4398 [1:04:47<1:21:57,  5.66s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.117739841646317, 'learning_rate': 9.893840362247809e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3529/4398 [1:04:47<1:21:57,  5.66s/it]
 0:  80%|  | 3530/4398 [1:04:52<1:21:27,  5.63s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4612, 'grad_norm': 1.0353399756592576, 'learning_rate': 9.871863088465245e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3530/4398 [1:04:52<1:21:27,  5.63s/it]
 0:  80%|  | 3531/4398 [1:04:58<1:21:14,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4173, 'grad_norm': 1.1494515337831248, 'learning_rate': 9.849907577094281e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3531/4398 [1:04:58<1:21:14,  5.62s/it]
 0:  80%|  | 3532/4398 [1:05:03<1:20:56,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4313, 'grad_norm': 1.1350146357064774, 'learning_rate': 9.827973840041905e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3532/4398 [1:05:03<1:20:56,  5.61s/it]
 0:  80%|  | 3533/4398 [1:05:09<1:20:49,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4256, 'grad_norm': 1.617978291079996, 'learning_rate': 9.806061889203277e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3533/4398 [1:05:09<1:20:49,  5.61s/it]
 0:  80%|  | 3534/4398 [1:05:14<1:20:43,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.438, 'grad_norm': 1.392929374289302, 'learning_rate': 9.784171736461762e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3534/4398 [1:05:14<1:20:43,  5.61s/it]
 0:  80%|  | 3535/4398 [1:05:20<1:20:37,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4571, 'grad_norm': 1.059099066018759, 'learning_rate': 9.762303393688887e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3535/4398 [1:05:20<1:20:37,  5.61s/it]
 0:  80%|  | 3536/4398 [1:05:26<1:20:21,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4113, 'grad_norm': 1.0249491817846053, 'learning_rate': 9.740456872744363e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3536/4398 [1:05:26<1:20:21,  5.59s/it]
 0:  80%|  | 3537/4398 [1:05:31<1:20:15,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4189, 'grad_norm': 1.0196027804849923, 'learning_rate': 9.71863218547605e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3537/4398 [1:05:31<1:20:15,  5.59s/it]
 0:  80%|  | 3538/4398 [1:05:37<1:20:00,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4196, 'grad_norm': 1.0781031909218384, 'learning_rate': 9.696829343719994e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3538/4398 [1:05:37<1:20:00,  5.58s/it]
 0:  80%|  | 3539/4398 [1:05:42<1:19:55,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.432, 'grad_norm': 1.006912942250418, 'learning_rate': 9.675048359300366e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3539/4398 [1:05:42<1:19:55,  5.58s/it]
 0:  80%|  | 3540/4398 [1:05:48<1:19:45,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4219, 'grad_norm': 1.0411717178689615, 'learning_rate': 9.653289244029497e-07, 'epoch': 0.8}
 0: 
 0:  80%|  | 3540/4398 [1:05:48<1:19:45,  5.58s/it]
 0:  81%|  | 3541/4398 [1:05:54<1:19:45,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4562, 'grad_norm': 1.072706920115522, 'learning_rate': 9.631552009707852e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3541/4398 [1:05:54<1:19:45,  5.58s/it]
 0:  81%|  | 3542/4398 [1:05:59<1:19:28,  5.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4162, 'grad_norm': 1.2064587979578396, 'learning_rate': 9.60983666812404e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3542/4398 [1:05:59<1:19:28,  5.57s/it]
 0:  81%|  | 3543/4398 [1:06:05<1:19:22,  5.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4166, 'grad_norm': 1.2379107718006843, 'learning_rate': 9.588143231054787e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3543/4398 [1:06:05<1:19:22,  5.57s/it]
 0:  81%|  | 3544/4398 [1:06:10<1:19:13,  5.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4287, 'grad_norm': 1.0536894825439709, 'learning_rate': 9.566471710264945e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3544/4398 [1:06:10<1:19:13,  5.57s/it]
 0:  81%|  | 3545/4398 [1:06:16<1:19:14,  5.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4465, 'grad_norm': 1.272906743262858, 'learning_rate': 9.544822117507474e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3545/4398 [1:06:16<1:19:14,  5.57s/it]
 0:  81%|  | 3546/4398 [1:06:21<1:19:14,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4212, 'grad_norm': 1.0827533512183354, 'learning_rate': 9.52319446452345e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3546/4398 [1:06:21<1:19:14,  5.58s/it]
 0:  81%|  | 3547/4398 [1:06:27<1:19:16,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4133, 'grad_norm': 1.2070733392818984, 'learning_rate': 9.50158876304203e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3547/4398 [1:06:27<1:19:16,  5.59s/it]
 0:  81%|  | 3548/4398 [1:06:33<1:19:26,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4491, 'grad_norm': 1.047150686797501, 'learning_rate': 9.480005024780514e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3548/4398 [1:06:33<1:19:26,  5.61s/it]
 0:  81%|  | 3549/4398 [1:06:38<1:19:15,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4634, 'grad_norm': 1.0414339863500999, 'learning_rate': 9.458443261444256e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3549/4398 [1:06:38<1:19:15,  5.60s/it]
 0:  81%|  | 3550/4398 [1:06:44<1:19:06,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4528, 'grad_norm': 1.0559855844161843, 'learning_rate': 9.436903484726672e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3550/4398 [1:06:44<1:19:06,  5.60s/it]
 0:  81%|  | 3551/4398 [1:06:49<1:18:58,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3817, 'grad_norm': 1.0237524323986247, 'learning_rate': 9.415385706309287e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3551/4398 [1:06:49<1:18:58,  5.59s/it]
 0:  81%|  | 3552/4398 [1:06:55<1:18:48,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4222, 'grad_norm': 3.3822630076777584, 'learning_rate': 9.393889937861694e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3552/4398 [1:06:55<1:18:48,  5.59s/it]
 0:  81%|  | 3553/4398 [1:07:01<1:18:43,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4009, 'grad_norm': 1.0531921737765693, 'learning_rate': 9.372416191041539e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3553/4398 [1:07:01<1:18:43,  5.59s/it]
 0:  81%|  | 3554/4398 [1:07:06<1:18:31,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4271, 'grad_norm': 1.0536512168834402, 'learning_rate': 9.350964477494528e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3554/4398 [1:07:06<1:18:31,  5.58s/it]
 0:  81%|  | 3555/4398 [1:07:12<1:18:30,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4744, 'grad_norm': 1.0425688533560702, 'learning_rate': 9.329534808854401e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3555/4398 [1:07:12<1:18:30,  5.59s/it]
 0:  81%|  | 3556/4398 [1:07:17<1:18:21,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4448, 'grad_norm': 0.9944844416912464, 'learning_rate': 9.308127196743e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3556/4398 [1:07:17<1:18:21,  5.58s/it]
 0:  81%|  | 3557/4398 [1:07:23<1:18:16,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4253, 'grad_norm': 1.0042263920987422, 'learning_rate': 9.286741652770143e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3557/4398 [1:07:23<1:18:16,  5.58s/it]
 0:  81%|  | 3558/4398 [1:07:28<1:18:00,  5.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4482, 'grad_norm': 1.212393033609336, 'learning_rate': 9.265378188533697e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3558/4398 [1:07:28<1:18:00,  5.57s/it]
 0:  81%|  | 3559/4398 [1:07:34<1:17:58,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4162, 'grad_norm': 1.2449251213271457, 'learning_rate': 9.244036815619572e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3559/4398 [1:07:34<1:17:58,  5.58s/it]
 0:  81%|  | 3560/4398 [1:07:40<1:17:53,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4395, 'grad_norm': 1.019852399546535, 'learning_rate': 9.222717545601678e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3560/4398 [1:07:40<1:17:53,  5.58s/it]
 0:  81%|  | 3561/4398 [1:07:45<1:17:55,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4628, 'grad_norm': 1.071075208198343, 'learning_rate': 9.201420390041965e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3561/4398 [1:07:45<1:17:55,  5.59s/it]
 0:  81%|  | 3562/4398 [1:07:51<1:17:42,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.455, 'grad_norm': 1.4384371332646668, 'learning_rate': 9.180145360490333e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3562/4398 [1:07:51<1:17:42,  5.58s/it]
 0:  81%|  | 3563/4398 [1:07:56<1:17:42,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4212, 'grad_norm': 1.1168615406170934, 'learning_rate': 9.158892468484732e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3563/4398 [1:07:56<1:17:42,  5.58s/it]
 0:  81%|  | 3564/4398 [1:08:02<1:17:31,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4278, 'grad_norm': 1.098797677474696, 'learning_rate': 9.137661725551111e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3564/4398 [1:08:02<1:17:31,  5.58s/it]
 0:  81%|  | 3565/4398 [1:08:08<1:17:36,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4264, 'grad_norm': 1.1128206869153985, 'learning_rate': 9.116453143203379e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3565/4398 [1:08:08<1:17:36,  5.59s/it]
 0:  81%|  | 3566/4398 [1:08:13<1:17:54,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4124, 'grad_norm': 1.318806621523084, 'learning_rate': 9.095266732943447e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3566/4398 [1:08:13<1:17:54,  5.62s/it]
 0:  81%|  | 3567/4398 [1:08:19<1:17:41,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3894, 'grad_norm': 1.0237414643856622, 'learning_rate': 9.074102506261179e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3567/4398 [1:08:19<1:17:41,  5.61s/it]
 0:  81%|  | 3568/4398 [1:08:24<1:17:28,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4691, 'grad_norm': 1.1181331546192244, 'learning_rate': 9.052960474634432e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3568/4398 [1:08:24<1:17:28,  5.60s/it]
 0:  81%|  | 3569/4398 [1:08:30<1:17:15,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4593, 'grad_norm': 1.0275067078642361, 'learning_rate': 9.031840649529012e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3569/4398 [1:08:30<1:17:15,  5.59s/it]
 0:  81%|  | 3570/4398 [1:08:36<1:17:07,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4489, 'grad_norm': 1.1413261782884667, 'learning_rate': 9.010743042398684e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3570/4398 [1:08:36<1:17:07,  5.59s/it]
 0:  81%|  | 3571/4398 [1:08:41<1:16:59,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4043, 'grad_norm': 1.073411835397161, 'learning_rate': 8.98966766468517e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3571/4398 [1:08:41<1:16:59,  5.59s/it]
 0:  81%|  | 3572/4398 [1:08:47<1:16:47,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4148, 'grad_norm': 1.1181269788882695, 'learning_rate': 8.968614527818132e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3572/4398 [1:08:47<1:16:47,  5.58s/it]
 0:  81%|  | 3573/4398 [1:08:58<1:41:20,  7.37s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4394, 'grad_norm': 1.0177265580488368, 'learning_rate': 8.947583643215163e-07, 'epoch': 0.81}
 0: 
 0:  81%|  | 3573/4398 [1:08:58<1:41:20,  7.37s/it]
 0:  81%| | 3574/4398 [1:10:04<5:42:17, 24.92s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3951, 'grad_norm': 1.0388282377986044, 'learning_rate': 8.926575022281808e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3574/4398 [1:10:04<5:42:17, 24.92s/it]
 0:  81%| | 3575/4398 [1:11:26<9:37:26, 42.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4062, 'grad_norm': 1.2021741296919193, 'learning_rate': 8.905588676411514e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3575/4398 [1:11:26<9:37:26, 42.10s/it]
 0:  81%| | 3576/4398 [1:11:32<7:06:32, 31.14s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3916, 'grad_norm': 1.0705027488313361, 'learning_rate': 8.88462461698566e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3576/4398 [1:11:32<7:06:32, 31.14s/it]
 0:  81%| | 3577/4398 [1:11:37<5:21:08, 23.47s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4482, 'grad_norm': 3.418953473057843, 'learning_rate': 8.863682855373546e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3577/4398 [1:11:37<5:21:08, 23.47s/it]
 0:  81%| | 3578/4398 [1:11:43<4:07:20, 18.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.1499432549431052, 'learning_rate': 8.842763402932364e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3578/4398 [1:11:43<4:07:20, 18.10s/it]
 0:  81%| | 3579/4398 [1:11:49<3:15:44, 14.34s/it]
 0:                                                        
 0: 
 0: {'loss': 0.435, 'grad_norm': 1.0052774979605819, 'learning_rate': 8.821866271007218e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3579/4398 [1:11:49<3:15:44, 14.34s/it]
 0:  81%| | 3580/4398 [1:11:54<2:39:37, 11.71s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4399, 'grad_norm': 1.2121236139264644, 'learning_rate': 8.800991470931097e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3580/4398 [1:11:54<2:39:37, 11.71s/it]
 0:  81%| | 3581/4398 [1:12:00<2:14:21,  9.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4295, 'grad_norm': 1.0099573529178367, 'learning_rate': 8.78013901402488e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3581/4398 [1:12:00<2:14:21,  9.87s/it]
 0:  81%| | 3582/4398 [1:12:05<1:56:40,  8.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4607, 'grad_norm': 1.0528806305565073, 'learning_rate': 8.759308911597353e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3582/4398 [1:12:05<1:56:40,  8.58s/it]
 0:  81%| | 3583/4398 [1:12:11<1:44:19,  7.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4629, 'grad_norm': 1.0261706810481463, 'learning_rate': 8.738501174945152e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3583/4398 [1:12:11<1:44:19,  7.68s/it]
 0:  81%| | 3584/4398 [1:12:16<1:35:31,  7.04s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4474, 'grad_norm': 1.1563549532364281, 'learning_rate': 8.717715815352807e-07, 'epoch': 0.81}
 0: 
 0:  81%| | 3584/4398 [1:12:16<1:35:31,  7.04s/it]
 0:  82%| | 3585/4398 [1:12:22<1:29:26,  6.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4548, 'grad_norm': 0.9992471416732288, 'learning_rate': 8.696952844092659e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3585/4398 [1:12:22<1:29:26,  6.60s/it]
 0:  82%| | 3586/4398 [1:12:28<1:25:03,  6.29s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4346, 'grad_norm': 1.9096052430323833, 'learning_rate': 8.676212272424966e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3586/4398 [1:12:28<1:25:03,  6.29s/it]
 0:  82%| | 3587/4398 [1:12:33<1:22:04,  6.07s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4361, 'grad_norm': 1.052661896179663, 'learning_rate': 8.655494111597818e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3587/4398 [1:12:33<1:22:04,  6.07s/it]
 0:  82%| | 3588/4398 [1:12:39<1:19:53,  5.92s/it]
 0:                                                        
 0: 
 0: {'loss': 0.452, 'grad_norm': 1.0744395523262236, 'learning_rate': 8.634798372847148e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3588/4398 [1:12:39<1:19:53,  5.92s/it]
 0:  82%| | 3589/4398 [1:12:44<1:18:29,  5.82s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4406, 'grad_norm': 1.2192871778510994, 'learning_rate': 8.614125067396717e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3589/4398 [1:12:44<1:18:29,  5.82s/it]
 0:  82%| | 3590/4398 [1:12:50<1:17:17,  5.74s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3948, 'grad_norm': 1.0353266205464768, 'learning_rate': 8.593474206458163e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3590/4398 [1:12:50<1:17:17,  5.74s/it]
 0:  82%| | 3591/4398 [1:12:55<1:16:31,  5.69s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4341, 'grad_norm': 1.3661836759031236, 'learning_rate': 8.572845801230906e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3591/4398 [1:12:55<1:16:31,  5.69s/it]
 0:  82%| | 3592/4398 [1:13:01<1:15:59,  5.66s/it]
 0:                                                        
 0: 
 0: {'loss': 0.435, 'grad_norm': 1.8065251148358068, 'learning_rate': 8.552239862902207e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3592/4398 [1:13:01<1:15:59,  5.66s/it]
 0:  82%| | 3593/4398 [1:13:07<1:15:34,  5.63s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4311, 'grad_norm': 1.136719036689129, 'learning_rate': 8.531656402647137e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3593/4398 [1:13:07<1:15:34,  5.63s/it]
 0:  82%| | 3594/4398 [1:13:12<1:15:12,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4275, 'grad_norm': 1.3420690832619573, 'learning_rate': 8.511095431628591e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3594/4398 [1:13:12<1:15:12,  5.61s/it]
 0:  82%| | 3595/4398 [1:13:18<1:14:59,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4657, 'grad_norm': 1.2969535461489967, 'learning_rate': 8.490556960997242e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3595/4398 [1:13:18<1:14:59,  5.60s/it]
 0:  82%| | 3596/4398 [1:13:23<1:14:48,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4469, 'grad_norm': 1.8219311528530828, 'learning_rate': 8.470041001891593e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3596/4398 [1:13:23<1:14:48,  5.60s/it]
 0:  82%| | 3597/4398 [1:13:29<1:14:40,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4163, 'grad_norm': 0.9652052683610209, 'learning_rate': 8.449547565437887e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3597/4398 [1:13:29<1:14:40,  5.59s/it]
 0:  82%| | 3598/4398 [1:13:34<1:14:26,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4427, 'grad_norm': 1.0502031934871707, 'learning_rate': 8.429076662750218e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3598/4398 [1:13:34<1:14:26,  5.58s/it]
 0:  82%| | 3599/4398 [1:13:40<1:14:20,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4412, 'grad_norm': 1.332608784890158, 'learning_rate': 8.408628304930416e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3599/4398 [1:13:40<1:14:20,  5.58s/it]
 0:  82%| | 3600/4398 [1:13:46<1:14:09,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4541, 'grad_norm': 1.0560483916393284, 'learning_rate': 8.388202503068099e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3600/4398 [1:13:46<1:14:09,  5.58s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  82%| | 3601/4398 [1:14:43<4:42:05, 21.24s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4167, 'grad_norm': 1.1228870734774867, 'learning_rate': 8.36779926824065e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3601/4398 [1:14:43<4:42:05, 21.24s/it]
 0:  82%| | 3602/4398 [1:14:49<3:39:19, 16.53s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4207, 'grad_norm': 1.012721276554655, 'learning_rate': 8.347418611513203e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3602/4398 [1:14:49<3:39:19, 16.53s/it]
 0:  82%| | 3603/4398 [1:14:54<2:55:33, 13.25s/it]
 0:                                                        
 0: 
 0: {'loss': 0.41, 'grad_norm': 1.1177681218441342, 'learning_rate': 8.32706054393867e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3603/4398 [1:14:55<2:55:33, 13.25s/it]
 0:  82%| | 3604/4398 [1:15:00<2:24:51, 10.95s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4173, 'grad_norm': 1.2088565799083149, 'learning_rate': 8.306725076557687e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3604/4398 [1:15:00<2:24:51, 10.95s/it]
 0:  82%| | 3605/4398 [1:15:06<2:03:16,  9.33s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4301, 'grad_norm': 1.026496593786589, 'learning_rate': 8.286412220398654e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3605/4398 [1:15:06<2:03:16,  9.33s/it]
 0:  82%| | 3606/4398 [1:15:11<1:48:15,  8.20s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4278, 'grad_norm': 1.3330504904357556, 'learning_rate': 8.266121986477699e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3606/4398 [1:15:11<1:48:15,  8.20s/it]
 0:  82%| | 3607/4398 [1:15:17<1:37:46,  7.42s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4246, 'grad_norm': 1.0267161354761059, 'learning_rate': 8.24585438579868e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3607/4398 [1:15:17<1:37:46,  7.42s/it]
 0:  82%| | 3608/4398 [1:15:22<1:30:13,  6.85s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4104, 'grad_norm': 1.0831453316174815, 'learning_rate': 8.225609429353187e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3608/4398 [1:15:22<1:30:13,  6.85s/it]
 0:  82%| | 3609/4398 [1:15:28<1:25:07,  6.47s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4464, 'grad_norm': 1.1200995688712367, 'learning_rate': 8.205387128120518e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3609/4398 [1:15:28<1:25:07,  6.47s/it]
 0:  82%| | 3610/4398 [1:15:33<1:21:29,  6.21s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4312, 'grad_norm': 1.1407657882860818, 'learning_rate': 8.185187493067697e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3610/4398 [1:15:33<1:21:29,  6.21s/it]
 0:  82%| | 3611/4398 [1:15:39<1:18:49,  6.01s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4166, 'grad_norm': 1.0983493301342113, 'learning_rate': 8.165010535149454e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3611/4398 [1:15:39<1:18:49,  6.01s/it]
 0:  82%| | 3612/4398 [1:15:45<1:16:55,  5.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4053, 'grad_norm': 1.0428984047638228, 'learning_rate': 8.14485626530821e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3612/4398 [1:15:45<1:16:55,  5.87s/it]
 0:  82%| | 3613/4398 [1:15:50<1:15:44,  5.79s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4314, 'grad_norm': 1.210412984199851, 'learning_rate': 8.124724694474095e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3613/4398 [1:15:50<1:15:44,  5.79s/it]
 0:  82%| | 3614/4398 [1:15:56<1:14:45,  5.72s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4439, 'grad_norm': 1.1670716331088486, 'learning_rate': 8.104615833564916e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3614/4398 [1:15:56<1:14:45,  5.72s/it]
 0:  82%| | 3615/4398 [1:16:01<1:14:07,  5.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4382, 'grad_norm': 1.0699858490685044, 'learning_rate': 8.084529693486171e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3615/4398 [1:16:01<1:14:07,  5.68s/it]
 0:  82%| | 3616/4398 [1:16:07<1:13:32,  5.64s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3838, 'grad_norm': 1.0159109917795899, 'learning_rate': 8.064466285131045e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3616/4398 [1:16:07<1:13:32,  5.64s/it]
 0:  82%| | 3617/4398 [1:16:12<1:13:17,  5.63s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4668, 'grad_norm': 1.046872209235556, 'learning_rate': 8.044425619380386e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3617/4398 [1:16:12<1:13:17,  5.63s/it]
 0:  82%| | 3618/4398 [1:16:18<1:13:01,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4471, 'grad_norm': 1.1946668212931626, 'learning_rate': 8.0244077071027e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3618/4398 [1:16:18<1:13:01,  5.62s/it]
 0:  82%| | 3619/4398 [1:16:24<1:12:47,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4281, 'grad_norm': 1.4936304384051706, 'learning_rate': 8.004412559154179e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3619/4398 [1:16:24<1:12:47,  5.61s/it]
 0:  82%| | 3620/4398 [1:16:29<1:12:27,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4539, 'grad_norm': 1.2210177785777674, 'learning_rate': 7.984440186378622e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3620/4398 [1:16:29<1:12:27,  5.59s/it]
 0:  82%| | 3621/4398 [1:16:35<1:12:26,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4549, 'grad_norm': 1.0948413324198092, 'learning_rate': 7.964490599607522e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3621/4398 [1:16:35<1:12:26,  5.59s/it]
 0:  82%| | 3622/4398 [1:16:40<1:12:13,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4659, 'grad_norm': 1.065475360501335, 'learning_rate': 7.944563809659999e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3622/4398 [1:16:40<1:12:13,  5.58s/it]
 0:  82%| | 3623/4398 [1:16:46<1:12:17,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.081648282698769, 'learning_rate': 7.924659827342795e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3623/4398 [1:16:46<1:12:17,  5.60s/it]
 0:  82%| | 3624/4398 [1:16:52<1:12:09,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4572, 'grad_norm': 1.2735610226220708, 'learning_rate': 7.904778663450325e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3624/4398 [1:16:52<1:12:09,  5.59s/it]
 0:  82%| | 3625/4398 [1:16:57<1:12:07,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4502, 'grad_norm': 1.1410619528674981, 'learning_rate': 7.884920328764584e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3625/4398 [1:16:57<1:12:07,  5.60s/it]
 0:  82%| | 3626/4398 [1:17:03<1:11:58,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.1943482116814423, 'learning_rate': 7.865084834055209e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3626/4398 [1:17:03<1:11:58,  5.59s/it]
 0:  82%| | 3627/4398 [1:17:08<1:11:52,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.499, 'grad_norm': 1.1214564966001301, 'learning_rate': 7.845272190079444e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3627/4398 [1:17:08<1:11:52,  5.59s/it]
 0:  82%| | 3628/4398 [1:17:14<1:11:43,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4952, 'grad_norm': 1.1175097665874978, 'learning_rate': 7.825482407582136e-07, 'epoch': 0.82}
 0: 
 0:  82%| | 3628/4398 [1:17:14<1:11:43,  5.59s/it]
 0:  83%| | 3629/4398 [1:17:20<1:11:37,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4213, 'grad_norm': 1.1336014687007188, 'learning_rate': 7.805715497295746e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3629/4398 [1:17:20<1:11:37,  5.59s/it]
 0:  83%| | 3630/4398 [1:17:25<1:11:30,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4359, 'grad_norm': 1.2668279753895286, 'learning_rate': 7.785971469940323e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3630/4398 [1:17:25<1:11:30,  5.59s/it]
 0:  83%| | 3631/4398 [1:17:31<1:11:28,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4186, 'grad_norm': 1.3286774016050662, 'learning_rate': 7.766250336223508e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3631/4398 [1:17:31<1:11:28,  5.59s/it]
 0:  83%| | 3632/4398 [1:17:36<1:11:24,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3959, 'grad_norm': 1.0304422283829981, 'learning_rate': 7.746552106840515e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3632/4398 [1:17:36<1:11:24,  5.59s/it]
 0:  83%| | 3633/4398 [1:17:42<1:11:24,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.42, 'grad_norm': 1.1734242870641267, 'learning_rate': 7.726876792474164e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3633/4398 [1:17:42<1:11:24,  5.60s/it]
 0:  83%| | 3634/4398 [1:17:48<1:11:31,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.44, 'grad_norm': 1.035175678595709, 'learning_rate': 7.707224403794816e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3634/4398 [1:17:48<1:11:31,  5.62s/it]
 0:  83%| | 3635/4398 [1:17:53<1:11:23,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4535, 'grad_norm': 1.2100411679316845, 'learning_rate': 7.68759495146042e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3635/4398 [1:17:53<1:11:23,  5.61s/it]
 0:  83%| | 3636/4398 [1:17:59<1:11:12,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4397, 'grad_norm': 1.1884335518917222, 'learning_rate': 7.667988446116475e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3636/4398 [1:17:59<1:11:12,  5.61s/it]
 0:  83%| | 3637/4398 [1:18:04<1:11:09,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4518, 'grad_norm': 1.2460148032437355, 'learning_rate': 7.64840489839605e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3637/4398 [1:18:04<1:11:09,  5.61s/it]
 0:  83%| | 3638/4398 [1:18:10<1:10:56,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4178, 'grad_norm': 1.188360748860312, 'learning_rate': 7.628844318919743e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3638/4398 [1:18:10<1:10:56,  5.60s/it]
 0:  83%| | 3639/4398 [1:18:16<1:10:52,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4392, 'grad_norm': 1.1397654546133547, 'learning_rate': 7.609306718295711e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3639/4398 [1:18:16<1:10:52,  5.60s/it]
 0:  83%| | 3640/4398 [1:18:21<1:10:39,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4349, 'grad_norm': 1.1735617608394564, 'learning_rate': 7.589792107119643e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3640/4398 [1:18:21<1:10:39,  5.59s/it]
 0:  83%| | 3641/4398 [1:18:33<1:33:12,  7.39s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4235, 'grad_norm': 1.198419911163459, 'learning_rate': 7.570300495974748e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3641/4398 [1:18:33<1:33:12,  7.39s/it]
 0:  83%| | 3642/4398 [1:19:26<4:28:12, 21.29s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4028, 'grad_norm': 1.0810177361170943, 'learning_rate': 7.550831895431799e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3642/4398 [1:19:26<4:28:12, 21.29s/it]
 0:  83%| | 3643/4398 [1:20:44<8:01:11, 38.24s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4553, 'grad_norm': 1.1733316246455399, 'learning_rate': 7.531386316049067e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3643/4398 [1:20:44<8:01:11, 38.24s/it]
 0:  83%| | 3644/4398 [1:20:50<5:57:19, 28.43s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4587, 'grad_norm': 1.0578999284986346, 'learning_rate': 7.511963768372316e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3644/4398 [1:20:50<5:57:19, 28.43s/it]
 0:  83%| | 3645/4398 [1:20:55<4:30:49, 21.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4479, 'grad_norm': 1.0790638095363385, 'learning_rate': 7.492564262934848e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3645/4398 [1:20:55<4:30:49, 21.58s/it]
 0:  83%| | 3646/4398 [1:21:01<3:30:14, 16.77s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4019, 'grad_norm': 1.1209682931180829, 'learning_rate': 7.473187810257465e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3646/4398 [1:21:01<3:30:14, 16.77s/it]
 0:  83%| | 3647/4398 [1:21:07<2:47:58, 13.42s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3891, 'grad_norm': 1.1023331914436465, 'learning_rate': 7.45383442084846e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3647/4398 [1:21:07<2:47:58, 13.42s/it]
 0:  83%| | 3648/4398 [1:21:12<2:18:16, 11.06s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4161, 'grad_norm': 1.016041609097485, 'learning_rate': 7.434504105203622e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3648/4398 [1:21:12<2:18:16, 11.06s/it]
 0:  83%| | 3649/4398 [1:21:18<1:57:34,  9.42s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4651, 'grad_norm': 1.2049410893847015, 'learning_rate': 7.415196873806213e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3649/4398 [1:21:18<1:57:34,  9.42s/it]
 0:  83%| | 3650/4398 [1:21:23<1:43:00,  8.26s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4582, 'grad_norm': 1.0569191479257831, 'learning_rate': 7.395912737127009e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3650/4398 [1:21:23<1:43:00,  8.26s/it]
 0:  83%| | 3651/4398 [1:21:29<1:32:58,  7.47s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4223, 'grad_norm': 1.054842183434723, 'learning_rate': 7.376651705624239e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3651/4398 [1:21:29<1:32:58,  7.47s/it]
 0:  83%| | 3652/4398 [1:21:34<1:25:47,  6.90s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.2191572174714889, 'learning_rate': 7.357413789743595e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3652/4398 [1:21:34<1:25:47,  6.90s/it]
 0:  83%| | 3653/4398 [1:21:40<1:20:46,  6.50s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4638, 'grad_norm': 1.199484646791984, 'learning_rate': 7.338198999918245e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3653/4398 [1:21:40<1:20:46,  6.50s/it]
 0:  83%| | 3654/4398 [1:21:46<1:17:10,  6.22s/it]
 0:                                                        
 0: 
 0: {'loss': 0.441, 'grad_norm': 1.2227999684961086, 'learning_rate': 7.319007346568818e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3654/4398 [1:21:46<1:17:10,  6.22s/it]
 0:  83%| | 3655/4398 [1:21:51<1:14:45,  6.04s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4542, 'grad_norm': 1.116003988251027, 'learning_rate': 7.299838840103374e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3655/4398 [1:21:51<1:14:45,  6.04s/it]
 0:  83%| | 3656/4398 [1:21:57<1:12:52,  5.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4097, 'grad_norm': 1.1432061145208203, 'learning_rate': 7.28069349091744e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3656/4398 [1:21:57<1:12:52,  5.89s/it]
 0:  83%| | 3657/4398 [1:22:02<1:11:37,  5.80s/it]
 0:                                                        
 0: 
 0: {'loss': 0.433, 'grad_norm': 0.9903457663198323, 'learning_rate': 7.261571309393972e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3657/4398 [1:22:02<1:11:37,  5.80s/it]
 0:  83%| | 3658/4398 [1:22:08<1:10:40,  5.73s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4479, 'grad_norm': 1.2195002403536819, 'learning_rate': 7.242472305903387e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3658/4398 [1:22:08<1:10:40,  5.73s/it]
 0:  83%| | 3659/4398 [1:22:14<1:10:06,  5.69s/it]
 0:                                                        
 0: 
 0: {'loss': 0.468, 'grad_norm': 1.1782397061809942, 'learning_rate': 7.223396490803503e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3659/4398 [1:22:14<1:10:06,  5.69s/it]
 0:  83%| | 3660/4398 [1:22:19<1:09:32,  5.65s/it]
 0:                                                        
 0: 
 0: {'loss': 0.44, 'grad_norm': 1.2883452250414522, 'learning_rate': 7.204343874439578e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3660/4398 [1:22:19<1:09:32,  5.65s/it]
 0:  83%| | 3661/4398 [1:22:25<1:09:09,  5.63s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4538, 'grad_norm': 1.1336128707144544, 'learning_rate': 7.185314467144283e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3661/4398 [1:22:25<1:09:09,  5.63s/it]
 0:  83%| | 3662/4398 [1:22:30<1:08:53,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.461, 'grad_norm': 5.174462181380496, 'learning_rate': 7.1663082792377e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3662/4398 [1:22:30<1:08:53,  5.62s/it]
 0:  83%| | 3663/4398 [1:22:36<1:08:46,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4256, 'grad_norm': 1.0869649485932784, 'learning_rate': 7.147325321027327e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3663/4398 [1:22:36<1:08:46,  5.61s/it]
 0:  83%| | 3664/4398 [1:22:41<1:08:34,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4216, 'grad_norm': 1.0915963612230382, 'learning_rate': 7.128365602808051e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3664/4398 [1:22:41<1:08:34,  5.61s/it]
 0:  83%| | 3665/4398 [1:22:47<1:08:28,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4477, 'grad_norm': 1.2106624008872204, 'learning_rate': 7.109429134862167e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3665/4398 [1:22:47<1:08:28,  5.61s/it]
 0:  83%| | 3666/4398 [1:22:53<1:08:08,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4008, 'grad_norm': 1.1490796533891785, 'learning_rate': 7.090515927459357e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3666/4398 [1:22:53<1:08:08,  5.59s/it]
 0:  83%| | 3667/4398 [1:22:58<1:08:07,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4248, 'grad_norm': 1.5739855924785844, 'learning_rate': 7.071625990856695e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3667/4398 [1:22:58<1:08:07,  5.59s/it]
 0:  83%| | 3668/4398 [1:23:04<1:07:57,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4108, 'grad_norm': 1.1175159802332648, 'learning_rate': 7.05275933529862e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3668/4398 [1:23:04<1:07:57,  5.59s/it]
 0:  83%| | 3669/4398 [1:23:09<1:07:57,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.442, 'grad_norm': 1.4599913560896594, 'learning_rate': 7.033915971016952e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3669/4398 [1:23:09<1:07:57,  5.59s/it]
 0:  83%| | 3670/4398 [1:23:15<1:07:47,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3973, 'grad_norm': 1.2403658117841025, 'learning_rate': 7.015095908230884e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3670/4398 [1:23:15<1:07:47,  5.59s/it]
 0:  83%| | 3671/4398 [1:23:21<1:07:45,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4318, 'grad_norm': 2.777716122301975, 'learning_rate': 6.996299157146974e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3671/4398 [1:23:21<1:07:45,  5.59s/it]
 0:  83%| | 3672/4398 [1:23:26<1:07:40,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4616, 'grad_norm': 1.208361954401067, 'learning_rate': 6.977525727959128e-07, 'epoch': 0.83}
 0: 
 0:  83%| | 3672/4398 [1:23:26<1:07:40,  5.59s/it]
 0:  84%| | 3673/4398 [1:23:32<1:07:35,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4355, 'grad_norm': 1.1289028359840452, 'learning_rate': 6.958775630848602e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3673/4398 [1:23:32<1:07:35,  5.59s/it]
 0:  84%| | 3674/4398 [1:23:37<1:07:26,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.434, 'grad_norm': 1.0148762145537285, 'learning_rate': 6.940048875984018e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3674/4398 [1:23:37<1:07:26,  5.59s/it]
 0:  84%| | 3675/4398 [1:23:43<1:07:21,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4489, 'grad_norm': 1.0210392974314528, 'learning_rate': 6.921345473521302e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3675/4398 [1:23:43<1:07:21,  5.59s/it]
 0:  84%| | 3676/4398 [1:23:48<1:07:13,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4139, 'grad_norm': 1.0758093287437325, 'learning_rate': 6.902665433603772e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3676/4398 [1:23:48<1:07:13,  5.59s/it]
 0:  84%| | 3677/4398 [1:23:54<1:07:12,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4502, 'grad_norm': 1.7894407287199174, 'learning_rate': 6.884008766362022e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3677/4398 [1:23:54<1:07:12,  5.59s/it]
 0:  84%| | 3678/4398 [1:24:00<1:07:08,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4556, 'grad_norm': 1.0395961869796255, 'learning_rate': 6.865375481914017e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3678/4398 [1:24:00<1:07:08,  5.60s/it]
 0:  84%| | 3679/4398 [1:24:05<1:07:07,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4689, 'grad_norm': 1.2189750994730186, 'learning_rate': 6.846765590364979e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3679/4398 [1:24:05<1:07:07,  5.60s/it]
 0:  84%| | 3680/4398 [1:24:11<1:06:52,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4025, 'grad_norm': 1.1846320349135777, 'learning_rate': 6.828179101807497e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3680/4398 [1:24:11<1:06:52,  5.59s/it]
 0:  84%| | 3681/4398 [1:24:16<1:06:54,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4232, 'grad_norm': 0.9619082670223326, 'learning_rate': 6.809616026321453e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3681/4398 [1:24:16<1:06:54,  5.60s/it]
 0:  84%| | 3682/4398 [1:24:22<1:06:45,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4111, 'grad_norm': 1.0535015515373547, 'learning_rate': 6.791076373974026e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3682/4398 [1:24:22<1:06:45,  5.59s/it]
 0:  84%| | 3683/4398 [1:24:28<1:06:42,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4145, 'grad_norm': 1.0920024814014146, 'learning_rate': 6.77256015481969e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3683/4398 [1:24:28<1:06:42,  5.60s/it]
 0:  84%| | 3684/4398 [1:24:33<1:06:34,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4752, 'grad_norm': 1.4514629336477256, 'learning_rate': 6.75406737890023e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3684/4398 [1:24:33<1:06:34,  5.59s/it]
 0:  84%| | 3685/4398 [1:24:39<1:06:27,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4432, 'grad_norm': 1.2541584291467573, 'learning_rate': 6.735598056244708e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3685/4398 [1:24:39<1:06:27,  5.59s/it]
 0:  84%| | 3686/4398 [1:24:44<1:06:24,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4571, 'grad_norm': 1.191461999151831, 'learning_rate': 6.717152196869448e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3686/4398 [1:24:44<1:06:24,  5.60s/it]
 0:  84%| | 3687/4398 [1:24:50<1:06:14,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4132, 'grad_norm': 1.2522803836113867, 'learning_rate': 6.698729810778065e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3687/4398 [1:24:50<1:06:14,  5.59s/it]
 0:  84%| | 3688/4398 [1:24:56<1:06:08,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4629, 'grad_norm': 1.4611867522035584, 'learning_rate': 6.680330907961452e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3688/4398 [1:24:56<1:06:08,  5.59s/it]
 0:  84%| | 3689/4398 [1:25:01<1:06:07,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4448, 'grad_norm': 1.2086898225446827, 'learning_rate': 6.66195549839776e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3689/4398 [1:25:01<1:06:07,  5.60s/it]
 0:  84%| | 3690/4398 [1:25:07<1:05:56,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4451, 'grad_norm': 1.6757754500503907, 'learning_rate': 6.643603592052378e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3690/4398 [1:25:07<1:05:56,  5.59s/it]
 0:  84%| | 3691/4398 [1:25:12<1:05:51,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4595, 'grad_norm': 1.0694744726164314, 'learning_rate': 6.625275198877967e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3691/4398 [1:25:12<1:05:51,  5.59s/it]
 0:  84%| | 3692/4398 [1:25:18<1:05:41,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4459, 'grad_norm': 1.077027975851282, 'learning_rate': 6.606970328814455e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3692/4398 [1:25:18<1:05:41,  5.58s/it]
 0:  84%| | 3693/4398 [1:25:24<1:05:35,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4111, 'grad_norm': 1.1249839222186775, 'learning_rate': 6.588688991788983e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3693/4398 [1:25:24<1:05:35,  5.58s/it]
 0:  84%| | 3694/4398 [1:25:29<1:05:28,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4278, 'grad_norm': 1.0606855498513488, 'learning_rate': 6.570431197715943e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3694/4398 [1:25:29<1:05:28,  5.58s/it]
 0:  84%| | 3695/4398 [1:25:35<1:05:25,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.396, 'grad_norm': 1.0903797675946711, 'learning_rate': 6.552196956496959e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3695/4398 [1:25:35<1:05:25,  5.58s/it]
 0:  84%| | 3696/4398 [1:25:40<1:05:18,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4176, 'grad_norm': 1.070379863503096, 'learning_rate': 6.533986278020876e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3696/4398 [1:25:40<1:05:18,  5.58s/it]
 0:  84%| | 3697/4398 [1:25:46<1:05:18,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4387, 'grad_norm': 1.35559509601786, 'learning_rate': 6.515799172163762e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3697/4398 [1:25:46<1:05:18,  5.59s/it]
 0:  84%| | 3698/4398 [1:25:51<1:05:09,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.0731019731248095, 'learning_rate': 6.497635648788908e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3698/4398 [1:25:51<1:05:09,  5.59s/it]
 0:  84%| | 3699/4398 [1:25:57<1:05:05,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.0821421762342518, 'learning_rate': 6.479495717746809e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3699/4398 [1:25:57<1:05:05,  5.59s/it]
 0:  84%| | 3700/4398 [1:26:03<1:04:58,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.509, 'grad_norm': 1.157755041337674, 'learning_rate': 6.461379388875161e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3700/4398 [1:26:03<1:04:58,  5.59s/it]
 0:  84%| | 3701/4398 [1:26:08<1:04:53,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4156, 'grad_norm': 0.9532906943697004, 'learning_rate': 6.443286671998872e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3701/4398 [1:26:08<1:04:53,  5.59s/it]
 0:  84%| | 3702/4398 [1:26:14<1:04:45,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4721, 'grad_norm': 1.3343255490550496, 'learning_rate': 6.425217576930037e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3702/4398 [1:26:14<1:04:45,  5.58s/it]
 0:  84%| | 3703/4398 [1:26:19<1:04:44,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4582, 'grad_norm': 1.1803565629471249, 'learning_rate': 6.407172113467941e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3703/4398 [1:26:19<1:04:44,  5.59s/it]
 0:  84%| | 3704/4398 [1:26:25<1:04:35,  5.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4078, 'grad_norm': 1.9579690345729523, 'learning_rate': 6.389150291399054e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3704/4398 [1:26:25<1:04:35,  5.58s/it]
 0:  84%| | 3705/4398 [1:26:31<1:04:31,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.0952536998718723, 'learning_rate': 6.371152120497026e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3705/4398 [1:26:31<1:04:31,  5.59s/it]
 0:  84%| | 3706/4398 [1:26:36<1:04:25,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4316, 'grad_norm': 1.1885651227580085, 'learning_rate': 6.353177610522682e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3706/4398 [1:26:36<1:04:25,  5.59s/it]
 0:  84%| | 3707/4398 [1:26:42<1:04:20,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4364, 'grad_norm': 1.110472911508493, 'learning_rate': 6.335226771224001e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3707/4398 [1:26:42<1:04:20,  5.59s/it]
 0:  84%| | 3708/4398 [1:26:47<1:04:31,  5.61s/it]
 0:                                                        
 0: 
 0: {'loss': 0.479, 'grad_norm': 1.1141621664522119, 'learning_rate': 6.317299612336147e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3708/4398 [1:26:47<1:04:31,  5.61s/it]
 0:  84%| | 3709/4398 [1:27:00<1:28:00,  7.66s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4136, 'grad_norm': 1.0534392512446316, 'learning_rate': 6.299396143581421e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3709/4398 [1:27:00<1:28:00,  7.66s/it]
 0:  84%| | 3710/4398 [1:27:38<3:13:25, 16.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4572, 'grad_norm': 1.0286568868546282, 'learning_rate': 6.281516374669294e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3710/4398 [1:27:38<3:13:25, 16.87s/it]
 0:  84%| | 3711/4398 [1:29:15<7:48:09, 40.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3975, 'grad_norm': 1.0655994321967914, 'learning_rate': 6.263660315296382e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3711/4398 [1:29:15<7:48:09, 40.89s/it]
 0:  84%| | 3712/4398 [1:29:23<5:53:12, 30.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4653, 'grad_norm': 1.1803400235777757, 'learning_rate': 6.245827975146424e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3712/4398 [1:29:23<5:53:12, 30.89s/it]
 0:  84%| | 3713/4398 [1:29:28<4:25:58, 23.30s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4307, 'grad_norm': 1.1237397620053042, 'learning_rate': 6.228019363890325e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3713/4398 [1:29:28<4:25:58, 23.30s/it]
 0:  84%| | 3714/4398 [1:29:34<3:24:55, 17.98s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4553, 'grad_norm': 1.443820501613357, 'learning_rate': 6.210234491186079e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3714/4398 [1:29:34<3:24:55, 17.98s/it]
 0:  84%| | 3715/4398 [1:29:39<2:42:20, 14.26s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4289, 'grad_norm': 1.0435646043753513, 'learning_rate': 6.192473366678848e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3715/4398 [1:29:39<2:42:20, 14.26s/it]
 0:  84%| | 3716/4398 [1:29:45<2:12:42, 11.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4102, 'grad_norm': 1.3343142247759419, 'learning_rate': 6.174736000000891e-07, 'epoch': 0.84}
 0: 
 0:  84%| | 3716/4398 [1:29:45<2:12:42, 11.68s/it]
 0:  85%| | 3717/4398 [1:29:51<1:51:49,  9.85s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4185, 'grad_norm': 1.2512775404048053, 'learning_rate': 6.157022400771584e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3717/4398 [1:29:51<1:51:49,  9.85s/it]
 0:  85%| | 3718/4398 [1:29:56<1:37:05,  8.57s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4305, 'grad_norm': 1.6366896393282668, 'learning_rate': 6.139332578597446e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3718/4398 [1:29:56<1:37:05,  8.57s/it]
 0:  85%| | 3719/4398 [1:30:02<1:26:45,  7.67s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3951, 'grad_norm': 3.611830986202169, 'learning_rate': 6.121666543072052e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3719/4398 [1:30:02<1:26:45,  7.67s/it]
 0:  85%| | 3720/4398 [1:30:07<1:19:28,  7.03s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4182, 'grad_norm': 1.1372965902112366, 'learning_rate': 6.104024303776107e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3720/4398 [1:30:07<1:19:28,  7.03s/it]
 0:  85%| | 3721/4398 [1:30:13<1:14:29,  6.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4285, 'grad_norm': 1.0905864690666056, 'learning_rate': 6.086405870277395e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3721/4398 [1:30:13<1:14:29,  6.60s/it]
 0:  85%| | 3722/4398 [1:30:19<1:10:57,  6.30s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4534, 'grad_norm': 1.0679980974771333, 'learning_rate': 6.068811252130813e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3722/4398 [1:30:19<1:10:57,  6.30s/it]
 0:  85%| | 3723/4398 [1:30:24<1:08:27,  6.09s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4444, 'grad_norm': 1.1831575024860994, 'learning_rate': 6.051240458878316e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3723/4398 [1:30:24<1:08:27,  6.09s/it]
 0:  85%| | 3724/4398 [1:30:30<1:06:36,  5.93s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3918, 'grad_norm': 1.0939269513101628, 'learning_rate': 6.033693500048948e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3724/4398 [1:30:30<1:06:36,  5.93s/it]
 0:  85%| | 3725/4398 [1:30:35<1:05:20,  5.83s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4089, 'grad_norm': 1.0072248316916739, 'learning_rate': 6.016170385158832e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3725/4398 [1:30:35<1:05:20,  5.83s/it]
 0:  85%| | 3726/4398 [1:30:41<1:04:22,  5.75s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.2551973347741214, 'learning_rate': 5.998671123711158e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3726/4398 [1:30:41<1:04:22,  5.75s/it]
 0:  85%| | 3727/4398 [1:30:46<1:03:48,  5.71s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4698, 'grad_norm': 1.086458440388073, 'learning_rate': 5.981195725196176e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3727/4398 [1:30:46<1:03:48,  5.71s/it]
 0:  85%| | 3728/4398 [1:30:52<1:03:16,  5.67s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4303, 'grad_norm': 1.2763679402581414, 'learning_rate': 5.963744199091198e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3728/4398 [1:30:52<1:03:16,  5.67s/it]
 0:  85%| | 3729/4398 [1:30:58<1:02:56,  5.65s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4268, 'grad_norm': 1.0814803992928887, 'learning_rate': 5.946316554860582e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3729/4398 [1:30:58<1:02:56,  5.65s/it]
 0:  85%| | 3730/4398 [1:31:03<1:02:39,  5.63s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4626, 'grad_norm': 1.163357113669038, 'learning_rate': 5.928912801955749e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3730/4398 [1:31:03<1:02:39,  5.63s/it]
 0:  85%| | 3731/4398 [1:31:09<1:02:29,  5.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4719, 'grad_norm': 1.1499979545624468, 'learning_rate': 5.911532949815146e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3731/4398 [1:31:09<1:02:29,  5.62s/it]
 0:  85%| | 3732/4398 [1:31:14<1:02:11,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4394, 'grad_norm': 1.0799207208933133, 'learning_rate': 5.894177007864272e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3732/4398 [1:31:14<1:02:11,  5.60s/it]
 0:  85%| | 3733/4398 [1:31:20<1:02:05,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3873, 'grad_norm': 1.1906783836953108, 'learning_rate': 5.876844985515656e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3733/4398 [1:31:20<1:02:05,  5.60s/it]
 0:  85%| | 3734/4398 [1:31:26<1:01:56,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4481, 'grad_norm': 2.026753799707528, 'learning_rate': 5.859536892168843e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3734/4398 [1:31:26<1:01:56,  5.60s/it]
 0:  85%| | 3735/4398 [1:31:31<1:01:47,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4339, 'grad_norm': 1.1069020917125894, 'learning_rate': 5.842252737210413e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3735/4398 [1:31:31<1:01:47,  5.59s/it]
 0:  85%| | 3736/4398 [1:31:37<1:01:39,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4698, 'grad_norm': 1.04988238204747, 'learning_rate': 5.824992530013979e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3736/4398 [1:31:37<1:01:39,  5.59s/it]
 0:  85%| | 3737/4398 [1:31:42<1:01:36,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4025, 'grad_norm': 1.1675154553701077, 'learning_rate': 5.807756279940124e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3737/4398 [1:31:42<1:01:36,  5.59s/it]
 0:  85%| | 3738/4398 [1:31:48<1:01:27,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3851, 'grad_norm': 0.9966576917000551, 'learning_rate': 5.790543996336468e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3738/4398 [1:31:48<1:01:27,  5.59s/it]
 0:  85%| | 3739/4398 [1:31:54<1:01:25,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4173, 'grad_norm': 0.9852959234718405, 'learning_rate': 5.773355688537635e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3739/4398 [1:31:54<1:01:25,  5.59s/it]
 0:  85%| | 3740/4398 [1:31:59<1:01:14,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4601, 'grad_norm': 1.2233856774137837, 'learning_rate': 5.756191365865238e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3740/4398 [1:31:59<1:01:14,  5.59s/it]
 0:  85%| | 3741/4398 [1:32:05<1:01:14,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4176, 'grad_norm': 1.5259374342902845, 'learning_rate': 5.739051037627885e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3741/4398 [1:32:05<1:01:14,  5.59s/it]
 0:  85%| | 3742/4398 [1:32:10<1:01:06,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4469, 'grad_norm': 2.2101750365273722, 'learning_rate': 5.721934713121163e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3742/4398 [1:32:10<1:01:06,  5.59s/it]
 0:  85%| | 3743/4398 [1:32:16<1:00:59,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4056, 'grad_norm': 1.0388109700958814, 'learning_rate': 5.704842401627647e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3743/4398 [1:32:16<1:00:59,  5.59s/it]
 0:  85%| | 3744/4398 [1:32:21<1:00:52,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4384, 'grad_norm': 1.1075125031388, 'learning_rate': 5.687774112416905e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3744/4398 [1:32:21<1:00:52,  5.59s/it]
 0:  85%| | 3745/4398 [1:32:27<1:00:49,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4515, 'grad_norm': 1.108405130591801, 'learning_rate': 5.670729854745454e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3745/4398 [1:32:27<1:00:49,  5.59s/it]
 0:  85%| | 3746/4398 [1:32:33<1:00:41,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4668, 'grad_norm': 1.2642779903136205, 'learning_rate': 5.653709637856791e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3746/4398 [1:32:33<1:00:41,  5.59s/it]
 0:  85%| | 3747/4398 [1:32:38<1:00:44,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.461, 'grad_norm': 1.0778365988111736, 'learning_rate': 5.636713470981364e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3747/4398 [1:32:38<1:00:44,  5.60s/it]
 0:  85%| | 3748/4398 [1:32:44<1:00:32,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4501, 'grad_norm': 1.1859227807923975, 'learning_rate': 5.6197413633366e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3748/4398 [1:32:44<1:00:32,  5.59s/it]
 0:  85%| | 3749/4398 [1:32:49<1:00:32,  5.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4745, 'grad_norm': 1.0629639890632714, 'learning_rate': 5.602793324126842e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3749/4398 [1:32:49<1:00:32,  5.60s/it]
 0:  85%| | 3750/4398 [1:32:55<1:00:20,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4156, 'grad_norm': 1.0841764368692133, 'learning_rate': 5.585869362543416e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3750/4398 [1:32:55<1:00:20,  5.59s/it]
 0:  85%| | 3751/4398 [1:33:01<1:00:18,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.439, 'grad_norm': 1.1438666789233753, 'learning_rate': 5.568969487764558e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3751/4398 [1:33:01<1:00:18,  5.59s/it]
 0:  85%| | 3752/4398 [1:33:06<1:00:12,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4199, 'grad_norm': 1.0811753831946398, 'learning_rate': 5.552093708955486e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3752/4398 [1:33:06<1:00:12,  5.59s/it]
 0:  85%| | 3753/4398 [1:33:12<1:00:07,  5.59s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4128, 'grad_norm': 1.1126784187339294, 'learning_rate': 5.535242035268307e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3753/4398 [1:33:12<1:00:07,  5.59s/it]
 0:  85%| | 3754/4398 [1:33:17<59:58,  5.59s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4349, 'grad_norm': 1.02716517549041, 'learning_rate': 5.518414475842072e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3754/4398 [1:33:17<59:58,  5.59s/it]
 0:  85%| | 3755/4398 [1:33:23<59:53,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3868, 'grad_norm': 0.9983935781861472, 'learning_rate': 5.501611039802757e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3755/4398 [1:33:23<59:53,  5.59s/it]
 0:  85%| | 3756/4398 [1:33:29<59:46,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4105, 'grad_norm': 1.0412558885871563, 'learning_rate': 5.484831736263246e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3756/4398 [1:33:29<59:46,  5.59s/it]
 0:  85%| | 3757/4398 [1:33:34<59:46,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4238, 'grad_norm': 1.161046072758336, 'learning_rate': 5.46807657432335e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3757/4398 [1:33:34<59:46,  5.59s/it]
 0:  85%| | 3758/4398 [1:33:40<59:39,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.402, 'grad_norm': 1.1199569196160395, 'learning_rate': 5.451345563069771e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3758/4398 [1:33:40<59:39,  5.59s/it]
 0:  85%| | 3759/4398 [1:33:45<59:37,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4506, 'grad_norm': 1.121477292747616, 'learning_rate': 5.434638711576124e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3759/4398 [1:33:45<59:37,  5.60s/it]
 0:  85%| | 3760/4398 [1:33:51<59:23,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4361, 'grad_norm': 1.0550098131425885, 'learning_rate': 5.417956028902921e-07, 'epoch': 0.85}
 0: 
 0:  85%| | 3760/4398 [1:33:51<59:23,  5.59s/it]
 0:  86%| | 3761/4398 [1:33:57<59:22,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4669, 'grad_norm': 1.0551857387415662, 'learning_rate': 5.401297524097565e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3761/4398 [1:33:57<59:22,  5.59s/it]
 0:  86%| | 3762/4398 [1:34:02<59:15,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4991, 'grad_norm': 1.2594754864175473, 'learning_rate': 5.38466320619434e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3762/4398 [1:34:02<59:15,  5.59s/it]
 0:  86%| | 3763/4398 [1:34:08<59:12,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4327, 'grad_norm': 1.2402573558285876, 'learning_rate': 5.36805308421442e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3763/4398 [1:34:08<59:12,  5.59s/it]
 0:  86%| | 3764/4398 [1:34:13<58:58,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4465, 'grad_norm': 1.1701325516308967, 'learning_rate': 5.351467167165864e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3764/4398 [1:34:13<58:58,  5.58s/it]
 0:  86%| | 3765/4398 [1:34:19<58:57,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4119, 'grad_norm': 1.0667233229050683, 'learning_rate': 5.334905464043588e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3765/4398 [1:34:19<58:57,  5.59s/it]
 0:  86%| | 3766/4398 [1:34:24<58:49,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4315, 'grad_norm': 1.0162823237574263, 'learning_rate': 5.318367983829393e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3766/4398 [1:34:24<58:49,  5.58s/it]
 0:  86%| | 3767/4398 [1:34:30<58:44,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4292, 'grad_norm': 1.1134643375361888, 'learning_rate': 5.301854735491924e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3767/4398 [1:34:30<58:44,  5.59s/it]
 0:  86%| | 3768/4398 [1:34:36<58:39,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4308, 'grad_norm': 1.3825912831867784, 'learning_rate': 5.285365727986708e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3768/4398 [1:34:36<58:39,  5.59s/it]
 0:  86%| | 3769/4398 [1:34:41<58:36,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4004, 'grad_norm': 1.0622173437587281, 'learning_rate': 5.268900970256092e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3769/4398 [1:34:41<58:36,  5.59s/it]
 0:  86%| | 3770/4398 [1:34:47<58:27,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4195, 'grad_norm': 1.050896582582706, 'learning_rate': 5.252460471229315e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3770/4398 [1:34:47<58:27,  5.58s/it]
 0:  86%| | 3771/4398 [1:34:52<58:24,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.421, 'grad_norm': 1.030448917075274, 'learning_rate': 5.236044239822441e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3771/4398 [1:34:52<58:24,  5.59s/it]
 0:  86%| | 3772/4398 [1:34:58<58:21,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4372, 'grad_norm': 1.1799514272766682, 'learning_rate': 5.219652284938348e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3772/4398 [1:34:58<58:21,  5.59s/it]
 0:  86%| | 3773/4398 [1:35:04<58:18,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4542, 'grad_norm': 1.0612794041238, 'learning_rate': 5.203284615466786e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3773/4398 [1:35:04<58:18,  5.60s/it]
 0:  86%| | 3774/4398 [1:35:09<58:26,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.466, 'grad_norm': 1.0891339882083901, 'learning_rate': 5.18694124028431e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3774/4398 [1:35:09<58:26,  5.62s/it]
 0:  86%| | 3775/4398 [1:35:15<58:14,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4207, 'grad_norm': 1.1633116140496702, 'learning_rate': 5.170622168254313e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3775/4398 [1:35:15<58:14,  5.61s/it]
 0:  86%| | 3776/4398 [1:35:20<58:04,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4161, 'grad_norm': 1.078842626131637, 'learning_rate': 5.154327408227e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3776/4398 [1:35:20<58:04,  5.60s/it]
 0:  86%| | 3777/4398 [1:35:28<1:04:15,  6.21s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4365, 'grad_norm': 1.2065924563089585, 'learning_rate': 5.138056969039384e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3777/4398 [1:35:28<1:04:15,  6.21s/it]
 0:  86%| | 3778/4398 [1:35:54<2:05:11, 12.12s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4223, 'grad_norm': 1.2050639004195483, 'learning_rate': 5.121810859515325e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3778/4398 [1:35:54<2:05:11, 12.12s/it]
 0:  86%| | 3779/4398 [1:37:33<6:35:21, 38.32s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4098, 'grad_norm': 1.23532660765093, 'learning_rate': 5.105589088465446e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3779/4398 [1:37:33<6:35:21, 38.32s/it]
 0:  86%| | 3780/4398 [1:37:51<5:29:52, 32.03s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4618, 'grad_norm': 1.2220456569084022, 'learning_rate': 5.089391664687182e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3780/4398 [1:37:51<5:29:52, 32.03s/it]
 0:  86%| | 3781/4398 [1:37:56<4:07:49, 24.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.1065518923795494, 'learning_rate': 5.073218596964779e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3781/4398 [1:37:56<4:07:49, 24.10s/it]
 0:  86%| | 3782/4398 [1:38:02<3:10:16, 18.53s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3954, 'grad_norm': 1.064987266395561, 'learning_rate': 5.057069894069261e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3782/4398 [1:38:02<3:10:16, 18.53s/it]
 0:  86%| | 3783/4398 [1:38:07<2:30:08, 14.65s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4041, 'grad_norm': 4.579910922727023, 'learning_rate': 5.040945564758448e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3783/4398 [1:38:07<2:30:08, 14.65s/it]
 0:  86%| | 3784/4398 [1:38:13<2:02:04, 11.93s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4705, 'grad_norm': 1.138416660328542, 'learning_rate': 5.024845617776913e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3784/4398 [1:38:13<2:02:04, 11.93s/it]
 0:  86%| | 3785/4398 [1:38:19<1:42:27, 10.03s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.0459764989740974, 'learning_rate': 5.008770061856033e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3785/4398 [1:38:19<1:42:27, 10.03s/it]
 0:  86%| | 3786/4398 [1:38:24<1:28:40,  8.69s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.2614308568225143, 'learning_rate': 4.992718905713967e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3786/4398 [1:38:24<1:28:40,  8.69s/it]
 0:  86%| | 3787/4398 [1:38:30<1:19:06,  7.77s/it]
 0:                                                        
 0: 
 0: {'loss': 0.414, 'grad_norm': 1.4176629799433424, 'learning_rate': 4.976692158055618e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3787/4398 [1:38:30<1:19:06,  7.77s/it]
 0:  86%| | 3788/4398 [1:38:35<1:12:11,  7.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4601, 'grad_norm': 1.0707538872223294, 'learning_rate': 4.960689827572657e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3788/4398 [1:38:35<1:12:11,  7.10s/it]
 0:  86%| | 3789/4398 [1:38:41<1:07:28,  6.65s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4144, 'grad_norm': 1.0838805868434946, 'learning_rate': 4.944711922943524e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3789/4398 [1:38:41<1:07:28,  6.65s/it]
 0:  86%| | 3790/4398 [1:38:47<1:04:04,  6.32s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4356, 'grad_norm': 1.1068955766101312, 'learning_rate': 4.928758452833393e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3790/4398 [1:38:47<1:04:04,  6.32s/it]
 0:  86%| | 3791/4398 [1:38:52<1:01:45,  6.11s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4035, 'grad_norm': 1.058623391147817, 'learning_rate': 4.912829425894211e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3791/4398 [1:38:52<1:01:45,  6.11s/it]
 0:  86%| | 3792/4398 [1:38:58<1:00:04,  5.95s/it]
 0:                                                        
 0: 
 0: {'loss': 0.381, 'grad_norm': 0.9457039653879163, 'learning_rate': 4.89692485076464e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3792/4398 [1:38:58<1:00:04,  5.95s/it]
 0:  86%| | 3793/4398 [1:39:03<58:58,  5.85s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4847, 'grad_norm': 1.1349192815192546, 'learning_rate': 4.881044736070112e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3793/4398 [1:39:03<58:58,  5.85s/it]
 0:  86%| | 3794/4398 [1:39:09<58:03,  5.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4523, 'grad_norm': 1.2051328495510554, 'learning_rate': 4.865189090422767e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3794/4398 [1:39:09<58:03,  5.77s/it]
 0:  86%| | 3795/4398 [1:39:15<57:27,  5.72s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.0892857298942433, 'learning_rate': 4.849357922421494e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3795/4398 [1:39:15<57:27,  5.72s/it]
 0:  86%| | 3796/4398 [1:39:20<56:58,  5.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4222, 'grad_norm': 1.1167240729691763, 'learning_rate': 4.833551240651896e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3796/4398 [1:39:20<56:58,  5.68s/it]
 0:  86%| | 3797/4398 [1:39:26<56:42,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4121, 'grad_norm': 1.0392637465517514, 'learning_rate': 4.8177690536863e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3797/4398 [1:39:26<56:42,  5.66s/it]
 0:  86%| | 3798/4398 [1:39:31<56:22,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4764, 'grad_norm': 1.1160752254042483, 'learning_rate': 4.802011370083748e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3798/4398 [1:39:31<56:22,  5.64s/it]
 0:  86%| | 3799/4398 [1:39:37<56:09,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4648, 'grad_norm': 1.1394732013420596, 'learning_rate': 4.786278198389999e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3799/4398 [1:39:37<56:09,  5.63s/it]
 0:  86%| | 3800/4398 [1:39:43<55:57,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.3082212000125635, 'learning_rate': 4.77056954713751e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3800/4398 [1:39:43<55:57,  5.61s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  86%| | 3801/4398 [1:40:41<3:32:15, 21.33s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4154, 'grad_norm': 0.9599615787408327, 'learning_rate': 4.75488542484544e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3801/4398 [1:40:41<3:32:15, 21.33s/it]
 0:  86%| | 3802/4398 [1:40:46<2:44:56, 16.60s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4206, 'grad_norm': 1.0577192209579371, 'learning_rate': 4.7392258400196565e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3802/4398 [1:40:46<2:44:56, 16.60s/it]
 0:  86%| | 3803/4398 [1:40:52<2:11:51, 13.30s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.0769177883218328, 'learning_rate': 4.723590801152711e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3803/4398 [1:40:52<2:11:51, 13.30s/it]
 0:  86%| | 3804/4398 [1:40:57<1:48:43, 10.98s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.038254682606279, 'learning_rate': 4.707980316723837e-07, 'epoch': 0.86}
 0: 
 0:  86%| | 3804/4398 [1:40:57<1:48:43, 10.98s/it]
 0:  87%| | 3805/4398 [1:41:03<1:32:38,  9.37s/it]
 0:                                                        
 0: 
 0: {'loss': 0.382, 'grad_norm': 0.9666287953689453, 'learning_rate': 4.6923943951989735e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3805/4398 [1:41:03<1:32:38,  9.37s/it]
 0:  87%| | 3806/4398 [1:41:08<1:21:11,  8.23s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4617, 'grad_norm': 1.180328254584972, 'learning_rate': 4.6768330450307154e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3806/4398 [1:41:08<1:21:11,  8.23s/it]
 0:  87%| | 3807/4398 [1:41:14<1:13:20,  7.45s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4365, 'grad_norm': 1.0346781068196615, 'learning_rate': 4.66129627465835e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3807/4398 [1:41:14<1:13:20,  7.45s/it]
 0:  87%| | 3808/4398 [1:41:20<1:07:43,  6.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3996, 'grad_norm': 1.076108118683898, 'learning_rate': 4.6457840925078046e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3808/4398 [1:41:20<1:07:43,  6.89s/it]
 0:  87%| | 3809/4398 [1:41:25<1:03:51,  6.51s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4393, 'grad_norm': 1.3946883064427287, 'learning_rate': 4.6302965069917105e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3809/4398 [1:41:25<1:03:51,  6.51s/it]
 0:  87%| | 3810/4398 [1:41:31<1:00:56,  6.22s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4418, 'grad_norm': 1.0428029025604415, 'learning_rate': 4.6148335265093324e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3810/4398 [1:41:31<1:00:56,  6.22s/it]
 0:  87%| | 3811/4398 [1:41:36<59:03,  6.04s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.3806, 'grad_norm': 1.1389921432629435, 'learning_rate': 4.599395159446607e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3811/4398 [1:41:36<59:03,  6.04s/it]
 0:  87%| | 3812/4398 [1:41:42<57:37,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.448, 'grad_norm': 1.094363309960619, 'learning_rate': 4.583981414176103e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3812/4398 [1:41:42<57:37,  5.90s/it]
 0:  87%| | 3813/4398 [1:41:48<56:43,  5.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4472, 'grad_norm': 1.017228987385048, 'learning_rate': 4.5685922990570675e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3813/4398 [1:41:48<56:43,  5.82s/it]
 0:  87%| | 3814/4398 [1:41:53<55:58,  5.75s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4315, 'grad_norm': 0.9758791940612709, 'learning_rate': 4.5532278224353633e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3814/4398 [1:41:53<55:58,  5.75s/it]
 0:  87%| | 3815/4398 [1:41:59<55:29,  5.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4249, 'grad_norm': 1.0671316599752847, 'learning_rate': 4.537887992643497e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3815/4398 [1:41:59<55:29,  5.71s/it]
 0:  87%| | 3816/4398 [1:42:04<55:01,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4385, 'grad_norm': 1.1951044351384346, 'learning_rate': 4.5225728180006146e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3816/4398 [1:42:04<55:01,  5.67s/it]
 0:  87%| | 3817/4398 [1:42:10<54:47,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4368, 'grad_norm': 1.1613366920909374, 'learning_rate': 4.507282306812488e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3817/4398 [1:42:10<54:47,  5.66s/it]
 0:  87%| | 3818/4398 [1:42:16<54:31,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4229, 'grad_norm': 1.0164070387978736, 'learning_rate': 4.492016467371513e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3818/4398 [1:42:16<54:31,  5.64s/it]
 0:  87%| | 3819/4398 [1:42:21<54:17,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4464, 'grad_norm': 1.484085389788429, 'learning_rate': 4.476775307956699e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3819/4398 [1:42:21<54:17,  5.63s/it]
 0:  87%| | 3820/4398 [1:42:27<54:01,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4383, 'grad_norm': 1.0120342670662448, 'learning_rate': 4.4615588368336735e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3820/4398 [1:42:27<54:01,  5.61s/it]
 0:  87%| | 3821/4398 [1:42:32<53:58,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.429, 'grad_norm': 1.1319253982641988, 'learning_rate': 4.4463670622546975e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3821/4398 [1:42:32<53:58,  5.61s/it]
 0:  87%| | 3822/4398 [1:42:38<53:47,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4173, 'grad_norm': 0.9665661855845626, 'learning_rate': 4.431199992458607e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3822/4398 [1:42:38<53:47,  5.60s/it]
 0:  87%| | 3823/4398 [1:42:44<53:48,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.479, 'grad_norm': 1.0796072886594117, 'learning_rate': 4.416057635670856e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3823/4398 [1:42:44<53:48,  5.61s/it]
 0:  87%| | 3824/4398 [1:42:49<53:36,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4157, 'grad_norm': 1.0420813470696477, 'learning_rate': 4.400940000103493e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3824/4398 [1:42:49<53:36,  5.60s/it]
 0:  87%| | 3825/4398 [1:42:55<53:34,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4348, 'grad_norm': 0.9918782595227786, 'learning_rate': 4.3858470939551535e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3825/4398 [1:42:55<53:34,  5.61s/it]
 0:  87%| | 3826/4398 [1:43:00<53:26,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4494, 'grad_norm': 1.0876389093342016, 'learning_rate': 4.370778925411079e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3826/4398 [1:43:00<53:26,  5.61s/it]
 0:  87%| | 3827/4398 [1:43:06<53:36,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3966, 'grad_norm': 0.9645753895819391, 'learning_rate': 4.3557355026430714e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3827/4398 [1:43:06<53:36,  5.63s/it]
 0:  87%| | 3828/4398 [1:43:12<53:24,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4204, 'grad_norm': 1.0798367653649643, 'learning_rate': 4.3407168338095327e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3828/4398 [1:43:12<53:24,  5.62s/it]
 0:  87%| | 3829/4398 [1:43:17<53:16,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4145, 'grad_norm': 1.1180070721922915, 'learning_rate': 4.325722927055437e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3829/4398 [1:43:17<53:16,  5.62s/it]
 0:  87%| | 3830/4398 [1:43:23<53:04,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.5176852875996214, 'learning_rate': 4.310753790512312e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3830/4398 [1:43:23<53:04,  5.61s/it]
 0:  87%| | 3831/4398 [1:43:29<52:56,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4677, 'grad_norm': 1.308607375960688, 'learning_rate': 4.2958094322982703e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3831/4398 [1:43:29<52:56,  5.60s/it]
 0:  87%| | 3832/4398 [1:43:34<52:49,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4322, 'grad_norm': 1.2450771857657252, 'learning_rate': 4.280889860517989e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3832/4398 [1:43:34<52:49,  5.60s/it]
 0:  87%| | 3833/4398 [1:43:40<52:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.44, 'grad_norm': 1.1240450200996812, 'learning_rate': 4.265995083262681e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3833/4398 [1:43:40<52:42,  5.60s/it]
 0:  87%| | 3834/4398 [1:43:45<52:34,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.157993208657764, 'learning_rate': 4.251125108610138e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3834/4398 [1:43:45<52:34,  5.59s/it]
 0:  87%| | 3835/4398 [1:43:51<52:30,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4581, 'grad_norm': 1.0251757831772603, 'learning_rate': 4.2362799446246825e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3835/4398 [1:43:51<52:30,  5.60s/it]
 0:  87%| | 3836/4398 [1:43:56<52:21,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4026, 'grad_norm': 1.0447839227880709, 'learning_rate': 4.2214595993571916e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3836/4398 [1:43:56<52:21,  5.59s/it]
 0:  87%| | 3837/4398 [1:44:02<52:14,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4081, 'grad_norm': 0.9886986193144909, 'learning_rate': 4.2066640808450796e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3837/4398 [1:44:02<52:14,  5.59s/it]
 0:  87%| | 3838/4398 [1:44:08<52:08,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4137, 'grad_norm': 0.9684414744583218, 'learning_rate': 4.1918933971122844e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3838/4398 [1:44:08<52:08,  5.59s/it]
 0:  87%| | 3839/4398 [1:44:13<52:05,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4461, 'grad_norm': 1.087276854847447, 'learning_rate': 4.177147556169309e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3839/4398 [1:44:13<52:05,  5.59s/it]
 0:  87%| | 3840/4398 [1:44:19<51:58,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4353, 'grad_norm': 1.10886822268517, 'learning_rate': 4.16242656601315e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3840/4398 [1:44:19<51:58,  5.59s/it]
 0:  87%| | 3841/4398 [1:44:24<51:58,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.0304814186385758, 'learning_rate': 4.147730434627345e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3841/4398 [1:44:24<51:58,  5.60s/it]
 0:  87%| | 3842/4398 [1:44:30<51:47,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4841, 'grad_norm': 1.2149758679781355, 'learning_rate': 4.1330591699819455e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3842/4398 [1:44:30<51:47,  5.59s/it]
 0:  87%| | 3843/4398 [1:44:36<51:50,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4129, 'grad_norm': 1.1358723950108083, 'learning_rate': 4.1184127800334985e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3843/4398 [1:44:36<51:50,  5.60s/it]
 0:  87%| | 3844/4398 [1:44:41<51:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4135, 'grad_norm': 0.9514224115033185, 'learning_rate': 4.103791272725094e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3844/4398 [1:44:41<51:42,  5.60s/it]
 0:  87%| | 3845/4398 [1:44:49<57:11,  6.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4493, 'grad_norm': 1.0416375949885825, 'learning_rate': 4.089194655986306e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3845/4398 [1:44:49<57:11,  6.21s/it]
 0:  87%| | 3846/4398 [1:45:11<1:39:46, 10.85s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.0685449176013952, 'learning_rate': 4.074622937733208e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3846/4398 [1:45:11<1:39:46, 10.85s/it]
 0:  87%| | 3847/4398 [1:47:10<6:39:55, 43.55s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4011, 'grad_norm': 1.0971612735233651, 'learning_rate': 4.060076125868395e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3847/4398 [1:47:10<6:39:55, 43.55s/it]
 0:  87%| | 3848/4398 [1:47:32<5:38:00, 36.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4562, 'grad_norm': 1.0799528100899138, 'learning_rate': 4.045554228280935e-07, 'epoch': 0.87}
 0: 
 0:  87%| | 3848/4398 [1:47:32<5:38:00, 36.87s/it]
 0:  88%| | 3849/4398 [1:47:37<4:11:34, 27.49s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4342, 'grad_norm': 1.0944254947011332, 'learning_rate': 4.031057252846371e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3849/4398 [1:47:37<4:11:34, 27.49s/it]
 0:  88%| | 3850/4398 [1:47:43<3:11:02, 20.92s/it]
 0:                                                        
 0: 
 0: {'loss': 0.428, 'grad_norm': 1.024148889328673, 'learning_rate': 4.016585207426765e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3850/4398 [1:47:43<3:11:02, 20.92s/it]
 0:  88%| | 3851/4398 [1:47:48<2:28:49, 16.32s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4364, 'grad_norm': 1.0704373261082771, 'learning_rate': 4.0021380998706285e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3851/4398 [1:47:48<2:28:49, 16.32s/it]
 0:  88%| | 3852/4398 [1:47:54<1:59:11, 13.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.1570584699891364, 'learning_rate': 3.987715938012965e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3852/4398 [1:47:54<1:59:11, 13.10s/it]
 0:  88%| | 3853/4398 [1:48:00<1:38:33, 10.85s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4315, 'grad_norm': 1.06727090147544, 'learning_rate': 3.9733187296752406e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3853/4398 [1:48:00<1:38:33, 10.85s/it]
 0:  88%| | 3854/4398 [1:48:05<1:24:01,  9.27s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4108, 'grad_norm': 1.8591358142996153, 'learning_rate': 3.958946482665399e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3854/4398 [1:48:05<1:24:01,  9.27s/it]
 0:  88%| | 3855/4398 [1:48:11<1:13:58,  8.17s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4238, 'grad_norm': 1.1043719994363825, 'learning_rate': 3.9445992047778325e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3855/4398 [1:48:11<1:13:58,  8.17s/it]
 0:  88%| | 3856/4398 [1:48:16<1:07:02,  7.42s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4652, 'grad_norm': 1.2212095567695773, 'learning_rate': 3.9302769037934064e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3856/4398 [1:48:17<1:07:02,  7.42s/it]
 0:  88%| | 3857/4398 [1:48:22<1:01:59,  6.88s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4268, 'grad_norm': 1.1146219424042025, 'learning_rate': 3.9159795874794295e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3857/4398 [1:48:22<1:01:59,  6.88s/it]
 0:  88%| | 3858/4398 [1:48:28<58:22,  6.49s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4131, 'grad_norm': 1.3335826038058018, 'learning_rate': 3.9017072635896716e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3858/4398 [1:48:28<58:22,  6.49s/it]
 0:  88%| | 3859/4398 [1:48:33<55:51,  6.22s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4331, 'grad_norm': 1.0484326658005392, 'learning_rate': 3.88745993986433e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3859/4398 [1:48:33<55:51,  6.22s/it]
 0:  88%| | 3860/4398 [1:48:39<54:03,  6.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4547, 'grad_norm': 1.1335398126538925, 'learning_rate': 3.8732376240300683e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3860/4398 [1:48:39<54:03,  6.03s/it]
 0:  88%| | 3861/4398 [1:48:44<52:50,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4305, 'grad_norm': 1.3810269241322533, 'learning_rate': 3.859040323799973e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3861/4398 [1:48:44<52:50,  5.90s/it]
 0:  88%| | 3862/4398 [1:48:50<51:52,  5.81s/it]
 0:                                                      
 0: 
 0: {'loss': 0.448, 'grad_norm': 1.1267480631975126, 'learning_rate': 3.844868046873562e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3862/4398 [1:48:50<51:52,  5.81s/it]
 0:  88%| | 3863/4398 [1:48:56<51:11,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4392, 'grad_norm': 1.2018412660087185, 'learning_rate': 3.830720800936788e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3863/4398 [1:48:56<51:11,  5.74s/it]
 0:  88%| | 3864/4398 [1:49:01<50:40,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4144, 'grad_norm': 1.347366224310704, 'learning_rate': 3.816598593662024e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3864/4398 [1:49:01<50:40,  5.69s/it]
 0:  88%| | 3865/4398 [1:49:07<50:24,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4425, 'grad_norm': 1.1596394336300329, 'learning_rate': 3.802501432708089e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3865/4398 [1:49:07<50:24,  5.67s/it]
 0:  88%| | 3866/4398 [1:49:12<50:07,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4316, 'grad_norm': 1.0220214732984985, 'learning_rate': 3.788429325720172e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3866/4398 [1:49:12<50:07,  5.65s/it]
 0:  88%| | 3867/4398 [1:49:18<49:55,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4376, 'grad_norm': 1.1764973742427058, 'learning_rate': 3.7743822803299077e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3867/4398 [1:49:18<49:55,  5.64s/it]
 0:  88%| | 3868/4398 [1:49:24<49:40,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3967, 'grad_norm': 1.1055846772115567, 'learning_rate': 3.7603603041553416e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3868/4398 [1:49:24<49:40,  5.62s/it]
 0:  88%| | 3869/4398 [1:49:29<49:35,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4646, 'grad_norm': 1.0350924208961256, 'learning_rate': 3.746363404800901e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3869/4398 [1:49:29<49:35,  5.63s/it]
 0:  88%| | 3870/4398 [1:49:35<49:23,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4312, 'grad_norm': 1.0518316198241071, 'learning_rate': 3.7323915898574316e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3870/4398 [1:49:35<49:23,  5.61s/it]
 0:  88%| | 3871/4398 [1:49:40<49:19,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4335, 'grad_norm': 1.0917435614263162, 'learning_rate': 3.7184448669021766e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3871/4398 [1:49:40<49:19,  5.62s/it]
 0:  88%| | 3872/4398 [1:49:46<49:10,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.456, 'grad_norm': 1.101048105838373, 'learning_rate': 3.704523243498742e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3872/4398 [1:49:46<49:10,  5.61s/it]
 0:  88%| | 3873/4398 [1:49:52<49:03,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4435, 'grad_norm': 1.0472030097094653, 'learning_rate': 3.690626727197172e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3873/4398 [1:49:52<49:03,  5.61s/it]
 0:  88%| | 3874/4398 [1:49:57<48:54,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.1133727111293321, 'learning_rate': 3.6767553255338596e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3874/4398 [1:49:57<48:54,  5.60s/it]
 0:  88%| | 3875/4398 [1:50:03<48:50,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4008, 'grad_norm': 1.0436880395775958, 'learning_rate': 3.6629090460315763e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3875/4398 [1:50:03<48:50,  5.60s/it]
 0:  88%| | 3876/4398 [1:50:08<48:40,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3974, 'grad_norm': 1.0180592074159454, 'learning_rate': 3.649087896199488e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3876/4398 [1:50:08<48:40,  5.60s/it]
 0:  88%| | 3877/4398 [1:50:14<48:41,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4358, 'grad_norm': 1.0742508857290907, 'learning_rate': 3.635291883533121e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3877/4398 [1:50:14<48:41,  5.61s/it]
 0:  88%| | 3878/4398 [1:50:20<48:33,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4192, 'grad_norm': 1.0452114508165073, 'learning_rate': 3.6215210155143565e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3878/4398 [1:50:20<48:33,  5.60s/it]
 0:  88%| | 3879/4398 [1:50:25<48:24,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4176, 'grad_norm': 1.1192917929179977, 'learning_rate': 3.6077752996114656e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3879/4398 [1:50:25<48:24,  5.60s/it]
 0:  88%| | 3880/4398 [1:50:31<48:20,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4872, 'grad_norm': 1.1071411085686234, 'learning_rate': 3.5940547432790573e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3880/4398 [1:50:31<48:20,  5.60s/it]
 0:  88%| | 3881/4398 [1:50:36<48:15,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4495, 'grad_norm': 1.0231205749032457, 'learning_rate': 3.5803593539581126e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3881/4398 [1:50:36<48:15,  5.60s/it]
 0:  88%| | 3882/4398 [1:50:42<48:06,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4169, 'grad_norm': 1.055199537243707, 'learning_rate': 3.566689139075963e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3882/4398 [1:50:42<48:06,  5.59s/it]
 0:  88%| | 3883/4398 [1:50:48<48:03,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4141, 'grad_norm': 1.079165242796429, 'learning_rate': 3.553044106046266e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3883/4398 [1:50:48<48:03,  5.60s/it]
 0:  88%| | 3884/4398 [1:50:53<47:53,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3967, 'grad_norm': 0.9761614125036894, 'learning_rate': 3.539424262269042e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3884/4398 [1:50:53<47:53,  5.59s/it]
 0:  88%| | 3885/4398 [1:50:59<47:52,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3774, 'grad_norm': 1.0125480189786982, 'learning_rate': 3.5258296151306495e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3885/4398 [1:50:59<47:52,  5.60s/it]
 0:  88%| | 3886/4398 [1:51:04<47:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4359, 'grad_norm': 1.1239415474972625, 'learning_rate': 3.512260172003773e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3886/4398 [1:51:04<47:45,  5.60s/it]
 0:  88%| | 3887/4398 [1:51:10<47:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4654, 'grad_norm': 1.26627950172357, 'learning_rate': 3.498715940247438e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3887/4398 [1:51:10<47:42,  5.60s/it]
 0:  88%| | 3888/4398 [1:51:16<47:33,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4838, 'grad_norm': 1.4077592049013556, 'learning_rate': 3.485196927206985e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3888/4398 [1:51:16<47:33,  5.59s/it]
 0:  88%| | 3889/4398 [1:51:21<47:44,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4483, 'grad_norm': 1.176405580374664, 'learning_rate': 3.471703140214094e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3889/4398 [1:51:21<47:44,  5.63s/it]
 0:  88%| | 3890/4398 [1:51:27<47:31,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4188, 'grad_norm': 1.2134007805020814, 'learning_rate': 3.458234586586756e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3890/4398 [1:51:27<47:31,  5.61s/it]
 0:  88%| | 3891/4398 [1:51:33<47:21,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4347, 'grad_norm': 1.200485377970758, 'learning_rate': 3.4447912736292724e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3891/4398 [1:51:33<47:21,  5.61s/it]
 0:  88%| | 3892/4398 [1:51:38<47:11,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4263, 'grad_norm': 1.08700261561849, 'learning_rate': 3.431373208632266e-07, 'epoch': 0.88}
 0: 
 0:  88%| | 3892/4398 [1:51:38<47:11,  5.60s/it]
 0:  89%| | 3893/4398 [1:51:44<47:07,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.419, 'grad_norm': 0.957235353461744, 'learning_rate': 3.4179803988726557e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3893/4398 [1:51:44<47:07,  5.60s/it]
 0:  89%| | 3894/4398 [1:51:49<46:59,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4124, 'grad_norm': 1.0985856089519745, 'learning_rate': 3.404612851613676e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3894/4398 [1:51:49<46:59,  5.59s/it]
 0:  89%| | 3895/4398 [1:51:55<46:52,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4361, 'grad_norm': 1.1148972207211763, 'learning_rate': 3.3912705741048547e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3895/4398 [1:51:55<46:52,  5.59s/it]
 0:  89%| | 3896/4398 [1:52:00<46:45,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.445, 'grad_norm': 1.2106106755736095, 'learning_rate': 3.3779535735820103e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3896/4398 [1:52:00<46:45,  5.59s/it]
 0:  89%| | 3897/4398 [1:52:06<46:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3955, 'grad_norm': 1.1844268591596196, 'learning_rate': 3.364661857267265e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3897/4398 [1:52:06<46:45,  5.60s/it]
 0:  89%| | 3898/4398 [1:52:12<46:37,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4372, 'grad_norm': 1.0604826614582188, 'learning_rate': 3.351395432369009e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3898/4398 [1:52:12<46:37,  5.59s/it]
 0:  89%| | 3899/4398 [1:52:17<46:34,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4851, 'grad_norm': 1.175886078935856, 'learning_rate': 3.3381543060819533e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3899/4398 [1:52:17<46:34,  5.60s/it]
 0:  89%| | 3900/4398 [1:52:23<46:31,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4428, 'grad_norm': 1.296329000599405, 'learning_rate': 3.324938485587059e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3900/4398 [1:52:23<46:31,  5.60s/it]
 0:  89%| | 3901/4398 [1:52:28<46:24,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4306, 'grad_norm': 1.0131942322843361, 'learning_rate': 3.3117479780515495e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3901/4398 [1:52:28<46:24,  5.60s/it]
 0:  89%| | 3902/4398 [1:52:34<46:15,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4134, 'grad_norm': 1.0481195789521363, 'learning_rate': 3.298582790628957e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3902/4398 [1:52:34<46:15,  5.60s/it]
 0:  89%| | 3903/4398 [1:52:40<46:10,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4426, 'grad_norm': 1.1079239045168936, 'learning_rate': 3.285442930459065e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3903/4398 [1:52:40<46:10,  5.60s/it]
 0:  89%| | 3904/4398 [1:52:45<46:06,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4038, 'grad_norm': 1.060669011854628, 'learning_rate': 3.2723284046679184e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3904/4398 [1:52:45<46:06,  5.60s/it]
 0:  89%| | 3905/4398 [1:52:51<46:02,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4276, 'grad_norm': 1.1665655508865007, 'learning_rate': 3.259239220367827e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3905/4398 [1:52:51<46:02,  5.60s/it]
 0:  89%| | 3906/4398 [1:52:56<45:53,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4573, 'grad_norm': 1.071108034455215, 'learning_rate': 3.2461753846573463e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3906/4398 [1:52:56<45:53,  5.60s/it]
 0:  89%| | 3907/4398 [1:53:02<45:51,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4212, 'grad_norm': 1.0664569907878965, 'learning_rate': 3.233136904621309e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3907/4398 [1:53:02<45:51,  5.60s/it]
 0:  89%| | 3908/4398 [1:53:08<45:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4488, 'grad_norm': 1.273309486343306, 'learning_rate': 3.2201237873307865e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3908/4398 [1:53:08<45:45,  5.60s/it]
 0:  89%| | 3909/4398 [1:53:13<45:39,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4174, 'grad_norm': 1.1960533139364564, 'learning_rate': 3.2071360398430783e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3909/4398 [1:53:13<45:39,  5.60s/it]
 0:  89%| | 3910/4398 [1:53:19<45:32,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.396, 'grad_norm': 1.0166964994682348, 'learning_rate': 3.19417366920175e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3910/4398 [1:53:19<45:32,  5.60s/it]
 0:  89%| | 3911/4398 [1:53:24<45:29,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4642, 'grad_norm': 1.037250202827856, 'learning_rate': 3.1812366824365835e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3911/4398 [1:53:24<45:29,  5.60s/it]
 0:  89%| | 3912/4398 [1:53:30<45:24,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4602, 'grad_norm': 1.0313084055719337, 'learning_rate': 3.168325086563612e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3912/4398 [1:53:30<45:24,  5.61s/it]
 0:  89%| | 3913/4398 [1:53:38<50:10,  6.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4213, 'grad_norm': 1.058329008254423, 'learning_rate': 3.155438888585083e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3913/4398 [1:53:38<50:10,  6.21s/it]
 0:  89%| | 3914/4398 [1:53:53<1:12:32,  8.99s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4154, 'grad_norm': 1.0044610953195552, 'learning_rate': 3.142578095489479e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3914/4398 [1:53:53<1:12:32,  8.99s/it]
 0:  89%| | 3915/4398 [1:55:41<5:11:40, 38.72s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4375, 'grad_norm': 1.0013925783934245, 'learning_rate': 3.1297427142515105e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3915/4398 [1:55:41<5:11:40, 38.72s/it]
 0:  89%| | 3916/4398 [1:56:13<4:55:21, 36.77s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4178, 'grad_norm': 1.0592515419223367, 'learning_rate': 3.116932751832097e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3916/4398 [1:56:13<4:55:21, 36.77s/it]
 0:  89%| | 3917/4398 [1:56:19<3:39:47, 27.42s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4562, 'grad_norm': 1.106581902278964, 'learning_rate': 3.104148215178371e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3917/4398 [1:56:19<3:39:47, 27.42s/it]
 0:  89%| | 3918/4398 [1:56:25<2:46:55, 20.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4771, 'grad_norm': 1.067561935258208, 'learning_rate': 3.0913891112236915e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3918/4398 [1:56:25<2:46:55, 20.87s/it]
 0:  89%| | 3919/4398 [1:56:30<2:10:01, 16.29s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4317, 'grad_norm': 1.0466614251990718, 'learning_rate': 3.078655446887602e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3919/4398 [1:56:30<2:10:01, 16.29s/it]
 0:  89%| | 3920/4398 [1:56:36<1:44:08, 13.07s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4602, 'grad_norm': 1.1691129425575402, 'learning_rate': 3.065947229075872e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3920/4398 [1:56:36<1:44:08, 13.07s/it]
 0:  89%| | 3921/4398 [1:56:41<1:26:06, 10.83s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4338, 'grad_norm': 1.1659722879543022, 'learning_rate': 3.0532644646804555e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3921/4398 [1:56:41<1:26:06, 10.83s/it]
 0:  89%| | 3922/4398 [1:56:47<1:13:23,  9.25s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4191, 'grad_norm': 1.0671768419745373, 'learning_rate': 3.0406071605795006e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3922/4398 [1:56:47<1:13:23,  9.25s/it]
 0:  89%| | 3923/4398 [1:56:53<1:04:33,  8.15s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4495, 'grad_norm': 1.1924539186551721, 'learning_rate': 3.027975323637367e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3923/4398 [1:56:53<1:04:33,  8.15s/it]
 0:  89%| | 3924/4398 [1:56:58<58:17,  7.38s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4162, 'grad_norm': 1.218801509356042, 'learning_rate': 3.015368960704584e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3924/4398 [1:56:58<58:17,  7.38s/it]
 0:  89%| | 3925/4398 [1:57:04<53:59,  6.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4701, 'grad_norm': 1.2006803227501732, 'learning_rate': 3.002788078617874e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3925/4398 [1:57:04<53:59,  6.85s/it]
 0:  89%| | 3926/4398 [1:57:09<50:51,  6.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4258, 'grad_norm': 1.1959695892528681, 'learning_rate': 2.990232684200134e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3926/4398 [1:57:09<50:51,  6.47s/it]
 0:  89%| | 3927/4398 [1:57:15<48:42,  6.21s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4306, 'grad_norm': 0.9966482949348029, 'learning_rate': 2.9777027842604444e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3927/4398 [1:57:15<48:42,  6.21s/it]
 0:  89%| | 3928/4398 [1:57:21<47:08,  6.02s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3933, 'grad_norm': 1.0129488375358326, 'learning_rate': 2.9651983855940645e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3928/4398 [1:57:21<47:08,  6.02s/it]
 0:  89%| | 3929/4398 [1:57:26<46:05,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4553, 'grad_norm': 1.1012932623807572, 'learning_rate': 2.9527194949824145e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3929/4398 [1:57:26<46:05,  5.90s/it]
 0:  89%| | 3930/4398 [1:57:32<45:28,  5.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.399, 'grad_norm': 1.0809939001497884, 'learning_rate': 2.9402661191930803e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3930/4398 [1:57:32<45:28,  5.83s/it]
 0:  89%| | 3931/4398 [1:57:38<45:04,  5.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4326, 'grad_norm': 1.0879026887324135, 'learning_rate': 2.9278382649798154e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3931/4398 [1:57:38<45:04,  5.79s/it]
 0:  89%| | 3932/4398 [1:57:43<44:28,  5.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4606, 'grad_norm': 1.1009059591153951, 'learning_rate': 2.9154359390825224e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3932/4398 [1:57:43<44:28,  5.73s/it]
 0:  89%| | 3933/4398 [1:57:49<44:05,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4244, 'grad_norm': 1.058008051890708, 'learning_rate': 2.9030591482272883e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3933/4398 [1:57:49<44:05,  5.69s/it]
 0:  89%| | 3934/4398 [1:57:54<43:43,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4238, 'grad_norm': 1.00817179019186, 'learning_rate': 2.8907078991263214e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3934/4398 [1:57:54<43:43,  5.65s/it]
 0:  89%| | 3935/4398 [1:58:00<43:32,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4246, 'grad_norm': 1.0217808478937933, 'learning_rate': 2.8783821984779804e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3935/4398 [1:58:00<43:32,  5.64s/it]
 0:  89%| | 3936/4398 [1:58:05<43:20,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3965, 'grad_norm': 1.0991810007093237, 'learning_rate': 2.8660820529667953e-07, 'epoch': 0.89}
 0: 
 0:  89%| | 3936/4398 [1:58:05<43:20,  5.63s/it]
 0:  90%| | 3937/4398 [1:58:11<43:12,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3799, 'grad_norm': 1.0378133616699454, 'learning_rate': 2.853807469263398e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3937/4398 [1:58:11<43:12,  5.62s/it]
 0:  90%| | 3938/4398 [1:58:17<43:01,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3963, 'grad_norm': 1.1337345755898618, 'learning_rate': 2.8415584540245843e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3938/4398 [1:58:17<43:01,  5.61s/it]
 0:  90%| | 3939/4398 [1:58:22<42:55,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4322, 'grad_norm': 1.0139586759712214, 'learning_rate': 2.8293350138932805e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3939/4398 [1:58:22<42:55,  5.61s/it]
 0:  90%| | 3940/4398 [1:58:28<42:44,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4689, 'grad_norm': 1.1417090948332562, 'learning_rate': 2.8171371554985226e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3940/4398 [1:58:28<42:44,  5.60s/it]
 0:  90%| | 3941/4398 [1:58:33<42:38,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4071, 'grad_norm': 1.1150161830792016, 'learning_rate': 2.8049648854555187e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3941/4398 [1:58:33<42:38,  5.60s/it]
 0:  90%| | 3942/4398 [1:58:39<42:31,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4668, 'grad_norm': 1.0500913493544526, 'learning_rate': 2.79281821036555e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3942/4398 [1:58:39<42:31,  5.60s/it]
 0:  90%| | 3943/4398 [1:58:45<42:24,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4215, 'grad_norm': 1.0226432236847731, 'learning_rate': 2.780697136816046e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3943/4398 [1:58:45<42:24,  5.59s/it]
 0:  90%| | 3944/4398 [1:58:50<42:19,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4409, 'grad_norm': 1.0817572825344623, 'learning_rate': 2.7686016713805444e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3944/4398 [1:58:50<42:19,  5.59s/it]
 0:  90%| | 3945/4398 [1:58:56<42:14,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4223, 'grad_norm': 1.017913469923899, 'learning_rate': 2.756531820618685e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3945/4398 [1:58:56<42:14,  5.59s/it]
 0:  90%| | 3946/4398 [1:59:01<42:08,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4236, 'grad_norm': 2.465331329245636, 'learning_rate': 2.7444875910762334e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3946/4398 [1:59:01<42:08,  5.59s/it]
 0:  90%| | 3947/4398 [1:59:07<42:04,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3923, 'grad_norm': 1.1455854421718965, 'learning_rate': 2.732468989285064e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3947/4398 [1:59:07<42:04,  5.60s/it]
 0:  90%| | 3948/4398 [1:59:13<41:56,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.1290430240097744, 'learning_rate': 2.7204760217631074e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3948/4398 [1:59:13<41:56,  5.59s/it]
 0:  90%| | 3949/4398 [1:59:18<42:04,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4169, 'grad_norm': 1.0225252830549598, 'learning_rate': 2.7085086950144504e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3949/4398 [1:59:18<42:04,  5.62s/it]
 0:  90%| | 3950/4398 [1:59:24<41:50,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4221, 'grad_norm': 1.1459235471240832, 'learning_rate': 2.69656701552925e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3950/4398 [1:59:24<41:50,  5.60s/it]
 0:  90%| | 3951/4398 [1:59:29<41:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4588, 'grad_norm': 1.3124179613664562, 'learning_rate': 2.684650989783749e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3951/4398 [1:59:29<41:42,  5.60s/it]
 0:  90%| | 3952/4398 [1:59:35<41:34,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4223, 'grad_norm': 1.0563497891667528, 'learning_rate': 2.672760624240278e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3952/4398 [1:59:35<41:34,  5.59s/it]
 0:  90%| | 3953/4398 [1:59:41<41:33,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4078, 'grad_norm': 1.197237794096527, 'learning_rate': 2.660895925347262e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3953/4398 [1:59:41<41:33,  5.60s/it]
 0:  90%| | 3954/4398 [1:59:46<41:27,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4452, 'grad_norm': 0.9955242019678048, 'learning_rate': 2.6490568995391986e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3954/4398 [1:59:46<41:27,  5.60s/it]
 0:  90%| | 3955/4398 [1:59:52<41:21,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4317, 'grad_norm': 1.0654798653845066, 'learning_rate': 2.637243553236662e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3955/4398 [1:59:52<41:21,  5.60s/it]
 0:  90%| | 3956/4398 [1:59:57<41:14,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4319, 'grad_norm': 0.9913309293607774, 'learning_rate': 2.6254558928463014e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3956/4398 [1:59:57<41:14,  5.60s/it]
 0:  90%| | 3957/4398 [2:00:03<41:10,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4525, 'grad_norm': 1.070958067980879, 'learning_rate': 2.613693924760835e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3957/4398 [2:00:03<41:10,  5.60s/it]
 0:  90%| | 3958/4398 [2:00:09<41:03,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.43, 'grad_norm': 1.2004965953100841, 'learning_rate': 2.60195765535905e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3958/4398 [2:00:09<41:03,  5.60s/it]
 0:  90%| | 3959/4398 [2:00:14<41:01,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4301, 'grad_norm': 1.0584639345318771, 'learning_rate': 2.5902470910058043e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3959/4398 [2:00:14<41:01,  5.61s/it]
 0:  90%| | 3960/4398 [2:00:20<40:55,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4363, 'grad_norm': 1.110675400326525, 'learning_rate': 2.578562238051996e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3960/4398 [2:00:20<40:55,  5.61s/it]
 0:  90%| | 3961/4398 [2:00:25<40:51,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4404, 'grad_norm': 1.0330172601394985, 'learning_rate': 2.566903102834589e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3961/4398 [2:00:25<40:51,  5.61s/it]
 0:  90%| | 3962/4398 [2:00:31<40:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4512, 'grad_norm': 1.2148203678715215, 'learning_rate': 2.555269691676604e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3962/4398 [2:00:31<40:42,  5.60s/it]
 0:  90%| | 3963/4398 [2:00:37<40:36,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4058, 'grad_norm': 1.1580028487620626, 'learning_rate': 2.5436620108871045e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3963/4398 [2:00:37<40:36,  5.60s/it]
 0:  90%| | 3964/4398 [2:00:42<40:27,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4203, 'grad_norm': 1.1172321927531228, 'learning_rate': 2.532080066761211e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3964/4398 [2:00:42<40:27,  5.59s/it]
 0:  90%| | 3965/4398 [2:00:48<40:23,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4072, 'grad_norm': 1.050595501241167, 'learning_rate': 2.520523865580066e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3965/4398 [2:00:48<40:23,  5.60s/it]
 0:  90%| | 3966/4398 [2:00:53<40:14,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4085, 'grad_norm': 1.1406514961472236, 'learning_rate': 2.5089934136108665e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3966/4398 [2:00:53<40:14,  5.59s/it]
 0:  90%| | 3967/4398 [2:00:59<40:12,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4048, 'grad_norm': 1.0830659829175286, 'learning_rate': 2.4974887171068495e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3967/4398 [2:00:59<40:12,  5.60s/it]
 0:  90%| | 3968/4398 [2:01:05<40:04,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4381, 'grad_norm': 1.572363846492489, 'learning_rate': 2.4860097823072695e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3968/4398 [2:01:05<40:04,  5.59s/it]
 0:  90%| | 3969/4398 [2:01:10<40:00,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4595, 'grad_norm': 1.158942094967842, 'learning_rate': 2.474556615437423e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3969/4398 [2:01:10<40:00,  5.59s/it]
 0:  90%| | 3970/4398 [2:01:16<39:54,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4195, 'grad_norm': 1.1140133608412897, 'learning_rate': 2.4631292227086213e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3970/4398 [2:01:16<39:54,  5.59s/it]
 0:  90%| | 3971/4398 [2:01:21<39:48,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4345, 'grad_norm': 1.1400557357626888, 'learning_rate': 2.451727610318205e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3971/4398 [2:01:21<39:48,  5.59s/it]
 0:  90%| | 3972/4398 [2:01:27<39:38,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.1321403059989474, 'learning_rate': 2.440351784449524e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3972/4398 [2:01:27<39:38,  5.58s/it]
 0:  90%| | 3973/4398 [2:01:33<39:33,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4314, 'grad_norm': 1.0950840482737072, 'learning_rate': 2.429001751271959e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3973/4398 [2:01:33<39:33,  5.58s/it]
 0:  90%| | 3974/4398 [2:01:38<39:25,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3965, 'grad_norm': 1.0433251602855236, 'learning_rate': 2.4176775169408804e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3974/4398 [2:01:38<39:25,  5.58s/it]
 0:  90%| | 3975/4398 [2:01:44<39:26,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4641, 'grad_norm': 1.0958758702243567, 'learning_rate': 2.4063790875976986e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3975/4398 [2:01:44<39:26,  5.59s/it]
 0:  90%| | 3976/4398 [2:01:49<39:15,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4099, 'grad_norm': 1.1189307031456752, 'learning_rate': 2.3951064693698014e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3976/4398 [2:01:49<39:15,  5.58s/it]
 0:  90%| | 3977/4398 [2:01:55<39:10,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4313, 'grad_norm': 1.1211494319465463, 'learning_rate': 2.3838596683705917e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3977/4398 [2:01:55<39:10,  5.58s/it]
 0:  90%| | 3978/4398 [2:02:00<39:06,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4035, 'grad_norm': 0.980478432782356, 'learning_rate': 2.3726386906994692e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3978/4398 [2:02:00<39:06,  5.59s/it]
 0:  90%| | 3979/4398 [2:02:06<39:00,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4302, 'grad_norm': 1.0915214650635041, 'learning_rate': 2.3614435424418214e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3979/4398 [2:02:06<39:00,  5.59s/it]
 0:  90%| | 3980/4398 [2:02:12<38:55,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3771, 'grad_norm': 0.912174050068075, 'learning_rate': 2.3502742296690473e-07, 'epoch': 0.9}
 0: 
 0:  90%| | 3980/4398 [2:02:12<38:55,  5.59s/it]
 0:  91%| | 3981/4398 [2:02:19<43:00,  6.19s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4613, 'grad_norm': 1.1818461506044615, 'learning_rate': 2.339130758438507e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3981/4398 [2:02:19<43:00,  6.19s/it]
 0:  91%| | 3982/4398 [2:02:35<1:02:20,  8.99s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4203, 'grad_norm': 1.2695783305277322, 'learning_rate': 2.3280131347935775e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3982/4398 [2:02:35<1:02:20,  8.99s/it]
 0:  91%| | 3983/4398 [2:04:09<3:59:34, 34.64s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4424, 'grad_norm': 1.0505214081241907, 'learning_rate': 2.3169213647635913e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3983/4398 [2:04:09<3:59:34, 34.64s/it]
 0:  91%| | 3984/4398 [2:05:05<4:41:43, 40.83s/it]
 0:                                                        
 0: 
 0: {'loss': 0.441, 'grad_norm': 1.1287286898652213, 'learning_rate': 2.30585545436387e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3984/4398 [2:05:05<4:41:43, 40.83s/it]
 0:  91%| | 3985/4398 [2:05:10<3:28:18, 30.26s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4167, 'grad_norm': 1.0457752640316607, 'learning_rate': 2.2948154095957132e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3985/4398 [2:05:10<3:28:18, 30.26s/it]
 0:  91%| | 3986/4398 [2:05:16<2:37:00, 22.87s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4102, 'grad_norm': 1.0536160791844051, 'learning_rate': 2.2838012364463925e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3986/4398 [2:05:16<2:37:00, 22.87s/it]
 0:  91%| | 3987/4398 [2:05:21<2:01:12, 17.69s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4604, 'grad_norm': 1.181773424178258, 'learning_rate': 2.2728129408891463e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3987/4398 [2:05:21<2:01:12, 17.69s/it]
 0:  91%| | 3988/4398 [2:05:27<1:36:16, 14.09s/it]
 0:                                                        
 0: 
 0: {'loss': 0.463, 'grad_norm': 1.3233480381179141, 'learning_rate': 2.2618505288831803e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3988/4398 [2:05:27<1:36:16, 14.09s/it]
 0:  91%| | 3989/4398 [2:05:33<1:18:46, 11.56s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4445, 'grad_norm': 1.1123045084317487, 'learning_rate': 2.2509140063736668e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3989/4398 [2:05:33<1:18:46, 11.56s/it]
 0:  91%| | 3990/4398 [2:05:38<1:06:24,  9.77s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4327, 'grad_norm': 1.0938102400316545, 'learning_rate': 2.2400033792917275e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3990/4398 [2:05:38<1:06:24,  9.77s/it]
 0:  91%| | 3991/4398 [2:05:44<57:49,  8.53s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4169, 'grad_norm': 1.1180467165312196, 'learning_rate': 2.2291186535544464e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3991/4398 [2:05:44<57:49,  8.53s/it]
 0:  91%| | 3992/4398 [2:05:50<51:45,  7.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4448, 'grad_norm': 1.027841432008783, 'learning_rate': 2.2182598350648566e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3992/4398 [2:05:50<51:45,  7.65s/it]
 0:  91%| | 3993/4398 [2:05:55<47:30,  7.04s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4093, 'grad_norm': 1.0825611439354061, 'learning_rate': 2.2074269297119588e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3993/4398 [2:05:55<47:30,  7.04s/it]
 0:  91%| | 3994/4398 [2:06:01<44:31,  6.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4185, 'grad_norm': 1.1190492856711722, 'learning_rate': 2.1966199433706814e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3994/4398 [2:06:01<44:31,  6.61s/it]
 0:  91%| | 3995/4398 [2:06:06<42:27,  6.32s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4023, 'grad_norm': 1.1518368920692255, 'learning_rate': 2.1858388819018972e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3995/4398 [2:06:06<42:27,  6.32s/it]
 0:  91%| | 3996/4398 [2:06:12<40:53,  6.10s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4184, 'grad_norm': 1.0615405726990106, 'learning_rate': 2.1750837511524235e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3996/4398 [2:06:12<40:53,  6.10s/it]
 0:  91%| | 3997/4398 [2:06:18<39:49,  5.96s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4357, 'grad_norm': 1.0450542156392186, 'learning_rate': 2.1643545569550172e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3997/4398 [2:06:18<39:49,  5.96s/it]
 0:  91%| | 3998/4398 [2:06:23<39:03,  5.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4521, 'grad_norm': 1.0664579091034223, 'learning_rate': 2.153651305128368e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3998/4398 [2:06:23<39:03,  5.86s/it]
 0:  91%| | 3999/4398 [2:06:29<38:31,  5.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4473, 'grad_norm': 1.3286758219143038, 'learning_rate': 2.1429740014770994e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 3999/4398 [2:06:29<38:31,  5.79s/it]
 0:  91%| | 4000/4398 [2:06:34<38:04,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4363, 'grad_norm': 1.0794613376549804, 'learning_rate': 2.1323226517917407e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4000/4398 [2:06:35<38:04,  5.74s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  91%| | 4001/4398 [2:07:28<2:12:16, 19.99s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4168, 'grad_norm': 2.571724171019494, 'learning_rate': 2.1216972618487874e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4001/4398 [2:07:28<2:12:16, 19.99s/it]
 0:  91%| | 4002/4398 [2:07:33<1:43:28, 15.68s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4475, 'grad_norm': 1.110230107471904, 'learning_rate': 2.1110978374106195e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4002/4398 [2:07:33<1:43:28, 15.68s/it]
 0:  91%| | 4003/4398 [2:07:39<1:23:20, 12.66s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4023, 'grad_norm': 1.3639400404961841, 'learning_rate': 2.1005243842255552e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4003/4398 [2:07:39<1:23:20, 12.66s/it]
 0:  91%| | 4004/4398 [2:07:45<1:09:15, 10.55s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3923, 'grad_norm': 1.0353258943539052, 'learning_rate': 2.0899769080278187e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4004/4398 [2:07:45<1:09:15, 10.55s/it]
 0:  91%| | 4005/4398 [2:07:50<59:34,  9.09s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4532, 'grad_norm': 1.0513226520349692, 'learning_rate': 2.0794554145375457e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4005/4398 [2:07:50<59:34,  9.09s/it]
 0:  91%| | 4006/4398 [2:07:56<52:33,  8.04s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4134, 'grad_norm': 1.1490042302577237, 'learning_rate': 2.0689599094607936e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4006/4398 [2:07:56<52:33,  8.04s/it]
 0:  91%| | 4007/4398 [2:08:02<47:44,  7.33s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4687, 'grad_norm': 1.0826032946119533, 'learning_rate': 2.0584903984895043e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4007/4398 [2:08:02<47:44,  7.33s/it]
 0:  91%| | 4008/4398 [2:08:07<44:17,  6.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4612, 'grad_norm': 1.0354971645574726, 'learning_rate': 2.0480468873015303e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4008/4398 [2:08:07<44:17,  6.82s/it]
 0:  91%| | 4009/4398 [2:08:13<41:51,  6.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 1.2499170811940048, 'learning_rate': 2.0376293815606464e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4009/4398 [2:08:13<41:51,  6.46s/it]
 0:  91%| | 4010/4398 [2:08:18<40:05,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4292, 'grad_norm': 0.9939500560053683, 'learning_rate': 2.0272378869164945e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4010/4398 [2:08:18<40:05,  6.20s/it]
 0:  91%| | 4011/4398 [2:08:24<38:53,  6.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4386, 'grad_norm': 1.5587828222530264, 'learning_rate': 2.0168724090046222e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4011/4398 [2:08:24<38:53,  6.03s/it]
 0:  91%| | 4012/4398 [2:08:30<38:00,  5.91s/it]
 0:                                                      
 0: 
 0: {'loss': 0.448, 'grad_norm': 1.0399205745917932, 'learning_rate': 2.0065329534464717e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4012/4398 [2:08:30<38:00,  5.91s/it]
 0:  91%| | 4013/4398 [2:08:35<37:21,  5.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4309, 'grad_norm': 1.0734639795383827, 'learning_rate': 1.9962195258493578e-07, 'epoch': 0.91}
 0: 
 0:  91%| | 4013/4398 [2:08:35<37:21,  5.82s/it]
 0:  91%|| 4014/4398 [2:08:41<36:51,  5.76s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4299, 'grad_norm': 1.0321031362428796, 'learning_rate': 1.985932131806495e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4014/4398 [2:08:41<36:51,  5.76s/it]
 0:  91%|| 4015/4398 [2:08:47<36:33,  5.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4587, 'grad_norm': 1.1543025626832035, 'learning_rate': 1.9756707768969764e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4015/4398 [2:08:47<36:33,  5.73s/it]
 0:  91%|| 4016/4398 [2:08:52<36:13,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4428, 'grad_norm': 1.0973434858836315, 'learning_rate': 1.9654354666857667e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4016/4398 [2:08:52<36:13,  5.69s/it]
 0:  91%|| 4017/4398 [2:08:58<36:02,  5.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4245, 'grad_norm': 1.0318902257354634, 'learning_rate': 1.9552262067237094e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4017/4398 [2:08:58<36:02,  5.67s/it]
 0:  91%|| 4018/4398 [2:09:03<35:59,  5.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4428, 'grad_norm': 1.0651198396147608, 'learning_rate': 1.9450430025475253e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4018/4398 [2:09:03<35:59,  5.68s/it]
 0:  91%|| 4019/4398 [2:09:09<35:51,  5.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4092, 'grad_norm': 1.0687903699267887, 'learning_rate': 1.934885859679797e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4019/4398 [2:09:09<35:51,  5.68s/it]
 0:  91%|| 4020/4398 [2:09:15<35:36,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.447, 'grad_norm': 1.3024348240714205, 'learning_rate': 1.9247547836289792e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4020/4398 [2:09:15<35:36,  5.65s/it]
 0:  91%|| 4021/4398 [2:09:20<35:29,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.461, 'grad_norm': 1.1205408276894204, 'learning_rate': 1.9146497798893825e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4021/4398 [2:09:20<35:29,  5.65s/it]
 0:  91%|| 4022/4398 [2:09:26<35:17,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4181, 'grad_norm': 1.1977532981896521, 'learning_rate': 1.90457085394119e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4022/4398 [2:09:26<35:17,  5.63s/it]
 0:  91%|| 4023/4398 [2:09:32<35:14,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3977, 'grad_norm': 1.0063218066843114, 'learning_rate': 1.894518011250429e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4023/4398 [2:09:32<35:14,  5.64s/it]
 0:  91%|| 4024/4398 [2:09:37<35:05,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4263, 'grad_norm': 0.9994795796497846, 'learning_rate': 1.8844912572689887e-07, 'epoch': 0.91}
 0: 
 0:  91%|| 4024/4398 [2:09:37<35:05,  5.63s/it]
 0:  92%|| 4025/4398 [2:09:43<35:10,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4627, 'grad_norm': 1.101553365283445, 'learning_rate': 1.8744905974346083e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4025/4398 [2:09:43<35:10,  5.66s/it]
 0:  92%|| 4026/4398 [2:09:49<34:56,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4052, 'grad_norm': 1.0685547555963402, 'learning_rate': 1.8645160371708716e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4026/4398 [2:09:49<34:56,  5.64s/it]
 0:  92%|| 4027/4398 [2:09:54<34:50,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4224, 'grad_norm': 1.1467265684177017, 'learning_rate': 1.8545675818872234e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4027/4398 [2:09:54<34:50,  5.63s/it]
 0:  92%|| 4028/4398 [2:10:00<34:43,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4283, 'grad_norm': 1.094668782296532, 'learning_rate': 1.844645236978937e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4028/4398 [2:10:00<34:43,  5.63s/it]
 0:  92%|| 4029/4398 [2:10:05<34:36,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4498, 'grad_norm': 1.0913469883785014, 'learning_rate': 1.8347490078271247e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4029/4398 [2:10:05<34:36,  5.63s/it]
 0:  92%|| 4030/4398 [2:10:11<34:29,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4606, 'grad_norm': 1.0365597954637427, 'learning_rate': 1.8248788997987377e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4030/4398 [2:10:11<34:29,  5.62s/it]
 0:  92%|| 4031/4398 [2:10:17<34:25,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4093, 'grad_norm': 1.1477780137632783, 'learning_rate': 1.815034918246561e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4031/4398 [2:10:17<34:25,  5.63s/it]
 0:  92%|| 4032/4398 [2:10:22<34:18,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4768, 'grad_norm': 1.1123072400393565, 'learning_rate': 1.805217068509213e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4032/4398 [2:10:22<34:18,  5.62s/it]
 0:  92%|| 4033/4398 [2:10:28<34:12,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4268, 'grad_norm': 1.101661687039969, 'learning_rate': 1.7954253559111456e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4033/4398 [2:10:28<34:12,  5.62s/it]
 0:  92%|| 4034/4398 [2:10:33<34:04,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4279, 'grad_norm': 1.0946324985913334, 'learning_rate': 1.7856597857626167e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4034/4398 [2:10:34<34:04,  5.62s/it]
 0:  92%|| 4035/4398 [2:10:39<33:58,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4288, 'grad_norm': 1.0606472008365486, 'learning_rate': 1.77592036335974e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4035/4398 [2:10:39<33:58,  5.62s/it]
 0:  92%|| 4036/4398 [2:10:45<33:56,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4542, 'grad_norm': 1.1465611653631975, 'learning_rate': 1.7662070939844123e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4036/4398 [2:10:45<33:56,  5.62s/it]
 0:  92%|| 4037/4398 [2:10:50<33:52,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4262, 'grad_norm': 1.3343635232226272, 'learning_rate': 1.7565199829043644e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4037/4398 [2:10:50<33:52,  5.63s/it]
 0:  92%|| 4038/4398 [2:10:56<33:45,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4619, 'grad_norm': 1.4356990093123738, 'learning_rate': 1.7468590353731495e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4038/4398 [2:10:56<33:45,  5.63s/it]
 0:  92%|| 4039/4398 [2:11:02<33:43,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 0.97147300878872, 'learning_rate': 1.73722425663011e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4039/4398 [2:11:02<33:43,  5.64s/it]
 0:  92%|| 4040/4398 [2:11:07<33:32,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4566, 'grad_norm': 1.1454008944254919, 'learning_rate': 1.7276156519004161e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4040/4398 [2:11:07<33:32,  5.62s/it]
 0:  92%|| 4041/4398 [2:11:13<33:29,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4354, 'grad_norm': 0.9935630989418944, 'learning_rate': 1.7180332263950384e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4041/4398 [2:11:13<33:29,  5.63s/it]
 0:  92%|| 4042/4398 [2:11:19<33:34,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4381, 'grad_norm': 1.0754549607553907, 'learning_rate': 1.7084769853107252e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4042/4398 [2:11:19<33:34,  5.66s/it]
 0:  92%|| 4043/4398 [2:11:24<33:24,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4261, 'grad_norm': 1.0348207264680889, 'learning_rate': 1.6989469338300702e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4043/4398 [2:11:24<33:24,  5.65s/it]
 0:  92%|| 4044/4398 [2:11:30<33:14,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4547, 'grad_norm': 1.0139100025417649, 'learning_rate': 1.6894430771214277e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4044/4398 [2:11:30<33:14,  5.63s/it]
 0:  92%|| 4045/4398 [2:11:36<33:12,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.427, 'grad_norm': 1.0320826227361977, 'learning_rate': 1.6799654203389582e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4045/4398 [2:11:36<33:12,  5.64s/it]
 0:  92%|| 4046/4398 [2:11:41<33:04,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 1.6588258991296154, 'learning_rate': 1.6705139686226224e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4046/4398 [2:11:41<33:04,  5.64s/it]
 0:  92%|| 4047/4398 [2:11:47<32:56,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4225, 'grad_norm': 1.1199652186868319, 'learning_rate': 1.6610887270981425e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4047/4398 [2:11:47<32:56,  5.63s/it]
 0:  92%|| 4048/4398 [2:11:54<36:20,  6.23s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4301, 'grad_norm': 1.0349357429808554, 'learning_rate': 1.6516897008770628e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4048/4398 [2:11:54<36:20,  6.23s/it]
 0:  92%|| 4049/4398 [2:12:00<35:11,  6.05s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4357, 'grad_norm': 1.1399038867522524, 'learning_rate': 1.642316895056678e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4049/4398 [2:12:00<35:11,  6.05s/it]
 0:  92%|| 4050/4398 [2:12:14<48:13,  8.31s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4312, 'grad_norm': 1.1350724706849782, 'learning_rate': 1.6329703147200783e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4050/4398 [2:12:14<48:13,  8.31s/it]
 0:  92%|| 4051/4398 [2:13:54<3:28:19, 36.02s/it]
 0:                                                        
 0: 
 0: {'loss': 0.449, 'grad_norm': 1.5841802904087978, 'learning_rate': 1.6236499649361358e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4051/4398 [2:13:54<3:28:19, 36.02s/it]
 0:  92%|| 4052/4398 [2:14:48<3:59:02, 41.45s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4439, 'grad_norm': 1.0128688251555633, 'learning_rate': 1.6143558507594804e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4052/4398 [2:14:48<3:59:02, 41.45s/it]
 0:  92%|| 4053/4398 [2:14:54<2:56:30, 30.70s/it]
 0:                                                        
 0: 
 0: {'loss': 0.3703, 'grad_norm': 1.0148291426048894, 'learning_rate': 1.6050879772305406e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4053/4398 [2:14:54<2:56:30, 30.70s/it]
 0:  92%|| 4054/4398 [2:15:00<2:12:49, 23.17s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4287, 'grad_norm': 1.2277405679329083, 'learning_rate': 1.5958463493754795e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4054/4398 [2:15:00<2:12:49, 23.17s/it]
 0:  92%|| 4055/4398 [2:15:05<1:42:18, 17.90s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4434, 'grad_norm': 1.1626419101175247, 'learning_rate': 1.586630972206249e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4055/4398 [2:15:05<1:42:18, 17.90s/it]
 0:  92%|| 4056/4398 [2:15:11<1:20:56, 14.20s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4299, 'grad_norm': 1.1153271566465497, 'learning_rate': 1.577441850720568e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4056/4398 [2:15:11<1:20:56, 14.20s/it]
 0:  92%|| 4057/4398 [2:15:16<1:06:01, 11.62s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4228, 'grad_norm': 1.0258349001716192, 'learning_rate': 1.5682789899019002e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4057/4398 [2:15:16<1:06:01, 11.62s/it]
 0:  92%|| 4058/4398 [2:15:22<55:32,  9.80s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4744, 'grad_norm': 1.1031670246092202, 'learning_rate': 1.5591423947194817e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4058/4398 [2:15:22<55:32,  9.80s/it]
 0:  92%|| 4059/4398 [2:15:28<48:15,  8.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4442, 'grad_norm': 1.1820087687255962, 'learning_rate': 1.5500320701282933e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4059/4398 [2:15:28<48:15,  8.54s/it]
 0:  92%|| 4060/4398 [2:15:33<43:06,  7.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4652, 'grad_norm': 1.1234783282842142, 'learning_rate': 1.5409480210690665e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4060/4398 [2:15:33<43:06,  7.65s/it]
 0:  92%|| 4061/4398 [2:15:39<39:32,  7.04s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4483, 'grad_norm': 1.3311761696339537, 'learning_rate': 1.5318902524683098e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4061/4398 [2:15:39<39:32,  7.04s/it]
 0:  92%|| 4062/4398 [2:15:44<36:58,  6.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4688, 'grad_norm': 1.06433022505961, 'learning_rate': 1.5228587692382447e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4062/4398 [2:15:44<36:58,  6.60s/it]
 0:  92%|| 4063/4398 [2:15:50<35:10,  6.30s/it]
 0:                                                      
 0: 
 0: {'loss': 0.414, 'grad_norm': 1.031329513237976, 'learning_rate': 1.513853576276858e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4063/4398 [2:15:50<35:10,  6.30s/it]
 0:  92%|| 4064/4398 [2:15:55<33:50,  6.08s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4409, 'grad_norm': 1.120974885141224, 'learning_rate': 1.504874678467877e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4064/4398 [2:15:55<33:50,  6.08s/it]
 0:  92%|| 4065/4398 [2:16:01<32:58,  5.94s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4082, 'grad_norm': 1.1358770667660418, 'learning_rate': 1.495922080680756e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4065/4398 [2:16:01<32:58,  5.94s/it]
 0:  92%|| 4066/4398 [2:16:07<32:17,  5.84s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4301, 'grad_norm': 1.145011691613053, 'learning_rate': 1.4869957877706898e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4066/4398 [2:16:07<32:17,  5.84s/it]
 0:  92%|| 4067/4398 [2:16:12<31:49,  5.77s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4075, 'grad_norm': 1.240901181024139, 'learning_rate': 1.4780958045786274e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4067/4398 [2:16:12<31:49,  5.77s/it]
 0:  92%|| 4068/4398 [2:16:18<31:24,  5.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4672, 'grad_norm': 1.0271332470578147, 'learning_rate': 1.4692221359312199e-07, 'epoch': 0.92}
 0: 
 0:  92%|| 4068/4398 [2:16:18<31:24,  5.71s/it]
 0:  93%|| 4069/4398 [2:16:23<31:10,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4329, 'grad_norm': 1.0661022441886454, 'learning_rate': 1.4603747866408724e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4069/4398 [2:16:24<31:10,  5.69s/it]
 0:  93%|| 4070/4398 [2:16:29<30:55,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4198, 'grad_norm': 1.0992594643338327, 'learning_rate': 1.451553761505703e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4070/4398 [2:16:29<30:55,  5.66s/it]
 0:  93%|| 4071/4398 [2:16:35<30:44,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4355, 'grad_norm': 1.0771548741876742, 'learning_rate': 1.4427590653095569e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4071/4398 [2:16:35<30:44,  5.64s/it]
 0:  93%|| 4072/4398 [2:16:40<30:31,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4135, 'grad_norm': 1.060106691215036, 'learning_rate': 1.4339907028219968e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4072/4398 [2:16:40<30:31,  5.62s/it]
 0:  93%|| 4073/4398 [2:16:46<30:26,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4121, 'grad_norm': 1.230840092067953, 'learning_rate': 1.4252486787983134e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4073/4398 [2:16:46<30:26,  5.62s/it]
 0:  93%|| 4074/4398 [2:16:51<30:19,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4168, 'grad_norm': 1.0592261976379826, 'learning_rate': 1.4165329979794972e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4074/4398 [2:16:51<30:19,  5.62s/it]
 0:  93%|| 4075/4398 [2:16:57<30:13,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4149, 'grad_norm': 1.051746833581427, 'learning_rate': 1.4078436650922734e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4075/4398 [2:16:57<30:13,  5.62s/it]
 0:  93%|| 4076/4398 [2:17:03<30:06,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4458, 'grad_norm': 1.025775184409672, 'learning_rate': 1.3991806848490662e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4076/4398 [2:17:03<30:06,  5.61s/it]
 0:  93%|| 4077/4398 [2:17:08<30:03,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4308, 'grad_norm': 1.017351748276954, 'learning_rate': 1.3905440619480015e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4077/4398 [2:17:08<30:03,  5.62s/it]
 0:  93%|| 4078/4398 [2:17:14<29:53,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4246, 'grad_norm': 1.0549363105950067, 'learning_rate': 1.381933801072921e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4078/4398 [2:17:14<29:53,  5.61s/it]
 0:  93%|| 4079/4398 [2:17:20<29:50,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4416, 'grad_norm': 1.04241785536187, 'learning_rate': 1.373349906893373e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4079/4398 [2:17:20<29:50,  5.61s/it]
 0:  93%|| 4080/4398 [2:17:25<29:44,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4073, 'grad_norm': 0.9849184338380922, 'learning_rate': 1.3647923840645948e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4080/4398 [2:17:25<29:44,  5.61s/it]
 0:  93%|| 4081/4398 [2:17:31<29:40,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4411, 'grad_norm': 1.07236283188453, 'learning_rate': 1.356261237227524e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4081/4398 [2:17:31<29:40,  5.62s/it]
 0:  93%|| 4082/4398 [2:17:36<29:31,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4181, 'grad_norm': 1.1245498548781319, 'learning_rate': 1.3477564710088097e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4082/4398 [2:17:36<29:31,  5.61s/it]
 0:  93%|| 4083/4398 [2:17:42<29:26,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.0313368745448863, 'learning_rate': 1.3392780900207735e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4083/4398 [2:17:42<29:26,  5.61s/it]
 0:  93%|| 4084/4398 [2:17:48<29:19,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3964, 'grad_norm': 1.0391820945729198, 'learning_rate': 1.3308260988614375e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4084/4398 [2:17:48<29:19,  5.60s/it]
 0:  93%|| 4085/4398 [2:17:53<29:17,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3962, 'grad_norm': 1.0553802642431833, 'learning_rate': 1.3224005021145182e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4085/4398 [2:17:53<29:17,  5.61s/it]
 0:  93%|| 4086/4398 [2:17:59<29:10,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4381, 'grad_norm': 1.1791264608958696, 'learning_rate': 1.3140013043493992e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4086/4398 [2:17:59<29:10,  5.61s/it]
 0:  93%|| 4087/4398 [2:18:04<29:05,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4713, 'grad_norm': 1.0259608823197741, 'learning_rate': 1.3056285101211598e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4087/4398 [2:18:04<29:05,  5.61s/it]
 0:  93%|| 4088/4398 [2:18:10<28:59,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4637, 'grad_norm': 1.0348885024361767, 'learning_rate': 1.2972821239705724e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4088/4398 [2:18:10<28:59,  5.61s/it]
 0:  93%|| 4089/4398 [2:18:16<28:54,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4487, 'grad_norm': 1.038481278793145, 'learning_rate': 1.2889621504240557e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4089/4398 [2:18:16<28:54,  5.61s/it]
 0:  93%|| 4090/4398 [2:18:21<28:46,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 1.0821016803905201, 'learning_rate': 1.280668593993739e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4090/4398 [2:18:21<28:46,  5.60s/it]
 0:  93%|| 4091/4398 [2:18:27<28:43,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.44, 'grad_norm': 1.2545498156234087, 'learning_rate': 1.272401459177397e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4091/4398 [2:18:27<28:43,  5.61s/it]
 0:  93%|| 4092/4398 [2:18:32<28:34,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3983, 'grad_norm': 0.9852677558078834, 'learning_rate': 1.264160750458493e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4092/4398 [2:18:32<28:34,  5.60s/it]
 0:  93%|| 4093/4398 [2:18:38<28:30,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4086, 'grad_norm': 0.9872029455931692, 'learning_rate': 1.255946472306152e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4093/4398 [2:18:38<28:30,  5.61s/it]
 0:  93%|| 4094/4398 [2:18:44<28:23,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4482, 'grad_norm': 1.0814044510127847, 'learning_rate': 1.247758629175161e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4094/4398 [2:18:44<28:23,  5.60s/it]
 0:  93%|| 4095/4398 [2:18:49<28:18,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4183, 'grad_norm': 1.1914757684216266, 'learning_rate': 1.2395972255059786e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4095/4398 [2:18:49<28:18,  5.60s/it]
 0:  93%|| 4096/4398 [2:18:55<28:12,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4449, 'grad_norm': 1.2907618211035676, 'learning_rate': 1.231462265724731e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4096/4398 [2:18:55<28:12,  5.60s/it]
 0:  93%|| 4097/4398 [2:19:01<28:09,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4154, 'grad_norm': 1.173589625020529, 'learning_rate': 1.2233537542431785e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4097/4398 [2:19:01<28:09,  5.61s/it]
 0:  93%|| 4098/4398 [2:19:06<28:02,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4303, 'grad_norm': 1.0887273206455457, 'learning_rate': 1.2152716954587695e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4098/4398 [2:19:06<28:02,  5.61s/it]
 0:  93%|| 4099/4398 [2:19:12<27:58,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4363, 'grad_norm': 1.0344268174692695, 'learning_rate': 1.2072160937545762e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4099/4398 [2:19:12<27:58,  5.61s/it]
 0:  93%|| 4100/4398 [2:19:17<27:51,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.437, 'grad_norm': 1.164967899337058, 'learning_rate': 1.1991869534993484e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4100/4398 [2:19:17<27:51,  5.61s/it]
 0:  93%|| 4101/4398 [2:19:23<27:46,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4404, 'grad_norm': 1.1286028890339197, 'learning_rate': 1.1911842790474637e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4101/4398 [2:19:23<27:46,  5.61s/it]
 0:  93%|| 4102/4398 [2:19:29<27:39,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4471, 'grad_norm': 1.184658188191961, 'learning_rate': 1.1832080747389507e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4102/4398 [2:19:29<27:39,  5.61s/it]
 0:  93%|| 4103/4398 [2:19:34<27:33,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4142, 'grad_norm': 1.0436505625540191, 'learning_rate': 1.1752583448995102e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4103/4398 [2:19:34<27:33,  5.60s/it]
 0:  93%|| 4104/4398 [2:19:40<27:27,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4486, 'grad_norm': 1.124551017275389, 'learning_rate': 1.1673350938404493e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4104/4398 [2:19:40<27:27,  5.60s/it]
 0:  93%|| 4105/4398 [2:19:45<27:23,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4038, 'grad_norm': 1.3593708703043017, 'learning_rate': 1.159438325858725e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4105/4398 [2:19:45<27:23,  5.61s/it]
 0:  93%|| 4106/4398 [2:19:51<27:16,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4242, 'grad_norm': 1.0995304220567699, 'learning_rate': 1.1515680452369504e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4106/4398 [2:19:51<27:16,  5.60s/it]
 0:  93%|| 4107/4398 [2:19:57<27:12,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4428, 'grad_norm': 1.1983933283701498, 'learning_rate': 1.1437242562433503e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4107/4398 [2:19:57<27:12,  5.61s/it]
 0:  93%|| 4108/4398 [2:20:02<27:05,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4646, 'grad_norm': 1.0954710198384159, 'learning_rate': 1.1359069631317888e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4108/4398 [2:20:02<27:05,  5.60s/it]
 0:  93%|| 4109/4398 [2:20:08<27:00,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4117, 'grad_norm': 1.0874609055814612, 'learning_rate': 1.1281161701417742e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4109/4398 [2:20:08<27:00,  5.61s/it]
 0:  93%|| 4110/4398 [2:20:13<26:53,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4543, 'grad_norm': 1.24743671790649, 'learning_rate': 1.1203518814984216e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4110/4398 [2:20:13<26:53,  5.60s/it]
 0:  93%|| 4111/4398 [2:20:19<26:49,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4323, 'grad_norm': 1.034226401410654, 'learning_rate': 1.1126141014124903e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4111/4398 [2:20:19<26:49,  5.61s/it]
 0:  93%|| 4112/4398 [2:20:25<26:44,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.419, 'grad_norm': 1.1865629863111236, 'learning_rate': 1.1049028340803569e-07, 'epoch': 0.93}
 0: 
 0:  93%|| 4112/4398 [2:20:25<26:44,  5.61s/it]
 0:  94%|| 4113/4398 [2:20:30<26:38,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4309, 'grad_norm': 1.0345278770804374, 'learning_rate': 1.0972180836840152e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4113/4398 [2:20:30<26:38,  5.61s/it]
 0:  94%|| 4114/4398 [2:20:36<26:30,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4545, 'grad_norm': 1.2241674204634871, 'learning_rate': 1.0895598543910868e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4114/4398 [2:20:36<26:30,  5.60s/it]
 0:  94%|| 4115/4398 [2:20:41<26:27,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.449, 'grad_norm': 1.092688892535524, 'learning_rate': 1.0819281503547996e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4115/4398 [2:20:41<26:27,  5.61s/it]
 0:  94%|| 4116/4398 [2:20:49<29:08,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4509, 'grad_norm': 1.033095755813305, 'learning_rate': 1.0743229757140039e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4116/4398 [2:20:49<29:08,  6.20s/it]
 0:  94%|| 4117/4398 [2:20:55<28:11,  6.02s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3882, 'grad_norm': 1.012025367224969, 'learning_rate': 1.0667443345931672e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4117/4398 [2:20:55<28:11,  6.02s/it]
 0:  94%|| 4118/4398 [2:21:04<33:05,  7.09s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4536, 'grad_norm': 0.97462067293843, 'learning_rate': 1.0591922311023517e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4118/4398 [2:21:04<33:05,  7.09s/it]
 0:  94%|| 4119/4398 [2:22:27<2:18:50, 29.86s/it]
 0:                                                        
 0: 
 0: {'loss': 0.408, 'grad_norm': 1.0161708941537955, 'learning_rate': 1.0516666693372424e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4119/4398 [2:22:27<2:18:50, 29.86s/it]
 0:  94%|| 4120/4398 [2:23:38<3:14:54, 42.07s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4139, 'grad_norm': 1.058509050931297, 'learning_rate': 1.0441676533791189e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4120/4398 [2:23:38<3:14:54, 42.07s/it]
 0:  94%|| 4121/4398 [2:23:45<2:26:29, 31.73s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4378, 'grad_norm': 1.0400175342573865, 'learning_rate': 1.0366951872948727e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4121/4398 [2:23:45<2:26:29, 31.73s/it]
 0:  94%|| 4122/4398 [2:23:51<1:49:53, 23.89s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4206, 'grad_norm': 1.159914282024038, 'learning_rate': 1.0292492751370009e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4122/4398 [2:23:51<1:49:53, 23.89s/it]
 0:  94%|| 4123/4398 [2:23:57<1:24:22, 18.41s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4355, 'grad_norm': 1.135028999831139, 'learning_rate': 1.02182992094359e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4123/4398 [2:23:57<1:24:22, 18.41s/it]
 0:  94%|| 4124/4398 [2:24:02<1:06:27, 14.55s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4128, 'grad_norm': 1.027926769600886, 'learning_rate': 1.0144371287383215e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4124/4398 [2:24:02<1:06:27, 14.55s/it]
 0:  94%|| 4125/4398 [2:24:08<54:00, 11.87s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4146, 'grad_norm': 1.0598591830980992, 'learning_rate': 1.007070902530477e-07, 'epoch': 0.94}
 0: 
 0:  94%|| 4125/4398 [2:24:08<54:00, 11.87s/it]
 0:  94%|| 4126/4398 [2:24:13<45:14,  9.98s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4193, 'grad_norm': 1.2114169633943819, 'learning_rate': 9.997312463149389e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4126/4398 [2:24:13<45:14,  9.98s/it]
 0:  94%|| 4127/4398 [2:24:19<39:07,  8.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4098, 'grad_norm': 0.9748047139436496, 'learning_rate': 9.924181640721565e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4127/4398 [2:24:19<39:07,  8.66s/it]
 0:  94%|| 4128/4398 [2:24:24<34:49,  7.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.48, 'grad_norm': 1.1422431445188728, 'learning_rate': 9.851316597681959e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4128/4398 [2:24:24<34:49,  7.74s/it]
 0:  94%|| 4129/4398 [2:24:30<31:49,  7.10s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4279, 'grad_norm': 1.0296688717547677, 'learning_rate': 9.778717373546854e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4129/4398 [2:24:30<31:49,  7.10s/it]
 0:  94%|| 4130/4398 [2:24:36<29:41,  6.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4201, 'grad_norm': 1.0935694459170866, 'learning_rate': 9.706384007688585e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4130/4398 [2:24:36<29:41,  6.65s/it]
 0:  94%|| 4131/4398 [2:24:41<28:12,  6.34s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3946, 'grad_norm': 1.1198574626654303, 'learning_rate': 9.634316539335164e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4131/4398 [2:24:41<28:12,  6.34s/it]
 0:  94%|| 4132/4398 [2:24:47<27:11,  6.13s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4009, 'grad_norm': 1.0456449026167571, 'learning_rate': 9.562515007570439e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4132/4398 [2:24:47<27:11,  6.13s/it]
 0:  94%|| 4133/4398 [2:24:53<26:24,  5.98s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4324, 'grad_norm': 1.0709833821602754, 'learning_rate': 9.490979451333982e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4133/4398 [2:24:53<26:24,  5.98s/it]
 0:  94%|| 4134/4398 [2:24:58<25:46,  5.86s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3993, 'grad_norm': 1.2589274638549837, 'learning_rate': 9.419709909421204e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4134/4398 [2:24:58<25:46,  5.86s/it]
 0:  94%|| 4135/4398 [2:25:04<25:22,  5.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4465, 'grad_norm': 1.041703847413518, 'learning_rate': 9.3487064204833e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4135/4398 [2:25:04<25:22,  5.79s/it]
 0:  94%|| 4136/4398 [2:25:09<25:01,  5.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4364, 'grad_norm': 1.4272081135541166, 'learning_rate': 9.277969023026967e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4136/4398 [2:25:09<25:01,  5.73s/it]
 0:  94%|| 4137/4398 [2:25:15<24:46,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4418, 'grad_norm': 1.0565387863056184, 'learning_rate': 9.207497755414685e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4137/4398 [2:25:15<24:46,  5.70s/it]
 0:  94%|| 4138/4398 [2:25:21<24:32,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4537, 'grad_norm': 1.0345547278030065, 'learning_rate': 9.137292655864826e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4138/4398 [2:25:21<24:32,  5.66s/it]
 0:  94%|| 4139/4398 [2:25:26<24:23,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.418, 'grad_norm': 1.0921243830284475, 'learning_rate': 9.067353762451104e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4139/4398 [2:25:26<24:23,  5.65s/it]
 0:  94%|| 4140/4398 [2:25:32<24:13,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4311, 'grad_norm': 1.1743490014474904, 'learning_rate': 8.997681113103007e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4140/4398 [2:25:32<24:13,  5.63s/it]
 0:  94%|| 4141/4398 [2:25:37<24:05,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4594, 'grad_norm': 1.351019039613363, 'learning_rate': 8.928274745605648e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4141/4398 [2:25:37<24:05,  5.63s/it]
 0:  94%|| 4142/4398 [2:25:43<23:55,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3914, 'grad_norm': 0.9664517214551122, 'learning_rate': 8.859134697599636e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4142/4398 [2:25:43<23:55,  5.61s/it]
 0:  94%|| 4143/4398 [2:25:49<23:48,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4022, 'grad_norm': 1.047823973302779, 'learning_rate': 8.790261006581313e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4143/4398 [2:25:49<23:48,  5.60s/it]
 0:  94%|| 4144/4398 [2:25:54<23:41,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4063, 'grad_norm': 0.9555201801192389, 'learning_rate': 8.721653709902356e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4144/4398 [2:25:54<23:41,  5.60s/it]
 0:  94%|| 4145/4398 [2:26:00<23:38,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4185, 'grad_norm': 1.3173962904052492, 'learning_rate': 8.653312844770112e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4145/4398 [2:26:00<23:38,  5.61s/it]
 0:  94%|| 4146/4398 [2:26:05<23:29,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4491, 'grad_norm': 1.1001362698840151, 'learning_rate': 8.585238448247434e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4146/4398 [2:26:05<23:29,  5.59s/it]
 0:  94%|| 4147/4398 [2:26:11<23:25,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4488, 'grad_norm': 1.2112642871971515, 'learning_rate': 8.517430557252626e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4147/4398 [2:26:11<23:25,  5.60s/it]
 0:  94%|| 4148/4398 [2:26:17<23:19,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4239, 'grad_norm': 1.0703513875857331, 'learning_rate': 8.449889208559492e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4148/4398 [2:26:17<23:19,  5.60s/it]
 0:  94%|| 4149/4398 [2:26:22<23:13,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4273, 'grad_norm': 1.1987960411982186, 'learning_rate': 8.382614438797176e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4149/4398 [2:26:22<23:13,  5.60s/it]
 0:  94%|| 4150/4398 [2:26:28<23:07,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4177, 'grad_norm': 1.097038303091079, 'learning_rate': 8.315606284450383e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4150/4398 [2:26:28<23:07,  5.59s/it]
 0:  94%|| 4151/4398 [2:26:33<23:02,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4521, 'grad_norm': 1.338051811903985, 'learning_rate': 8.248864781859212e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4151/4398 [2:26:33<23:02,  5.60s/it]
 0:  94%|| 4152/4398 [2:26:39<22:56,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4097, 'grad_norm': 0.974867198807631, 'learning_rate': 8.18238996721904e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4152/4398 [2:26:39<22:56,  5.59s/it]
 0:  94%|| 4153/4398 [2:26:45<22:52,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4292, 'grad_norm': 1.1328953815905776, 'learning_rate': 8.116181876580753e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4153/4398 [2:26:45<22:52,  5.60s/it]
 0:  94%|| 4154/4398 [2:26:50<22:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4165, 'grad_norm': 1.0874273235170446, 'learning_rate': 8.050240545850463e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4154/4398 [2:26:50<22:45,  5.60s/it]
 0:  94%|| 4155/4398 [2:26:56<22:41,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4008, 'grad_norm': 1.0399449889240886, 'learning_rate': 7.984566010789673e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4155/4398 [2:26:56<22:41,  5.60s/it]
 0:  94%|| 4156/4398 [2:27:01<22:33,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4511, 'grad_norm': 1.3425679030484283, 'learning_rate': 7.91915830701523e-08, 'epoch': 0.94}
 0: 
 0:  94%|| 4156/4398 [2:27:01<22:33,  5.59s/it]
 0:  95%|| 4157/4398 [2:27:07<22:29,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4293, 'grad_norm': 1.0287252785437626, 'learning_rate': 7.854017469999254e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4157/4398 [2:27:07<22:29,  5.60s/it]
 0:  95%|| 4158/4398 [2:27:12<22:21,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4454, 'grad_norm': 1.02857358414894, 'learning_rate': 7.789143535069154e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4158/4398 [2:27:12<22:21,  5.59s/it]
 0:  95%|| 4159/4398 [2:27:18<22:18,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4359, 'grad_norm': 1.0653869823788447, 'learning_rate': 7.724536537407457e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4159/4398 [2:27:18<22:18,  5.60s/it]
 0:  95%|| 4160/4398 [2:27:24<22:11,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4469, 'grad_norm': 1.0160400836936991, 'learning_rate': 7.660196512052021e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4160/4398 [2:27:24<22:11,  5.59s/it]
 0:  95%|| 4161/4398 [2:27:29<22:08,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4555, 'grad_norm': 1.1809658033332084, 'learning_rate': 7.59612349389599e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4161/4398 [2:27:29<22:08,  5.60s/it]
 0:  95%|| 4162/4398 [2:27:35<22:02,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.433, 'grad_norm': 1.0297890708645436, 'learning_rate': 7.532317517687626e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4162/4398 [2:27:35<22:02,  5.60s/it]
 0:  95%|| 4163/4398 [2:27:41<21:57,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4379, 'grad_norm': 1.08340514981628, 'learning_rate': 7.468778618030304e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4163/4398 [2:27:41<21:57,  5.61s/it]
 0:  95%|| 4164/4398 [2:27:46<21:50,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4756, 'grad_norm': 1.0594566328938215, 'learning_rate': 7.405506829382736e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4164/4398 [2:27:46<21:50,  5.60s/it]
 0:  95%|| 4165/4398 [2:27:52<21:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.429, 'grad_norm': 1.0961667544815643, 'learning_rate': 7.342502186058587e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4165/4398 [2:27:52<21:45,  5.60s/it]
 0:  95%|| 4166/4398 [2:27:57<21:38,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4412, 'grad_norm': 1.1007894915073229, 'learning_rate': 7.279764722226801e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4166/4398 [2:27:57<21:38,  5.60s/it]
 0:  95%|| 4167/4398 [2:28:03<21:41,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4437, 'grad_norm': 1.0646336548237911, 'learning_rate': 7.217294471911274e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4167/4398 [2:28:03<21:41,  5.63s/it]
 0:  95%|| 4168/4398 [2:28:09<21:33,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3986, 'grad_norm': 1.0953530337706006, 'learning_rate': 7.155091468991071e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4168/4398 [2:28:09<21:33,  5.62s/it]
 0:  95%|| 4169/4398 [2:28:14<21:34,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4426, 'grad_norm': 1.0424189808318653, 'learning_rate': 7.093155747200376e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4169/4398 [2:28:14<21:34,  5.65s/it]
 0:  95%|| 4170/4398 [2:28:20<21:23,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4068, 'grad_norm': 1.36380837892303, 'learning_rate': 7.031487340128318e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4170/4398 [2:28:20<21:23,  5.63s/it]
 0:  95%|| 4171/4398 [2:28:26<21:17,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4352, 'grad_norm': 1.095441255021787, 'learning_rate': 6.97008628121898e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4171/4398 [2:28:26<21:17,  5.63s/it]
 0:  95%|| 4172/4398 [2:28:31<21:08,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4052, 'grad_norm': 1.0210014633988613, 'learning_rate': 6.908952603771779e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4172/4398 [2:28:31<21:08,  5.61s/it]
 0:  95%|| 4173/4398 [2:28:37<21:01,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.43, 'grad_norm': 1.398392906992977, 'learning_rate': 6.848086340940752e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4173/4398 [2:28:37<21:01,  5.61s/it]
 0:  95%|| 4174/4398 [2:28:42<20:53,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 1.108935644455328, 'learning_rate': 6.787487525735159e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4174/4398 [2:28:42<20:53,  5.60s/it]
 0:  95%|| 4175/4398 [2:28:48<20:49,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4221, 'grad_norm': 1.0323897837357736, 'learning_rate': 6.727156191019047e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4175/4398 [2:28:48<20:49,  5.60s/it]
 0:  95%|| 4176/4398 [2:28:53<20:41,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4473, 'grad_norm': 1.1668518964104517, 'learning_rate': 6.667092369511574e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4176/4398 [2:28:53<20:41,  5.59s/it]
 0:  95%|| 4177/4398 [2:28:59<20:37,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3929, 'grad_norm': 1.0932521415337408, 'learning_rate': 6.607296093786741e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4177/4398 [2:28:59<20:37,  5.60s/it]
 0:  95%|| 4178/4398 [2:29:05<20:30,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.414, 'grad_norm': 1.0751210172218673, 'learning_rate': 6.54776739627333e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4178/4398 [2:29:05<20:30,  5.59s/it]
 0:  95%|| 4179/4398 [2:29:10<20:25,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4245, 'grad_norm': 1.0292287687055215, 'learning_rate': 6.488506309255238e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4179/4398 [2:29:10<20:25,  5.60s/it]
 0:  95%|| 4180/4398 [2:29:16<20:19,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3961, 'grad_norm': 1.08630326542953, 'learning_rate': 6.429512864871034e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4180/4398 [2:29:16<20:19,  5.59s/it]
 0:  95%|| 4181/4398 [2:29:21<20:14,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4152, 'grad_norm': 1.0796912766512432, 'learning_rate': 6.370787095114295e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4181/4398 [2:29:21<20:14,  5.60s/it]
 0:  95%|| 4182/4398 [2:29:27<20:08,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4585, 'grad_norm': 1.0633168564931865, 'learning_rate': 6.31232903183332e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4182/4398 [2:29:27<20:08,  5.60s/it]
 0:  95%|| 4183/4398 [2:29:33<20:03,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4128, 'grad_norm': 1.109404148035354, 'learning_rate': 6.25413870673125e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4183/4398 [2:29:33<20:03,  5.60s/it]
 0:  95%|| 4184/4398 [2:29:40<22:05,  6.19s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 1.047107071881163, 'learning_rate': 6.196216151366007e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4184/4398 [2:29:40<22:05,  6.19s/it]
 0:  95%|| 4185/4398 [2:29:46<21:20,  6.01s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4264, 'grad_norm': 1.2141575694066156, 'learning_rate': 6.138561397150411e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4185/4398 [2:29:46<21:20,  6.01s/it]
 0:  95%|| 4186/4398 [2:29:53<22:51,  6.47s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4411, 'grad_norm': 0.9484606811653234, 'learning_rate': 6.081174475351892e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4186/4398 [2:29:53<22:51,  6.47s/it]
 0:  95%|| 4187/4398 [2:31:10<1:37:15, 27.66s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4281, 'grad_norm': 1.0203608947436087, 'learning_rate': 6.024055417092723e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4187/4398 [2:31:10<1:37:15, 27.66s/it]
 0:  95%|| 4188/4398 [2:33:03<3:05:50, 53.10s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4226, 'grad_norm': 0.997030947208047, 'learning_rate': 5.9672042533499e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4188/4398 [2:33:03<3:05:50, 53.10s/it]
 0:  95%|| 4189/4398 [2:33:17<2:24:40, 41.53s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4273, 'grad_norm': 1.2185134648779006, 'learning_rate': 5.9106210149550424e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4189/4398 [2:33:17<2:24:40, 41.53s/it]
 0:  95%|| 4190/4398 [2:33:23<1:46:35, 30.75s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4577, 'grad_norm': 1.1065521743201019, 'learning_rate': 5.8543057325946006e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4190/4398 [2:33:23<1:46:35, 30.75s/it]
 0:  95%|| 4191/4398 [2:33:29<1:20:04, 23.21s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4336, 'grad_norm': 0.9916863934815919, 'learning_rate': 5.7982584368097004e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4191/4398 [2:33:29<1:20:04, 23.21s/it]
 0:  95%|| 4192/4398 [2:33:34<1:01:32, 17.93s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4028, 'grad_norm': 1.2745907679236963, 'learning_rate': 5.742479157995973e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4192/4398 [2:33:34<1:01:32, 17.93s/it]
 0:  95%|| 4193/4398 [2:33:40<48:38, 14.24s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4034, 'grad_norm': 1.0862197753621148, 'learning_rate': 5.686967926403941e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4193/4398 [2:33:40<48:38, 14.24s/it]
 0:  95%|| 4194/4398 [2:33:45<39:33, 11.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4533, 'grad_norm': 1.1638244490387326, 'learning_rate': 5.631724772138469e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4194/4398 [2:33:45<39:33, 11.64s/it]
 0:  95%|| 4195/4398 [2:33:51<33:15,  9.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4631, 'grad_norm': 1.0504090370113104, 'learning_rate': 5.5767497251592584e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4195/4398 [2:33:51<33:15,  9.83s/it]
 0:  95%|| 4196/4398 [2:33:57<28:47,  8.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.426, 'grad_norm': 1.1254958774551305, 'learning_rate': 5.522042815280515e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4196/4398 [2:33:57<28:47,  8.55s/it]
 0:  95%|| 4197/4398 [2:34:02<25:41,  7.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4062, 'grad_norm': 1.1657673623397409, 'learning_rate': 5.467604072171062e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4197/4398 [2:34:02<25:41,  7.67s/it]
 0:  95%|| 4198/4398 [2:34:08<23:27,  7.04s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4478, 'grad_norm': 1.1339768565455197, 'learning_rate': 5.413433525354339e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4198/4398 [2:34:08<23:27,  7.04s/it]
 0:  95%|| 4199/4398 [2:34:13<21:55,  6.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.449, 'grad_norm': 1.0942176299588187, 'learning_rate': 5.359531204208235e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4199/4398 [2:34:13<21:55,  6.61s/it]
 0:  95%|| 4200/4398 [2:34:19<20:49,  6.31s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4245, 'grad_norm': 1.1152490096832361, 'learning_rate': 5.305897137965199e-08, 'epoch': 0.95}
 0: 
 0:  95%|| 4200/4398 [2:34:19<20:49,  6.31s/it]
 0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 0:   return fn(*args, **kwargs)
 0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
 0:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
 0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 0:  96%|| 4201/4398 [2:35:12<1:06:50, 20.36s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4418, 'grad_norm': 1.0048426670099377, 'learning_rate': 5.252531355712298e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4201/4398 [2:35:12<1:06:50, 20.36s/it]
 0:  96%|| 4202/4398 [2:35:18<52:01, 15.93s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4362, 'grad_norm': 1.1134609922507075, 'learning_rate': 5.199433886390937e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4202/4398 [2:35:18<52:01, 15.93s/it]
 0:  96%|| 4203/4398 [2:35:23<41:46, 12.85s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4222, 'grad_norm': 1.1629511097061052, 'learning_rate': 5.14660475879708e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4203/4398 [2:35:23<41:46, 12.85s/it]
 0:  96%|| 4204/4398 [2:35:29<34:29, 10.67s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4148, 'grad_norm': 1.064251757422611, 'learning_rate': 5.0940440015813105e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4204/4398 [2:35:29<34:29, 10.67s/it]
 0:  96%|| 4205/4398 [2:35:35<29:27,  9.16s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4443, 'grad_norm': 1.2317009864638306, 'learning_rate': 5.0417516432483826e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4205/4398 [2:35:35<29:27,  9.16s/it]
 0:  96%|| 4206/4398 [2:35:40<25:51,  8.08s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4244, 'grad_norm': 1.1083650465738695, 'learning_rate': 4.989727712157721e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4206/4398 [2:35:40<25:51,  8.08s/it]
 0:  96%|| 4207/4398 [2:35:46<23:23,  7.35s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4225, 'grad_norm': 1.0728210041088333, 'learning_rate': 4.937972236523092e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4207/4398 [2:35:46<23:23,  7.35s/it]
 0:  96%|| 4208/4398 [2:35:51<21:36,  6.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4394, 'grad_norm': 1.0722535260472363, 'learning_rate': 4.886485244412709e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4208/4398 [2:35:51<21:36,  6.82s/it]
 0:  96%|| 4209/4398 [2:35:57<20:21,  6.46s/it]
 0:                                                      
 0: 
 0: {'loss': 0.403, 'grad_norm': 1.0672063691948408, 'learning_rate': 4.8352667637490694e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4209/4398 [2:35:57<20:21,  6.46s/it]
 0:  96%|| 4210/4398 [2:36:03<19:24,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4321, 'grad_norm': 1.059511267585521, 'learning_rate': 4.784316822309232e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4210/4398 [2:36:03<19:24,  6.20s/it]
 0:  96%|| 4211/4398 [2:36:08<18:46,  6.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4638, 'grad_norm': 1.3532379067164573, 'learning_rate': 4.733635447724427e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4211/4398 [2:36:08<18:46,  6.03s/it]
 0:  96%|| 4212/4398 [2:36:14<18:22,  5.93s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3967, 'grad_norm': 1.044065597268735, 'learning_rate': 4.6832226674803916e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4212/4398 [2:36:14<18:22,  5.93s/it]
 0:  96%|| 4213/4398 [2:36:20<17:59,  5.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.0301952185958303, 'learning_rate': 4.633078508917144e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4213/4398 [2:36:20<17:59,  5.83s/it]
 0:  96%|| 4214/4398 [2:36:25<17:39,  5.76s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4018, 'grad_norm': 1.132352085910262, 'learning_rate': 4.583202999228986e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4214/4398 [2:36:25<17:39,  5.76s/it]
 0:  96%|| 4215/4398 [2:36:31<17:25,  5.71s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4325, 'grad_norm': 1.5564338832414042, 'learning_rate': 4.5335961654645044e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4215/4398 [2:36:31<17:25,  5.71s/it]
 0:  96%|| 4216/4398 [2:36:36<17:13,  5.68s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4547, 'grad_norm': 1.197961638351886, 'learning_rate': 4.484258034526734e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4216/4398 [2:36:36<17:13,  5.68s/it]
 0:  96%|| 4217/4398 [2:36:42<17:03,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4211, 'grad_norm': 1.0641195085885515, 'learning_rate': 4.435188633172882e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4217/4398 [2:36:42<17:03,  5.66s/it]
 0:  96%|| 4218/4398 [2:36:48<16:55,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4094, 'grad_norm': 1.173359184407529, 'learning_rate': 4.3863879880142737e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4218/4398 [2:36:48<16:55,  5.64s/it]
 0:  96%|| 4219/4398 [2:36:53<16:48,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4262, 'grad_norm': 1.1360847430533159, 'learning_rate': 4.3378561255167375e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4219/4398 [2:36:53<16:48,  5.64s/it]
 0:  96%|| 4220/4398 [2:36:59<16:40,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4239, 'grad_norm': 1.1629197602458061, 'learning_rate': 4.289593072000164e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4220/4398 [2:36:59<16:40,  5.62s/it]
 0:  96%|| 4221/4398 [2:37:04<16:36,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4302, 'grad_norm': 1.138793418377759, 'learning_rate': 4.241598853638784e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4221/4398 [2:37:04<16:36,  5.63s/it]
 0:  96%|| 4222/4398 [2:37:10<16:27,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4782, 'grad_norm': 1.1503589758084403, 'learning_rate': 4.193873496460887e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4222/4398 [2:37:10<16:27,  5.61s/it]
 0:  96%|| 4223/4398 [2:37:16<16:23,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4131, 'grad_norm': 1.0628176782227303, 'learning_rate': 4.146417026349048e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4223/4398 [2:37:16<16:23,  5.62s/it]
 0:  96%|| 4224/4398 [2:37:21<16:16,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4523, 'grad_norm': 1.1526711141939856, 'learning_rate': 4.099229469040011e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4224/4398 [2:37:21<16:16,  5.61s/it]
 0:  96%|| 4225/4398 [2:37:27<16:11,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.434, 'grad_norm': 0.9703423915366377, 'learning_rate': 4.052310850124752e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4225/4398 [2:37:27<16:11,  5.62s/it]
 0:  96%|| 4226/4398 [2:37:32<16:03,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4685, 'grad_norm': 1.1200591014321377, 'learning_rate': 4.005661195048194e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4226/4398 [2:37:32<16:03,  5.60s/it]
 0:  96%|| 4227/4398 [2:37:38<15:59,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4148, 'grad_norm': 1.0462153493155772, 'learning_rate': 3.959280529109599e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4227/4398 [2:37:38<15:59,  5.61s/it]
 0:  96%|| 4228/4398 [2:37:44<15:52,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4403, 'grad_norm': 1.1594822263381912, 'learning_rate': 3.9131688774622366e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4228/4398 [2:37:44<15:52,  5.60s/it]
 0:  96%|| 4229/4398 [2:37:49<15:47,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.0790373480650304, 'learning_rate': 3.867326265113547e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4229/4398 [2:37:49<15:47,  5.61s/it]
 0:  96%|| 4230/4398 [2:37:55<15:41,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4248, 'grad_norm': 1.131458554948445, 'learning_rate': 3.821752716924976e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4230/4398 [2:37:55<15:41,  5.61s/it]
 0:  96%|| 4231/4398 [2:38:01<15:37,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4072, 'grad_norm': 1.0517607462167287, 'learning_rate': 3.776448257612086e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4231/4398 [2:38:01<15:37,  5.61s/it]
 0:  96%|| 4232/4398 [2:38:06<15:30,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4247, 'grad_norm': 1.086121750061808, 'learning_rate': 3.7314129117446694e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4232/4398 [2:38:06<15:30,  5.60s/it]
 0:  96%|| 4233/4398 [2:38:12<15:24,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4369, 'grad_norm': 1.1838947291311679, 'learning_rate': 3.686646703746299e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4233/4398 [2:38:12<15:24,  5.60s/it]
 0:  96%|| 4234/4398 [2:38:17<15:17,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.46, 'grad_norm': 1.197381537899362, 'learning_rate': 3.642149657894778e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4234/4398 [2:38:17<15:17,  5.59s/it]
 0:  96%|| 4235/4398 [2:38:23<15:12,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.45, 'grad_norm': 1.0336147522384267, 'learning_rate': 3.5979217983218574e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4235/4398 [2:38:23<15:12,  5.60s/it]
 0:  96%|| 4236/4398 [2:38:28<15:06,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4388, 'grad_norm': 1.0389090529279703, 'learning_rate': 3.553963149013295e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4236/4398 [2:38:28<15:06,  5.59s/it]
 0:  96%|| 4237/4398 [2:38:34<15:01,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3696, 'grad_norm': 1.0269347026313784, 'learning_rate': 3.510273733808911e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4237/4398 [2:38:34<15:01,  5.60s/it]
 0:  96%|| 4238/4398 [2:38:40<14:55,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4276, 'grad_norm': 1.0435069026191957, 'learning_rate': 3.466853576402529e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4238/4398 [2:38:40<14:55,  5.60s/it]
 0:  96%|| 4239/4398 [2:38:45<14:51,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4602, 'grad_norm': 1.090392188416408, 'learning_rate': 3.423702700341813e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4239/4398 [2:38:45<14:51,  5.61s/it]
 0:  96%|| 4240/4398 [2:38:51<14:45,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4302, 'grad_norm': 1.173629204995084, 'learning_rate': 3.3808211290284886e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4240/4398 [2:38:51<14:45,  5.60s/it]
 0:  96%|| 4241/4398 [2:38:56<14:39,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4319, 'grad_norm': 1.5497937020125738, 'learning_rate': 3.338208885718286e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4241/4398 [2:38:56<14:39,  5.60s/it]
 0:  96%|| 4242/4398 [2:39:02<14:34,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4574, 'grad_norm': 1.0016247418615525, 'learning_rate': 3.2958659935207196e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4242/4398 [2:39:02<14:34,  5.60s/it]
 0:  96%|| 4243/4398 [2:39:08<14:30,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4434, 'grad_norm': 1.0902260509915018, 'learning_rate': 3.25379247539942e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4243/4398 [2:39:08<14:30,  5.62s/it]
 0:  96%|| 4244/4398 [2:39:13<14:23,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4076, 'grad_norm': 1.219201567838871, 'learning_rate': 3.211988354171691e-08, 'epoch': 0.96}
 0: 
 0:  96%|| 4244/4398 [2:39:13<14:23,  5.61s/it]
 0:  97%|| 4245/4398 [2:39:19<14:19,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4497, 'grad_norm': 1.1252360081626747, 'learning_rate': 3.170453652508954e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4245/4398 [2:39:19<14:19,  5.62s/it]
 0:  97%|| 4246/4398 [2:39:25<14:13,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4815, 'grad_norm': 1.1063196802410362, 'learning_rate': 3.1291883929364105e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4246/4398 [2:39:25<14:13,  5.61s/it]
 0:  97%|| 4247/4398 [2:39:30<14:08,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4477, 'grad_norm': 1.082215356320847, 'learning_rate': 3.0881925978331615e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4247/4398 [2:39:30<14:08,  5.62s/it]
 0:  97%|| 4248/4398 [2:39:36<14:00,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4426, 'grad_norm': 1.074992436511885, 'learning_rate': 3.047466289432144e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4248/4398 [2:39:36<14:00,  5.61s/it]
 0:  97%|| 4249/4398 [2:39:41<13:56,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3793, 'grad_norm': 1.0268923810232373, 'learning_rate': 3.007009489820245e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4249/4398 [2:39:41<13:56,  5.62s/it]
 0:  97%|| 4250/4398 [2:39:47<13:50,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4344, 'grad_norm': 0.9737408685836404, 'learning_rate': 2.966822220938026e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4250/4398 [2:39:47<13:50,  5.61s/it]
 0:  97%|| 4251/4398 [2:39:53<13:46,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4443, 'grad_norm': 1.1045915912559623, 'learning_rate': 2.9269045045800527e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4251/4398 [2:39:53<13:46,  5.62s/it]
 0:  97%|| 4252/4398 [2:39:58<13:38,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4557, 'grad_norm': 1.206196122966449, 'learning_rate': 2.8872563623945638e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4252/4398 [2:39:58<13:38,  5.61s/it]
 0:  97%|| 4253/4398 [2:40:06<15:03,  6.23s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4276, 'grad_norm': 1.0887532370965562, 'learning_rate': 2.8478778158836927e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4253/4398 [2:40:06<15:03,  6.23s/it]
 0:  97%|| 4254/4398 [2:40:14<15:56,  6.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4374, 'grad_norm': 1.1761618765621746, 'learning_rate': 2.8087688864033014e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4254/4398 [2:40:14<15:56,  6.64s/it]
 0:  97%|| 4255/4398 [2:41:18<57:20, 24.06s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4011, 'grad_norm': 1.1803388919076807, 'learning_rate': 2.76992959516309e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4255/4398 [2:41:18<57:20, 24.06s/it]
 0:  97%|| 4256/4398 [2:42:57<1:49:42, 46.36s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4592, 'grad_norm': 1.4107111242759387, 'learning_rate': 2.731359963226432e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4256/4398 [2:42:57<1:49:42, 46.36s/it]
 0:  97%|| 4257/4398 [2:43:07<1:23:36, 35.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4771, 'grad_norm': 1.1966337437041497, 'learning_rate': 2.6930600115105954e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4257/4398 [2:43:07<1:23:36, 35.58s/it]
 0:  97%|| 4258/4398 [2:43:13<1:02:01, 26.58s/it]
 0:                                                        
 0: 
 0: {'loss': 0.4289, 'grad_norm': 1.1849353603981323, 'learning_rate': 2.6550297607865206e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4258/4398 [2:43:13<1:02:01, 26.58s/it]
 0:  97%|| 4259/4398 [2:43:18<46:59, 20.28s/it]  
 0:                                                      
 0: 
 0: {'loss': 0.4133, 'grad_norm': 0.9278099314697652, 'learning_rate': 2.617269231678876e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4259/4398 [2:43:18<46:59, 20.28s/it]
 0:  97%|| 4260/4398 [2:43:24<36:29, 15.87s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4308, 'grad_norm': 1.0352626221702605, 'learning_rate': 2.579778444666059e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4260/4398 [2:43:24<36:29, 15.87s/it]
 0:  97%|| 4261/4398 [2:43:29<29:12, 12.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4284, 'grad_norm': 1.2885248362334156, 'learning_rate': 2.542557420080194e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4261/4398 [2:43:29<29:12, 12.79s/it]
 0:  97%|| 4262/4398 [2:43:35<24:04, 10.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.472, 'grad_norm': 1.1188176709152469, 'learning_rate': 2.5056061781070785e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4262/4398 [2:43:35<24:04, 10.62s/it]
 0:  97%|| 4263/4398 [2:43:41<20:31,  9.12s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4433, 'grad_norm': 1.2020163757371725, 'learning_rate': 2.4689247387862934e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4263/4398 [2:43:41<20:31,  9.12s/it]
 0:  97%|| 4264/4398 [2:43:46<18:00,  8.06s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4398, 'grad_norm': 1.113190474883022, 'learning_rate': 2.4325131220109267e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4264/4398 [2:43:46<18:00,  8.06s/it]
 0:  97%|| 4265/4398 [2:43:52<16:13,  7.32s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4305, 'grad_norm': 1.1496243520680631, 'learning_rate': 2.396371347527848e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4265/4398 [2:43:52<16:13,  7.32s/it]
 0:  97%|| 4266/4398 [2:43:57<14:57,  6.80s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4402, 'grad_norm': 1.1438183667696333, 'learning_rate': 2.3604994349376576e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4266/4398 [2:43:57<14:57,  6.80s/it]
 0:  97%|| 4267/4398 [2:44:03<14:03,  6.44s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4576, 'grad_norm': 1.0423714966328148, 'learning_rate': 2.3248974036944593e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4267/4398 [2:44:03<14:03,  6.44s/it]
 0:  97%|| 4268/4398 [2:44:08<13:23,  6.18s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4048, 'grad_norm': 1.0010549745217674, 'learning_rate': 2.2895652731060313e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4268/4398 [2:44:09<13:23,  6.18s/it]
 0:  97%|| 4269/4398 [2:44:14<12:53,  6.00s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4336, 'grad_norm': 1.1482938442559325, 'learning_rate': 2.2545030623338237e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4269/4398 [2:44:14<12:53,  6.00s/it]
 0:  97%|| 4270/4398 [2:44:20<12:31,  5.87s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4633, 'grad_norm': 1.0683051011425913, 'learning_rate': 2.2197107903929038e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4270/4398 [2:44:20<12:31,  5.87s/it]
 0:  97%|| 4271/4398 [2:44:25<12:15,  5.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4662, 'grad_norm': 1.0518250910589377, 'learning_rate': 2.1851884761519004e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4271/4398 [2:44:25<12:15,  5.79s/it]
 0:  97%|| 4272/4398 [2:44:31<12:02,  5.73s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4295, 'grad_norm': 1.0346621978549402, 'learning_rate': 2.1509361383330597e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4272/4398 [2:44:31<12:02,  5.73s/it]
 0:  97%|| 4273/4398 [2:44:36<11:52,  5.70s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4473, 'grad_norm': 1.2591367588892357, 'learning_rate': 2.1169537955121887e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4273/4398 [2:44:36<11:52,  5.70s/it]
 0:  97%|| 4274/4398 [2:44:42<11:42,  5.66s/it]
 0:                                                      
 0: 
 0: {'loss': 0.464, 'grad_norm': 1.1427997595066164, 'learning_rate': 2.0832414661187682e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4274/4398 [2:44:42<11:42,  5.66s/it]
 0:  97%|| 4275/4398 [2:44:48<11:34,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4735, 'grad_norm': 1.1389433195212395, 'learning_rate': 2.0497991684356732e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4275/4398 [2:44:48<11:34,  5.65s/it]
 0:  97%|| 4276/4398 [2:44:53<11:27,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4175, 'grad_norm': 0.9953653981385205, 'learning_rate': 2.016626920599507e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4276/4398 [2:44:53<11:27,  5.63s/it]
 0:  97%|| 4277/4398 [2:44:59<11:20,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4437, 'grad_norm': 1.177846749357381, 'learning_rate': 1.983724740600268e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4277/4398 [2:44:59<11:20,  5.63s/it]
 0:  97%|| 4278/4398 [2:45:04<11:13,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4531, 'grad_norm': 1.1861146824018733, 'learning_rate': 1.9510926462816825e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4278/4398 [2:45:04<11:13,  5.61s/it]
 0:  97%|| 4279/4398 [2:45:10<11:07,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4612, 'grad_norm': 1.1377903110695484, 'learning_rate': 1.9187306553407613e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4279/4398 [2:45:10<11:07,  5.61s/it]
 0:  97%|| 4280/4398 [2:45:16<11:00,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3989, 'grad_norm': 1.1591113916439293, 'learning_rate': 1.8866387853281876e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4280/4398 [2:45:16<11:00,  5.60s/it]
 0:  97%|| 4281/4398 [2:45:21<10:55,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3993, 'grad_norm': 1.074884109448405, 'learning_rate': 1.8548170536482058e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4281/4398 [2:45:21<10:55,  5.60s/it]
 0:  97%|| 4282/4398 [2:45:27<10:49,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4513, 'grad_norm': 1.115115023751008, 'learning_rate': 1.8232654775583446e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4282/4398 [2:45:27<10:49,  5.60s/it]
 0:  97%|| 4283/4398 [2:45:32<10:44,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4237, 'grad_norm': 1.08000735179395, 'learning_rate': 1.7919840741698613e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4283/4398 [2:45:32<10:44,  5.60s/it]
 0:  97%|| 4284/4398 [2:45:38<10:37,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4241, 'grad_norm': 1.0757747273413771, 'learning_rate': 1.7609728604472966e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4284/4398 [2:45:38<10:37,  5.59s/it]
 0:  97%|| 4285/4398 [2:45:44<10:32,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4181, 'grad_norm': 1.0185020634660011, 'learning_rate': 1.7302318532088082e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4285/4398 [2:45:44<10:32,  5.60s/it]
 0:  97%|| 4286/4398 [2:45:49<10:26,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4094, 'grad_norm': 1.2574046481184156, 'learning_rate': 1.6997610691258935e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4286/4398 [2:45:49<10:26,  5.60s/it]
 0:  97%|| 4287/4398 [2:45:55<10:21,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4619, 'grad_norm': 1.102527875375427, 'learning_rate': 1.6695605247236125e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4287/4398 [2:45:55<10:21,  5.60s/it]
 0:  97%|| 4288/4398 [2:46:00<10:15,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4284, 'grad_norm': 1.1303218794718055, 'learning_rate': 1.6396302363804185e-08, 'epoch': 0.97}
 0: 
 0:  97%|| 4288/4398 [2:46:00<10:15,  5.59s/it]
 0:  98%|| 4289/4398 [2:46:06<10:09,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4324, 'grad_norm': 1.0979738456667878, 'learning_rate': 1.6099702203281055e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4289/4398 [2:46:06<10:09,  5.60s/it]
 0:  98%|| 4290/4398 [2:46:12<10:04,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4111, 'grad_norm': 1.073095873563212, 'learning_rate': 1.580580492652084e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4290/4398 [2:46:12<10:04,  5.59s/it]
 0:  98%|| 4291/4398 [2:46:17<09:59,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4147, 'grad_norm': 1.0889272938083445, 'learning_rate': 1.5514610692910493e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4291/4398 [2:46:17<09:59,  5.60s/it]
 0:  98%|| 4292/4398 [2:46:23<09:53,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4286, 'grad_norm': 1.3305901540198528, 'learning_rate': 1.522611966037091e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4292/4398 [2:46:23<09:53,  5.60s/it]
 0:  98%|| 4293/4398 [2:46:28<09:48,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4194, 'grad_norm': 1.0627368995052575, 'learning_rate': 1.4940331985358046e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4293/4398 [2:46:28<09:48,  5.60s/it]
 0:  98%|| 4294/4398 [2:46:34<09:42,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4538, 'grad_norm': 1.0661518209942424, 'learning_rate': 1.465724782286071e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4294/4398 [2:46:34<09:42,  5.60s/it]
 0:  98%|| 4295/4398 [2:46:40<09:36,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4583, 'grad_norm': 1.226750143777998, 'learning_rate': 1.43768673264022e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4295/4398 [2:46:40<09:36,  5.60s/it]
 0:  98%|| 4296/4398 [2:46:45<09:30,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4375, 'grad_norm': 0.9789730718140505, 'learning_rate': 1.4099190648039218e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4296/4398 [2:46:45<09:30,  5.59s/it]
 0:  98%|| 4297/4398 [2:46:51<09:25,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4358, 'grad_norm': 1.2453163722223695, 'learning_rate': 1.3824217938361861e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4297/4398 [2:46:51<09:25,  5.60s/it]
 0:  98%|| 4298/4398 [2:46:56<09:19,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4059, 'grad_norm': 1.21527515019449, 'learning_rate': 1.355194934649473e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4298/4398 [2:46:56<09:19,  5.59s/it]
 0:  98%|| 4299/4398 [2:47:02<09:17,  5.64s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4188, 'grad_norm': 1.7852409965283957, 'learning_rate': 1.3282385020095267e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4299/4398 [2:47:02<09:17,  5.64s/it]
 0:  98%|| 4300/4398 [2:47:08<09:11,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4363, 'grad_norm': 1.0595761716386536, 'learning_rate': 1.301552510535431e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4300/4398 [2:47:08<09:11,  5.63s/it]
 0:  98%|| 4301/4398 [2:47:13<09:05,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4252, 'grad_norm': 0.9137191202701851, 'learning_rate': 1.2751369746995535e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4301/4398 [2:47:13<09:05,  5.62s/it]
 0:  98%|| 4302/4398 [2:47:19<08:58,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4516, 'grad_norm': 1.0599194215760406, 'learning_rate': 1.248991908827768e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4302/4398 [2:47:19<08:58,  5.61s/it]
 0:  98%|| 4303/4398 [2:47:25<08:53,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.431, 'grad_norm': 1.0686223347664339, 'learning_rate': 1.2231173270990104e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4303/4398 [2:47:25<08:53,  5.61s/it]
 0:  98%|| 4304/4398 [2:47:30<08:46,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4498, 'grad_norm': 1.0314478765667743, 'learning_rate': 1.197513243545778e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4304/4398 [2:47:30<08:46,  5.60s/it]
 0:  98%|| 4305/4398 [2:47:36<08:41,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4505, 'grad_norm': 1.041270880006512, 'learning_rate': 1.172179672053686e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4305/4398 [2:47:36<08:41,  5.61s/it]
 0:  98%|| 4306/4398 [2:47:41<08:35,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3886, 'grad_norm': 0.9751567600681997, 'learning_rate': 1.1471166263617439e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4306/4398 [2:47:41<08:35,  5.60s/it]
 0:  98%|| 4307/4398 [2:47:47<08:29,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4489, 'grad_norm': 1.0672975393847624, 'learning_rate': 1.1223241200621904e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4307/4398 [2:47:47<08:29,  5.60s/it]
 0:  98%|| 4308/4398 [2:47:53<08:23,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4346, 'grad_norm': 1.0974316666197825, 'learning_rate': 1.0978021666005479e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4308/4398 [2:47:53<08:23,  5.60s/it]
 0:  98%|| 4309/4398 [2:47:58<08:18,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4324, 'grad_norm': 1.1328887118656397, 'learning_rate': 1.0735507792757338e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4309/4398 [2:47:58<08:18,  5.60s/it]
 0:  98%|| 4310/4398 [2:48:04<08:12,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4195, 'grad_norm': 0.9563927651074838, 'learning_rate': 1.0495699712397279e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4310/4398 [2:48:04<08:12,  5.59s/it]
 0:  98%|| 4311/4398 [2:48:09<08:06,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4101, 'grad_norm': 1.0262531326302105, 'learning_rate': 1.0258597554979599e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4311/4398 [2:48:09<08:06,  5.60s/it]
 0:  98%|| 4312/4398 [2:48:15<08:00,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4043, 'grad_norm': 1.7463238333192266, 'learning_rate': 1.0024201449089777e-08, 'epoch': 0.98}
 0: 
 0:  98%|| 4312/4398 [2:48:15<08:00,  5.59s/it]
 0:  98%|| 4313/4398 [2:48:21<07:58,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4908, 'grad_norm': 1.2795722350556074, 'learning_rate': 9.792511521846126e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4313/4398 [2:48:21<07:58,  5.63s/it]
 0:  98%|| 4314/4398 [2:48:26<07:51,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4152, 'grad_norm': 1.0987290031372254, 'learning_rate': 9.563527898899804e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4314/4398 [2:48:26<07:51,  5.62s/it]
 0:  98%|| 4315/4398 [2:48:32<07:46,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4219, 'grad_norm': 1.0731174032120734, 'learning_rate': 9.337250704433698e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4315/4398 [2:48:32<07:46,  5.61s/it]
 0:  98%|| 4316/4398 [2:48:37<07:39,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.415, 'grad_norm': 1.0702153303610444, 'learning_rate': 9.113680061162977e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4316/4398 [2:48:37<07:39,  5.61s/it]
 0:  98%|| 4317/4398 [2:48:43<07:33,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4298, 'grad_norm': 0.968933290008239, 'learning_rate': 8.8928160903351e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4317/4398 [2:48:43<07:33,  5.60s/it]
 0:  98%|| 4318/4398 [2:48:49<07:27,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4613, 'grad_norm': 1.2409823181831938, 'learning_rate': 8.674658911729805e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4318/4398 [2:48:49<07:27,  5.60s/it]
 0:  98%|| 4319/4398 [2:48:54<07:22,  5.61s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4875, 'grad_norm': 1.2296581045669945, 'learning_rate': 8.459208643659122e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4319/4398 [2:48:54<07:22,  5.61s/it]
 0:  98%|| 4320/4398 [2:49:02<08:03,  6.20s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4523, 'grad_norm': 1.067053032775491, 'learning_rate': 8.246465402966252e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4320/4398 [2:49:02<08:03,  6.20s/it]
 0:  98%|| 4321/4398 [2:49:07<07:44,  6.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4001, 'grad_norm': 0.9875159056948081, 'learning_rate': 8.036429305026683e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4321/4398 [2:49:07<07:44,  6.03s/it]
 0:  98%|| 4322/4398 [2:49:13<07:28,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3796, 'grad_norm': 0.9891398967352836, 'learning_rate': 7.829100463748185e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4322/4398 [2:49:13<07:28,  5.90s/it]
 0:  98%|| 4323/4398 [2:50:08<25:46, 20.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4575, 'grad_norm': 1.108100370782277, 'learning_rate': 7.624478991569706e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4323/4398 [2:50:08<25:46, 20.63s/it]
 0:  98%|| 4324/4398 [2:51:53<56:30, 45.82s/it]
 0:                                                      
 0: 
 0: {'loss': 0.422, 'grad_norm': 1.1689328729962525, 'learning_rate': 7.422564999462478e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4324/4398 [2:51:53<56:30, 45.82s/it]
 0:  98%|| 4325/4398 [2:52:18<48:21, 39.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.416, 'grad_norm': 1.026769620863099, 'learning_rate': 7.223358596928354e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4325/4398 [2:52:18<48:21, 39.74s/it]
 0:  98%|| 4326/4398 [2:52:24<35:23, 29.49s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4531, 'grad_norm': 1.0242014524494352, 'learning_rate': 7.02685989200258e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4326/4398 [2:52:24<35:23, 29.49s/it]
 0:  98%|| 4327/4398 [2:52:29<26:24, 22.32s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4393, 'grad_norm': 1.1771656312439867, 'learning_rate': 6.8330689912499134e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4327/4398 [2:52:29<26:24, 22.32s/it]
 0:  98%|| 4328/4398 [2:52:35<20:11, 17.30s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 1.0886270582713562, 'learning_rate': 6.641985999768507e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4328/4398 [2:52:35<20:11, 17.30s/it]
 0:  98%|| 4329/4398 [2:52:41<15:51, 13.80s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4204, 'grad_norm': 0.9824417900154399, 'learning_rate': 6.453611021186579e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4329/4398 [2:52:41<15:51, 13.80s/it]
 0:  98%|| 4330/4398 [2:52:46<12:50, 11.33s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4397, 'grad_norm': 1.0934453789617553, 'learning_rate': 6.267944157664074e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4330/4398 [2:52:46<12:50, 11.33s/it]
 0:  98%|| 4331/4398 [2:52:52<10:43,  9.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4389, 'grad_norm': 1.0117390263656778, 'learning_rate': 6.084985509892671e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4331/4398 [2:52:52<10:43,  9.60s/it]
 0:  98%|| 4332/4398 [2:52:57<09:13,  8.39s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4055, 'grad_norm': 1.53236155826397, 'learning_rate': 5.904735177095222e-09, 'epoch': 0.98}
 0: 
 0:  98%|| 4332/4398 [2:52:57<09:13,  8.39s/it]
 0:  99%|| 4333/4398 [2:53:03<08:10,  7.55s/it]
 0:                                                      
 0: 
 0: {'loss': 0.441, 'grad_norm': 1.0675587350183409, 'learning_rate': 5.727193257025199e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4333/4398 [2:53:03<08:10,  7.55s/it]
 0:  99%|| 4334/4398 [2:53:08<07:24,  6.95s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4486, 'grad_norm': 1.1421489097042814, 'learning_rate': 5.552359845968358e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4334/4398 [2:53:08<07:24,  6.95s/it]
 0:  99%|| 4335/4398 [2:53:14<06:52,  6.54s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4176, 'grad_norm': 1.0611043424193511, 'learning_rate': 5.380235038739967e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4335/4398 [2:53:14<06:52,  6.54s/it]
 0:  99%|| 4336/4398 [2:53:20<06:27,  6.25s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4371, 'grad_norm': 1.0195885529353, 'learning_rate': 5.210818928687578e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4336/4398 [2:53:20<06:27,  6.25s/it]
 0:  99%|| 4337/4398 [2:53:25<06:09,  6.05s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4499, 'grad_norm': 1.064159971705206, 'learning_rate': 5.044111607689916e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4337/4398 [2:53:25<06:09,  6.05s/it]
 0:  99%|| 4338/4398 [2:53:31<05:54,  5.90s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4315, 'grad_norm': 1.1925528110997048, 'learning_rate': 4.8801131661557756e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4338/4398 [2:53:31<05:54,  5.90s/it]
 0:  99%|| 4339/4398 [2:53:36<05:42,  5.81s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4525, 'grad_norm': 1.137518238811657, 'learning_rate': 4.7188236930251204e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4339/4398 [2:53:36<05:42,  5.81s/it]
 0:  99%|| 4340/4398 [2:53:42<05:32,  5.74s/it]
 0:                                                      
 0: 
 0: {'loss': 0.437, 'grad_norm': 0.9619805377102234, 'learning_rate': 4.560243275769094e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4340/4398 [2:53:42<05:32,  5.74s/it]
 0:  99%|| 4341/4398 [2:53:47<05:24,  5.69s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4159, 'grad_norm': 1.0331975448164155, 'learning_rate': 4.404372000390011e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4341/4398 [2:53:47<05:24,  5.69s/it]
 0:  99%|| 4342/4398 [2:53:53<05:16,  5.65s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4468, 'grad_norm': 1.209851443735207, 'learning_rate': 4.2512099514196994e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4342/4398 [2:53:53<05:16,  5.65s/it]
 0:  99%|| 4343/4398 [2:53:59<05:09,  5.63s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4392, 'grad_norm': 1.0728097769457872, 'learning_rate': 4.1007572119222685e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4343/4398 [2:53:59<05:09,  5.63s/it]
 0:  99%|| 4344/4398 [2:54:04<05:03,  5.62s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4053, 'grad_norm': 1.0942455386491694, 'learning_rate': 3.953013863490784e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4344/4398 [2:54:04<05:03,  5.62s/it]
 0:  99%|| 4345/4398 [2:54:10<04:57,  5.60s/it]
 0:                                                      
 0: 
 0: {'loss': 0.454, 'grad_norm': 1.0615685503519585, 'learning_rate': 3.807979986250043e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4345/4398 [2:54:10<04:57,  5.60s/it]
 0:  99%|| 4346/4398 [2:54:15<04:50,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4671, 'grad_norm': 1.1969623640287776, 'learning_rate': 3.6656556588560155e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4346/4398 [2:54:15<04:50,  5.59s/it]
 0:  99%|| 4347/4398 [2:54:21<04:45,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4351, 'grad_norm': 1.066567993736618, 'learning_rate': 3.526040958494181e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4347/4398 [2:54:21<04:45,  5.59s/it]
 0:  99%|| 4348/4398 [2:54:26<04:39,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4414, 'grad_norm': 1.101624795227344, 'learning_rate': 3.3891359608800854e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4348/4398 [2:54:26<04:39,  5.59s/it]
 0:  99%|| 4349/4398 [2:54:32<04:33,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4282, 'grad_norm': 1.1180100069822374, 'learning_rate': 3.2549407402610035e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4349/4398 [2:54:32<04:33,  5.58s/it]
 0:  99%|| 4350/4398 [2:54:38<04:27,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.444, 'grad_norm': 1.2477664920289073, 'learning_rate': 3.1234553694137193e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4350/4398 [2:54:38<04:27,  5.58s/it]
 0:  99%|| 4351/4398 [2:54:43<04:22,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4525, 'grad_norm': 1.1202837481657923, 'learning_rate': 2.994679919646748e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4351/4398 [2:54:43<04:22,  5.58s/it]
 0:  99%|| 4352/4398 [2:54:49<04:16,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3971, 'grad_norm': 0.9543661852288453, 'learning_rate': 2.8686144607970035e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4352/4398 [2:54:49<04:16,  5.58s/it]
 0:  99%|| 4353/4398 [2:54:54<04:11,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4669, 'grad_norm': 1.221372074469495, 'learning_rate': 2.745259061232575e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4353/4398 [2:54:54<04:11,  5.59s/it]
 0:  99%|| 4354/4398 [2:55:00<04:05,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4199, 'grad_norm': 1.2448896577133526, 'learning_rate': 2.6246137878527254e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4354/4398 [2:55:00<04:05,  5.59s/it]
 0:  99%|| 4355/4398 [2:55:06<04:00,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4577, 'grad_norm': 1.127874510416397, 'learning_rate': 2.5066787060862298e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4355/4398 [2:55:06<04:00,  5.59s/it]
 0:  99%|| 4356/4398 [2:55:11<03:54,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4525, 'grad_norm': 1.0668423377406444, 'learning_rate': 2.391453879890815e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4356/4398 [2:55:11<03:54,  5.59s/it]
 0:  99%|| 4357/4398 [2:55:17<03:49,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4455, 'grad_norm': 1.073984937768653, 'learning_rate': 2.2789393717564943e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4357/4398 [2:55:17<03:49,  5.59s/it]
 0:  99%|| 4358/4398 [2:55:22<03:43,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4028, 'grad_norm': 1.0347340580420819, 'learning_rate': 2.1691352427027913e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4358/4398 [2:55:22<03:43,  5.59s/it]
 0:  99%|| 4359/4398 [2:55:28<03:38,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4227, 'grad_norm': 1.0526016668716525, 'learning_rate': 2.062041552277627e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4359/4398 [2:55:28<03:38,  5.59s/it]
 0:  99%|| 4360/4398 [2:55:33<03:32,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4261, 'grad_norm': 1.1657425725966326, 'learning_rate': 1.9576583585617647e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4360/4398 [2:55:34<03:32,  5.59s/it]
 0:  99%|| 4361/4398 [2:55:39<03:26,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4358, 'grad_norm': 1.118708182969226, 'learning_rate': 1.855985718164366e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4361/4398 [2:55:39<03:26,  5.58s/it]
 0:  99%|| 4362/4398 [2:55:45<03:20,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4236, 'grad_norm': 1.0973897714701948, 'learning_rate': 1.757023686224102e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4362/4398 [2:55:45<03:20,  5.58s/it]
 0:  99%|| 4363/4398 [2:55:50<03:15,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.462, 'grad_norm': 0.9623540176720867, 'learning_rate': 1.6607723164108191e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4363/4398 [2:55:50<03:15,  5.58s/it]
 0:  99%|| 4364/4398 [2:55:56<03:09,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4225, 'grad_norm': 1.3901284288046711, 'learning_rate': 1.5672316609238736e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4364/4398 [2:55:56<03:09,  5.58s/it]
 0:  99%|| 4365/4398 [2:56:01<03:04,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4575, 'grad_norm': 1.0397125258803765, 'learning_rate': 1.4764017704926857e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4365/4398 [2:56:01<03:04,  5.58s/it]
 0:  99%|| 4366/4398 [2:56:07<02:58,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4435, 'grad_norm': 1.0603121308530794, 'learning_rate': 1.3882826943761863e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4366/4398 [2:56:07<02:58,  5.59s/it]
 0:  99%|| 4367/4398 [2:56:13<02:53,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4397, 'grad_norm': 1.0446356404659367, 'learning_rate': 1.3028744803628146e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4367/4398 [2:56:13<02:53,  5.59s/it]
 0:  99%|| 4368/4398 [2:56:18<02:47,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4106, 'grad_norm': 1.203370472385933, 'learning_rate': 1.220177174772741e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4368/4398 [2:56:18<02:47,  5.59s/it]
 0:  99%|| 4369/4398 [2:56:24<02:42,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4565, 'grad_norm': 1.1392475738565693, 'learning_rate': 1.1401908224534242e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4369/4398 [2:56:24<02:42,  5.59s/it]
 0:  99%|| 4370/4398 [2:56:29<02:36,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4494, 'grad_norm': 1.1176458180743303, 'learning_rate': 1.0629154667840535e-09, 'epoch': 0.99}
 0: 
 0:  99%|| 4370/4398 [2:56:29<02:36,  5.58s/it]
 0:  99%|| 4371/4398 [2:56:35<02:30,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4745, 'grad_norm': 1.0613744528412232, 'learning_rate': 9.883511496722176e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4371/4398 [2:56:35<02:30,  5.59s/it]
 0:  99%|| 4372/4398 [2:56:41<02:25,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4294, 'grad_norm': 0.9945293155367787, 'learning_rate': 9.164979115561245e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4372/4398 [2:56:41<02:25,  5.59s/it]
 0:  99%|| 4373/4398 [2:56:46<02:19,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4405, 'grad_norm': 1.1272395159231874, 'learning_rate': 8.473557914034925e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4373/4398 [2:56:46<02:19,  5.58s/it]
 0: wandb: WARNING Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
 0:  99%|| 4374/4398 [2:56:52<02:13,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4413, 'grad_norm': 1.1743179764059435, 'learning_rate': 7.809248267121039e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4374/4398 [2:56:52<02:13,  5.58s/it]
 0:  99%|| 4375/4398 [2:56:57<02:08,  5.59s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4219, 'grad_norm': 0.9296644342200716, 'learning_rate': 7.172050535081409e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4375/4398 [2:56:57<02:08,  5.59s/it]
 0:  99%|| 4376/4398 [2:57:03<02:02,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4304, 'grad_norm': 1.32821865648531, 'learning_rate': 6.561965063489606e-10, 'epoch': 0.99}
 0: 
 0:  99%|| 4376/4398 [2:57:03<02:02,  5.58s/it]
 0: 100%|| 4377/4398 [2:57:08<01:57,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4302, 'grad_norm': 2.349326192810805, 'learning_rate': 5.978992183203192e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4377/4398 [2:57:08<01:57,  5.58s/it]
 0: 100%|| 4378/4398 [2:57:14<01:51,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4359, 'grad_norm': 1.035583805647346, 'learning_rate': 5.423132210385928e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4378/4398 [2:57:14<01:51,  5.58s/it]
 0: 100%|| 4379/4398 [2:57:20<01:46,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4426, 'grad_norm': 1.0954859156649812, 'learning_rate': 4.894385446491123e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4379/4398 [2:57:20<01:46,  5.58s/it]
 0: 100%|| 4380/4398 [2:57:25<01:40,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4229, 'grad_norm': 1.447369889451147, 'learning_rate': 4.392752178278281e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4380/4398 [2:57:25<01:40,  5.57s/it]
 0: 100%|| 4381/4398 [2:57:31<01:34,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4127, 'grad_norm': 1.1902204822118385, 'learning_rate': 3.9182326777853495e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4381/4398 [2:57:31<01:34,  5.58s/it]
 0: 100%|| 4382/4398 [2:57:36<01:29,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4498, 'grad_norm': 1.6207974633110258, 'learning_rate': 3.470827202356475e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4382/4398 [2:57:36<01:29,  5.58s/it]
 0: 100%|| 4383/4398 [2:57:42<01:23,  5.58s/it]
 0:                                                      
 0: 
 0: {'loss': 0.464, 'grad_norm': 1.2484145239624749, 'learning_rate': 3.050535994630899e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4383/4398 [2:57:42<01:23,  5.58s/it]
 0: 100%|| 4384/4398 [2:57:47<01:17,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4449, 'grad_norm': 1.0581064787344105, 'learning_rate': 2.6573592825429593e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4384/4398 [2:57:47<01:17,  5.57s/it]
 0: 100%|| 4385/4398 [2:57:53<01:12,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4661, 'grad_norm': 1.2081476307226509, 'learning_rate': 2.2912972793220912e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4385/4398 [2:57:53<01:12,  5.57s/it]
 0: 100%|| 4386/4398 [2:57:59<01:06,  5.57s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4218, 'grad_norm': 1.0393157886698954, 'learning_rate': 1.9523501834928239e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4386/4398 [2:57:59<01:06,  5.57s/it]
 0: 100%|| 4387/4398 [2:58:04<01:01,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4172, 'grad_norm': 0.9937560305559543, 'learning_rate': 1.6405181788692327e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4387/4398 [2:58:04<01:01,  5.56s/it]
 0: 100%|| 4388/4398 [2:58:10<00:55,  5.56s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4259, 'grad_norm': 1.0008889746437808, 'learning_rate': 1.355801434571591e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4388/4398 [2:58:10<00:55,  5.56s/it]
 0: 100%|| 4389/4398 [2:58:17<00:55,  6.18s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4023, 'grad_norm': 1.1320751648064948, 'learning_rate': 1.0982001050041657e-10, 'epoch': 1.0}
 0: 
 0: 100%|| 4389/4398 [2:58:17<00:55,  6.18s/it]
 0: 100%|| 4390/4398 [2:58:23<00:47,  6.00s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4084, 'grad_norm': 1.0839905855057625, 'learning_rate': 8.677143298718715e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4390/4398 [2:58:23<00:47,  6.00s/it]
 0: 100%|| 4391/4398 [2:59:02<01:52, 16.05s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4321, 'grad_norm': 1.1090849411733914, 'learning_rate': 6.643442341691675e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4391/4398 [2:59:02<01:52, 16.05s/it]
 0: 100%|| 4392/4398 [3:01:02<04:43, 47.18s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3928, 'grad_norm': 1.1680428802545495, 'learning_rate': 4.8808992819116e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4392/4398 [3:01:02<04:43, 47.18s/it]
 0: 100%|| 4393/4398 [3:01:42<03:44, 44.83s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4286, 'grad_norm': 1.4104132100294156, 'learning_rate': 3.389515075225003e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4393/4398 [3:01:42<03:44, 44.83s/it]
 0: 100%|| 4394/4398 [3:01:47<02:12, 33.04s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4542, 'grad_norm': 1.088678160513243, 'learning_rate': 2.169290530484869e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4394/4398 [3:01:47<02:12, 33.04s/it]
 0: 100%|| 4395/4398 [3:01:53<01:14, 24.79s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4405, 'grad_norm': 1.1175676419624367, 'learning_rate': 1.2202263094396317e-11, 'epoch': 1.0}
 0: 
 0: 100%|| 4395/4398 [3:01:53<01:14, 24.79s/it]
 0: 100%|| 4396/4398 [3:01:58<00:38, 19.03s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4182, 'grad_norm': 1.0458589992079115, 'learning_rate': 5.423229267331742e-12, 'epoch': 1.0}
 0: 
 0: 100%|| 4396/4398 [3:01:58<00:38, 19.03s/it]
 0: 100%|| 4397/4398 [3:02:04<00:14, 14.99s/it]
 0:                                                      
 0: 
 0: {'loss': 0.3958, 'grad_norm': 0.9965691461324194, 'learning_rate': 1.355807500713624e-12, 'epoch': 1.0}
 0: 
 0: 100%|| 4397/4398 [3:02:04<00:14, 14.99s/it]
 0: 100%|| 4398/4398 [3:02:09<00:00, 12.15s/it]
 0:                                                      
 0: 
 0: {'loss': 0.4103, 'grad_norm': 1.0159183178400095, 'learning_rate': 0.0, 'epoch': 1.0}
 0: 
 0: 100%|| 4398/4398 [3:02:09<00:00, 12.15s/it]
 0:                                                      
 0: 
 0: {'train_runtime': 10931.7794, 'train_samples_per_second': 103.012, 'train_steps_per_second': 0.402, 'train_loss': 0.13814298585633247, 'epoch': 1.0}
 0: 
 0: 100%|| 4398/4398 [3:02:09<00:00, 12.15s/it]
 0: 100%|| 4398/4398 [3:02:09<00:00,  2.49s/it]
 0: 
 0: Rank 0: 
 0:  
 0: Only save projectors: False
 0: 
 0: Rank 0: 
 0:  
 0: Model saved to /capstor/scratch/cscs/ndeperr/checkpoints/radvlm-sft
 0: 
 0: [1;34mwandb[0m: 
 0: [1;34mwandb[0m:  View run [33mradvlm-sft[0m at: [34mhttps://wandb.ai/krauthammerlab/huggingface/runs/5zockb19[0m
 0: [1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250624_192900-5zockb19/logs[0m
