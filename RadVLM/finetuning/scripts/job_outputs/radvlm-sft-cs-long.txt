+ cat /var/spool/slurmd/job546602/slurm_script
#!/bin/bash
#SBATCH -A a135
#SBATCH --job-name=radvlm-sft-cs-long  # Job name
#SBATCH --nodes=4    # Number of nodes
#SBATCH --ntasks-per-node=1                  # Number of tasks per node (1 process per node)
#SBATCH --gpus-per-task=4                  # Number of GPUs per ta
#SBATCH --time=06:00:00                      # Time limit
#SBATCH --output=job_outputs/%x.txt    # Standard output and error log
#SBATCH --mem=460000
#SBATCH --partition=normal  

# Initialization
set -x
cat $0
export MASTER_PORT=29500
#export MASTER_ADDR=$(hostname)
export MASTER_ADDR=$(scontrol show hostname | head -n 1)
export HF_HOME=$SCRATCH/huggingface_home

PROMPT_VERSION="qwen_1_5"

RUN_NAME=${SLURM_JOB_NAME}
echo "RUN_NAME: ${RUN_NAME}"
CKPT_PATH="lmms-lab/llava-onevision-qwen2-7b-si" # this could also be the previous stage checkpoint

NUM_EPOCHS=1
LR=1e-5
SAVE_STEPS=200

WORKDIR="$SCRATCH/code/RadVLM"

# Run main script
srun -ul  --environment=llava_env_clariden bash -c "
  pip install accelerate==0.28.0
  cd $WORKDIR  # Change cwd and run the main training script.
  export PYTHONPATH=$WORKDIR/finetuning
  export WANDB_API_KEY=81291d9e2d99efeb2a4e3f4d507abe879e646a22
  TORCHRUN_ARGS=\"
   --node-rank=\${SLURM_PROCID} \
   --master-addr=\${MASTER_ADDR} \
   --master-port=\${MASTER_PORT} \
   --nnodes=\${SLURM_NNODES} \
   --nproc-per-node=4 \
  \"

 ACCELERATE_CPU_AFFINITY=1  torchrun \${TORCHRUN_ARGS} finetuning/llava/train/train_mem.py \
    --deepspeed finetuning/scripts/zero3.json \
    --model_name_or_path $CKPT_PATH \
    --version ${PROMPT_VERSION} \
    --data_path radvlm/data/llava_datasets/cold_start_long.json \
    --image_folder . \
    --mm_tunable_parts="mm_vision_tower,mm_mlp_adapter,mm_language_model" \
    --mm_vision_tower_lr=2e-6 \
    --vision_tower google/siglip-so400m-patch14-384 \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --group_by_modality_length True \
   --image_aspect_ratio anyres_max_9 \
    --image_grid_pinpoints \"(1x1),...,(6x6)\" \
    --mm_patch_merge_type spatial_unpad \
    --bf16 True \
    --run_name $RUN_NAME \
    --output_dir="$SCRATCH/checkpoints/${RUN_NAME}" \
    --num_train_epochs $NUM_EPOCHS \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 2 \
    --evaluation_strategy \"no\" \
    --save_strategy \"steps\" \
    --save_steps $SAVE_STEPS \
    --save_total_limit 1 \
    --learning_rate $LR \
    --warmup_ratio 0.03 \
    --lr_scheduler_type \"cosine\" \
    --weight_decay 0. \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 32768 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb \
    --torch_compile True \
    --torch_compile_backend \"inductor\" \
    --dataloader_drop_last True \
    --frames_upbound 32
"


+ export MASTER_PORT=29500
+ MASTER_PORT=29500
++ scontrol show hostname
++ head -n 1
+ export MASTER_ADDR=nid005687
+ MASTER_ADDR=nid005687
+ export HF_HOME=/capstor/scratch/cscs/ndeperr/huggingface_home
+ HF_HOME=/capstor/scratch/cscs/ndeperr/huggingface_home
+ PROMPT_VERSION=qwen_1_5
+ RUN_NAME=radvlm-sft-cs-long
+ echo 'RUN_NAME: radvlm-sft-cs-long'
RUN_NAME: radvlm-sft-cs-long
+ CKPT_PATH=lmms-lab/llava-onevision-qwen2-7b-si
+ NUM_EPOCHS=1
+ LR=1e-5
+ SAVE_STEPS=200
+ WORKDIR=/capstor/scratch/cscs/ndeperr/code/RadVLM
+ srun -ul --environment=llava_env_clariden bash -c '
  pip install accelerate==0.28.0
  cd /capstor/scratch/cscs/ndeperr/code/RadVLM  # Change cwd and run the main training script.
  export PYTHONPATH=/capstor/scratch/cscs/ndeperr/code/RadVLM/finetuning
  export WANDB_API_KEY=81291d9e2d99efeb2a4e3f4d507abe879e646a22
  TORCHRUN_ARGS="
   --node-rank=${SLURM_PROCID}    --master-addr=${MASTER_ADDR}    --master-port=${MASTER_PORT}    --nnodes=${SLURM_NNODES}    --nproc-per-node=4   "

 ACCELERATE_CPU_AFFINITY=1  torchrun ${TORCHRUN_ARGS} finetuning/llava/train/train_mem.py     --deepspeed finetuning/scripts/zero3.json     --model_name_or_path lmms-lab/llava-onevision-qwen2-7b-si     --version qwen_1_5     --data_path radvlm/data/llava_datasets/cold_start_long.json     --image_folder .     --mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model     --mm_vision_tower_lr=2e-6     --vision_tower google/siglip-so400m-patch14-384     --mm_projector_type mlp2x_gelu     --mm_vision_select_layer -2     --mm_use_im_start_end False     --mm_use_im_patch_token False     --group_by_modality_length True    --image_aspect_ratio anyres_max_9     --image_grid_pinpoints "(1x1),...,(6x6)"     --mm_patch_merge_type spatial_unpad     --bf16 True     --run_name radvlm-sft-cs-long     --output_dir=/capstor/scratch/cscs/ndeperr/checkpoints/radvlm-sft-cs-long     --num_train_epochs 1     --per_device_train_batch_size 1     --per_device_eval_batch_size 2     --gradient_accumulation_steps 2     --evaluation_strategy "no"     --save_strategy "steps"     --save_steps 200     --save_total_limit 1     --learning_rate 1e-5     --warmup_ratio 0.03     --lr_scheduler_type "cosine"     --weight_decay 0.     --logging_steps 1     --tf32 True     --model_max_length 32768     --gradient_checkpointing True     --dataloader_num_workers 4     --lazy_preprocess True     --report_to wandb     --torch_compile True     --torch_compile_backend "inductor"     --dataloader_drop_last True     --frames_upbound 32
'
3: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
2: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
1: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
0: Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
1: Collecting accelerate==0.28.0
2: Collecting accelerate==0.28.0
3: Collecting accelerate==0.28.0
1:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
1: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
1: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
1: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
1: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
1: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
1: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
1: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
2:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
2: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
2: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
2: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
2: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
2: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
2: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
2: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
3:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
3: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
3: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
3: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
3: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
3: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
3: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
3: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
1: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
3: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
1: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
2: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
3: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
1: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
2: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
3: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
1: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
2: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
3: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
1: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
2: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
3: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
1: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
2: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
3: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
2: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
1: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
2: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
1: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
3: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
2: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
3: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
1: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
2: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
3: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
1: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
3: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
2: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
1: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
3: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
2: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
1: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
3: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
2: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
1: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
3: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
2: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
1: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
3: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
2: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
1: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
1: [?25l
1:    [38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
2: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
2: [?25l
2:    [38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
3: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
3: [?25l
3:    [38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
1: [2K   [38;5;70mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m290.1/290.1 kB[0m [31m9.4 MB/s[0m eta [36m0:00:00[0m
1: [?25h
2: [2K   [38;5;70mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m290.1/290.1 kB[0m [31m8.7 MB/s[0m eta [36m0:00:00[0m
2: [?25h
3: [2K   [38;5;70mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m290.1/290.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
3: [?25h
0: Collecting accelerate==0.28.0
0:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)
0: Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (1.26.1)
0: Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (24.0)
0: Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.8)
0: Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (6.0.1)
0: Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (2.4.0a0+3bcc3cddb5.nv24.7)
0: Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.31.4)
0: Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (0.5.3)
0: Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.15.4)
0: Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (4.13.2)
0: Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (1.14.0)
0: Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.3)
0: Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.4)
0: Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.28.0) (2023.10.0)
0: Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (2.32.3)
0: Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)
0: Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)
0: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)
0: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.7)
0: Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (1.26.20)
0: Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.7.4)
0: Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)
0: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)
0: [?25l
0:    [38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/290.1 kB[0m [31m?[0m eta [36m-:--:--[0m
0: [2K   [38;5;70mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m290.1/290.1 kB[0m [31m9.1 MB/s[0m eta [36m0:00:00[0m
0: [?25h
3: Installing collected packages: accelerate
2: Installing collected packages: accelerate
2:   Attempting uninstall: accelerate
1: Installing collected packages: accelerate
1:   Attempting uninstall: accelerate
3:   Attempting uninstall: accelerate
2:     Found existing installation: accelerate 1.7.0
0: Installing collected packages: accelerate
3:     Found existing installation: accelerate 1.7.0
1:     Found existing installation: accelerate 1.7.0
0:   Attempting uninstall: accelerate
0:     Found existing installation: accelerate 1.7.0
3:     Uninstalling accelerate-1.7.0:
2:     Uninstalling accelerate-1.7.0:
1:     Uninstalling accelerate-1.7.0:
0:     Uninstalling accelerate-1.7.0:
3:       Successfully uninstalled accelerate-1.7.0
2:       Successfully uninstalled accelerate-1.7.0
1:       Successfully uninstalled accelerate-1.7.0
0:       Successfully uninstalled accelerate-1.7.0
2: Successfully installed accelerate-0.28.0
1: Successfully installed accelerate-0.28.0
3: Successfully installed accelerate-0.28.0
0: Successfully installed accelerate-0.28.0
1: 
1: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
1: [notice] To update, run: python -m pip install --upgrade pip
2: 
2: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
2: [notice] To update, run: python -m pip install --upgrade pip
0: 
0: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
0: [notice] To update, run: python -m pip install --upgrade pip
3: 
3: [notice] A new release of pip is available: 24.1.2 -> 25.1.1
3: [notice] To update, run: python -m pip install --upgrade pip
1: W0709 22:52:24.287000 70369047480416 torch/distributed/run.py:778] 
1: W0709 22:52:24.287000 70369047480416 torch/distributed/run.py:778] *****************************************
1: W0709 22:52:24.287000 70369047480416 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
1: W0709 22:52:24.287000 70369047480416 torch/distributed/run.py:778] *****************************************
2: W0709 22:52:24.288000 70368793528416 torch/distributed/run.py:778] 
2: W0709 22:52:24.288000 70368793528416 torch/distributed/run.py:778] *****************************************
2: W0709 22:52:24.288000 70368793528416 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
2: W0709 22:52:24.288000 70368793528416 torch/distributed/run.py:778] *****************************************
3: W0709 22:52:24.293000 70368792152160 torch/distributed/run.py:778] 
3: W0709 22:52:24.293000 70368792152160 torch/distributed/run.py:778] *****************************************
3: W0709 22:52:24.293000 70368792152160 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
3: W0709 22:52:24.293000 70368792152160 torch/distributed/run.py:778] *****************************************
0: W0709 22:52:24.478000 70369272006752 torch/distributed/run.py:778] 
0: W0709 22:52:24.478000 70369272006752 torch/distributed/run.py:778] *****************************************
0: W0709 22:52:24.478000 70369272006752 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
0: W0709 22:52:24.478000 70369272006752 torch/distributed/run.py:778] *****************************************
3: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
3: 
3: 
3: 
1: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
1: 
1: 
1: 
2: Please install pyav to use video processing functions.Please install pyav to use video processing functions.Please install pyav to use video processing functions.
2: 
2: Please install pyav to use video processing functions.
2: 
0: Please install pyav to use video processing functions.Please install pyav to use video processing functions.
0: 
0: Please install pyav to use video processing functions.
0: Please install pyav to use video processing functions.
1: [2025-07-09 22:52:40,692] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-09 22:52:40,700] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-09 22:52:40,711] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-09 22:52:40,727] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-09 22:52:40,735] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-09 22:52:40,746] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: df: /users/ndeperr/.triton/autotune
1: : No such file or directory
1: [2025-07-09 22:52:40,763] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-09 22:52:40,771] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-09 22:52:40,781] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-09 22:52:40,799] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: df: /users/ndeperr/.triton/autotune
2: : No such file or directory
3: [2025-07-09 22:52:40,805] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: df: /users/ndeperr/.triton/autotune
3: : No such file or directory
2: [2025-07-09 22:52:40,818] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
2: 
2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
2: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
3: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
3: 
3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
1: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
2: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
2: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
2: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
3: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
3: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
3: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: [2025-07-09 22:52:41,449] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
1: 
1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: [2025-07-09 22:52:41,485] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-09 22:52:41,522] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-09 22:52:41,558] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
1: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
1: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
0: df: /users/ndeperr/.triton/autotune
0: : No such file or directory
0: df: /users/ndeperr/.triton/autotune
0: : No such file or directory
3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
3:   def forward(ctx, input, weight, bias=None):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
3:   def backward(ctx, grad_output):
1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
3:   def forward(ctx, input, weight, bias=None):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
3:   def backward(ctx, grad_output):
2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
2: 
2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
2: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
2: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
1:   def forward(ctx, input, weight, bias=None):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
1:   def backward(ctx, grad_output):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
1:   def forward(ctx, input, weight, bias=None):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
1:   def backward(ctx, grad_output):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
2:   def forward(ctx, input, weight, bias=None):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
2:   def backward(ctx, grad_output):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
2:   def forward(ctx, input, weight, bias=None):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
2:   def backward(ctx, grad_output):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
2:   def forward(ctx, input, weight, bias=None):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
2:   def backward(ctx, grad_output):
3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
2:   def forward(ctx, input, weight, bias=None):
2: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
2:   def backward(ctx, grad_output):
1: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
1: 
1: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
3: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
3: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
1:   def forward(ctx, input, weight, bias=None):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
1:   def backward(ctx, grad_output):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
1:   def forward(ctx, input, weight, bias=None):
1: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
1:   def backward(ctx, grad_output):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
3:   def forward(ctx, input, weight, bias=None):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
3:   def backward(ctx, grad_output):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
3:   def forward(ctx, input, weight, bias=None):
3: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
3:   def backward(ctx, grad_output):
0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
0: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
0: 
0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
0: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
0: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
0: 
0: 
0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: 
0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
0: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
0: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
0: [93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
0:   def forward(ctx, input, weight, bias=None):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
0:   def forward(ctx, input, weight, bias=None):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
0:   def backward(ctx, grad_output):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
0:   def backward(ctx, grad_output):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
0:   def forward(ctx, input, weight, bias=None):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
0:   def backward(ctx, grad_output):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
0:   def forward(ctx, input, weight, bias=None):
0: /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
0:   def backward(ctx, grad_output):
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
1: [2025-07-09 22:52:44,334] [INFO] [comm.py:637:init_distributed] cdb=None
3: [2025-07-09 22:52:44,336] [INFO] [comm.py:637:init_distributed] cdb=None
1: [2025-07-09 22:52:44,336] [INFO] [comm.py:637:init_distributed] cdb=None
3: [2025-07-09 22:52:44,337] [INFO] [comm.py:637:init_distributed] cdb=None
2: [2025-07-09 22:52:44,337] [INFO] [comm.py:637:init_distributed] cdb=None
2: [2025-07-09 22:52:44,337] [INFO] [comm.py:637:init_distributed] cdb=None
2: [2025-07-09 22:52:44,338] [INFO] [comm.py:637:init_distributed] cdb=None
2: [2025-07-09 22:52:44,338] [INFO] [comm.py:637:init_distributed] cdb=None
1: [2025-07-09 22:52:44,338] [INFO] [comm.py:637:init_distributed] cdb=None
1: [2025-07-09 22:52:44,340] [INFO] [comm.py:637:init_distributed] cdb=None
3: [2025-07-09 22:52:44,341] [INFO] [comm.py:637:init_distributed] cdb=None
3: [2025-07-09 22:52:44,363] [INFO] [comm.py:637:init_distributed] cdb=None
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
3:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
1:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
2:   warnings.warn(
2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
2:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
1:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
1:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
3:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
1:   warnings.warn(
1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
1: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
1:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
3:   warnings.warn(
3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
2:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
3:   warnings.warn(
2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
3: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
2:   warnings.warn(
2: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
3: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
3:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
2:   warnings.warn(
0: [2025-07-09 22:52:45,574] [INFO] [comm.py:637:init_distributed] cdb=None
0: [2025-07-09 22:52:45,574] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
0: [2025-07-09 22:52:45,575] [INFO] [comm.py:637:init_distributed] cdb=None
0: [2025-07-09 22:52:45,575] [INFO] [comm.py:637:init_distributed] cdb=None
0: [2025-07-09 22:52:45,635] [INFO] [comm.py:637:init_distributed] cdb=None
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
0:   warnings.warn(
0: Rank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}
0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: nid005687:269560:269560 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid005687:269560:269560 [0] NCCL INFO Bootstrap : Using hsn0:172.28.20.68<0>
0: nid005687:269560:269560 [0] NCCL INFO cudaDriverVersion 12050
0: nid005687:269560:269560 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
2: nid005692:108973:108973 [0] NCCL INFO cudaDriverVersion 12050
2: nid005692:108973:108973 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid005694:125983:125983 [3] NCCL INFO cudaDriverVersion 12050
2: nid005692:108973:108973 [0] NCCL INFO Bootstrap : Using hsn0:172.28.20.88<0>
2: nid005692:108973:108973 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
3: nid005694:125983:125983 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid005694:125983:125983 [3] NCCL INFO Bootstrap : Using hsn0:172.28.20.96<0>
3: nid005694:125983:125983 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
1: nid005689:27135:27135 [3] NCCL INFO cudaDriverVersion 12050
1: nid005689:27135:27135 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid005692:108976:108976 [3] NCCL INFO cudaDriverVersion 12050
2: nid005692:108976:108976 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid005689:27135:27135 [3] NCCL INFO Bootstrap : Using hsn0:172.28.20.76<0>
1: nid005689:27135:27135 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
2: nid005692:108976:108976 [3] NCCL INFO Bootstrap : Using hsn0:172.28.20.88<0>
2: nid005692:108976:108976 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
2: nid005692:108975:108975 [2] NCCL INFO cudaDriverVersion 12050
2: nid005692:108974:108974 [1] NCCL INFO cudaDriverVersion 12050
1: nid005689:27133:27133 [1] NCCL INFO cudaDriverVersion 12050
1: nid005689:27134:27134 [2] NCCL INFO cudaDriverVersion 12050
1: nid005689:27132:27132 [0] NCCL INFO cudaDriverVersion 12050
3: nid005694:125982:125982 [2] NCCL INFO cudaDriverVersion 12050
3: nid005694:125981:125981 [1] NCCL INFO cudaDriverVersion 12050
3: nid005694:125980:125980 [0] NCCL INFO cudaDriverVersion 12050
3: nid005694:125982:125982 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid005694:125982:125982 [2] NCCL INFO Bootstrap : Using hsn0:172.28.20.96<0>
3: nid005694:125982:125982 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
2: nid005692:108975:108975 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid005692:108975:108975 [2] NCCL INFO Bootstrap : Using hsn0:172.28.20.88<0>
2: nid005692:108975:108975 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
1: nid005689:27133:27133 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid005689:27133:27133 [1] NCCL INFO Bootstrap : Using hsn0:172.28.20.76<0>
1: nid005689:27133:27133 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
3: nid005694:125980:125980 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid005694:125980:125980 [0] NCCL INFO Bootstrap : Using hsn0:172.28.20.96<0>
3: nid005694:125980:125980 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
2: nid005692:108974:108974 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid005692:108974:108974 [1] NCCL INFO Bootstrap : Using hsn0:172.28.20.88<0>
2: nid005692:108974:108974 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
1: nid005689:27134:27134 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid005689:27134:27134 [2] NCCL INFO Bootstrap : Using hsn0:172.28.20.76<0>
1: nid005689:27134:27134 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
3: nid005694:125981:125981 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid005694:125981:125981 [1] NCCL INFO Bootstrap : Using hsn0:172.28.20.96<0>
3: nid005694:125981:125981 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
1: nid005689:27132:27132 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid005689:27132:27132 [0] NCCL INFO Bootstrap : Using hsn0:172.28.20.76<0>
1: nid005689:27132:27132 [0] NCCL INFO NCCL version 2.22.3+cuda12.5
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
0:   warnings.warn(
0: nid005687:269560:270017 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269560:270017 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid005687:269560:270017 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
3: nid005694:125980:126459 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125980:126459 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid005694:125980:126459 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
3: nid005694:125983:126457 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125983:126457 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid005694:125983:126457 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid005694:125982:126458 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid005694:125982:126458 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid005694:125982:126458 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: 
0: nid005687:269560:270017 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: 
3: nid005694:125980:126459 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Creating one domain per process
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: 
3: nid005694:125983:126457 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: 
3: nid005694:125982:126458 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid005694:125981:126460 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125981:126460 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid005694:125981:126460 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
1: nid005689:27135:27614 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27135:27614 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid005689:27135:27614 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Support for global registrations: false
0: nid005687:269560:270017 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Creating one domain per process
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid005687:269560:270017 [0] NCCL INFO Using network AWS Libfabric
1: nid005689:27134:27616 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27132:27617 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27134:27616 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid005689:27134:27616 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid005689:27132:27617 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid005689:27132:27617 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
1: nid005689:27133:27615 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27133:27615 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid005689:27133:27615 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
1: 
1: nid005689:27135:27614 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Support for global registrations: false
3: nid005694:125980:126459 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: 
3: nid005694:125981:126460 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
1: 
1: nid005689:27132:27617 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
1: 
1: nid005689:27134:27616 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Creating one domain per process
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Creating one domain per process
1: 
1: nid005689:27133:27615 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid005694:125980:126459 [0] NCCL INFO Using network AWS Libfabric
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Creating one domain per process
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Creating one domain per process
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Support for global registrations: false
3: nid005694:125982:126458 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Support for global registrations: false
3: nid005694:125983:126457 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Support for global registrations: false
3: nid005694:125981:126460 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Creating one domain per process
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Creating one domain per process
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Creating one domain per process
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid005694:125982:126458 [2] NCCL INFO Using network AWS Libfabric
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid005694:125983:126457 [3] NCCL INFO Using network AWS Libfabric
3: nid005694:125981:126460 [1] NCCL INFO Using network AWS Libfabric
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Support for global registrations: false
1: nid005689:27135:27614 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27135:27614 [3] NCCL INFO Using network AWS Libfabric
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Support for global registrations: false
1: nid005689:27134:27616 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Support for global registrations: false
1: nid005689:27133:27615 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Support for global registrations: false
1: nid005689:27132:27617 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid005689:27134:27616 [2] NCCL INFO Using network AWS Libfabric
1: nid005689:27133:27615 [1] NCCL INFO Using network AWS Libfabric
1: nid005689:27132:27617 [0] NCCL INFO Using network AWS Libfabric
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
0:   warnings.warn(
0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
0:   warnings.warn(
0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
2: nid005692:108976:109411 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108976:109411 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid005692:108976:109411 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
2: nid005692:108973:109410 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108974:109413 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108975:109412 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108974:109413 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid005692:108973:109410 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid005692:108975:109412 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid005692:108974:109413 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid005692:108973:109410 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid005692:108975:109412 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
2: 
2: nid005692:108976:109411 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: 
2: nid005692:108973:109410 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: 
2: nid005692:108974:109413 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: 
2: nid005692:108975:109412 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Creating one domain per process
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Creating one domain per process
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Support for global registrations: false
2: nid005692:108976:109411 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Creating one domain per process
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Creating one domain per process
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid005692:108976:109411 [3] NCCL INFO Using network AWS Libfabric
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Support for global registrations: false
2: nid005692:108973:109410 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Support for global registrations: false
2: nid005692:108974:109413 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Support for global registrations: false
2: nid005692:108975:109412 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid005692:108973:109410 [0] NCCL INFO Using network AWS Libfabric
2: nid005692:108974:109413 [1] NCCL INFO Using network AWS Libfabric
2: nid005692:108975:109412 [2] NCCL INFO Using network AWS Libfabric
0: nid005687:269560:270017 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid005687:269562:269562 [2] NCCL INFO cudaDriverVersion 12050
0: nid005687:269562:269562 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid005687:269562:269562 [2] NCCL INFO Bootstrap : Using hsn0:172.28.20.68<0>
0: nid005687:269562:269562 [2] NCCL INFO NCCL version 2.22.3+cuda12.5
0: nid005687:269563:269563 [3] NCCL INFO cudaDriverVersion 12050
0: nid005687:269563:269563 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid005687:269561:269561 [1] NCCL INFO cudaDriverVersion 12050
0: nid005687:269561:269561 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid005687:269561:269561 [1] NCCL INFO Bootstrap : Using hsn0:172.28.20.68<0>
0: nid005687:269563:269563 [3] NCCL INFO Bootstrap : Using hsn0:172.28.20.68<0>
0: nid005687:269561:269561 [1] NCCL INFO NCCL version 2.22.3+cuda12.5
0: nid005687:269563:269563 [3] NCCL INFO NCCL version 2.22.3+cuda12.5
0: nid005687:269561:270023 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269562:270022 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269562:270022 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid005687:269561:270023 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid005687:269562:270022 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid005687:269561:270023 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: nid005687:269563:270024 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269563:270024 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid005687:269563:270024 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Using CUDA driver version 12050 with runtime 12050
0: 
0: nid005687:269561:270023 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: 
0: nid005687:269562:270022 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: 
0: nid005687:269563:270024 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Creating one domain per process
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Creating one domain per process
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Creating one domain per process
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Support for global registrations: false
0: nid005687:269561:270023 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Support for global registrations: false
0: nid005687:269562:270022 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Support for global registrations: false
0: nid005687:269563:270024 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid005687:269561:270023 [1] NCCL INFO Using network AWS Libfabric
0: nid005687:269562:270022 [2] NCCL INFO Using network AWS Libfabric
0: nid005687:269563:270024 [3] NCCL INFO Using network AWS Libfabric
3: nid005694:125980:126459 [0] NCCL INFO DMA-BUF is available on GPU device 0
3: nid005694:125982:126458 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: nid005694:125983:126457 [3] NCCL INFO DMA-BUF is available on GPU device 3
3: nid005694:125981:126460 [1] NCCL INFO DMA-BUF is available on GPU device 1
1: nid005689:27135:27614 [3] NCCL INFO DMA-BUF is available on GPU device 3
1: nid005689:27134:27616 [2] NCCL INFO DMA-BUF is available on GPU device 2
1: nid005689:27133:27615 [1] NCCL INFO DMA-BUF is available on GPU device 1
1: nid005689:27132:27617 [0] NCCL INFO DMA-BUF is available on GPU device 0
2: nid005692:108976:109411 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid005692:108973:109410 [0] NCCL INFO DMA-BUF is available on GPU device 0
2: nid005692:108974:109413 [1] NCCL INFO DMA-BUF is available on GPU device 1
2: nid005692:108975:109412 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid005687:269561:270023 [1] NCCL INFO DMA-BUF is available on GPU device 1
0: nid005687:269562:270022 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid005687:269563:270024 [3] NCCL INFO DMA-BUF is available on GPU device 3
0: nid005687:269563:270024 [3] NCCL INFO ncclCommInitRank comm 0xaaaafed45590 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init START
0: nid005687:269562:270022 [2] NCCL INFO ncclCommInitRank comm 0xaaaabf426c60 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init START
2: nid005692:108976:109411 [3] NCCL INFO ncclCommInitRank comm 0xaaab09369150 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init START
0: nid005687:269561:270023 [1] NCCL INFO ncclCommInitRank comm 0xaaaae4747fe0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init START
1: nid005689:27133:27615 [1] NCCL INFO ncclCommInitRank comm 0xaaaad5ee6bc0 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init START
1: nid005689:27135:27614 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf0cb7810 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init START
1: nid005689:27134:27616 [2] NCCL INFO ncclCommInitRank comm 0xaaab041a9900 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init START
1: nid005689:27132:27617 [0] NCCL INFO ncclCommInitRank comm 0xaaab011c2b20 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init START
2: nid005692:108975:109412 [2] NCCL INFO ncclCommInitRank comm 0xaaab08c77530 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init START
2: nid005692:108974:109413 [1] NCCL INFO ncclCommInitRank comm 0xaaaadb715620 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init START
2: nid005692:108973:109410 [0] NCCL INFO ncclCommInitRank comm 0xaaab0c2c5a50 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init START
0: nid005687:269560:270017 [0] NCCL INFO ncclCommInitRank comm 0xaaaac5a75d30 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init START
3: nid005694:125983:126457 [3] NCCL INFO ncclCommInitRank comm 0xaaab090a68e0 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init START
3: nid005694:125980:126459 [0] NCCL INFO ncclCommInitRank comm 0xaaab051c3880 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init START
3: nid005694:125982:126458 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf5095b90 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init START
3: nid005694:125981:126460 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf54480e0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init START
1: nid005689:27132:27617 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
1: nid005689:27134:27616 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
1: nid005689:27133:27615 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
1: nid005689:27135:27614 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
1: nid005689:27135:27614 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: nid005689:27135:27614 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid005689:27134:27616 [2] NCCL INFO NVLS multicast support is not available on dev 2
1: nid005689:27133:27615 [1] NCCL INFO NVLS multicast support is not available on dev 1
1: nid005689:27134:27616 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid005689:27133:27615 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid005689:27132:27617 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: nid005689:27132:27617 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid005694:125980:126459 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
3: nid005694:125980:126459 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: nid005694:125980:126459 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid005694:125983:126457 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
3: nid005694:125983:126457 [3] NCCL INFO NVLS multicast support is not available on dev 3
3: nid005694:125983:126457 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid005694:125982:126458 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
3: nid005694:125981:126460 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
3: nid005694:125982:126458 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid005694:125982:126458 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid005694:125981:126460 [1] NCCL INFO NVLS multicast support is not available on dev 1
3: nid005694:125981:126460 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid005692:108973:109410 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
2: nid005692:108973:109410 [0] NCCL INFO NVLS multicast support is not available on dev 0
2: nid005692:108973:109410 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid005692:108976:109411 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
2: nid005692:108976:109411 [3] NCCL INFO NVLS multicast support is not available on dev 3
2: nid005692:108976:109411 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid005692:108974:109413 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
2: nid005692:108975:109412 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
2: nid005692:108975:109412 [2] NCCL INFO NVLS multicast support is not available on dev 2
2: nid005692:108975:109412 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid005692:108974:109413 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: nid005692:108974:109413 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid005687:269563:270024 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
0: nid005687:269563:270024 [3] NCCL INFO NVLS multicast support is not available on dev 3
0: nid005687:269563:270024 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid005687:269560:270017 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
0: nid005687:269560:270017 [0] NCCL INFO NVLS multicast support is not available on dev 0
0: nid005687:269560:270017 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid005687:269561:270023 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
0: nid005687:269561:270023 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid005687:269561:270023 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid005687:269562:270022 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
0: nid005687:269562:270022 [2] NCCL INFO NVLS multicast support is not available on dev 2
0: nid005687:269562:270022 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid005687:269563:270024 [3] NCCL INFO comm 0xaaaafed45590 rank 3 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
0: nid005687:269562:270022 [2] NCCL INFO comm 0xaaaabf426c60 rank 2 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
0: nid005687:269563:270024 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
0: nid005687:269563:270024 [3] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269562:270022 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/10/-1->2->-1 [3] 3/10/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
0: nid005687:269561:270023 [1] NCCL INFO comm 0xaaaae4747fe0 rank 1 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
0: nid005687:269562:270022 [2] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269560:270017 [0] NCCL INFO comm 0xaaaac5a75d30 rank 0 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
0: nid005687:269561:270023 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
0: nid005687:269561:270023 [1] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269560:270017 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12
3: nid005694:125983:126457 [3] NCCL INFO comm 0xaaab090a68e0 rank 15 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
0: nid005687:269560:270017 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15   3   2   1
0: nid005687:269560:270017 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14   2
0: nid005687:269560:270017 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13   1
0: nid005687:269560:270017 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12
0: nid005687:269560:270017 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15   3   2   1
0: nid005687:269560:270017 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14   2
0: nid005687:269560:270017 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13   1
3: nid005694:125983:126457 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
3: nid005694:125983:126457 [3] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269560:270017 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/8/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
0: nid005687:269560:270017 [0] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125982:126458 [2] NCCL INFO comm 0xaaaaf5095b90 rank 14 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
3: nid005694:125981:126460 [1] NCCL INFO comm 0xaaaaf54480e0 rank 13 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
3: nid005694:125982:126458 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/6/-1->14->-1 [7] 15/6/-1->14->-1
3: nid005694:125982:126458 [2] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125981:126460 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
3: nid005694:125981:126460 [1] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27135:27614 [3] NCCL INFO comm 0xaaaaf0cb7810 rank 7 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
1: nid005689:27134:27616 [2] NCCL INFO comm 0xaaab041a9900 rank 6 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
2: nid005692:108976:109411 [3] NCCL INFO comm 0xaaab09369150 rank 11 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
2: nid005692:108975:109412 [2] NCCL INFO comm 0xaaab08c77530 rank 10 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
3: nid005694:125980:126459 [0] NCCL INFO comm 0xaaab051c3880 rank 12 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
3: nid005694:125980:126459 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/4/-1->12->-1 [5] 13/4/-1->12->-1 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
3: nid005694:125980:126459 [0] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27133:27615 [1] NCCL INFO comm 0xaaaad5ee6bc0 rank 5 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
1: nid005689:27135:27614 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
1: nid005689:27132:27617 [0] NCCL INFO comm 0xaaab011c2b20 rank 4 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
2: nid005692:108976:109411 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
1: nid005689:27135:27614 [3] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27133:27615 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
2: nid005692:108976:109411 [3] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27134:27616 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
1: nid005689:27133:27615 [1] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27134:27616 [2] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108974:109413 [1] NCCL INFO comm 0xaaaadb715620 rank 9 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
2: nid005692:108975:109412 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->2 [3] 11/6/14->10->2 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
2: nid005692:108975:109412 [2] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108974:109413 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
2: nid005692:108974:109413 [1] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27132:27617 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
1: nid005689:27132:27617 [0] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108973:109410 [0] NCCL INFO comm 0xaaab0c2c5a50 rank 8 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
2: nid005692:108973:109410 [0] NCCL INFO Trees [0] 9/4/12->8->0 [1] 9/4/12->8->0 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
2: nid005692:108973:109410 [0] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125980:126459 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125980:126459 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108976:109411 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108976:109411 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108975:109412 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108975:109412 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269562:270022 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269562:270022 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27135:27614 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27135:27614 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125983:126457 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125983:126457 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125982:126458 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125982:126458 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125981:126460 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125981:126460 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269561:270023 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269561:270023 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27133:27615 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27133:27615 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108974:109413 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108974:109413 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27134:27616 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27134:27616 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269560:270017 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269560:270017 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108973:109410 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108973:109410 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269560:270017 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
1: nid005689:27132:27617 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27132:27617 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269563:270024 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269563:270024 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125980:126459 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125980:126459 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid005694:125980:126459 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid005694:125980:126459 [0] NCCL INFO ncclCommInitRank comm 0xaaab051c3880 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init COMPLETE
3: nid005694:125980:126459 [0] NCCL INFO Init timings: rank 12 nranks 16 total 2.68 (kernels 0.15, bootstrap 2.17, allgathers 0.02, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108976:109411 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108976:109411 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid005692:108976:109411 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid005694:125982:126458 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125982:126458 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid005694:125982:126458 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid005694:125982:126458 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf5095b90 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init COMPLETE
3: nid005694:125982:126458 [2] NCCL INFO Init timings: rank 14 nranks 16 total 2.70 (kernels 0.17, bootstrap 2.17, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108976:109411 [3] NCCL INFO ncclCommInitRank comm 0xaaab09369150 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init COMPLETE
2: nid005692:108976:109411 [3] NCCL INFO Init timings: rank 11 nranks 16 total 2.80 (kernels 0.37, bootstrap 2.07, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125981:126460 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125983:126457 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid005694:125981:126460 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid005694:125983:126457 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid005694:125981:126460 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid005694:125983:126457 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid005694:125981:126460 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf54480e0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init COMPLETE
3: nid005694:125983:126457 [3] NCCL INFO ncclCommInitRank comm 0xaaab090a68e0 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init COMPLETE
3: nid005694:125981:126460 [1] NCCL INFO Init timings: rank 13 nranks 16 total 2.64 (kernels 0.12, bootstrap 2.16, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125983:126457 [3] NCCL INFO Init timings: rank 15 nranks 16 total 2.80 (kernels 0.27, bootstrap 2.17, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27135:27614 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27135:27614 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid005689:27135:27614 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid005689:27135:27614 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf0cb7810 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init COMPLETE
1: nid005689:27135:27614 [3] NCCL INFO Init timings: rank 7 nranks 16 total 2.80 (kernels 0.28, bootstrap 2.16, allgathers 0.02, topo 0.30, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108975:109412 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108975:109412 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid005692:108975:109412 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid005692:108975:109412 [2] NCCL INFO ncclCommInitRank comm 0xaaab08c77530 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init COMPLETE
2: nid005692:108975:109412 [2] NCCL INFO Init timings: rank 10 nranks 16 total 2.70 (kernels 0.27, bootstrap 2.07, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108973:109410 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108974:109413 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid005692:108973:109410 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid005692:108974:109413 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid005692:108973:109410 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid005692:108974:109413 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid005692:108973:109410 [0] NCCL INFO ncclCommInitRank comm 0xaaab0c2c5a50 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init COMPLETE
2: nid005692:108974:109413 [1] NCCL INFO ncclCommInitRank comm 0xaaaadb715620 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init COMPLETE
2: nid005692:108974:109413 [1] NCCL INFO Init timings: rank 9 nranks 16 total 2.68 (kernels 0.25, bootstrap 2.07, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108973:109410 [0] NCCL INFO Init timings: rank 8 nranks 16 total 2.80 (kernels 0.37, bootstrap 2.07, allgathers 0.01, topo 0.31, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27134:27616 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27134:27616 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid005689:27134:27616 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid005689:27134:27616 [2] NCCL INFO ncclCommInitRank comm 0xaaab041a9900 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init COMPLETE
1: nid005689:27132:27617 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27133:27615 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid005689:27134:27616 [2] NCCL INFO Init timings: rank 6 nranks 16 total 2.67 (kernels 0.15, bootstrap 2.16, allgathers 0.02, topo 0.30, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27133:27615 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid005689:27132:27617 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid005689:27133:27615 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid005689:27132:27617 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid005689:27133:27615 [1] NCCL INFO ncclCommInitRank comm 0xaaaad5ee6bc0 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init COMPLETE
1: nid005689:27132:27617 [0] NCCL INFO ncclCommInitRank comm 0xaaab011c2b20 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init COMPLETE
1: nid005689:27133:27615 [1] NCCL INFO Init timings: rank 5 nranks 16 total 2.70 (kernels 0.18, bootstrap 2.16, allgathers 0.02, topo 0.30, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27132:27617 [0] NCCL INFO Init timings: rank 4 nranks 16 total 2.64 (kernels 0.12, bootstrap 2.16, allgathers 0.02, topo 0.30, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269562:270022 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269562:270022 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid005687:269562:270022 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid005687:269562:270022 [2] NCCL INFO ncclCommInitRank comm 0xaaaabf426c60 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 commId 0xf743d35b1eadafab - Init COMPLETE
0: nid005687:269560:270017 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269562:270022 [2] NCCL INFO Init timings: rank 2 nranks 16 total 1.98 (kernels 0.07, bootstrap 1.55, allgathers 0.00, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269560:270017 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid005687:269560:270017 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid005687:269560:270017 [0] NCCL INFO ncclCommInitRank comm 0xaaaac5a75d30 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 commId 0xf743d35b1eadafab - Init COMPLETE
0: nid005687:269560:270017 [0] NCCL INFO Init timings: rank 0 nranks 16 total 2.82 (kernels 0.28, bootstrap 2.18, allgathers 0.00, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269561:270023 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269561:270023 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid005687:269561:270023 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid005687:269561:270023 [1] NCCL INFO ncclCommInitRank comm 0xaaaae4747fe0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 commId 0xf743d35b1eadafab - Init COMPLETE
0: nid005687:269561:270023 [1] NCCL INFO Init timings: rank 1 nranks 16 total 1.98 (kernels 0.07, bootstrap 1.55, allgathers 0.00, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269563:270024 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid005687:269563:270024 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid005687:269563:270024 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid005687:269563:270024 [3] NCCL INFO ncclCommInitRank comm 0xaaaafed45590 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 commId 0xf743d35b1eadafab - Init COMPLETE
0: nid005687:269563:270024 [3] NCCL INFO Init timings: rank 3 nranks 16 total 1.98 (kernels 0.07, bootstrap 1.55, allgathers 0.00, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125980:126475 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
0: nid005687:269560:270039 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270039 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:109464 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:27634 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27133:27636 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108975:109464 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27634 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:109464 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
1: nid005689:27133:27636 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
1: nid005689:27134:27634 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid005687:269561:270038 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
0: nid005687:269562:270037 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27133:27636 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
1: nid005689:27133:27636 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
2: nid005692:108974:109463 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108974:109463 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108974:109463 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
2: nid005692:108974:109463 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
0: nid005687:269561:270038 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
0: nid005687:269562:270037 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27135:27633 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
1: nid005689:27132:27635 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27135:27633 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125982:126476 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27132:27635 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27135:27633 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
3: nid005694:125982:126476 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27132:27635 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27135:27633 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
3: nid005694:125982:126476 [2] NCCL INFO Channel 02/0 : 14[2] -> 2[2] [send] via NET/AWS Libfabric/2
3: nid005694:125982:126476 [2] NCCL INFO Channel 06/0 : 14[2] -> 2[2] [send] via NET/AWS Libfabric/2
0: nid005687:269560:270039 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125981:126478 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108973:109462 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108976:109461 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125981:126478 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
0: nid005687:269560:270039 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
3: nid005694:125981:126478 [1] NCCL INFO Channel 03/0 : 13[1] -> 1[1] [send] via NET/AWS Libfabric/1
2: nid005692:108975:109464 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
3: nid005694:125981:126478 [1] NCCL INFO Channel 07/0 : 13[1] -> 1[1] [send] via NET/AWS Libfabric/1
0: nid005687:269560:270039 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid005692:108973:109462 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109462 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid005687:269560:270039 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
2: nid005692:108973:109462 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
3: nid005694:125983:126477 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125983:126477 [3] NCCL INFO Channel 01/0 : 15[3] -> 3[3] [send] via NET/AWS Libfabric/3
3: nid005694:125983:126477 [3] NCCL INFO Channel 05/0 : 15[3] -> 3[3] [send] via NET/AWS Libfabric/3
2: nid005692:108976:109461 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
3: nid005694:125980:126475 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:126475 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:126475 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [send] via NET/AWS Libfabric/0
3: nid005694:125980:126475 [0] NCCL INFO Channel 04/0 : 12[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108976:109461 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108976:109461 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108974:109463 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
2: nid005692:108975:109464 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
0: nid005687:269562:270037 [2] NCCL INFO Channel 02/0 : 14[2] -> 2[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270037 [2] NCCL INFO Channel 06/0 : 14[2] -> 2[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270037 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269561:270038 [1] NCCL INFO Channel 03/0 : 13[1] -> 1[1] [receive] via NET/AWS Libfabric/1
0: nid005687:269562:270037 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269561:270038 [1] NCCL INFO Channel 07/0 : 13[1] -> 1[1] [receive] via NET/AWS Libfabric/1
0: nid005687:269561:270038 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
0: nid005687:269563:270040 [3] NCCL INFO Channel 01/0 : 15[3] -> 3[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269562:270037 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
0: nid005687:269561:270038 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
1: nid005689:27134:27634 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 05/0 : 15[3] -> 3[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269563:270040 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
0: nid005687:269563:270040 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
0: nid005687:269562:270037 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
1: nid005689:27135:27633 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
1: nid005689:27135:27633 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
0: nid005687:269560:270039 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125983:126477 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269560:270039 [0] NCCL INFO Channel 04/0 : 12[0] -> 0[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27135:27633 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid005687:269560:270039 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid005687:269560:270039 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid005687:269563:270040 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:27633 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid005687:269561:270038 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:27633 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269561:270038 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid005687:269563:270040 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27135:27633 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid005694:125983:126477 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269561:270038 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27133:27636 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
3: nid005694:125981:126478 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid005687:269561:270038 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
0: nid005687:269562:270037 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
3: nid005694:125982:126476 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:27634 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
0: nid005687:269562:270037 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
1: nid005689:27132:27635 [0] NCCL INFO Connected all rings
1: nid005689:27135:27633 [3] NCCL INFO Connected all rings
1: nid005689:27134:27634 [2] NCCL INFO Connected all rings
1: nid005689:27133:27636 [1] NCCL INFO Connected all rings
0: nid005687:269560:270039 [0] NCCL INFO Connected all rings
0: nid005687:269563:270040 [3] NCCL INFO Connected all rings
0: nid005687:269561:270038 [1] NCCL INFO Connected all rings
0: nid005687:269562:270037 [2] NCCL INFO Connected all rings
3: nid005694:125980:126475 [0] NCCL INFO Connected all rings
3: nid005694:125983:126477 [3] NCCL INFO Connected all rings
3: nid005694:125982:126476 [2] NCCL INFO Connected all rings
3: nid005694:125981:126478 [1] NCCL INFO Connected all rings
2: nid005692:108973:109462 [0] NCCL INFO Connected all rings
2: nid005692:108974:109463 [1] NCCL INFO Connected all rings
2: nid005692:108976:109461 [3] NCCL INFO Connected all rings
2: nid005692:108975:109464 [2] NCCL INFO Connected all rings
0: Rank 0:  Loading vision tower: google/siglip-so400m-patch14-384
0: [2025-07-09 22:52:57,892] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 1131, num_elems = 15.68B
3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
3: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
2: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
1: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
1: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
3: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
1: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
3: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
2: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
2: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.08s/it]
0: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.09s/it]
0: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.09s/it]
0: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.10s/it]
0: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.15s/it]
3: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.20s/it]
3: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.27s/it]
2: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.28s/it]
1: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.28s/it]
2: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.29s/it]
1: Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:05<00:15,  5.29s/it]
1: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
2: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
3: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
3: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
2: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
1: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.09s/it]
0: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.10s/it]
0: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.10s/it]
0: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.11s/it]
0: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.13s/it]
3: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.14s/it]
3: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.16s/it]
2: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.17s/it]
1: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.16s/it]
1: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.17s/it]
2: Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:12<00:12,  6.17s/it]
1: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
3: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
3: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
1: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
2: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
2: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
0: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
0: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.39s/it]
0: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.38s/it]
0: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.40s/it]
3: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.41s/it]
3: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.42s/it]
1: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.42s/it]
2: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.42s/it]
2: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.42s/it]
1: Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:18<00:06,  6.42s/it]
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
3: 
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
2: 
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
1: 
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
3: 
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
2: 
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
1: 
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.88s/it]
0: 
0: 
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.22s/it]
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.89s/it]
0: 
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.23s/it]
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.90s/it]
3: 
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.24s/it]
3: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.92s/it]
3: 
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.24s/it]
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.92s/it]
2: 
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.24s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.24s/it]
2: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.92s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.92s/it]
2: 
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.24s/it]
1: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:19<00:00,  4.92s/it]
0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  4.53s/it]
0: Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.08s/it]
0: 
0: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: Rank 0:  Prompt version: qwen_1_5
0: Rank 0:  google/siglip-so400m-patch14-384 is already loaded, `load_model` called again, skipping.
0: 
0: Rank 0: 
0:  Using mm_tunable_parts: mm_vision_tower,mm_mlp_adapter,mm_language_model
0: 
0: Rank 0:  Total parameters: ~8030.35 MB)
0: 
0: Rank 0:  Trainable parameters: ~8030.35 MB)
0: 
0: Rank 0:  Loading radvlm/data/llava_datasets/cold_start_long.json
2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
2:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
3:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
1:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
0:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
3:   warnings.warn(
3: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
3: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
3:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
2:   warnings.warn(
2: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
2: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
2:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
1:   warnings.warn(
1: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
1: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
1:   warnings.warn(
0: Rank 0: 
0:  Loaded 71172 samples from radvlm/data/llava_datasets/cold_start_long.json
0: Rank 0:  Loaded 71172 samples from radvlm/data/llava_datasets/cold_start_long.json
0: Rank 0:  Formatting inputs...Skip in lazy mode
0: 
0: Rank 0: 
0:  Setting NCCL timeout to INF to avoid running errors.
0: 
0: /usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
0: dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)
0:   warnings.warn(
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
0: nid005687:269561:270113 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
0: nid005687:269561:270113 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269563:270116 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
3: nid005694:125981:126580 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid005694:125981:126580 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125981:126580 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid005694:125981:126580 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid005694:125981:126580 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid005694:125981:126580 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125983:126583 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
2: nid005692:108974:109544 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108974:109544 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
2: nid005692:108974:109544 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108974:109544 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108974:109544 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108974:109544 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:126576 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:126576 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
3: nid005694:125980:126576 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108976:109545 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108976:109545 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:126577 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:126577 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
3: nid005694:125982:126577 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid005689:27133:27741 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
1: nid005689:27133:27741 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27133:27741 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27133:27741 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
1: nid005689:27133:27741 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27133:27741 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108973:109538 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:126577 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:126577 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108973:109538 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:27734 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108975:109539 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:109539 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269562:270119 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
0: nid005687:269562:270119 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
3: nid005694:125982:126577 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid005689:27134:27734 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
2: nid005692:108975:109539 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
0: Parameter Offload: Total persistent parameters: 728992 in 407 params
0: nid005687:269560:270142 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid005687:269560:270142 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269560:270142 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269560:270142 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269560:270142 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0
0: nid005687:269561:270113 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid005687:269560:270142 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269561:270113 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269561:270113 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid005694:125980:126576 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
3: nid005694:125980:126576 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269561:270113 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
2: nid005692:108973:109538 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:109538 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:27738 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
3: nid005694:125983:126583 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid005694:125983:126583 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid005687:269563:270116 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid005689:27135:27735 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid005692:108976:109545 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid005687:269560:270142 [0] NCCL INFO Connected all trees
3: nid005694:125980:126576 [0] NCCL INFO Connected all trees
2: nid005692:108973:109538 [0] NCCL INFO Connected all trees
1: nid005689:27132:27738 [0] NCCL INFO Connected all trees
0: nid005687:269563:270116 [3] NCCL INFO Connected all trees
0: nid005687:269562:270119 [2] NCCL INFO Connected all trees
3: nid005694:125981:126580 [1] NCCL INFO Connected all trees
1: nid005689:27133:27741 [1] NCCL INFO Connected all trees
2: nid005692:108974:109544 [1] NCCL INFO Connected all trees
3: nid005694:125983:126583 [3] NCCL INFO Connected all trees
1: nid005689:27135:27735 [3] NCCL INFO Connected all trees
2: nid005692:108976:109545 [3] NCCL INFO Connected all trees
3: nid005694:125982:126577 [2] NCCL INFO Connected all trees
2: nid005692:108975:109539 [2] NCCL INFO Connected all trees
1: nid005689:27134:27734 [2] NCCL INFO Connected all trees
0: nid005687:269561:270113 [1] NCCL INFO Connected all trees
0: wandb: Currently logged in as: nicolas-deperrois (krauthammerlab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
0: wandb: Tracking run with wandb version 0.19.11
0: wandb: Run data is saved locally in /capstor/scratch/cscs/ndeperr/code/RadVLM/wandb/run-20250709_225327-006og4d0
0: wandb: Run `wandb offline` to turn off syncing.
0: wandb: Syncing run radvlm-sft-cs-long
0: wandb: â­ï¸ View project at https://wandb.ai/krauthammerlab/huggingface
0: wandb: ðŸš€ View run at https://wandb.ai/krauthammerlab/huggingface/runs/006og4d0
0:   0%|          | 0/2224 [00:00<?, ?it/s]
2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
2:   return fn(*args, **kwargs)
2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
2:   return fn(*args, **kwargs)
2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
2:   return fn(*args, **kwargs)
2: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
2:   return fn(*args, **kwargs)
3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
3:   return fn(*args, **kwargs)
3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
3:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
1:   return fn(*args, **kwargs)
3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
3:   return fn(*args, **kwargs)
3: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
3:   return fn(*args, **kwargs)
1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
1:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
1:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
1: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
1:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
1: nid005689:27135:29107 [3] NCCL INFO Using network AWS Libfabric
1: nid005689:27134:29106 [2] NCCL INFO Using network AWS Libfabric
3: nid005694:125982:127938 [2] NCCL INFO Using network AWS Libfabric
1: nid005689:27135:29107 [3] NCCL INFO DMA-BUF is available on GPU device 3
1: nid005689:27134:29106 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: nid005694:125980:127936 [0] NCCL INFO Using network AWS Libfabric
1: nid005689:27133:29109 [1] NCCL INFO Using network AWS Libfabric
1: nid005689:27132:29108 [0] NCCL INFO Using network AWS Libfabric
0: nid005687:269560:271822 [0] NCCL INFO Using network AWS Libfabric
3: nid005694:125982:127938 [2] NCCL INFO DMA-BUF is available on GPU device 2
1: nid005689:27133:29109 [1] NCCL INFO DMA-BUF is available on GPU device 1
0: nid005687:269561:271823 [1] NCCL INFO Using network AWS Libfabric
0: nid005687:269563:271824 [3] NCCL INFO Using network AWS Libfabric
3: nid005694:125983:127935 [3] NCCL INFO Using network AWS Libfabric
3: nid005694:125980:127936 [0] NCCL INFO DMA-BUF is available on GPU device 0
1: nid005689:27132:29108 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid005687:269562:271825 [2] NCCL INFO Using network AWS Libfabric
3: nid005694:125981:127937 [1] NCCL INFO Using network AWS Libfabric
0: nid005687:269560:271822 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid005687:269561:271823 [1] NCCL INFO DMA-BUF is available on GPU device 1
3: nid005694:125983:127935 [3] NCCL INFO DMA-BUF is available on GPU device 3
3: nid005694:125981:127937 [1] NCCL INFO DMA-BUF is available on GPU device 1
0: nid005687:269563:271824 [3] NCCL INFO DMA-BUF is available on GPU device 3
0: nid005687:269562:271825 [2] NCCL INFO DMA-BUF is available on GPU device 2
2: nid005692:108975:110903 [2] NCCL INFO Using network AWS Libfabric
2: nid005692:108976:110906 [3] NCCL INFO Using network AWS Libfabric
2: nid005692:108974:110904 [1] NCCL INFO Using network AWS Libfabric
2: nid005692:108973:110905 [0] NCCL INFO Using network AWS Libfabric
2: nid005692:108975:110903 [2] NCCL INFO DMA-BUF is available on GPU device 2
2: nid005692:108976:110906 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid005692:108974:110904 [1] NCCL INFO DMA-BUF is available on GPU device 1
2: nid005692:108973:110905 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid005687:269561:271823 [1] NCCL INFO bootstrapSplit: comm 0x400c8157caf0 parent 0xaaaae4747fe0 rank 1 nranks 16 color 1197013201 key 1 prev 0 next 2 - DONE
0: nid005687:269560:271822 [0] NCCL INFO bootstrapSplit: comm 0x400c60a9fb50 parent 0xaaaac5a75d30 rank 0 nranks 16 color 1197013201 key 0 prev 15 next 1 - DONE
0: nid005687:269561:271823 [1] NCCL INFO ncclCommSplit comm 0x400c8157caf0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae4747fe0 color 1197013201 key 1 commId 0xbe0cb0130ee69ba9 - Init START
0: nid005687:269560:271822 [0] NCCL INFO ncclCommSplit comm 0x400c60a9fb50 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac5a75d30 color 1197013201 key 0 commId 0xbe0cb0130ee69ba9 - Init START
0: nid005687:269563:271824 [3] NCCL INFO bootstrapSplit: comm 0x400ce157d3c0 parent 0xaaaafed45590 rank 3 nranks 16 color 1197013201 key 3 prev 2 next 4 - DONE
3: nid005694:125980:127936 [0] NCCL INFO bootstrapSplit: comm 0x400478b92fe0 parent 0xaaab051c3880 rank 12 nranks 16 color 1197013201 key 12 prev 11 next 13 - DONE
3: nid005694:125982:127938 [2] NCCL INFO bootstrapSplit: comm 0x400c7557cb20 parent 0xaaaaf5095b90 rank 14 nranks 16 color 1197013201 key 14 prev 13 next 15 - DONE
3: nid005694:125983:127935 [3] NCCL INFO bootstrapSplit: comm 0x400c84b93620 parent 0xaaab090a68e0 rank 15 nranks 16 color 1197013201 key 15 prev 14 next 0 - DONE
1: nid005689:27135:29107 [3] NCCL INFO bootstrapSplit: comm 0x400ca557cbf0 parent 0xaaaaf0cb7810 rank 7 nranks 16 color 1197013201 key 7 prev 6 next 8 - DONE
1: nid005689:27135:29107 [3] NCCL INFO ncclCommSplit comm 0x400ca557cbf0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf0cb7810 color 1197013201 key 7 commId 0xbe0cb0130ee69ba9 - Init START
0: nid005687:269562:271825 [2] NCCL INFO bootstrapSplit: comm 0x400caca9f3d0 parent 0xaaaabf426c60 rank 2 nranks 16 color 1197013201 key 2 prev 1 next 3 - DONE
1: nid005689:27134:29106 [2] NCCL INFO bootstrapSplit: comm 0x40049d57cb00 parent 0xaaab041a9900 rank 6 nranks 16 color 1197013201 key 6 prev 5 next 7 - DONE
0: nid005687:269563:271824 [3] NCCL INFO ncclCommSplit comm 0x400ce157d3c0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafed45590 color 1197013201 key 3 commId 0xbe0cb0130ee69ba9 - Init START
0: nid005687:269562:271825 [2] NCCL INFO ncclCommSplit comm 0x400caca9f3d0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaabf426c60 color 1197013201 key 2 commId 0xbe0cb0130ee69ba9 - Init START
3: nid005694:125981:127937 [1] NCCL INFO bootstrapSplit: comm 0x40047d57cba0 parent 0xaaaaf54480e0 rank 13 nranks 16 color 1197013201 key 13 prev 12 next 14 - DONE
3: nid005694:125980:127936 [0] NCCL INFO ncclCommSplit comm 0x400478b92fe0 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab051c3880 color 1197013201 key 12 commId 0xbe0cb0130ee69ba9 - Init START
3: nid005694:125982:127938 [2] NCCL INFO ncclCommSplit comm 0x400c7557cb20 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf5095b90 color 1197013201 key 14 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108976:110906 [3] NCCL INFO bootstrapSplit: comm 0x40079d57cd20 parent 0xaaab09369150 rank 11 nranks 16 color 1197013201 key 11 prev 10 next 12 - DONE
2: nid005692:108975:110903 [2] NCCL INFO bootstrapSplit: comm 0x400c6557ccc0 parent 0xaaab08c77530 rank 10 nranks 16 color 1197013201 key 10 prev 9 next 11 - DONE
3: nid005694:125983:127935 [3] NCCL INFO ncclCommSplit comm 0x400c84b93620 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab090a68e0 color 1197013201 key 15 commId 0xbe0cb0130ee69ba9 - Init START
3: nid005694:125981:127937 [1] NCCL INFO ncclCommSplit comm 0x40047d57cba0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf54480e0 color 1197013201 key 13 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108974:110904 [1] NCCL INFO bootstrapSplit: comm 0x4007e4a9f590 parent 0xaaaadb715620 rank 9 nranks 16 color 1197013201 key 9 prev 8 next 10 - DONE
2: nid005692:108976:110906 [3] NCCL INFO ncclCommSplit comm 0x40079d57cd20 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab09369150 color 1197013201 key 11 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108973:110905 [0] NCCL INFO bootstrapSplit: comm 0x4003dd57cc60 parent 0xaaab0c2c5a50 rank 8 nranks 16 color 1197013201 key 8 prev 7 next 9 - DONE
1: nid005689:27134:29106 [2] NCCL INFO ncclCommSplit comm 0x40049d57cb00 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab041a9900 color 1197013201 key 6 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108975:110903 [2] NCCL INFO ncclCommSplit comm 0x400c6557ccc0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab08c77530 color 1197013201 key 10 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108974:110904 [1] NCCL INFO ncclCommSplit comm 0x4007e4a9f590 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadb715620 color 1197013201 key 9 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108973:110905 [0] NCCL INFO ncclCommSplit comm 0x4003dd57cc60 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0c2c5a50 color 1197013201 key 8 commId 0xbe0cb0130ee69ba9 - Init START
1: nid005689:27132:29108 [0] NCCL INFO bootstrapSplit: comm 0x400790a9f240 parent 0xaaab011c2b20 rank 4 nranks 16 color 1197013201 key 4 prev 3 next 5 - DONE
1: nid005689:27133:29109 [1] NCCL INFO bootstrapSplit: comm 0x4007e557d440 parent 0xaaaad5ee6bc0 rank 5 nranks 16 color 1197013201 key 5 prev 4 next 6 - DONE
1: nid005689:27132:29108 [0] NCCL INFO ncclCommSplit comm 0x400790a9f240 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab011c2b20 color 1197013201 key 4 commId 0xbe0cb0130ee69ba9 - Init START
1: nid005689:27133:29109 [1] NCCL INFO ncclCommSplit comm 0x4007e557d440 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad5ee6bc0 color 1197013201 key 5 commId 0xbe0cb0130ee69ba9 - Init START
2: nid005692:108974:110904 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
2: nid005692:108974:110904 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: nid005692:108975:110903 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
2: nid005692:108973:110905 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
2: nid005692:108976:110906 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
2: nid005692:108975:110903 [2] NCCL INFO NVLS multicast support is not available on dev 2
2: nid005692:108973:110905 [0] NCCL INFO NVLS multicast support is not available on dev 0
2: nid005692:108976:110906 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: nid005689:27135:29107 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
1: nid005689:27135:29107 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: nid005689:27132:29108 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
1: nid005689:27133:29109 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
1: nid005689:27134:29106 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
1: nid005689:27132:29108 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: nid005689:27133:29109 [1] NCCL INFO NVLS multicast support is not available on dev 1
1: nid005689:27134:29106 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid005694:125980:127936 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
3: nid005694:125980:127936 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: nid005694:125982:127938 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
3: nid005694:125982:127938 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid005694:125983:127935 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
3: nid005694:125983:127935 [3] NCCL INFO NVLS multicast support is not available on dev 3
3: nid005694:125981:127937 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
3: nid005694:125981:127937 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid005687:269561:271823 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
0: nid005687:269561:271823 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid005687:269560:271822 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
0: nid005687:269563:271824 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
0: nid005687:269560:271822 [0] NCCL INFO NVLS multicast support is not available on dev 0
0: nid005687:269563:271824 [3] NCCL INFO NVLS multicast support is not available on dev 3
0: nid005687:269562:271825 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
0: nid005687:269562:271825 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid005694:125980:127936 [0] NCCL INFO comm 0x400478b92fe0 rank 12 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
3: nid005694:125981:127937 [1] NCCL INFO comm 0x40047d57cba0 rank 13 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
3: nid005694:125983:127935 [3] NCCL INFO comm 0x400c84b93620 rank 15 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
3: nid005694:125982:127938 [2] NCCL INFO comm 0x400c7557cb20 rank 14 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
2: nid005692:108976:110906 [3] NCCL INFO comm 0x40079d57cd20 rank 11 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
2: nid005692:108975:110903 [2] NCCL INFO comm 0x400c6557ccc0 rank 10 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
2: nid005692:108974:110904 [1] NCCL INFO comm 0x4007e4a9f590 rank 9 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
2: nid005692:108976:110906 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
2: nid005692:108976:110906 [3] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125980:127936 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/4/-1->12->-1 [5] 13/4/-1->12->-1 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
3: nid005694:125983:127935 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
3: nid005694:125980:127936 [0] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27135:29107 [3] NCCL INFO comm 0x400ca557cbf0 rank 7 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
2: nid005692:108974:110904 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
2: nid005692:108974:110904 [1] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108975:110903 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->2 [3] 11/6/14->10->2 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
3: nid005694:125982:127938 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/6/-1->14->-1 [7] 15/6/-1->14->-1
3: nid005694:125983:127935 [3] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125981:127937 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
3: nid005694:125982:127938 [2] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125981:127937 [1] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108973:110905 [0] NCCL INFO comm 0x4003dd57cc60 rank 8 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
2: nid005692:108975:110903 [2] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27134:29106 [2] NCCL INFO comm 0x40049d57cb00 rank 6 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
1: nid005689:27135:29107 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
1: nid005689:27135:29107 [3] NCCL INFO P2P Chunksize set to 131072
2: nid005692:108973:110905 [0] NCCL INFO Trees [0] 9/4/12->8->0 [1] 9/4/12->8->0 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
2: nid005692:108973:110905 [0] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27134:29106 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
1: nid005689:27134:29106 [2] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269563:271824 [3] NCCL INFO comm 0x400ce157d3c0 rank 3 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
0: nid005687:269560:271822 [0] NCCL INFO comm 0x400c60a9fb50 rank 0 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
0: nid005687:269563:271824 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
0: nid005687:269561:271823 [1] NCCL INFO comm 0x400c8157caf0 rank 1 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
1: nid005689:27133:29109 [1] NCCL INFO comm 0x4007e557d440 rank 5 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
1: nid005689:27132:29108 [0] NCCL INFO comm 0x400790a9f240 rank 4 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
1: nid005689:27133:29109 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
1: nid005689:27133:29109 [1] NCCL INFO P2P Chunksize set to 131072
1: nid005689:27132:29108 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
1: nid005689:27132:29108 [0] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269562:271825 [2] NCCL INFO comm 0x400caca9f3d0 rank 2 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
0: nid005687:269563:271824 [3] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269561:271823 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
0: nid005687:269561:271823 [1] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269560:271822 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12
0: nid005687:269562:271825 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/10/-1->2->-1 [3] 3/10/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
0: nid005687:269560:271822 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15   3   2   1
0: nid005687:269562:271825 [2] NCCL INFO P2P Chunksize set to 131072
0: nid005687:269560:271822 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14   2
0: nid005687:269560:271822 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13   1
0: nid005687:269560:271822 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12
0: nid005687:269560:271822 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15   3   2   1
0: nid005687:269560:271822 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14   2
0: nid005687:269560:271822 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13   1
0: nid005687:269560:271822 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/8/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
0: nid005687:269560:271822 [0] NCCL INFO P2P Chunksize set to 131072
3: nid005694:125980:127936 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125980:127936 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27135:29107 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27135:29107 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125983:127935 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125983:127935 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108976:110906 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108976:110906 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125982:127938 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125982:127938 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27132:29108 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27132:29108 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27133:29109 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27133:29109 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269563:271824 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269563:271824 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269561:271823 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269561:271823 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27134:29106 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
1: nid005689:27134:29106 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108975:110903 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108975:110903 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108974:110904 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108974:110904 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
2: nid005692:108973:110905 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
2: nid005692:108973:110905 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
0: nid005687:269562:271825 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269562:271825 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
3: nid005694:125981:127937 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
3: nid005694:125981:127937 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27133:29109 [1] NCCL INFO ncclCommSplit comm 0x4007e557d440 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaad5ee6bc0 color 1197013201 key 5 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269560:271822 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
0: nid005687:269560:271822 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
1: nid005689:27135:29107 [3] NCCL INFO ncclCommSplit comm 0x400ca557cbf0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf0cb7810 color 1197013201 key 7 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
1: nid005689:27133:29109 [1] NCCL INFO Init timings: rank 5 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27132:29108 [0] NCCL INFO ncclCommSplit comm 0x400790a9f240 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab011c2b20 color 1197013201 key 4 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
1: nid005689:27135:29107 [3] NCCL INFO Init timings: rank 7 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27132:29108 [0] NCCL INFO Init timings: rank 4 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27134:29106 [2] NCCL INFO ncclCommSplit comm 0x40049d57cb00 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab041a9900 color 1197013201 key 6 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
1: nid005689:27134:29106 [2] NCCL INFO Init timings: rank 6 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108974:110904 [1] NCCL INFO ncclCommSplit comm 0x4007e4a9f590 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaadb715620 color 1197013201 key 9 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
2: nid005692:108973:110905 [0] NCCL INFO ncclCommSplit comm 0x4003dd57cc60 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0c2c5a50 color 1197013201 key 8 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269560:271822 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
2: nid005692:108974:110904 [1] NCCL INFO Init timings: rank 9 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108976:110906 [3] NCCL INFO ncclCommSplit comm 0x40079d57cd20 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab09369150 color 1197013201 key 11 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
2: nid005692:108973:110905 [0] NCCL INFO Init timings: rank 8 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108976:110906 [3] NCCL INFO Init timings: rank 11 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
2: nid005692:108975:110903 [2] NCCL INFO ncclCommSplit comm 0x400c6557ccc0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab08c77530 color 1197013201 key 10 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
2: nid005692:108975:110903 [2] NCCL INFO Init timings: rank 10 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.25, topo 0.32, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125980:127936 [0] NCCL INFO ncclCommSplit comm 0x400478b92fe0 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab051c3880 color 1197013201 key 12 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
3: nid005694:125982:127938 [2] NCCL INFO ncclCommSplit comm 0x400c7557cb20 rank 14 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf5095b90 color 1197013201 key 14 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
3: nid005694:125980:127936 [0] NCCL INFO Init timings: rank 12 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.24, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125982:127938 [2] NCCL INFO Init timings: rank 14 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.24, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125983:127935 [3] NCCL INFO ncclCommSplit comm 0x400c84b93620 rank 15 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab090a68e0 color 1197013201 key 15 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
3: nid005694:125983:127935 [3] NCCL INFO Init timings: rank 15 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.24, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
3: nid005694:125981:127937 [1] NCCL INFO ncclCommSplit comm 0x40047d57cba0 rank 13 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf54480e0 color 1197013201 key 13 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
3: nid005694:125981:127937 [1] NCCL INFO Init timings: rank 13 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.24, topo 0.33, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27132:29119 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
3: nid005694:125980:127948 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid005687:269563:271824 [3] NCCL INFO ncclCommSplit comm 0x400ce157d3c0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaafed45590 color 1197013201 key 3 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269561:271823 [1] NCCL INFO ncclCommSplit comm 0x400c8157caf0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaae4747fe0 color 1197013201 key 1 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269563:271824 [3] NCCL INFO Init timings: rank 3 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.57, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269561:271823 [1] NCCL INFO Init timings: rank 1 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.57, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269562:271825 [2] NCCL INFO ncclCommSplit comm 0x400caca9f3d0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaabf426c60 color 1197013201 key 2 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269562:271825 [2] NCCL INFO Init timings: rank 2 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.00, allgathers 0.00, topo 0.57, graphs 0.02, connections 0.01, rest 0.00)
0: nid005687:269560:271822 [0] NCCL INFO ncclCommSplit comm 0x400c60a9fb50 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac5a75d30 color 1197013201 key 0 commId 0xbe0cb0130ee69ba9 - Init COMPLETE
0: nid005687:269560:271822 [0] NCCL INFO Init timings: rank 0 nranks 16 total 0.61 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.57, graphs 0.02, connections 0.01, rest 0.00)
1: nid005689:27132:29119 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
3: nid005694:125980:127948 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
0: nid005687:269560:271836 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
3: nid005694:125982:127949 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
2: nid005692:108975:110917 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
0: nid005687:269560:271836 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
1: nid005689:27132:29119 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
2: nid005692:108975:110917 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
3: nid005694:125982:127949 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
1: nid005689:27132:29119 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid005689:27132:29119 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
3: nid005694:125980:127948 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
0: nid005687:269562:271837 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
1: nid005689:27132:29119 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
3: nid005694:125980:127948 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
0: nid005687:269562:271837 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
1: nid005689:27134:29121 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125980:127948 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
2: nid005692:108973:110916 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:29121 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:29121 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid005687:269560:271836 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid005692:108975:110917 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125980:127948 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
3: nid005694:125982:127949 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108973:110916 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108975:110917 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108974:110915 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
3: nid005694:125982:127949 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125981:127950 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
2: nid005692:108975:110917 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108974:110915 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
3: nid005694:125982:127949 [2] NCCL INFO Channel 02/0 : 14[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:110917 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
3: nid005694:125981:127950 [1] NCCL INFO Channel 03/0 : 13[1] -> 1[1] [send] via NET/AWS Libfabric/1
2: nid005692:108974:110915 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
3: nid005694:125982:127949 [2] NCCL INFO Channel 06/0 : 14[2] -> 2[2] [send] via NET/AWS Libfabric/2
3: nid005694:125981:127950 [1] NCCL INFO Channel 07/0 : 13[1] -> 1[1] [send] via NET/AWS Libfabric/1
0: nid005687:269560:271836 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
1: nid005689:27132:29119 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108975:110917 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269561:271835 [1] NCCL INFO Channel 03/0 : 13[1] -> 1[1] [receive] via NET/AWS Libfabric/1
1: nid005689:27132:29119 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27135:29120 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269560:271836 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
1: nid005689:27132:29119 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27135:29120 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
0: nid005687:269561:271835 [1] NCCL INFO Channel 07/0 : 13[1] -> 1[1] [receive] via NET/AWS Libfabric/1
0: nid005687:269562:271837 [2] NCCL INFO Channel 02/0 : 14[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27132:29119 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27135:29120 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
0: nid005687:269561:271835 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
0: nid005687:269562:271837 [2] NCCL INFO Channel 06/0 : 14[2] -> 2[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:271837 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269561:271835 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
2: nid005692:108975:110917 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125980:127948 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269562:271837 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid005692:108976:110918 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269560:271836 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108976:110918 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
3: nid005694:125980:127948 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125983:127947 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
2: nid005692:108973:110916 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108976:110918 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
3: nid005694:125980:127948 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110916 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
3: nid005694:125983:127947 [3] NCCL INFO Channel 01/0 : 15[3] -> 3[3] [send] via NET/AWS Libfabric/3
3: nid005694:125982:127949 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
0: nid005687:269562:271837 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 05/0 : 15[3] -> 3[3] [send] via NET/AWS Libfabric/3
3: nid005694:125980:127948 [0] NCCL INFO Channel 04/0 : 12[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110916 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:29121 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
3: nid005694:125982:127949 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
0: nid005687:269562:271837 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid005687:269560:271836 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269560:271836 [0] NCCL INFO Channel 04/0 : 12[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269563:271834 [3] NCCL INFO Channel 01/0 : 15[3] -> 3[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269560:271836 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid005687:269563:271834 [3] NCCL INFO Channel 05/0 : 15[3] -> 3[3] [receive] via NET/AWS Libfabric/3
0: nid005687:269560:271836 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid005687:269563:271834 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
0: nid005687:269563:271834 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
2: nid005692:108976:110918 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
2: nid005692:108976:110918 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
2: nid005692:108975:110917 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
2: nid005692:108975:110917 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
2: nid005692:108974:110915 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid005694:125983:127947 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29120 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid005687:269563:271834 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid005694:125982:127949 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269562:271837 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid005694:125982:127949 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:29121 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid005689:27133:29118 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
3: nid005694:125981:127950 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid005687:269562:271837 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
0: nid005687:269561:271835 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
2: nid005692:108973:110916 [0] NCCL INFO Connected all rings
2: nid005692:108976:110918 [3] NCCL INFO Connected all rings
2: nid005692:108975:110917 [2] NCCL INFO Connected all rings
2: nid005692:108974:110915 [1] NCCL INFO Connected all rings
2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
2: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
2:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
1: nid005689:27132:29119 [0] NCCL INFO Connected all rings
1: nid005689:27135:29120 [3] NCCL INFO Connected all rings
1: nid005689:27133:29118 [1] NCCL INFO Connected all rings
0: nid005687:269560:271836 [0] NCCL INFO Connected all rings
0: nid005687:269563:271834 [3] NCCL INFO Connected all rings
0: nid005687:269561:271835 [1] NCCL INFO Connected all rings
1: nid005689:27134:29121 [2] NCCL INFO Connected all rings
3: nid005694:125980:127948 [0] NCCL INFO Connected all rings
0: nid005687:269562:271837 [2] NCCL INFO Connected all rings
3: nid005694:125983:127947 [3] NCCL INFO Connected all rings
3: nid005694:125982:127949 [2] NCCL INFO Connected all rings
3: nid005694:125981:127950 [1] NCCL INFO Connected all rings
3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
3: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
3:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
1: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
1:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
3: nid005694:125980:127977 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid005694:125981:127979 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
3: nid005694:125981:127979 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
1: nid005689:27133:29129 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
0: nid005687:269561:271864 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid005689:27133:29129 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:271866 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid005687:269561:271864 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125983:127980 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid005694:125981:127979 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
0: nid005687:269562:271866 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269560:271863 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:29126 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
3: nid005694:125981:127979 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
1: nid005689:27133:29129 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125981:127979 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27133:29129 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125981:127979 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid005689:27133:29129 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269560:271863 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0
0: nid005687:269562:271866 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid005687:269560:271863 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid005689:27133:29129 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269561:271864 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid005687:269563:271865 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid005687:269561:271864 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid005687:269561:271864 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid005687:269561:271864 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
2: nid005692:108974:110945 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108974:110945 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108973:110947 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
2: nid005692:108975:110944 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108974:110945 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:127977 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27132:29128 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:127977 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108974:110945 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
3: nid005694:125980:127977 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0
3: nid005694:125980:127977 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27132:29128 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
2: nid005692:108974:110945 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/AWS Libfabric/0
3: nid005694:125982:127978 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108974:110945 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:127978 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid005692:108973:110947 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125982:127978 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269560:271863 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125980:127977 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
1: nid005689:27132:29128 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
0: nid005687:269560:271863 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125982:127978 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
3: nid005694:125980:127977 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108973:110947 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:29128 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/AWS Libfabric/2
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27132:29128 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid005694:125982:127978 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27132:29128 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108973:110947 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid005689:27134:29126 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
3: nid005694:125982:127978 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid005689:27132:29128 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:271866 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
0: nid005687:269562:271866 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid005689:27134:29126 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid005692:108975:110944 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
3: nid005694:125983:127980 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269562:271866 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
3: nid005694:125982:127978 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid005689:27134:29126 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid005692:108975:110944 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
3: nid005694:125983:127980 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
0: nid005687:269563:271865 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid005689:27135:29127 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid005692:108976:110946 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
3: nid005694:125980:127977 [0] NCCL INFO Connected all trees
2: nid005692:108973:110947 [0] NCCL INFO Connected all trees
1: nid005689:27132:29128 [0] NCCL INFO Connected all trees
0: nid005687:269563:271865 [3] NCCL INFO Connected all trees
0: nid005687:269562:271866 [2] NCCL INFO Connected all trees
3: nid005694:125981:127979 [1] NCCL INFO Connected all trees
1: nid005689:27133:29129 [1] NCCL INFO Connected all trees
2: nid005692:108974:110945 [1] NCCL INFO Connected all trees
3: nid005694:125983:127980 [3] NCCL INFO Connected all trees
1: nid005689:27135:29127 [3] NCCL INFO Connected all trees
3: nid005694:125982:127978 [2] NCCL INFO Connected all trees
2: nid005692:108976:110946 [3] NCCL INFO Connected all trees
2: nid005692:108975:110944 [2] NCCL INFO Connected all trees
1: nid005689:27134:29126 [2] NCCL INFO Connected all trees
0: nid005687:269560:271863 [0] NCCL INFO Connected all trees
0: nid005687:269561:271864 [1] NCCL INFO Connected all trees
0:   0%|          | 1/2224 [00:12<7:47:36, 12.62s/it]
0:                                                   
0: 
0: {'loss': 1.1705, 'grad_norm': 4.4609494504635565, 'learning_rate': 1.4925373134328358e-07, 'epoch': 0.0}
0: 
0:   0%|          | 1/2224 [00:12<7:47:36, 12.62s/it]
0:   0%|          | 2/2224 [00:17<5:01:16,  8.14s/it]
0:                                                   
0: 
0: {'loss': 1.181, 'grad_norm': 4.3899701688886195, 'learning_rate': 2.9850746268656716e-07, 'epoch': 0.0}
0: 
0:   0%|          | 2/2224 [00:17<5:01:16,  8.14s/it]
0:   0%|          | 3/2224 [00:22<4:13:35,  6.85s/it]
0:                                                   
0: 
0: {'loss': 1.1766, 'grad_norm': 4.5102986385590755, 'learning_rate': 4.4776119402985074e-07, 'epoch': 0.0}
0: 
0:   0%|          | 3/2224 [00:22<4:13:35,  6.85s/it]
0:   0%|          | 4/2224 [00:27<3:45:25,  6.09s/it]
0:                                                   
0: 
0: {'loss': 1.1799, 'grad_norm': 4.356711062016672, 'learning_rate': 5.970149253731343e-07, 'epoch': 0.0}
0: 
0:   0%|          | 4/2224 [00:27<3:45:25,  6.09s/it]
0:   0%|          | 5/2224 [00:33<3:33:15,  5.77s/it]
0:                                                   
0: 
0: {'loss': 1.1943, 'grad_norm': 4.624043776680894, 'learning_rate': 7.462686567164179e-07, 'epoch': 0.0}
0: 
0:   0%|          | 5/2224 [00:33<3:33:15,  5.77s/it]
0:   0%|          | 6/2224 [00:38<3:23:20,  5.50s/it]
0:                                                   
0: 
0: {'loss': 1.1632, 'grad_norm': 4.53537785669417, 'learning_rate': 8.955223880597015e-07, 'epoch': 0.0}
0: 
0:   0%|          | 6/2224 [00:38<3:23:20,  5.50s/it]
0:   0%|          | 7/2224 [00:42<3:16:38,  5.32s/it]
0:                                                   
0: 
0: {'loss': 1.1826, 'grad_norm': 4.511662335486716, 'learning_rate': 1.044776119402985e-06, 'epoch': 0.0}
0: 
0:   0%|          | 7/2224 [00:42<3:16:38,  5.32s/it]
0:   0%|          | 8/2224 [00:47<3:12:36,  5.22s/it]
0:                                                   
0: 
0: {'loss': 1.1756, 'grad_norm': 4.403212988401159, 'learning_rate': 1.1940298507462686e-06, 'epoch': 0.0}
0: 
0:   0%|          | 8/2224 [00:47<3:12:36,  5.22s/it]
0:   0%|          | 9/2224 [00:52<3:09:56,  5.15s/it]
0:                                                   
0: 
0: {'loss': 1.1514, 'grad_norm': 4.119487237392303, 'learning_rate': 1.3432835820895524e-06, 'epoch': 0.0}
0: 
0:   0%|          | 9/2224 [00:52<3:09:56,  5.15s/it]
0:   0%|          | 10/2224 [00:57<3:07:45,  5.09s/it]
0:                                                    
0: 
0: {'loss': 1.1159, 'grad_norm': 3.9202962791885363, 'learning_rate': 1.4925373134328358e-06, 'epoch': 0.0}
0: 
0:   0%|          | 10/2224 [00:57<3:07:45,  5.09s/it]
0:   0%|          | 11/2224 [01:02<3:06:39,  5.06s/it]
0:                                                    
0: 
0: {'loss': 1.1334, 'grad_norm': 4.4144583726468065, 'learning_rate': 1.6417910447761196e-06, 'epoch': 0.0}
0: 
0:   0%|          | 11/2224 [01:02<3:06:39,  5.06s/it]
0: [2025-07-09 22:54:36,710] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
0:   1%|          | 12/2224 [01:08<3:07:21,  5.08s/it]
0:                                                    
0: 
0: {'loss': 1.0205, 'grad_norm': 3.1356426881314783, 'learning_rate': 1.791044776119403e-06, 'epoch': 0.01}
0: 
0:   1%|          | 12/2224 [01:08<3:07:21,  5.08s/it]
0:   1%|          | 13/2224 [01:13<3:06:08,  5.05s/it]
0:                                                    
0: 
0: {'loss': 1.0316, 'grad_norm': 3.0160213478561446, 'learning_rate': 1.9402985074626867e-06, 'epoch': 0.01}
0: 
0:   1%|          | 13/2224 [01:13<3:06:08,  5.05s/it]
0:   1%|          | 14/2224 [01:17<3:04:50,  5.02s/it]
0:                                                    
0: 
0: {'loss': 0.9804, 'grad_norm': 2.763535798279711, 'learning_rate': 2.08955223880597e-06, 'epoch': 0.01}
0: 
0:   1%|          | 14/2224 [01:17<3:04:50,  5.02s/it]
0:   1%|          | 15/2224 [01:22<3:04:09,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.9884, 'grad_norm': 2.8277211843553927, 'learning_rate': 2.238805970149254e-06, 'epoch': 0.01}
0: 
0:   1%|          | 15/2224 [01:22<3:04:09,  5.00s/it]
0:   1%|          | 16/2224 [01:27<3:03:51,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.921, 'grad_norm': 2.32849724821535, 'learning_rate': 2.3880597014925373e-06, 'epoch': 0.01}
0: 
0:   1%|          | 16/2224 [01:27<3:03:51,  5.00s/it]
0:   1%|          | 17/2224 [01:32<3:04:06,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.8519, 'grad_norm': 3.053027654319761, 'learning_rate': 2.537313432835821e-06, 'epoch': 0.01}
0: 
0:   1%|          | 17/2224 [01:32<3:04:06,  5.01s/it]
0:   1%|          | 18/2224 [01:37<3:02:49,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.8771, 'grad_norm': 3.0625288990991026, 'learning_rate': 2.686567164179105e-06, 'epoch': 0.01}
0: 
0:   1%|          | 18/2224 [01:37<3:02:49,  4.97s/it]
0:   1%|          | 19/2224 [01:42<3:04:27,  5.02s/it]
0:                                                    
0: 
0: {'loss': 0.8703, 'grad_norm': 2.7390104849450116, 'learning_rate': 2.835820895522388e-06, 'epoch': 0.01}
0: 
0:   1%|          | 19/2224 [01:42<3:04:27,  5.02s/it]
0:   1%|          | 20/2224 [01:48<3:04:23,  5.02s/it]
0:                                                    
0: 
0: {'loss': 0.8421, 'grad_norm': 2.7387097029333267, 'learning_rate': 2.9850746268656716e-06, 'epoch': 0.01}
0: 
0:   1%|          | 20/2224 [01:48<3:04:23,  5.02s/it]
0:   1%|          | 21/2224 [01:53<3:04:43,  5.03s/it]
0:                                                    
0: 
0: {'loss': 0.8019, 'grad_norm': 2.2663850382497004, 'learning_rate': 3.1343283582089558e-06, 'epoch': 0.01}
0: 
0:   1%|          | 21/2224 [01:53<3:04:43,  5.03s/it]
0:   1%|          | 22/2224 [01:57<3:03:35,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.7333, 'grad_norm': 1.9125285956978009, 'learning_rate': 3.283582089552239e-06, 'epoch': 0.01}
0: 
0:   1%|          | 22/2224 [01:58<3:03:35,  5.00s/it]
0:   1%|          | 23/2224 [02:02<3:03:13,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.7662, 'grad_norm': 2.04010292686846, 'learning_rate': 3.4328358208955225e-06, 'epoch': 0.01}
0: 
0:   1%|          | 23/2224 [02:02<3:03:13,  4.99s/it]
0:   1%|          | 24/2224 [02:08<3:04:28,  5.03s/it]
0:                                                    
0: 
0: {'loss': 0.7299, 'grad_norm': 2.298370447451649, 'learning_rate': 3.582089552238806e-06, 'epoch': 0.01}
0: 
0:   1%|          | 24/2224 [02:08<3:04:28,  5.03s/it]
0:   1%|          | 25/2224 [02:13<3:03:52,  5.02s/it]
0:                                                    
0: 
0: {'loss': 0.7203, 'grad_norm': 2.126338585314279, 'learning_rate': 3.73134328358209e-06, 'epoch': 0.01}
0: 
0:   1%|          | 25/2224 [02:13<3:03:52,  5.02s/it]
0:   1%|          | 26/2224 [02:17<3:02:26,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.6916, 'grad_norm': 2.014902265150716, 'learning_rate': 3.8805970149253735e-06, 'epoch': 0.01}
0: 
0:   1%|          | 26/2224 [02:17<3:02:26,  4.98s/it]
0:   1%|          | 27/2224 [02:23<3:03:16,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.6671, 'grad_norm': 1.6925747587648865, 'learning_rate': 4.029850746268657e-06, 'epoch': 0.01}
0: 
0:   1%|          | 27/2224 [02:23<3:03:16,  5.01s/it]
0:   1%|â–         | 28/2224 [02:28<3:03:24,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.6818, 'grad_norm': 1.5887840527599852, 'learning_rate': 4.17910447761194e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 28/2224 [02:28<3:03:24,  5.01s/it]
0:   1%|â–         | 29/2224 [02:33<3:03:30,  5.02s/it]
0:                                                    
0: 
0: {'loss': 0.697, 'grad_norm': 1.4996779626886214, 'learning_rate': 4.3283582089552236e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 29/2224 [02:33<3:03:30,  5.02s/it]
0:   1%|â–         | 30/2224 [02:38<3:03:17,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.6367, 'grad_norm': 1.4400241902963613, 'learning_rate': 4.477611940298508e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 30/2224 [02:38<3:03:17,  5.01s/it]
0:   1%|â–         | 31/2224 [02:43<3:03:01,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.6484, 'grad_norm': 1.418475299435297, 'learning_rate': 4.626865671641791e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 31/2224 [02:43<3:03:01,  5.01s/it]
0:   1%|â–         | 32/2224 [02:48<3:02:56,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.6411, 'grad_norm': 1.514118936256216, 'learning_rate': 4.7761194029850745e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 32/2224 [02:48<3:02:56,  5.01s/it]
0:   1%|â–         | 33/2224 [02:53<3:02:21,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5971, 'grad_norm': 1.4244627779664378, 'learning_rate': 4.925373134328359e-06, 'epoch': 0.01}
0: 
0:   1%|â–         | 33/2224 [02:53<3:02:21,  4.99s/it]
0:   2%|â–         | 34/2224 [02:57<3:01:22,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.6356, 'grad_norm': 1.4475124226323524, 'learning_rate': 5.074626865671642e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 34/2224 [02:57<3:01:22,  4.97s/it]
0:   2%|â–         | 35/2224 [03:03<3:02:04,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.617, 'grad_norm': 1.3581844867526183, 'learning_rate': 5.2238805970149255e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 35/2224 [03:03<3:02:04,  4.99s/it]
0:   2%|â–         | 36/2224 [03:07<3:00:50,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.596, 'grad_norm': 1.4289147140919853, 'learning_rate': 5.37313432835821e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 36/2224 [03:07<3:00:50,  4.96s/it]
0:   2%|â–         | 37/2224 [03:12<3:01:04,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5989, 'grad_norm': 1.3431419114703866, 'learning_rate': 5.522388059701493e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 37/2224 [03:12<3:01:04,  4.97s/it]
0:   2%|â–         | 38/2224 [03:17<3:01:16,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.5685, 'grad_norm': 1.3649400253238502, 'learning_rate': 5.671641791044776e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 38/2224 [03:17<3:01:16,  4.98s/it]
0:   2%|â–         | 39/2224 [03:22<3:01:35,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.6003, 'grad_norm': 1.486214780319393, 'learning_rate': 5.820895522388061e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 39/2224 [03:22<3:01:35,  4.99s/it]
0:   2%|â–         | 40/2224 [03:27<3:02:06,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5683, 'grad_norm': 1.3779045755529555, 'learning_rate': 5.970149253731343e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 40/2224 [03:27<3:02:06,  5.00s/it]
0:   2%|â–         | 41/2224 [03:32<3:01:57,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5541, 'grad_norm': 1.3811061172033323, 'learning_rate': 6.119402985074627e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 41/2224 [03:32<3:01:57,  5.00s/it]
0:   2%|â–         | 42/2224 [03:37<3:02:07,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.5261, 'grad_norm': 1.5053244048369587, 'learning_rate': 6.2686567164179116e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 42/2224 [03:37<3:02:07,  5.01s/it]
0:   2%|â–         | 43/2224 [03:42<3:00:48,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5383, 'grad_norm': 1.2667260807768923, 'learning_rate': 6.417910447761194e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 43/2224 [03:42<3:00:48,  4.97s/it]
0:   2%|â–         | 44/2224 [03:47<3:01:22,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5735, 'grad_norm': 1.357950472014993, 'learning_rate': 6.567164179104478e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 44/2224 [03:47<3:01:22,  4.99s/it]
0:   2%|â–         | 45/2224 [03:52<3:01:34,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5575, 'grad_norm': 1.3111861336320596, 'learning_rate': 6.7164179104477625e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 45/2224 [03:52<3:01:34,  5.00s/it]
0:   2%|â–         | 46/2224 [03:57<3:01:29,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5766, 'grad_norm': 1.353598748140235, 'learning_rate': 6.865671641791045e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 46/2224 [03:57<3:01:29,  5.00s/it]
0:   2%|â–         | 47/2224 [04:02<3:01:31,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5283, 'grad_norm': 1.30986040556185, 'learning_rate': 7.014925373134329e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 47/2224 [04:02<3:01:31,  5.00s/it]
0:   2%|â–         | 48/2224 [04:07<3:01:28,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5324, 'grad_norm': 1.2597072709914312, 'learning_rate': 7.164179104477612e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 48/2224 [04:07<3:01:28,  5.00s/it]
0:   2%|â–         | 49/2224 [04:12<3:00:00,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5364, 'grad_norm': 1.3760521152281406, 'learning_rate': 7.313432835820896e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 49/2224 [04:12<3:00:00,  4.97s/it]
0:   2%|â–         | 50/2224 [04:17<2:59:59,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5376, 'grad_norm': 1.349051065696405, 'learning_rate': 7.46268656716418e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 50/2224 [04:17<2:59:59,  4.97s/it]
0:   2%|â–         | 51/2224 [04:22<3:00:39,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5126, 'grad_norm': 1.4251842279630362, 'learning_rate': 7.611940298507463e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 51/2224 [04:22<3:00:39,  4.99s/it]
0:   2%|â–         | 52/2224 [04:27<3:01:07,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5092, 'grad_norm': 1.3950736486897464, 'learning_rate': 7.761194029850747e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 52/2224 [04:27<3:01:07,  5.00s/it]
0:   2%|â–         | 53/2224 [04:32<3:00:59,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5187, 'grad_norm': 1.2968653961436007, 'learning_rate': 7.91044776119403e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 53/2224 [04:32<3:00:59,  5.00s/it]
0:   2%|â–         | 54/2224 [04:37<3:01:00,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.552, 'grad_norm': 1.3378828695685747, 'learning_rate': 8.059701492537314e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 54/2224 [04:37<3:01:00,  5.00s/it]
0:   2%|â–         | 55/2224 [04:42<3:00:31,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5435, 'grad_norm': 1.3265433551796657, 'learning_rate': 8.208955223880599e-06, 'epoch': 0.02}
0: 
0:   2%|â–         | 55/2224 [04:42<3:00:31,  4.99s/it]
0:   3%|â–Ž         | 56/2224 [04:47<3:00:32,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5167, 'grad_norm': 1.3094736701586702, 'learning_rate': 8.35820895522388e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 56/2224 [04:47<3:00:32,  5.00s/it]
0:   3%|â–Ž         | 57/2224 [04:52<2:59:24,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5423, 'grad_norm': 1.2942122124744286, 'learning_rate': 8.507462686567165e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 57/2224 [04:52<2:59:24,  4.97s/it]
0:   3%|â–Ž         | 58/2224 [04:57<2:58:55,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.5455, 'grad_norm': 1.251317899420287, 'learning_rate': 8.656716417910447e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 58/2224 [04:57<2:58:55,  4.96s/it]
0:   3%|â–Ž         | 59/2224 [05:02<2:58:19,  4.94s/it]
0:                                                    
0: 
0: {'loss': 0.5403, 'grad_norm': 1.341302406366192, 'learning_rate': 8.805970149253732e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 59/2224 [05:02<2:58:19,  4.94s/it]
0:   3%|â–Ž         | 60/2224 [05:07<2:58:47,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.5192, 'grad_norm': 1.278327049694892, 'learning_rate': 8.955223880597016e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 60/2224 [05:07<2:58:47,  4.96s/it]
0:   3%|â–Ž         | 61/2224 [05:12<2:59:34,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.5073, 'grad_norm': 1.3188538247280854, 'learning_rate': 9.104477611940299e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 61/2224 [05:12<2:59:34,  4.98s/it]
0:   3%|â–Ž         | 62/2224 [05:17<2:59:58,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5205, 'grad_norm': 1.3548497712922845, 'learning_rate': 9.253731343283582e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 62/2224 [05:17<2:59:58,  4.99s/it]
0:   3%|â–Ž         | 63/2224 [05:22<2:59:53,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5111, 'grad_norm': 1.503100961960385, 'learning_rate': 9.402985074626867e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 63/2224 [05:22<2:59:53,  4.99s/it]
0:   3%|â–Ž         | 64/2224 [05:27<2:59:44,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5401, 'grad_norm': 1.3016525737487084, 'learning_rate': 9.552238805970149e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 64/2224 [05:27<2:59:44,  4.99s/it]
0:   3%|â–Ž         | 65/2224 [05:32<3:00:22,  5.01s/it]
0:                                                    
0: 
0: {'loss': 0.5034, 'grad_norm': 1.3390434472853188, 'learning_rate': 9.701492537313434e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 65/2224 [05:32<3:00:22,  5.01s/it]
0:   3%|â–Ž         | 66/2224 [05:37<2:59:34,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4782, 'grad_norm': 1.3681717799468136, 'learning_rate': 9.850746268656717e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 66/2224 [05:37<2:59:34,  4.99s/it]
0:   3%|â–Ž         | 67/2224 [05:42<2:59:31,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5003, 'grad_norm': 1.3564531668216393, 'learning_rate': 1e-05, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 67/2224 [05:42<2:59:31,  4.99s/it]
0:   3%|â–Ž         | 68/2224 [05:47<2:59:03,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.4861, 'grad_norm': 1.3194604777335082, 'learning_rate': 9.999994696783136e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 68/2224 [05:47<2:59:03,  4.98s/it]
0:   3%|â–Ž         | 69/2224 [05:52<2:59:07,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4914, 'grad_norm': 1.3682519059496698, 'learning_rate': 9.999978787143793e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 69/2224 [05:52<2:59:07,  4.99s/it]
0:   3%|â–Ž         | 70/2224 [05:57<2:59:30,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.5439, 'grad_norm': 1.2866883948661714, 'learning_rate': 9.999952271115718e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 70/2224 [05:57<2:59:30,  5.00s/it]
0:   3%|â–Ž         | 71/2224 [06:02<2:59:13,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.5304, 'grad_norm': 1.2888724069747088, 'learning_rate': 9.999915148755162e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 71/2224 [06:02<2:59:13,  4.99s/it]
0:   3%|â–Ž         | 72/2224 [06:07<2:59:07,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.499, 'grad_norm': 1.432894097826785, 'learning_rate': 9.999867420140871e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 72/2224 [06:07<2:59:07,  4.99s/it]
0:   3%|â–Ž         | 73/2224 [06:12<2:58:58,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4539, 'grad_norm': 1.218268623282174, 'learning_rate': 9.999809085374092e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 73/2224 [06:12<2:58:58,  4.99s/it]
0:   3%|â–Ž         | 74/2224 [06:17<2:57:31,  4.95s/it]
0:                                                    
0: 
0: {'loss': 0.4687, 'grad_norm': 1.1619823627327395, 'learning_rate': 9.999740144578568e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 74/2224 [06:17<2:57:31,  4.95s/it]
0:   3%|â–Ž         | 75/2224 [06:22<2:57:57,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5028, 'grad_norm': 1.2578952551086025, 'learning_rate': 9.999660597900543e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 75/2224 [06:22<2:57:57,  4.97s/it]
0:   3%|â–Ž         | 76/2224 [06:27<2:57:32,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4992, 'grad_norm': 1.3050515976849715, 'learning_rate': 9.999570445508757e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 76/2224 [06:27<2:57:32,  4.96s/it]
0:   3%|â–Ž         | 77/2224 [06:32<2:58:01,  4.97s/it]
0:                                                    
0: 
0: {'loss': 0.5154, 'grad_norm': 1.321156227516488, 'learning_rate': 9.999469687594452e-06, 'epoch': 0.03}
0: 
0:   3%|â–Ž         | 77/2224 [06:32<2:58:01,  4.97s/it]
0:   4%|â–Ž         | 78/2224 [06:37<2:56:51,  4.94s/it]
0:                                                    
0: 
0: {'loss': 0.4751, 'grad_norm': 1.285323980983135, 'learning_rate': 9.999358324371362e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 78/2224 [06:37<2:56:51,  4.94s/it]
0:   4%|â–Ž         | 79/2224 [06:42<2:57:28,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4935, 'grad_norm': 1.2939720857668309, 'learning_rate': 9.999236356075723e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 79/2224 [06:42<2:57:28,  4.96s/it]
0:   4%|â–Ž         | 80/2224 [06:47<2:56:33,  4.94s/it]
0:                                                    
0: 
0: {'loss': 0.4979, 'grad_norm': 1.3112051471369273, 'learning_rate': 9.999103782966262e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 80/2224 [06:47<2:56:33,  4.94s/it]
0:   4%|â–Ž         | 81/2224 [06:52<2:57:09,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4315, 'grad_norm': 1.2603641057768482, 'learning_rate': 9.998960605324204e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 81/2224 [06:52<2:57:09,  4.96s/it]
0:   4%|â–Ž         | 82/2224 [06:57<2:56:06,  4.93s/it]
0:                                                    
0: 
0: {'loss': 0.4848, 'grad_norm': 1.2567673282972334, 'learning_rate': 9.998806823453272e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 82/2224 [06:57<2:56:06,  4.93s/it]
0:   4%|â–Ž         | 83/2224 [07:01<2:56:33,  4.95s/it]
0:                                                    
0: 
0: {'loss': 0.475, 'grad_norm': 1.3140996233153412, 'learning_rate': 9.998642437679682e-06, 'epoch': 0.04}
0: 
0:   4%|â–Ž         | 83/2224 [07:01<2:56:33,  4.95s/it]
0:   4%|â–         | 84/2224 [07:06<2:56:57,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4705, 'grad_norm': 1.2941280631009904, 'learning_rate': 9.998467448352141e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 84/2224 [07:06<2:56:57,  4.96s/it]
0:   4%|â–         | 85/2224 [07:11<2:56:11,  4.94s/it]
0:                                                    
0: 
0: {'loss': 0.4477, 'grad_norm': 1.2606722496885203, 'learning_rate': 9.998281855841853e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 85/2224 [07:11<2:56:11,  4.94s/it]
0:   4%|â–         | 86/2224 [07:17<2:58:17,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.4702, 'grad_norm': 1.2434742812292994, 'learning_rate': 9.998085660542512e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 86/2224 [07:17<2:58:17,  5.00s/it]
0:   4%|â–         | 87/2224 [07:21<2:57:11,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.4879, 'grad_norm': 1.2866626099841698, 'learning_rate': 9.997878862870306e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 87/2224 [07:21<2:57:11,  4.98s/it]
0:   4%|â–         | 88/2224 [07:26<2:57:14,  4.98s/it]
0:                                                    
0: 
0: {'loss': 0.4607, 'grad_norm': 1.1874276222513747, 'learning_rate': 9.99766146326391e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 88/2224 [07:26<2:57:14,  4.98s/it]
0:   4%|â–         | 89/2224 [07:31<2:56:34,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4611, 'grad_norm': 1.1680789649751149, 'learning_rate': 9.997433462184494e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 89/2224 [07:31<2:56:34,  4.96s/it]
0:   4%|â–         | 90/2224 [07:36<2:56:25,  4.96s/it]
0:                                                    
0: 
0: {'loss': 0.4888, 'grad_norm': 1.268063077050707, 'learning_rate': 9.997194860115712e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 90/2224 [07:36<2:56:25,  4.96s/it]
0:   4%|â–         | 91/2224 [07:41<2:55:13,  4.93s/it]
0:                                                    
0: 
0: {'loss': 0.4678, 'grad_norm': 1.3117707039443167, 'learning_rate': 9.996945657563705e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 91/2224 [07:41<2:55:13,  4.93s/it]
0:   4%|â–         | 92/2224 [07:46<2:55:55,  4.95s/it]
0:                                                    
0: 
0: {'loss': 0.4554, 'grad_norm': 1.2316253441658909, 'learning_rate': 9.996685855057107e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 92/2224 [07:46<2:55:55,  4.95s/it]
0:   4%|â–         | 93/2224 [07:51<2:57:45,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.4997, 'grad_norm': 1.22440079825517, 'learning_rate': 9.99641545314703e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 93/2224 [07:51<2:57:45,  5.00s/it]
0:   4%|â–         | 94/2224 [07:56<2:57:35,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.4525, 'grad_norm': 1.2526869069296351, 'learning_rate': 9.996134452407077e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 94/2224 [07:56<2:57:35,  5.00s/it]
0:   4%|â–         | 95/2224 [08:01<2:57:03,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4993, 'grad_norm': 1.2351654077817444, 'learning_rate': 9.995842853433331e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 95/2224 [08:01<2:57:03,  4.99s/it]
0:   4%|â–         | 96/2224 [08:06<2:56:49,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4601, 'grad_norm': 1.2512486632555164, 'learning_rate': 9.995540656844356e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 96/2224 [08:06<2:56:49,  4.99s/it]
0:   4%|â–         | 97/2224 [08:11<2:57:19,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.4956, 'grad_norm': 1.1528492338264837, 'learning_rate': 9.995227863281197e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 97/2224 [08:11<2:57:19,  5.00s/it]
0:   4%|â–         | 98/2224 [08:16<2:57:16,  5.00s/it]
0:                                                    
0: 
0: {'loss': 0.4667, 'grad_norm': 1.2306373709628837, 'learning_rate': 9.994904473407379e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 98/2224 [08:16<2:57:16,  5.00s/it]
0:   4%|â–         | 99/2224 [08:21<2:56:51,  4.99s/it]
0:                                                    
0: 
0: {'loss': 0.4698, 'grad_norm': 1.2491108597634577, 'learning_rate': 9.994570487908906e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 99/2224 [08:21<2:56:51,  4.99s/it]
0:   4%|â–         | 100/2224 [08:26<2:56:13,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4546, 'grad_norm': 1.2671011471623344, 'learning_rate': 9.994225907494256e-06, 'epoch': 0.04}
0: 
0:   4%|â–         | 100/2224 [08:26<2:56:13,  4.98s/it]
0:   5%|â–         | 101/2224 [08:31<2:56:07,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4624, 'grad_norm': 1.226733352132196, 'learning_rate': 9.993870732894382e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 101/2224 [08:31<2:56:07,  4.98s/it]
0:   5%|â–         | 102/2224 [08:36<2:56:19,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4836, 'grad_norm': 1.2377506540584957, 'learning_rate': 9.993504964862714e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 102/2224 [08:36<2:56:19,  4.99s/it]
0:   5%|â–         | 103/2224 [08:41<2:57:57,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4938, 'grad_norm': 1.233493097628344, 'learning_rate': 9.993128604175148e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 103/2224 [08:41<2:57:57,  5.03s/it]
0:   5%|â–         | 104/2224 [08:46<2:57:26,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4386, 'grad_norm': 1.2704533028544538, 'learning_rate': 9.992741651630055e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 104/2224 [08:46<2:57:26,  5.02s/it]
0:   5%|â–         | 105/2224 [08:51<2:56:59,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4499, 'grad_norm': 1.2411360633802226, 'learning_rate': 9.992344108048271e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 105/2224 [08:51<2:56:59,  5.01s/it]
0:   5%|â–         | 106/2224 [08:56<2:55:14,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4634, 'grad_norm': 1.3005453842453685, 'learning_rate': 9.991935974273099e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 106/2224 [08:56<2:55:14,  4.96s/it]
0:   5%|â–         | 107/2224 [09:01<2:55:31,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4613, 'grad_norm': 1.3303647825345826, 'learning_rate': 9.991517251170311e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 107/2224 [09:01<2:55:31,  4.97s/it]
0:   5%|â–         | 108/2224 [09:06<2:56:18,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4468, 'grad_norm': 1.2185404528480368, 'learning_rate': 9.991087939628135e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 108/2224 [09:06<2:56:18,  5.00s/it]
0:   5%|â–         | 109/2224 [09:11<2:56:13,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4364, 'grad_norm': 1.2290867182455285, 'learning_rate': 9.990648040557268e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 109/2224 [09:11<2:56:13,  5.00s/it]
0:   5%|â–         | 110/2224 [09:16<2:56:17,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4375, 'grad_norm': 1.1875885674756388, 'learning_rate': 9.990197554890858e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 110/2224 [09:16<2:56:17,  5.00s/it]
0:   5%|â–         | 111/2224 [09:21<2:56:16,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4697, 'grad_norm': 1.2225389965287146, 'learning_rate': 9.989736483584515e-06, 'epoch': 0.05}
0: 
0:   5%|â–         | 111/2224 [09:21<2:56:16,  5.01s/it]
0:   5%|â–Œ         | 112/2224 [09:26<2:57:24,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.4475, 'grad_norm': 1.1939722768328294, 'learning_rate': 9.989264827616307e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 112/2224 [09:26<2:57:24,  5.04s/it]
0:   5%|â–Œ         | 113/2224 [09:31<2:56:43,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4206, 'grad_norm': 1.200284075091307, 'learning_rate': 9.988782587986749e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 113/2224 [09:31<2:56:43,  5.02s/it]
0:   5%|â–Œ         | 114/2224 [09:36<2:56:09,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4187, 'grad_norm': 1.2767047232495192, 'learning_rate': 9.988289765718808e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 114/2224 [09:36<2:56:09,  5.01s/it]
0:   5%|â–Œ         | 115/2224 [09:41<2:54:50,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4476, 'grad_norm': 1.3128099160896753, 'learning_rate': 9.987786361857904e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 115/2224 [09:41<2:54:50,  4.97s/it]
0:   5%|â–Œ         | 116/2224 [09:46<2:55:51,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4397, 'grad_norm': 1.3057638970811514, 'learning_rate': 9.9872723774719e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 116/2224 [09:46<2:55:51,  5.01s/it]
0:   5%|â–Œ         | 117/2224 [09:51<2:55:56,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4314, 'grad_norm': 1.2217184157631302, 'learning_rate': 9.986747813651103e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 117/2224 [09:51<2:55:56,  5.01s/it]
0:   5%|â–Œ         | 118/2224 [09:56<2:55:57,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4696, 'grad_norm': 1.2206621588060105, 'learning_rate': 9.986212671508265e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 118/2224 [09:56<2:55:57,  5.01s/it]
0:   5%|â–Œ         | 119/2224 [10:01<2:55:47,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.487, 'grad_norm': 1.2364052644286745, 'learning_rate': 9.985666952178576e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 119/2224 [10:01<2:55:47,  5.01s/it]
0:   5%|â–Œ         | 120/2224 [10:06<2:55:25,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4514, 'grad_norm': 1.1893276701883182, 'learning_rate': 9.985110656819662e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 120/2224 [10:06<2:55:25,  5.00s/it]
0:   5%|â–Œ         | 121/2224 [10:11<2:55:06,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4585, 'grad_norm': 1.1688103925775628, 'learning_rate': 9.984543786611585e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 121/2224 [10:11<2:55:06,  5.00s/it]
0:   5%|â–Œ         | 122/2224 [10:16<2:55:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4363, 'grad_norm': 1.1996795473373403, 'learning_rate': 9.98396634275684e-06, 'epoch': 0.05}
0: 
0:   5%|â–Œ         | 122/2224 [10:16<2:55:16,  5.00s/it]
0:   6%|â–Œ         | 123/2224 [10:21<2:54:54,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4487, 'grad_norm': 1.239698218677581, 'learning_rate': 9.983378326480352e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 123/2224 [10:21<2:54:54,  4.99s/it]
0:   6%|â–Œ         | 124/2224 [10:26<2:54:26,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4689, 'grad_norm': 1.2087444130147347, 'learning_rate': 9.98277973902947e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 124/2224 [10:26<2:54:26,  4.98s/it]
0:   6%|â–Œ         | 125/2224 [10:31<2:54:31,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.418, 'grad_norm': 1.2420715571093377, 'learning_rate': 9.98217058167397e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 125/2224 [10:31<2:54:31,  4.99s/it]
0:   6%|â–Œ         | 126/2224 [10:36<2:54:30,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4502, 'grad_norm': 1.2205248268810447, 'learning_rate': 9.98155085570605e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 126/2224 [10:36<2:54:30,  4.99s/it]
0:   6%|â–Œ         | 127/2224 [10:41<2:54:26,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4512, 'grad_norm': 1.1702620001742496, 'learning_rate': 9.980920562440329e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 127/2224 [10:41<2:54:26,  4.99s/it]
0:   6%|â–Œ         | 128/2224 [10:46<2:52:50,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.4361, 'grad_norm': 1.2030521650225752, 'learning_rate': 9.980279703213833e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 128/2224 [10:46<2:52:50,  4.95s/it]
0:   6%|â–Œ         | 129/2224 [10:51<2:53:28,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4202, 'grad_norm': 1.1663829867885602, 'learning_rate': 9.979628279386015e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 129/2224 [10:51<2:53:28,  4.97s/it]
0:   6%|â–Œ         | 130/2224 [10:56<2:54:16,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4564, 'grad_norm': 1.1951066720961756, 'learning_rate': 9.978966292338728e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 130/2224 [10:56<2:54:16,  4.99s/it]
0:   6%|â–Œ         | 131/2224 [11:01<2:53:54,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4245, 'grad_norm': 1.1865995040059487, 'learning_rate': 9.978293743476239e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 131/2224 [11:01<2:53:54,  4.99s/it]
0:   6%|â–Œ         | 132/2224 [11:06<2:54:06,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4435, 'grad_norm': 1.2171808210184978, 'learning_rate': 9.977610634225214e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 132/2224 [11:06<2:54:06,  4.99s/it]
0:   6%|â–Œ         | 133/2224 [11:11<2:54:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4436, 'grad_norm': 1.232067997354956, 'learning_rate': 9.976916966034725e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 133/2224 [11:11<2:54:16,  5.00s/it]
0:   6%|â–Œ         | 134/2224 [11:16<2:54:13,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4269, 'grad_norm': 1.207916636003252, 'learning_rate': 9.976212740376241e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 134/2224 [11:16<2:54:13,  5.00s/it]
0:   6%|â–Œ         | 135/2224 [11:21<2:54:19,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4289, 'grad_norm': 1.2654261242405145, 'learning_rate': 9.975497958743628e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 135/2224 [11:21<2:54:19,  5.01s/it]
0:   6%|â–Œ         | 136/2224 [11:26<2:53:43,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4331, 'grad_norm': 1.2311335126271388, 'learning_rate': 9.974772622653142e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 136/2224 [11:26<2:53:43,  4.99s/it]
0:   6%|â–Œ         | 137/2224 [11:31<2:53:29,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4457, 'grad_norm': 1.2833325356223808, 'learning_rate': 9.974036733643426e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 137/2224 [11:31<2:53:29,  4.99s/it]
0:   6%|â–Œ         | 138/2224 [11:36<2:53:28,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4627, 'grad_norm': 1.1898228302867093, 'learning_rate': 9.973290293275515e-06, 'epoch': 0.06}
0: 
0:   6%|â–Œ         | 138/2224 [11:36<2:53:28,  4.99s/it]
0:   6%|â–‹         | 139/2224 [11:41<2:53:44,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4007, 'grad_norm': 1.2387957572677708, 'learning_rate': 9.972533303132822e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 139/2224 [11:41<2:53:44,  5.00s/it]
0:   6%|â–‹         | 140/2224 [11:46<2:53:42,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4157, 'grad_norm': 1.1579388070248546, 'learning_rate': 9.971765764821142e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 140/2224 [11:46<2:53:42,  5.00s/it]
0:   6%|â–‹         | 141/2224 [11:51<2:53:54,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4767, 'grad_norm': 1.290996139804508, 'learning_rate': 9.97098767996864e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 141/2224 [11:51<2:53:54,  5.01s/it]
0:   6%|â–‹         | 142/2224 [11:56<2:52:29,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4286, 'grad_norm': 1.2139129189410658, 'learning_rate': 9.97019905022586e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 142/2224 [11:56<2:52:29,  4.97s/it]
0:   6%|â–‹         | 143/2224 [12:01<2:52:54,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4419, 'grad_norm': 1.263380246629134, 'learning_rate': 9.96939987726571e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 143/2224 [12:01<2:52:54,  4.99s/it]
0:   6%|â–‹         | 144/2224 [12:06<2:53:21,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4506, 'grad_norm': 1.129892410448688, 'learning_rate': 9.968590162783467e-06, 'epoch': 0.06}
0: 
0:   6%|â–‹         | 144/2224 [12:06<2:53:21,  5.00s/it]
0:   7%|â–‹         | 145/2224 [12:11<2:53:01,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4206, 'grad_norm': 1.2607010842479112, 'learning_rate': 9.967769908496765e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 145/2224 [12:11<2:53:01,  4.99s/it]
0:   7%|â–‹         | 146/2224 [12:16<2:52:39,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4186, 'grad_norm': 1.2328228769423557, 'learning_rate': 9.966939116145603e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 146/2224 [12:16<2:52:39,  4.99s/it]
0:   7%|â–‹         | 147/2224 [12:21<2:51:16,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.42, 'grad_norm': 1.1243358686819964, 'learning_rate': 9.966097787492324e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 147/2224 [12:21<2:51:16,  4.95s/it]
0:   7%|â–‹         | 148/2224 [12:26<2:51:28,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4166, 'grad_norm': 1.1678590748563642, 'learning_rate': 9.965245924321631e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 148/2224 [12:26<2:51:28,  4.96s/it]
0:   7%|â–‹         | 149/2224 [12:31<2:52:00,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4393, 'grad_norm': 1.216101963304606, 'learning_rate': 9.964383528440568e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 149/2224 [12:31<2:52:00,  4.97s/it]
0:   7%|â–‹         | 150/2224 [12:36<2:52:03,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4057, 'grad_norm': 1.1761518278582672, 'learning_rate': 9.963510601678527e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 150/2224 [12:36<2:52:03,  4.98s/it]
0:   7%|â–‹         | 151/2224 [12:41<2:51:05,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.4447, 'grad_norm': 1.1770914806905814, 'learning_rate': 9.962627145887233e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 151/2224 [12:41<2:51:05,  4.95s/it]
0:   7%|â–‹         | 152/2224 [12:46<2:52:07,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4549, 'grad_norm': 1.1587339424898957, 'learning_rate': 9.961733162940752e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 152/2224 [12:46<2:52:07,  4.98s/it]
0:   7%|â–‹         | 153/2224 [12:51<2:52:22,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4411, 'grad_norm': 1.1306785209464882, 'learning_rate': 9.960828654735477e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 153/2224 [12:51<2:52:22,  4.99s/it]
0:   7%|â–‹         | 154/2224 [12:56<2:52:01,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4194, 'grad_norm': 1.1453112364326892, 'learning_rate': 9.959913623190127e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 154/2224 [12:56<2:52:01,  4.99s/it]
0:   7%|â–‹         | 155/2224 [13:01<2:50:50,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.429, 'grad_norm': 1.1955369487540315, 'learning_rate': 9.958988070245748e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 155/2224 [13:01<2:50:50,  4.95s/it]
0:   7%|â–‹         | 156/2224 [13:06<2:51:28,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.403, 'grad_norm': 1.1366049321382028, 'learning_rate': 9.958051997865703e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 156/2224 [13:06<2:51:28,  4.97s/it]
0:   7%|â–‹         | 157/2224 [13:11<2:52:04,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4291, 'grad_norm': 1.1565131943588332, 'learning_rate': 9.95710540803567e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 157/2224 [13:11<2:52:04,  4.99s/it]
0:   7%|â–‹         | 158/2224 [13:16<2:52:18,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4353, 'grad_norm': 1.1470932506551341, 'learning_rate': 9.956148302763639e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 158/2224 [13:16<2:52:18,  5.00s/it]
0:   7%|â–‹         | 159/2224 [13:21<2:52:19,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4183, 'grad_norm': 1.1285083545095314, 'learning_rate': 9.955180684079901e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 159/2224 [13:21<2:52:19,  5.01s/it]
0:   7%|â–‹         | 160/2224 [13:26<2:51:58,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4033, 'grad_norm': 1.1720058983306572, 'learning_rate': 9.954202554037059e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 160/2224 [13:26<2:51:58,  5.00s/it]
0:   7%|â–‹         | 161/2224 [13:31<2:50:28,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4289, 'grad_norm': 1.2776113131025761, 'learning_rate': 9.95321391471e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 161/2224 [13:31<2:50:28,  4.96s/it]
0:   7%|â–‹         | 162/2224 [13:36<2:51:15,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4562, 'grad_norm': 1.1903632295592763, 'learning_rate': 9.952214768195914e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 162/2224 [13:36<2:51:15,  4.98s/it]
0:   7%|â–‹         | 163/2224 [13:41<2:50:13,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.421, 'grad_norm': 1.134719255953089, 'learning_rate': 9.95120511661428e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 163/2224 [13:41<2:50:13,  4.96s/it]
0:   7%|â–‹         | 164/2224 [13:46<2:50:33,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4135, 'grad_norm': 1.1366521924159312, 'learning_rate': 9.950184962106855e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 164/2224 [13:46<2:50:33,  4.97s/it]
0:   7%|â–‹         | 165/2224 [13:51<2:51:09,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4172, 'grad_norm': 1.2238620958841198, 'learning_rate': 9.949154306837682e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 165/2224 [13:51<2:51:09,  4.99s/it]
0:   7%|â–‹         | 166/2224 [13:56<2:51:22,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4078, 'grad_norm': 1.2009577811116752, 'learning_rate': 9.948113152993076e-06, 'epoch': 0.07}
0: 
0:   7%|â–‹         | 166/2224 [13:56<2:51:22,  5.00s/it]
0:   8%|â–Š         | 167/2224 [14:00<2:50:00,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4421, 'grad_norm': 1.0925332635926344, 'learning_rate': 9.94706150278162e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 167/2224 [14:00<2:50:00,  4.96s/it]
0:   8%|â–Š         | 168/2224 [14:05<2:50:20,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.42, 'grad_norm': 1.1775084052863363, 'learning_rate': 9.94599935843417e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 168/2224 [14:05<2:50:20,  4.97s/it]
0:   8%|â–Š         | 169/2224 [14:11<2:51:03,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4179, 'grad_norm': 1.1504985609324716, 'learning_rate': 9.944926722203833e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 169/2224 [14:11<2:51:03,  4.99s/it]
0:   8%|â–Š         | 170/2224 [14:16<2:51:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4408, 'grad_norm': 1.1113692888620905, 'learning_rate': 9.943843596365985e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 170/2224 [14:16<2:51:16,  5.00s/it]
0:   8%|â–Š         | 171/2224 [14:21<2:51:57,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4519, 'grad_norm': 1.1139722514998276, 'learning_rate': 9.94274998321824e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 171/2224 [14:21<2:51:57,  5.03s/it]
0:   8%|â–Š         | 172/2224 [14:26<2:50:16,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4009, 'grad_norm': 1.1485523025878026, 'learning_rate': 9.94164588508047e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 172/2224 [14:26<2:50:16,  4.98s/it]
0:   8%|â–Š         | 173/2224 [14:31<2:50:52,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4265, 'grad_norm': 1.1039670833708402, 'learning_rate': 9.940531304294781e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 173/2224 [14:31<2:50:52,  5.00s/it]
0:   8%|â–Š         | 174/2224 [14:36<2:51:04,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3981, 'grad_norm': 1.0898922495459484, 'learning_rate': 9.93940624322552e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 174/2224 [14:36<2:51:04,  5.01s/it]
0:   8%|â–Š         | 175/2224 [14:41<2:51:34,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4395, 'grad_norm': 1.108799189494607, 'learning_rate': 9.938270704259263e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 175/2224 [14:41<2:51:34,  5.02s/it]
0:   8%|â–Š         | 176/2224 [14:45<2:48:51,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.4208, 'grad_norm': 1.1233533810485168, 'learning_rate': 9.937124689804814e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 176/2224 [14:45<2:48:51,  4.95s/it]
0:   8%|â–Š         | 177/2224 [14:50<2:49:15,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4099, 'grad_norm': 1.127975972370144, 'learning_rate': 9.935968202293196e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 177/2224 [14:50<2:49:15,  4.96s/it]
0:   8%|â–Š         | 178/2224 [14:55<2:49:29,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4287, 'grad_norm': 1.1501883742995191, 'learning_rate': 9.934801244177655e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 178/2224 [14:55<2:49:29,  4.97s/it]
0:   8%|â–Š         | 179/2224 [15:00<2:49:41,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4338, 'grad_norm': 1.1468100890461905, 'learning_rate': 9.933623817933642e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 179/2224 [15:00<2:49:41,  4.98s/it]
0:   8%|â–Š         | 180/2224 [15:05<2:49:29,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.39, 'grad_norm': 1.163003973350506, 'learning_rate': 9.932435926058815e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 180/2224 [15:05<2:49:29,  4.98s/it]
0:   8%|â–Š         | 181/2224 [15:10<2:50:03,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4247, 'grad_norm': 1.0976458288535846, 'learning_rate': 9.931237571073034e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 181/2224 [15:10<2:50:03,  4.99s/it]
0:   8%|â–Š         | 182/2224 [15:15<2:50:13,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4176, 'grad_norm': 1.142149814251144, 'learning_rate': 9.930028755518352e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 182/2224 [15:15<2:50:13,  5.00s/it]
0:   8%|â–Š         | 183/2224 [15:20<2:50:05,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4203, 'grad_norm': 1.143275569412449, 'learning_rate': 9.928809481959016e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 183/2224 [15:20<2:50:05,  5.00s/it]
0:   8%|â–Š         | 184/2224 [15:25<2:50:27,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4095, 'grad_norm': 1.2345144112552082, 'learning_rate': 9.927579752981454e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 184/2224 [15:25<2:50:27,  5.01s/it]
0:   8%|â–Š         | 185/2224 [15:30<2:49:42,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3871, 'grad_norm': 1.2662568928514848, 'learning_rate': 9.926339571194273e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 185/2224 [15:30<2:49:42,  4.99s/it]
0:   8%|â–Š         | 186/2224 [15:35<2:49:54,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4202, 'grad_norm': 1.1866920305552429, 'learning_rate': 9.925088939228252e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 186/2224 [15:35<2:49:54,  5.00s/it]
0:   8%|â–Š         | 187/2224 [15:40<2:49:45,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4249, 'grad_norm': 1.1126509095265231, 'learning_rate': 9.923827859736344e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 187/2224 [15:40<2:49:45,  5.00s/it]
0:   8%|â–Š         | 188/2224 [15:45<2:49:28,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.415, 'grad_norm': 1.1021634706871473, 'learning_rate': 9.922556335393661e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 188/2224 [15:45<2:49:28,  4.99s/it]
0:   8%|â–Š         | 189/2224 [15:50<2:49:51,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3987, 'grad_norm': 1.063748472738315, 'learning_rate': 9.921274368897465e-06, 'epoch': 0.08}
0: 
0:   8%|â–Š         | 189/2224 [15:50<2:49:51,  5.01s/it]
0:   9%|â–Š         | 190/2224 [15:55<2:49:56,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3995, 'grad_norm': 1.1017933487277305, 'learning_rate': 9.91998196296718e-06, 'epoch': 0.09}
0: 
0:   9%|â–Š         | 190/2224 [15:55<2:49:56,  5.01s/it]
0:   9%|â–Š         | 191/2224 [16:00<2:49:57,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4113, 'grad_norm': 1.133820255968527, 'learning_rate': 9.918679120344367e-06, 'epoch': 0.09}
0: 
0:   9%|â–Š         | 191/2224 [16:00<2:49:57,  5.02s/it]
0:   9%|â–Š         | 192/2224 [16:05<2:49:39,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4121, 'grad_norm': 1.1119332801094353, 'learning_rate': 9.91736584379273e-06, 'epoch': 0.09}
0: 
0:   9%|â–Š         | 192/2224 [16:05<2:49:39,  5.01s/it]
0:   9%|â–Š         | 193/2224 [16:10<2:49:25,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3991, 'grad_norm': 1.1712112315203156, 'learning_rate': 9.916042136098102e-06, 'epoch': 0.09}
0: 
0:   9%|â–Š         | 193/2224 [16:10<2:49:25,  5.01s/it]
0:   9%|â–Š         | 194/2224 [16:15<2:49:27,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4362, 'grad_norm': 1.1403911348065574, 'learning_rate': 9.914708000068452e-06, 'epoch': 0.09}
0: 
0:   9%|â–Š         | 194/2224 [16:16<2:49:27,  5.01s/it]
0:   9%|â–‰         | 195/2224 [16:21<2:49:33,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4478, 'grad_norm': 1.087553101388696, 'learning_rate': 9.91336343853386e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 195/2224 [16:21<2:49:33,  5.01s/it]
0:   9%|â–‰         | 196/2224 [16:26<2:49:47,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4247, 'grad_norm': 1.082964161927727, 'learning_rate': 9.912008454346531e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 196/2224 [16:26<2:49:47,  5.02s/it]
0:   9%|â–‰         | 197/2224 [16:31<2:49:41,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4238, 'grad_norm': 1.069205393617196, 'learning_rate': 9.910643050380772e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 197/2224 [16:31<2:49:41,  5.02s/it]
0:   9%|â–‰         | 198/2224 [16:35<2:48:24,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4174, 'grad_norm': 1.0648453117823995, 'learning_rate': 9.909267229532996e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 198/2224 [16:35<2:48:24,  4.99s/it]
0:   9%|â–‰         | 199/2224 [16:40<2:47:27,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3898, 'grad_norm': 1.069873205146841, 'learning_rate': 9.907880994721714e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 199/2224 [16:40<2:47:27,  4.96s/it]
0:   9%|â–‰         | 200/2224 [16:45<2:47:35,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4065, 'grad_norm': 1.1191516301467044, 'learning_rate': 9.90648434888753e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 200/2224 [16:45<2:47:35,  4.97s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:   9%|â–‰         | 201/2224 [18:10<16:09:37, 28.76s/it]
0:                                                      
0: 
0: {'loss': 0.4026, 'grad_norm': 1.0669309661177682, 'learning_rate': 9.905077294993129e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 201/2224 [18:10<16:09:37, 28.76s/it]
0:   9%|â–‰         | 202/2224 [18:15<12:09:20, 21.64s/it]
0:                                                      
0: 
0: {'loss': 0.4089, 'grad_norm': 1.051273908665459, 'learning_rate': 9.903659836023273e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 202/2224 [18:15<12:09:20, 21.64s/it]
0:   9%|â–‰         | 203/2224 [18:20<9:20:45, 16.65s/it] 
0:                                                     
0: 
0: {'loss': 0.4316, 'grad_norm': 1.1258099091560811, 'learning_rate': 9.902231974984802e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 203/2224 [18:20<9:20:45, 16.65s/it]
0:   9%|â–‰         | 204/2224 [18:25<7:23:09, 13.16s/it]
0:                                                     
0: 
0: {'loss': 0.3909, 'grad_norm': 1.0658132136416596, 'learning_rate': 9.900793714906618e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 204/2224 [18:25<7:23:09, 13.16s/it]
0:   9%|â–‰         | 205/2224 [18:30<5:59:33, 10.69s/it]
0:                                                     
0: 
0: {'loss': 0.4032, 'grad_norm': 1.132677824675794, 'learning_rate': 9.899345058839682e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 205/2224 [18:30<5:59:33, 10.69s/it]
0:   9%|â–‰         | 206/2224 [18:35<5:02:00,  8.98s/it]
0:                                                     
0: 
0: {'loss': 0.4475, 'grad_norm': 1.1597493327455273, 'learning_rate': 9.897886009857011e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 206/2224 [18:35<5:02:00,  8.98s/it]
0:   9%|â–‰         | 207/2224 [18:40<4:21:44,  7.79s/it]
0:                                                     
0: 
0: {'loss': 0.4253, 'grad_norm': 1.0927359431163353, 'learning_rate': 9.896416571053665e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 207/2224 [18:40<4:21:44,  7.79s/it]
0:   9%|â–‰         | 208/2224 [18:45<3:53:42,  6.96s/it]
0:                                                     
0: 
0: {'loss': 0.4122, 'grad_norm': 1.11615548265225, 'learning_rate': 9.894936745546744e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 208/2224 [18:45<3:53:42,  6.96s/it]
0:   9%|â–‰         | 209/2224 [18:50<3:33:44,  6.36s/it]
0:                                                     
0: 
0: {'loss': 0.3622, 'grad_norm': 1.027154505938927, 'learning_rate': 9.893446536475383e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 209/2224 [18:50<3:33:44,  6.36s/it]
0:   9%|â–‰         | 210/2224 [18:54<3:18:17,  5.91s/it]
0:                                                     
0: 
0: {'loss': 0.4189, 'grad_norm': 1.1314658235646025, 'learning_rate': 9.891945947000742e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 210/2224 [18:54<3:18:17,  5.91s/it]
0:   9%|â–‰         | 211/2224 [18:59<3:09:11,  5.64s/it]
0:                                                     
0: 
0: {'loss': 0.4133, 'grad_norm': 1.1048099040067738, 'learning_rate': 9.890434980306005e-06, 'epoch': 0.09}
0: 
0:   9%|â–‰         | 211/2224 [18:59<3:09:11,  5.64s/it]
0:  10%|â–‰         | 212/2224 [19:04<3:01:38,  5.42s/it]
0:                                                     
0: 
0: {'loss': 0.401, 'grad_norm': 1.0903215257223104, 'learning_rate': 9.888913639596364e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 212/2224 [19:04<3:01:38,  5.42s/it]
0:  10%|â–‰         | 213/2224 [19:09<2:57:07,  5.28s/it]
0:                                                     
0: 
0: {'loss': 0.4191, 'grad_norm': 1.1112243430619761, 'learning_rate': 9.887381928099016e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 213/2224 [19:09<2:57:07,  5.28s/it]
0:  10%|â–‰         | 214/2224 [19:15<2:56:51,  5.28s/it]
0:                                                     
0: 
0: {'loss': 0.3908, 'grad_norm': 1.114114626793239, 'learning_rate': 9.885839849063163e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 214/2224 [19:15<2:56:51,  5.28s/it]
0:  10%|â–‰         | 215/2224 [19:20<2:54:01,  5.20s/it]
0:                                                     
0: 
0: {'loss': 0.4106, 'grad_norm': 1.1001592356323602, 'learning_rate': 9.884287405759997e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 215/2224 [19:20<2:54:01,  5.20s/it]
0:  10%|â–‰         | 216/2224 [19:25<2:52:49,  5.16s/it]
0:                                                     
0: 
0: {'loss': 0.3975, 'grad_norm': 1.0738280508637363, 'learning_rate': 9.882724601482694e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 216/2224 [19:25<2:52:49,  5.16s/it]
0:  10%|â–‰         | 217/2224 [19:30<2:50:09,  5.09s/it]
0:                                                     
0: 
0: {'loss': 0.3993, 'grad_norm': 1.0879126741408303, 'learning_rate': 9.88115143954641e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 217/2224 [19:30<2:50:09,  5.09s/it]
0:  10%|â–‰         | 218/2224 [19:35<2:48:18,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4503, 'grad_norm': 1.120503970220465, 'learning_rate': 9.879567923288275e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 218/2224 [19:35<2:48:18,  5.03s/it]
0:  10%|â–‰         | 219/2224 [19:40<2:48:04,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4261, 'grad_norm': 1.197008582847072, 'learning_rate': 9.877974056067379e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 219/2224 [19:40<2:48:04,  5.03s/it]
0:  10%|â–‰         | 220/2224 [19:45<2:48:08,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4042, 'grad_norm': 1.0772505351920274, 'learning_rate': 9.87636984126477e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 220/2224 [19:45<2:48:08,  5.03s/it]
0:  10%|â–‰         | 221/2224 [19:50<2:47:52,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4207, 'grad_norm': 1.1575510828610356, 'learning_rate': 9.87475528228345e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 221/2224 [19:50<2:47:52,  5.03s/it]
0:  10%|â–‰         | 222/2224 [19:55<2:47:51,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.425, 'grad_norm': 1.1213833984609647, 'learning_rate': 9.873130382548362e-06, 'epoch': 0.1}
0: 
0:  10%|â–‰         | 222/2224 [19:55<2:47:51,  5.03s/it]
0:  10%|â–ˆ         | 223/2224 [20:00<2:47:14,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4222, 'grad_norm': 1.0842919489828913, 'learning_rate': 9.87149514550638e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 223/2224 [20:00<2:47:14,  5.01s/it]
0:  10%|â–ˆ         | 224/2224 [20:05<2:48:12,  5.05s/it]
0:                                                     
0: 
0: {'loss': 0.397, 'grad_norm': 1.0524806223585266, 'learning_rate': 9.869849574626317e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 224/2224 [20:05<2:48:12,  5.05s/it]
0:  10%|â–ˆ         | 225/2224 [20:10<2:46:35,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4054, 'grad_norm': 1.0642787714771726, 'learning_rate': 9.868193673398897e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 225/2224 [20:10<2:46:35,  5.00s/it]
0:  10%|â–ˆ         | 226/2224 [20:15<2:46:11,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.398, 'grad_norm': 1.0539155349583635, 'learning_rate': 9.86652744533676e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 226/2224 [20:15<2:46:11,  4.99s/it]
0:  10%|â–ˆ         | 227/2224 [20:20<2:45:45,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4085, 'grad_norm': 1.1252218773577838, 'learning_rate': 9.864850893974457e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 227/2224 [20:20<2:45:45,  4.98s/it]
0:  10%|â–ˆ         | 228/2224 [20:25<2:49:24,  5.09s/it]
0:                                                     
0: 
0: {'loss': 0.3932, 'grad_norm': 1.1744879330609899, 'learning_rate': 9.863164022868432e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 228/2224 [20:25<2:49:24,  5.09s/it]
0:  10%|â–ˆ         | 229/2224 [20:30<2:48:58,  5.08s/it]
0:                                                     
0: 
0: {'loss': 0.4209, 'grad_norm': 1.1274950084931774, 'learning_rate': 9.861466835597022e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 229/2224 [20:30<2:48:58,  5.08s/it]
0:  10%|â–ˆ         | 230/2224 [20:35<2:47:42,  5.05s/it]
0:                                                     
0: 
0: {'loss': 0.3709, 'grad_norm': 1.052930626506067, 'learning_rate': 9.85975933576045e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 230/2224 [20:35<2:47:42,  5.05s/it]
0:  10%|â–ˆ         | 231/2224 [20:40<2:50:01,  5.12s/it]
0:                                                     
0: 
0: {'loss': 0.429, 'grad_norm': 1.1262771176774455, 'learning_rate': 9.858041526980811e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 231/2224 [20:40<2:50:01,  5.12s/it]
0:  10%|â–ˆ         | 232/2224 [20:45<2:48:39,  5.08s/it]
0:                                                     
0: 
0: {'loss': 0.4371, 'grad_norm': 1.092810803887153, 'learning_rate': 9.85631341290207e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 232/2224 [20:45<2:48:39,  5.08s/it]
0:  10%|â–ˆ         | 233/2224 [20:51<2:55:17,  5.28s/it]
0:                                                     
0: 
0: {'loss': 0.4255, 'grad_norm': 1.099852257713845, 'learning_rate': 9.854574997190053e-06, 'epoch': 0.1}
0: 
0:  10%|â–ˆ         | 233/2224 [20:51<2:55:17,  5.28s/it]
0:  11%|â–ˆ         | 234/2224 [20:56<2:52:36,  5.20s/it]
0:                                                     
0: 
0: {'loss': 0.3951, 'grad_norm': 1.0450918353999965, 'learning_rate': 9.852826283532439e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 234/2224 [20:56<2:52:36,  5.20s/it]
0:  11%|â–ˆ         | 235/2224 [21:02<2:57:22,  5.35s/it]
0:                                                     
0: 
0: {'loss': 0.4262, 'grad_norm': 1.089861584375774, 'learning_rate': 9.851067275638749e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 235/2224 [21:02<2:57:22,  5.35s/it]
0:  11%|â–ˆ         | 236/2224 [21:07<2:53:50,  5.25s/it]
0:                                                     
0: 
0: {'loss': 0.3993, 'grad_norm': 1.1089804422594438, 'learning_rate': 9.849297977240345e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 236/2224 [21:07<2:53:50,  5.25s/it]
0:  11%|â–ˆ         | 237/2224 [21:12<2:51:34,  5.18s/it]
0:                                                     
0: 
0: {'loss': 0.3797, 'grad_norm': 1.10962330678274, 'learning_rate': 9.847518392090416e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 237/2224 [21:12<2:51:34,  5.18s/it]
0:  11%|â–ˆ         | 238/2224 [21:17<2:50:29,  5.15s/it]
0:                                                     
0: 
0: {'loss': 0.4107, 'grad_norm': 1.0797772040494167, 'learning_rate': 9.84572852396397e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 238/2224 [21:17<2:50:29,  5.15s/it]
0:  11%|â–ˆ         | 239/2224 [21:22<2:49:01,  5.11s/it]
0:                                                     
0: 
0: {'loss': 0.3923, 'grad_norm': 1.062973159192743, 'learning_rate': 9.843928376657836e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 239/2224 [21:22<2:49:01,  5.11s/it]
0:  11%|â–ˆ         | 240/2224 [21:27<2:47:51,  5.08s/it]
0:                                                     
0: 
0: {'loss': 0.363, 'grad_norm': 1.0284893331770977, 'learning_rate': 9.842117953990637e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 240/2224 [21:27<2:47:51,  5.08s/it]
0:  11%|â–ˆ         | 241/2224 [21:32<2:46:40,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3967, 'grad_norm': 1.1364927411398453, 'learning_rate': 9.8402972598028e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 241/2224 [21:32<2:46:40,  5.04s/it]
0:  11%|â–ˆ         | 242/2224 [21:37<2:46:04,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4387, 'grad_norm': 1.0956895549422712, 'learning_rate': 9.83846629795654e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 242/2224 [21:37<2:46:04,  5.03s/it]
0:  11%|â–ˆ         | 243/2224 [21:42<2:46:08,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.4132, 'grad_norm': 1.149104104721662, 'learning_rate': 9.836625072335855e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 243/2224 [21:42<2:46:08,  5.03s/it]
0:  11%|â–ˆ         | 244/2224 [21:47<2:46:27,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.4533, 'grad_norm': 1.14395117214701, 'learning_rate': 9.834773586846509e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 244/2224 [21:47<2:46:27,  5.04s/it]
0:  11%|â–ˆ         | 245/2224 [21:52<2:46:05,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.4253, 'grad_norm': 1.090990169690643, 'learning_rate': 9.832911845416033e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 245/2224 [21:52<2:46:05,  5.04s/it]
0:  11%|â–ˆ         | 246/2224 [21:57<2:44:44,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4109, 'grad_norm': 1.097837715160688, 'learning_rate': 9.831039851993717e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 246/2224 [21:57<2:44:44,  5.00s/it]
0:  11%|â–ˆ         | 247/2224 [22:02<2:44:38,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4063, 'grad_norm': 1.0512890277603877, 'learning_rate': 9.829157610550594e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 247/2224 [22:02<2:44:38,  5.00s/it]
0:  11%|â–ˆ         | 248/2224 [22:07<2:44:56,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4219, 'grad_norm': 1.082876099187527, 'learning_rate': 9.827265125079438e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 248/2224 [22:07<2:44:56,  5.01s/it]
0:  11%|â–ˆ         | 249/2224 [22:12<2:44:45,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3956, 'grad_norm': 1.100238306666442, 'learning_rate': 9.825362399594756e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 249/2224 [22:12<2:44:45,  5.01s/it]
0:  11%|â–ˆ         | 250/2224 [22:17<2:44:40,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4118, 'grad_norm': 1.0844281546975572, 'learning_rate': 9.82344943813277e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆ         | 250/2224 [22:17<2:44:40,  5.01s/it]
0:  11%|â–ˆâ–        | 251/2224 [22:22<2:44:15,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4051, 'grad_norm': 1.0390852504471069, 'learning_rate': 9.821526244751426e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆâ–        | 251/2224 [22:22<2:44:15,  5.00s/it]
0:  11%|â–ˆâ–        | 252/2224 [22:27<2:43:13,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3819, 'grad_norm': 1.1165938596454934, 'learning_rate': 9.81959282353036e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆâ–        | 252/2224 [22:27<2:43:13,  4.97s/it]
0:  11%|â–ˆâ–        | 253/2224 [22:32<2:43:35,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3624, 'grad_norm': 0.9878650131226754, 'learning_rate': 9.817649178570918e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆâ–        | 253/2224 [22:32<2:43:35,  4.98s/it]
0:  11%|â–ˆâ–        | 254/2224 [22:37<2:42:43,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4105, 'grad_norm': 1.065972881294342, 'learning_rate': 9.81569531399613e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆâ–        | 254/2224 [22:37<2:42:43,  4.96s/it]
0:  11%|â–ˆâ–        | 255/2224 [22:42<2:43:08,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4143, 'grad_norm': 1.1130929553085833, 'learning_rate': 9.813731233950699e-06, 'epoch': 0.11}
0: 
0:  11%|â–ˆâ–        | 255/2224 [22:42<2:43:08,  4.97s/it]
0:  12%|â–ˆâ–        | 256/2224 [22:47<2:44:31,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3683, 'grad_norm': 1.0468703270583746, 'learning_rate': 9.811756942601004e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 256/2224 [22:47<2:44:31,  5.02s/it]
0:  12%|â–ˆâ–        | 257/2224 [22:52<2:43:46,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4227, 'grad_norm': 1.0537343941007833, 'learning_rate': 9.809772444135084e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 257/2224 [22:52<2:43:46,  5.00s/it]
0:  12%|â–ˆâ–        | 258/2224 [22:57<2:43:27,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4115, 'grad_norm': 1.1002576793808414, 'learning_rate': 9.807777742762627e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 258/2224 [22:57<2:43:27,  4.99s/it]
0:  12%|â–ˆâ–        | 259/2224 [23:02<2:43:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3968, 'grad_norm': 1.0863795329521149, 'learning_rate': 9.805772842714968e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 259/2224 [23:02<2:43:48,  5.00s/it]
0:  12%|â–ˆâ–        | 260/2224 [23:06<2:41:14,  4.93s/it]
0:                                                     
0: 
0: {'loss': 0.373, 'grad_norm': 1.0584336594630253, 'learning_rate': 9.803757748245075e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 260/2224 [23:06<2:41:14,  4.93s/it]
0:  12%|â–ˆâ–        | 261/2224 [23:11<2:42:03,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.4129, 'grad_norm': 1.080845040723121, 'learning_rate': 9.80173246362754e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 261/2224 [23:11<2:42:03,  4.95s/it]
0:  12%|â–ˆâ–        | 262/2224 [23:16<2:42:33,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4182, 'grad_norm': 1.0277543660574329, 'learning_rate': 9.799696993158575e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 262/2224 [23:16<2:42:33,  4.97s/it]
0:  12%|â–ˆâ–        | 263/2224 [23:21<2:42:41,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4465, 'grad_norm': 1.1193737931536976, 'learning_rate': 9.797651341155991e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 263/2224 [23:21<2:42:41,  4.98s/it]
0:  12%|â–ˆâ–        | 264/2224 [23:26<2:41:54,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3703, 'grad_norm': 1.0526384702747018, 'learning_rate': 9.795595511959208e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 264/2224 [23:26<2:41:54,  4.96s/it]
0:  12%|â–ˆâ–        | 265/2224 [23:31<2:42:28,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4089, 'grad_norm': 1.0866586537688017, 'learning_rate': 9.79352950992923e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 265/2224 [23:31<2:42:28,  4.98s/it]
0:  12%|â–ˆâ–        | 266/2224 [23:36<2:43:03,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4146, 'grad_norm': 1.073442080728484, 'learning_rate': 9.791453339448634e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 266/2224 [23:36<2:43:03,  5.00s/it]
0:  12%|â–ˆâ–        | 267/2224 [23:41<2:42:53,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3623, 'grad_norm': 1.1350160595114027, 'learning_rate': 9.789367004921578e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 267/2224 [23:41<2:42:53,  4.99s/it]
0:  12%|â–ˆâ–        | 268/2224 [23:46<2:43:21,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3726, 'grad_norm': 1.1040086256085075, 'learning_rate': 9.787270510773772e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 268/2224 [23:46<2:43:21,  5.01s/it]
0:  12%|â–ˆâ–        | 269/2224 [23:51<2:41:41,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3876, 'grad_norm': 1.0649034348804873, 'learning_rate': 9.785163861452485e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 269/2224 [23:51<2:41:41,  4.96s/it]
0:  12%|â–ˆâ–        | 270/2224 [23:56<2:41:45,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3685, 'grad_norm': 1.0306362155799693, 'learning_rate': 9.78304706142652e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 270/2224 [23:56<2:41:45,  4.97s/it]
0:  12%|â–ˆâ–        | 271/2224 [24:01<2:41:57,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4003, 'grad_norm': 1.0017404044515335, 'learning_rate': 9.780920115186221e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 271/2224 [24:01<2:41:57,  4.98s/it]
0:  12%|â–ˆâ–        | 272/2224 [24:06<2:41:05,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3954, 'grad_norm': 1.1123940123848666, 'learning_rate': 9.77878302724345e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 272/2224 [24:06<2:41:05,  4.95s/it]
0:  12%|â–ˆâ–        | 273/2224 [24:11<2:41:52,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4088, 'grad_norm': 1.046095043636718, 'learning_rate': 9.77663580213158e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 273/2224 [24:11<2:41:52,  4.98s/it]
0:  12%|â–ˆâ–        | 274/2224 [24:16<2:41:13,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3884, 'grad_norm': 1.0236327089017414, 'learning_rate': 9.774478444405494e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 274/2224 [24:16<2:41:13,  4.96s/it]
0:  12%|â–ˆâ–        | 275/2224 [24:21<2:41:33,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4004, 'grad_norm': 1.0243622027836055, 'learning_rate': 9.772310958641568e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 275/2224 [24:21<2:41:33,  4.97s/it]
0:  12%|â–ˆâ–        | 276/2224 [24:26<2:42:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3792, 'grad_norm': 1.0039094251298346, 'learning_rate': 9.770133349437657e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 276/2224 [24:26<2:42:16,  5.00s/it]
0:  12%|â–ˆâ–        | 277/2224 [24:31<2:42:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4049, 'grad_norm': 1.0227939663753722, 'learning_rate': 9.767945621413097e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–        | 277/2224 [24:31<2:42:16,  5.00s/it]
0:  12%|â–ˆâ–Ž        | 278/2224 [24:36<2:42:39,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4049, 'grad_norm': 1.0413232714989524, 'learning_rate': 9.765747779208685e-06, 'epoch': 0.12}
0: 
0:  12%|â–ˆâ–Ž        | 278/2224 [24:36<2:42:39,  5.02s/it]
0:  13%|â–ˆâ–Ž        | 279/2224 [24:41<2:42:17,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4029, 'grad_norm': 1.0528622905186655, 'learning_rate': 9.763539827486677e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 279/2224 [24:41<2:42:17,  5.01s/it]
0:  13%|â–ˆâ–Ž        | 280/2224 [24:46<2:41:07,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3817, 'grad_norm': 1.0315773827673766, 'learning_rate': 9.761321770930767e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 280/2224 [24:46<2:41:07,  4.97s/it]
0:  13%|â–ˆâ–Ž        | 281/2224 [24:51<2:41:46,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3976, 'grad_norm': 1.0520135939660362, 'learning_rate': 9.759093614246095e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 281/2224 [24:51<2:41:46,  5.00s/it]
0:  13%|â–ˆâ–Ž        | 282/2224 [24:56<2:42:25,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3877, 'grad_norm': 0.9877479727126616, 'learning_rate': 9.756855362159216e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 282/2224 [24:56<2:42:25,  5.02s/it]
0:  13%|â–ˆâ–Ž        | 283/2224 [25:01<2:42:28,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4073, 'grad_norm': 1.0163316955728956, 'learning_rate': 9.754607019418106e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 283/2224 [25:01<2:42:28,  5.02s/it]
0:  13%|â–ˆâ–Ž        | 284/2224 [25:06<2:41:18,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3932, 'grad_norm': 1.0384166300060287, 'learning_rate': 9.752348590792144e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 284/2224 [25:06<2:41:18,  4.99s/it]
0:  13%|â–ˆâ–Ž        | 285/2224 [25:11<2:41:26,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3952, 'grad_norm': 1.0772139350786, 'learning_rate': 9.750080081072105e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 285/2224 [25:11<2:41:26,  5.00s/it]
0:  13%|â–ˆâ–Ž        | 286/2224 [25:16<2:41:46,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3801, 'grad_norm': 1.0085238443263973, 'learning_rate': 9.747801495070149e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 286/2224 [25:16<2:41:46,  5.01s/it]
0:  13%|â–ˆâ–Ž        | 287/2224 [25:21<2:41:45,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3922, 'grad_norm': 1.0496822999323305, 'learning_rate': 9.74551283761981e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 287/2224 [25:21<2:41:45,  5.01s/it]
0:  13%|â–ˆâ–Ž        | 288/2224 [25:26<2:41:15,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4235, 'grad_norm': 1.1111963648050542, 'learning_rate': 9.743214113575987e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 288/2224 [25:26<2:41:15,  5.00s/it]
0:  13%|â–ˆâ–Ž        | 289/2224 [25:31<2:41:08,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4347, 'grad_norm': 1.0724860451395648, 'learning_rate': 9.740905327814931e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 289/2224 [25:31<2:41:08,  5.00s/it]
0:  13%|â–ˆâ–Ž        | 290/2224 [25:36<2:42:02,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3988, 'grad_norm': 1.0128996001515704, 'learning_rate': 9.738586485234242e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 290/2224 [25:36<2:42:02,  5.03s/it]
0:  13%|â–ˆâ–Ž        | 291/2224 [25:41<2:41:51,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3907, 'grad_norm': 1.026824652170196, 'learning_rate': 9.736257590752847e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 291/2224 [25:41<2:41:51,  5.02s/it]
0:  13%|â–ˆâ–Ž        | 292/2224 [25:46<2:40:19,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4089, 'grad_norm': 1.1455481502724458, 'learning_rate': 9.733918649311001e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 292/2224 [25:46<2:40:19,  4.98s/it]
0:  13%|â–ˆâ–Ž        | 293/2224 [25:51<2:39:22,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.399, 'grad_norm': 1.040543101139434, 'learning_rate': 9.73156966587027e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 293/2224 [25:51<2:39:22,  4.95s/it]
0:  13%|â–ˆâ–Ž        | 294/2224 [25:56<2:40:00,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4133, 'grad_norm': 1.0660051540071265, 'learning_rate': 9.729210645413518e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 294/2224 [25:56<2:40:00,  4.97s/it]
0:  13%|â–ˆâ–Ž        | 295/2224 [26:01<2:40:05,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3826, 'grad_norm': 1.0287128779582162, 'learning_rate': 9.726841592944907e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 295/2224 [26:01<2:40:05,  4.98s/it]
0:  13%|â–ˆâ–Ž        | 296/2224 [26:06<2:39:28,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3682, 'grad_norm': 1.0012943816553106, 'learning_rate': 9.724462513489874e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 296/2224 [26:06<2:39:28,  4.96s/it]
0:  13%|â–ˆâ–Ž        | 297/2224 [26:11<2:39:20,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.374, 'grad_norm': 1.028002028156316, 'learning_rate': 9.722073412095131e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 297/2224 [26:11<2:39:20,  4.96s/it]
0:  13%|â–ˆâ–Ž        | 298/2224 [26:16<2:39:41,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3619, 'grad_norm': 1.0542450557072414, 'learning_rate': 9.719674293828647e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 298/2224 [26:16<2:39:41,  4.97s/it]
0:  13%|â–ˆâ–Ž        | 299/2224 [26:21<2:41:07,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4401, 'grad_norm': 1.072813381353644, 'learning_rate': 9.71726516377964e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 299/2224 [26:21<2:41:07,  5.02s/it]
0:  13%|â–ˆâ–Ž        | 300/2224 [26:26<2:41:09,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.422, 'grad_norm': 1.0781082547037837, 'learning_rate': 9.714846027058564e-06, 'epoch': 0.13}
0: 
0:  13%|â–ˆâ–Ž        | 300/2224 [26:26<2:41:09,  5.03s/it]
0:  14%|â–ˆâ–Ž        | 301/2224 [26:31<2:40:51,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3607, 'grad_norm': 1.0952108286935547, 'learning_rate': 9.7124168887971e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–Ž        | 301/2224 [26:31<2:40:51,  5.02s/it]
0:  14%|â–ˆâ–Ž        | 302/2224 [26:36<2:40:32,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4052, 'grad_norm': 1.1021500133727808, 'learning_rate': 9.709977754148152e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–Ž        | 302/2224 [26:36<2:40:32,  5.01s/it]
0:  14%|â–ˆâ–Ž        | 303/2224 [26:41<2:40:22,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.412, 'grad_norm': 1.1121703646181949, 'learning_rate': 9.707528628285821e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–Ž        | 303/2224 [26:41<2:40:22,  5.01s/it]
0:  14%|â–ˆâ–Ž        | 304/2224 [26:46<2:40:20,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3626, 'grad_norm': 1.0283148219121379, 'learning_rate': 9.705069516405405e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–Ž        | 304/2224 [26:46<2:40:20,  5.01s/it]
0:  14%|â–ˆâ–Ž        | 305/2224 [26:51<2:40:03,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3903, 'grad_norm': 1.036458445076344, 'learning_rate': 9.702600423723388e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–Ž        | 305/2224 [26:51<2:40:03,  5.00s/it]
0:  14%|â–ˆâ–        | 306/2224 [26:56<2:40:29,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.405, 'grad_norm': 0.9891133750267616, 'learning_rate': 9.700121355477418e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 306/2224 [26:56<2:40:29,  5.02s/it]
0:  14%|â–ˆâ–        | 307/2224 [27:01<2:37:40,  4.94s/it]
0:                                                     
0: 
0: {'loss': 0.3542, 'grad_norm': 1.0387694846670479, 'learning_rate': 9.697632316926316e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 307/2224 [27:01<2:37:40,  4.94s/it]
0:  14%|â–ˆâ–        | 308/2224 [27:06<2:38:37,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3709, 'grad_norm': 0.982976210672763, 'learning_rate': 9.695133313350042e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 308/2224 [27:06<2:38:37,  4.97s/it]
0:  14%|â–ˆâ–        | 309/2224 [27:11<2:38:55,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4254, 'grad_norm': 0.9994605586487547, 'learning_rate': 9.692624350049701e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 309/2224 [27:11<2:38:55,  4.98s/it]
0:  14%|â–ˆâ–        | 310/2224 [27:16<2:38:46,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3772, 'grad_norm': 1.0799281064440394, 'learning_rate': 9.690105432347523e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 310/2224 [27:16<2:38:46,  4.98s/it]
0:  14%|â–ˆâ–        | 311/2224 [27:21<2:39:20,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3576, 'grad_norm': 1.0974344021113853, 'learning_rate': 9.687576565586856e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 311/2224 [27:21<2:39:20,  5.00s/it]
0:  14%|â–ˆâ–        | 312/2224 [27:26<2:38:14,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3489, 'grad_norm': 0.9771467261888115, 'learning_rate': 9.685037755132152e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 312/2224 [27:26<2:38:14,  4.97s/it]
0:  14%|â–ˆâ–        | 313/2224 [27:31<2:37:54,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.4059, 'grad_norm': 1.0112407195378992, 'learning_rate': 9.682489006368952e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 313/2224 [27:31<2:37:54,  4.96s/it]
0:  14%|â–ˆâ–        | 314/2224 [27:36<2:38:24,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3828, 'grad_norm': 1.0365020253046142, 'learning_rate': 9.679930324703888e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 314/2224 [27:36<2:38:24,  4.98s/it]
0:  14%|â–ˆâ–        | 315/2224 [27:41<2:37:28,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3722, 'grad_norm': 1.048743467123536, 'learning_rate': 9.677361715564653e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 315/2224 [27:41<2:37:28,  4.95s/it]
0:  14%|â–ˆâ–        | 316/2224 [27:46<2:38:02,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3994, 'grad_norm': 1.0564365961908646, 'learning_rate': 9.674783184400005e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 316/2224 [27:46<2:38:02,  4.97s/it]
0:  14%|â–ˆâ–        | 317/2224 [27:51<2:38:56,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3689, 'grad_norm': 1.0370797084591348, 'learning_rate': 9.67219473667975e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 317/2224 [27:51<2:38:56,  5.00s/it]
0:  14%|â–ˆâ–        | 318/2224 [27:56<2:39:02,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4005, 'grad_norm': 1.0106605677591487, 'learning_rate': 9.669596377894727e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 318/2224 [27:56<2:39:02,  5.01s/it]
0:  14%|â–ˆâ–        | 319/2224 [28:01<2:38:38,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3641, 'grad_norm': 0.9684528620542028, 'learning_rate': 9.666988113556799e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 319/2224 [28:01<2:38:38,  5.00s/it]
0:  14%|â–ˆâ–        | 320/2224 [28:06<2:38:15,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3977, 'grad_norm': 1.0123099902805195, 'learning_rate': 9.664369949198843e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 320/2224 [28:06<2:38:15,  4.99s/it]
0:  14%|â–ˆâ–        | 321/2224 [28:11<2:37:08,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3634, 'grad_norm': 1.0082977968003297, 'learning_rate': 9.661741890374737e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 321/2224 [28:11<2:37:08,  4.95s/it]
0:  14%|â–ˆâ–        | 322/2224 [28:16<2:37:33,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3797, 'grad_norm': 1.0630981329409313, 'learning_rate': 9.659103942659345e-06, 'epoch': 0.14}
0: 
0:  14%|â–ˆâ–        | 322/2224 [28:16<2:37:33,  4.97s/it]
0:  15%|â–ˆâ–        | 323/2224 [28:21<2:38:02,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.4287, 'grad_norm': 1.0577688185746952, 'learning_rate': 9.656456111648513e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 323/2224 [28:21<2:38:02,  4.99s/it]
0:  15%|â–ˆâ–        | 324/2224 [28:26<2:37:07,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3996, 'grad_norm': 1.049305391100619, 'learning_rate': 9.65379840295905e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 324/2224 [28:26<2:37:07,  4.96s/it]
0:  15%|â–ˆâ–        | 325/2224 [28:31<2:38:06,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3927, 'grad_norm': 1.04335828915382, 'learning_rate': 9.651130822228718e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 325/2224 [28:31<2:38:06,  5.00s/it]
0:  15%|â–ˆâ–        | 326/2224 [28:36<2:37:17,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.395, 'grad_norm': 1.0446011632144694, 'learning_rate': 9.648453375116217e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 326/2224 [28:36<2:37:17,  4.97s/it]
0:  15%|â–ˆâ–        | 327/2224 [28:41<2:37:27,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3791, 'grad_norm': 1.0171526060020784, 'learning_rate': 9.645766067301186e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 327/2224 [28:41<2:37:27,  4.98s/it]
0:  15%|â–ˆâ–        | 328/2224 [28:46<2:37:45,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3687, 'grad_norm': 0.9871526756696405, 'learning_rate': 9.643068904484172e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 328/2224 [28:46<2:37:45,  4.99s/it]
0:  15%|â–ˆâ–        | 329/2224 [28:51<2:37:33,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3941, 'grad_norm': 0.9914256253158359, 'learning_rate': 9.640361892386629e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 329/2224 [28:51<2:37:33,  4.99s/it]
0:  15%|â–ˆâ–        | 330/2224 [28:56<2:38:02,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3703, 'grad_norm': 1.0200739985996348, 'learning_rate': 9.637645036750912e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 330/2224 [28:56<2:38:02,  5.01s/it]
0:  15%|â–ˆâ–        | 331/2224 [29:01<2:38:20,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4073, 'grad_norm': 1.0194187297755457, 'learning_rate': 9.634918343340244e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 331/2224 [29:01<2:38:20,  5.02s/it]
0:  15%|â–ˆâ–        | 332/2224 [29:06<2:38:15,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.4042, 'grad_norm': 1.036658462848317, 'learning_rate': 9.632181817938728e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 332/2224 [29:06<2:38:15,  5.02s/it]
0:  15%|â–ˆâ–        | 333/2224 [29:11<2:37:40,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3635, 'grad_norm': 0.9790248429253176, 'learning_rate': 9.629435466351316e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–        | 333/2224 [29:11<2:37:40,  5.00s/it]
0:  15%|â–ˆâ–Œ        | 334/2224 [29:16<2:36:41,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3797, 'grad_norm': 1.0437134516720483, 'learning_rate': 9.626679294403809e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 334/2224 [29:16<2:36:41,  4.97s/it]
0:  15%|â–ˆâ–Œ        | 335/2224 [29:21<2:37:06,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3773, 'grad_norm': 1.0173928550921232, 'learning_rate': 9.62391330794284e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 335/2224 [29:21<2:37:06,  4.99s/it]
0:  15%|â–ˆâ–Œ        | 336/2224 [29:26<2:37:02,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.415, 'grad_norm': 1.0383587325895929, 'learning_rate': 9.621137512835855e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 336/2224 [29:26<2:37:02,  4.99s/it]
0:  15%|â–ˆâ–Œ        | 337/2224 [29:31<2:36:35,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.4001, 'grad_norm': 1.0151695561665464, 'learning_rate': 9.618351914971116e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 337/2224 [29:31<2:36:35,  4.98s/it]
0:  15%|â–ˆâ–Œ        | 338/2224 [29:36<2:36:53,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3725, 'grad_norm': 0.9673832667952196, 'learning_rate': 9.61555652025767e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 338/2224 [29:36<2:36:53,  4.99s/it]
0:  15%|â–ˆâ–Œ        | 339/2224 [29:41<2:37:14,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4143, 'grad_norm': 1.033113720731181, 'learning_rate': 9.612751334625355e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 339/2224 [29:41<2:37:14,  5.01s/it]
0:  15%|â–ˆâ–Œ        | 340/2224 [29:46<2:36:39,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3811, 'grad_norm': 1.0425485667240233, 'learning_rate': 9.609936364024772e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 340/2224 [29:46<2:36:39,  4.99s/it]
0:  15%|â–ˆâ–Œ        | 341/2224 [29:51<2:37:05,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.4018, 'grad_norm': 1.017982544298863, 'learning_rate': 9.60711161442728e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 341/2224 [29:51<2:37:05,  5.01s/it]
0:  15%|â–ˆâ–Œ        | 342/2224 [29:56<2:37:22,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.432, 'grad_norm': 1.0479400445509814, 'learning_rate': 9.604277091824984e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 342/2224 [29:56<2:37:22,  5.02s/it]
0:  15%|â–ˆâ–Œ        | 343/2224 [30:01<2:37:13,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3813, 'grad_norm': 1.022335963745467, 'learning_rate': 9.601432802230722e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 343/2224 [30:01<2:37:13,  5.02s/it]
0:  15%|â–ˆâ–Œ        | 344/2224 [30:06<2:36:16,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3734, 'grad_norm': 1.0027838195536947, 'learning_rate': 9.598578751678042e-06, 'epoch': 0.15}
0: 
0:  15%|â–ˆâ–Œ        | 344/2224 [30:06<2:36:16,  4.99s/it]
0:  16%|â–ˆâ–Œ        | 345/2224 [30:11<2:36:21,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3925, 'grad_norm': 1.0760779396671467, 'learning_rate': 9.595714946221209e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 345/2224 [30:11<2:36:21,  4.99s/it]
0:  16%|â–ˆâ–Œ        | 346/2224 [30:16<2:36:32,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4109, 'grad_norm': 1.045516482059243, 'learning_rate': 9.592841391935173e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 346/2224 [30:16<2:36:32,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 347/2224 [30:21<2:36:39,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.399, 'grad_norm': 1.052138320852491, 'learning_rate': 9.589958094915566e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 347/2224 [30:21<2:36:39,  5.01s/it]
0:  16%|â–ˆâ–Œ        | 348/2224 [30:26<2:36:20,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.383, 'grad_norm': 0.9837634747743013, 'learning_rate': 9.58706506127869e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 348/2224 [30:26<2:36:20,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 349/2224 [30:31<2:36:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3964, 'grad_norm': 1.0528403385514458, 'learning_rate': 9.584162297161494e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 349/2224 [30:31<2:36:16,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 350/2224 [30:36<2:35:21,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.4135, 'grad_norm': 1.0498462695551476, 'learning_rate': 9.58124980872158e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 350/2224 [30:36<2:35:21,  4.97s/it]
0:  16%|â–ˆâ–Œ        | 351/2224 [30:41<2:35:35,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3987, 'grad_norm': 1.014097202557773, 'learning_rate': 9.578327602137165e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 351/2224 [30:41<2:35:35,  4.98s/it]
0:  16%|â–ˆâ–Œ        | 352/2224 [30:46<2:35:58,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3858, 'grad_norm': 1.0491723474904213, 'learning_rate': 9.575395683607091e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 352/2224 [30:46<2:35:58,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 353/2224 [30:51<2:35:55,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3991, 'grad_norm': 0.98080327963675, 'learning_rate': 9.572454059350798e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 353/2224 [30:51<2:35:55,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 354/2224 [30:56<2:36:04,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3585, 'grad_norm': 0.9855857037192809, 'learning_rate': 9.569502735608314e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 354/2224 [30:56<2:36:04,  5.01s/it]
0:  16%|â–ˆâ–Œ        | 355/2224 [31:01<2:35:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3645, 'grad_norm': 1.0298879566765111, 'learning_rate': 9.56654171864024e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 355/2224 [31:01<2:35:48,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 356/2224 [31:06<2:34:50,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3785, 'grad_norm': 0.9914185334272348, 'learning_rate': 9.563571014727743e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 356/2224 [31:06<2:34:50,  4.97s/it]
0:  16%|â–ˆâ–Œ        | 357/2224 [31:11<2:35:18,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3983, 'grad_norm': 1.0630095932628838, 'learning_rate': 9.560590630172542e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 357/2224 [31:11<2:35:18,  4.99s/it]
0:  16%|â–ˆâ–Œ        | 358/2224 [31:16<2:35:29,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3722, 'grad_norm': 0.9852308840115119, 'learning_rate': 9.557600571296884e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 358/2224 [31:16<2:35:29,  5.00s/it]
0:  16%|â–ˆâ–Œ        | 359/2224 [31:20<2:34:19,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.364, 'grad_norm': 1.0082257547633429, 'learning_rate': 9.55460084444354e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 359/2224 [31:20<2:34:19,  4.96s/it]
0:  16%|â–ˆâ–Œ        | 360/2224 [31:26<2:34:46,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3859, 'grad_norm': 0.9683329991150077, 'learning_rate': 9.551591455975795e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 360/2224 [31:26<2:34:46,  4.98s/it]
0:  16%|â–ˆâ–Œ        | 361/2224 [31:31<2:35:21,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3765, 'grad_norm': 0.9680045229543177, 'learning_rate': 9.54857241227742e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–Œ        | 361/2224 [31:31<2:35:21,  5.00s/it]
0:  16%|â–ˆâ–‹        | 362/2224 [31:36<2:35:35,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3907, 'grad_norm': 1.019497136637388, 'learning_rate': 9.545543719752675e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–‹        | 362/2224 [31:36<2:35:35,  5.01s/it]
0:  16%|â–ˆâ–‹        | 363/2224 [31:41<2:35:29,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3669, 'grad_norm': 0.9523858661570214, 'learning_rate': 9.542505384826286e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–‹        | 363/2224 [31:41<2:35:29,  5.01s/it]
0:  16%|â–ˆâ–‹        | 364/2224 [31:46<2:35:14,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3912, 'grad_norm': 1.0149522548115748, 'learning_rate': 9.53945741394343e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–‹        | 364/2224 [31:46<2:35:14,  5.01s/it]
0:  16%|â–ˆâ–‹        | 365/2224 [31:51<2:35:20,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3678, 'grad_norm': 0.9756938112793732, 'learning_rate': 9.53639981356973e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–‹        | 365/2224 [31:51<2:35:20,  5.01s/it]
0:  16%|â–ˆâ–‹        | 366/2224 [31:56<2:35:31,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3832, 'grad_norm': 1.0282994596661268, 'learning_rate': 9.533332590191233e-06, 'epoch': 0.16}
0: 
0:  16%|â–ˆâ–‹        | 366/2224 [31:56<2:35:31,  5.02s/it]
0:  17%|â–ˆâ–‹        | 367/2224 [32:01<2:35:54,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3889, 'grad_norm': 1.0247410021608794, 'learning_rate': 9.530255750314396e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 367/2224 [32:01<2:35:54,  5.04s/it]
0:  17%|â–ˆâ–‹        | 368/2224 [32:06<2:35:53,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3775, 'grad_norm': 0.9809493840502477, 'learning_rate': 9.527169300466082e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 368/2224 [32:06<2:35:53,  5.04s/it]
0:  17%|â–ˆâ–‹        | 369/2224 [32:11<2:35:43,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3761, 'grad_norm': 1.0088946512066876, 'learning_rate': 9.524073247193535e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 369/2224 [32:11<2:35:43,  5.04s/it]
0:  17%|â–ˆâ–‹        | 370/2224 [32:16<2:35:01,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3705, 'grad_norm': 1.0221625744959324, 'learning_rate': 9.520967597064372e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 370/2224 [32:16<2:35:01,  5.02s/it]
0:  17%|â–ˆâ–‹        | 371/2224 [32:21<2:36:03,  5.05s/it]
0:                                                     
0: 
0: {'loss': 0.3906, 'grad_norm': 1.0276952171363203, 'learning_rate': 9.517852356666568e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 371/2224 [32:21<2:36:03,  5.05s/it]
0:  17%|â–ˆâ–‹        | 372/2224 [32:26<2:35:37,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3576, 'grad_norm': 1.0032876847087742, 'learning_rate': 9.51472753260844e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 372/2224 [32:26<2:35:37,  5.04s/it]
0:  17%|â–ˆâ–‹        | 373/2224 [32:31<2:34:58,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3616, 'grad_norm': 0.9835612468588811, 'learning_rate': 9.511593131518636e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 373/2224 [32:31<2:34:58,  5.02s/it]
0:  17%|â–ˆâ–‹        | 374/2224 [32:36<2:33:40,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3692, 'grad_norm': 0.9796760209489411, 'learning_rate': 9.508449160046121e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 374/2224 [32:36<2:33:40,  4.98s/it]
0:  17%|â–ˆâ–‹        | 375/2224 [32:41<2:34:09,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3843, 'grad_norm': 1.0114306115583682, 'learning_rate': 9.505295624860156e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 375/2224 [32:41<2:34:09,  5.00s/it]
0:  17%|â–ˆâ–‹        | 376/2224 [32:46<2:34:11,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3742, 'grad_norm': 1.0411689894486924, 'learning_rate': 9.502132532650298e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 376/2224 [32:46<2:34:11,  5.01s/it]
0:  17%|â–ˆâ–‹        | 377/2224 [32:51<2:34:24,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3874, 'grad_norm': 1.0264683221417852, 'learning_rate': 9.498959890126371e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 377/2224 [32:51<2:34:24,  5.02s/it]
0:  17%|â–ˆâ–‹        | 378/2224 [32:56<2:34:11,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3514, 'grad_norm': 0.9821928600113271, 'learning_rate': 9.495777704018458e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 378/2224 [32:56<2:34:11,  5.01s/it]
0:  17%|â–ˆâ–‹        | 379/2224 [33:01<2:33:59,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3602, 'grad_norm': 0.9884110361799295, 'learning_rate': 9.492585981076892e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 379/2224 [33:01<2:33:59,  5.01s/it]
0:  17%|â–ˆâ–‹        | 380/2224 [33:06<2:31:57,  4.94s/it]
0:                                                     
0: 
0: {'loss': 0.3739, 'grad_norm': 0.9839018539148332, 'learning_rate': 9.489384728072229e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 380/2224 [33:06<2:31:57,  4.94s/it]
0:  17%|â–ˆâ–‹        | 381/2224 [33:11<2:32:57,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3588, 'grad_norm': 1.0032783449627956, 'learning_rate': 9.486173951795246e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 381/2224 [33:11<2:32:57,  4.98s/it]
0:  17%|â–ˆâ–‹        | 382/2224 [33:16<2:32:18,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3668, 'grad_norm': 0.975222048234114, 'learning_rate': 9.48295365905692e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 382/2224 [33:16<2:32:18,  4.96s/it]
0:  17%|â–ˆâ–‹        | 383/2224 [33:21<2:32:36,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3681, 'grad_norm': 1.0110838267810889, 'learning_rate': 9.479723856688413e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 383/2224 [33:21<2:32:36,  4.97s/it]
0:  17%|â–ˆâ–‹        | 384/2224 [33:26<2:33:27,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3928, 'grad_norm': 1.0210029127544746, 'learning_rate': 9.476484551541065e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 384/2224 [33:26<2:33:27,  5.00s/it]
0:  17%|â–ˆâ–‹        | 385/2224 [33:31<2:32:51,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3844, 'grad_norm': 1.0095861626278584, 'learning_rate': 9.473235750486372e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 385/2224 [33:31<2:32:51,  4.99s/it]
0:  17%|â–ˆâ–‹        | 386/2224 [33:36<2:32:58,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.391, 'grad_norm': 0.9382412795273607, 'learning_rate': 9.46997746041597e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 386/2224 [33:36<2:32:58,  4.99s/it]
0:  17%|â–ˆâ–‹        | 387/2224 [33:41<2:32:17,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3797, 'grad_norm': 0.9816370248547481, 'learning_rate': 9.466709688241627e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 387/2224 [33:41<2:32:17,  4.97s/it]
0:  17%|â–ˆâ–‹        | 388/2224 [33:46<2:33:02,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3864, 'grad_norm': 0.9937737518299697, 'learning_rate': 9.463432440895224e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 388/2224 [33:46<2:33:02,  5.00s/it]
0:  17%|â–ˆâ–‹        | 389/2224 [33:51<2:32:01,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3695, 'grad_norm': 0.9849257192204042, 'learning_rate': 9.460145725328744e-06, 'epoch': 0.17}
0: 
0:  17%|â–ˆâ–‹        | 389/2224 [33:51<2:32:01,  4.97s/it]
0:  18%|â–ˆâ–Š        | 390/2224 [33:56<2:32:22,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3877, 'grad_norm': 1.0302072059686993, 'learning_rate': 9.456849548514254e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 390/2224 [33:56<2:32:22,  4.98s/it]
0:  18%|â–ˆâ–Š        | 391/2224 [34:01<2:33:01,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3931, 'grad_norm': 0.9678032785166966, 'learning_rate': 9.453543917443889e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 391/2224 [34:01<2:33:01,  5.01s/it]
0:  18%|â–ˆâ–Š        | 392/2224 [34:06<2:31:57,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.388, 'grad_norm': 0.958672038810975, 'learning_rate': 9.450228839129838e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 392/2224 [34:06<2:31:57,  4.98s/it]
0:  18%|â–ˆâ–Š        | 393/2224 [34:10<2:31:01,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3537, 'grad_norm': 1.0010392214467052, 'learning_rate': 9.446904320604337e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 393/2224 [34:11<2:31:01,  4.95s/it]
0:  18%|â–ˆâ–Š        | 394/2224 [34:15<2:31:08,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3716, 'grad_norm': 1.0437347902445773, 'learning_rate': 9.44357036891964e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 394/2224 [34:15<2:31:08,  4.96s/it]
0:  18%|â–ˆâ–Š        | 395/2224 [34:20<2:31:40,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3689, 'grad_norm': 1.0998886914134722, 'learning_rate': 9.440226991148016e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 395/2224 [34:20<2:31:40,  4.98s/it]
0:  18%|â–ˆâ–Š        | 396/2224 [34:25<2:31:52,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3684, 'grad_norm': 1.0050601295869759, 'learning_rate': 9.436874194381729e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 396/2224 [34:26<2:31:52,  4.98s/it]
0:  18%|â–ˆâ–Š        | 397/2224 [34:31<2:31:58,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3967, 'grad_norm': 0.9693619622462925, 'learning_rate': 9.433511985733017e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 397/2224 [34:31<2:31:58,  4.99s/it]
0:  18%|â–ˆâ–Š        | 398/2224 [34:36<2:32:11,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3735, 'grad_norm': 0.9802463097922643, 'learning_rate': 9.430140372334095e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 398/2224 [34:36<2:32:11,  5.00s/it]
0:  18%|â–ˆâ–Š        | 399/2224 [34:41<2:32:22,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.365, 'grad_norm': 1.0279939527810642, 'learning_rate': 9.426759361337119e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 399/2224 [34:41<2:32:22,  5.01s/it]
0:  18%|â–ˆâ–Š        | 400/2224 [34:46<2:32:01,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.377, 'grad_norm': 0.9774661986962327, 'learning_rate': 9.423368959914183e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 400/2224 [34:46<2:32:01,  5.00s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  18%|â–ˆâ–Š        | 401/2224 [36:04<13:42:51, 27.08s/it]
0:                                                      
0: 
0: {'loss': 0.3659, 'grad_norm': 0.9926414335815805, 'learning_rate': 9.419969175257302e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 401/2224 [36:04<13:42:51, 27.08s/it]
0:  18%|â–ˆâ–Š        | 402/2224 [36:09<10:20:17, 20.43s/it]
0:                                                      
0: 
0: {'loss': 0.3527, 'grad_norm': 0.9876246345473279, 'learning_rate': 9.41656001457839e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 402/2224 [36:09<10:20:17, 20.43s/it]
0:  18%|â–ˆâ–Š        | 403/2224 [36:14<7:59:49, 15.81s/it] 
0:                                                     
0: 
0: {'loss': 0.3496, 'grad_norm': 0.9885714121591775, 'learning_rate': 9.413141485109259e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 403/2224 [36:14<7:59:49, 15.81s/it]
0:  18%|â–ˆâ–Š        | 404/2224 [36:19<6:21:13, 12.57s/it]
0:                                                     
0: 
0: {'loss': 0.3516, 'grad_norm': 1.0062374407002102, 'learning_rate': 9.409713594101589e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 404/2224 [36:19<6:21:13, 12.57s/it]
0:  18%|â–ˆâ–Š        | 405/2224 [36:24<5:12:16, 10.30s/it]
0:                                                     
0: 
0: {'loss': 0.3691, 'grad_norm': 1.0362743662860585, 'learning_rate': 9.406276348826919e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 405/2224 [36:24<5:12:16, 10.30s/it]
0:  18%|â–ˆâ–Š        | 406/2224 [36:29<4:24:21,  8.72s/it]
0:                                                     
0: 
0: {'loss': 0.3523, 'grad_norm': 1.0310863368440883, 'learning_rate': 9.40282975657663e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 406/2224 [36:29<4:24:21,  8.72s/it]
0:  18%|â–ˆâ–Š        | 407/2224 [36:34<3:50:31,  7.61s/it]
0:                                                     
0: 
0: {'loss': 0.3778, 'grad_norm': 0.9884718160151773, 'learning_rate': 9.399373824661938e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 407/2224 [36:34<3:50:31,  7.61s/it]
0:  18%|â–ˆâ–Š        | 408/2224 [36:39<3:25:44,  6.80s/it]
0:                                                     
0: 
0: {'loss': 0.3768, 'grad_norm': 1.0756210563426694, 'learning_rate': 9.395908560413858e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 408/2224 [36:39<3:25:44,  6.80s/it]
0:  18%|â–ˆâ–Š        | 409/2224 [36:44<3:08:55,  6.25s/it]
0:                                                     
0: 
0: {'loss': 0.3714, 'grad_norm': 0.9941221252777619, 'learning_rate': 9.392433971183215e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 409/2224 [36:44<3:08:55,  6.25s/it]
0:  18%|â–ˆâ–Š        | 410/2224 [36:49<2:57:13,  5.86s/it]
0:                                                     
0: 
0: {'loss': 0.3816, 'grad_norm': 1.032107042942714, 'learning_rate': 9.388950064340608e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 410/2224 [36:49<2:57:13,  5.86s/it]
0:  18%|â–ˆâ–Š        | 411/2224 [36:54<2:49:12,  5.60s/it]
0:                                                     
0: 
0: {'loss': 0.3611, 'grad_norm': 0.9463590578690143, 'learning_rate': 9.385456847276401e-06, 'epoch': 0.18}
0: 
0:  18%|â–ˆâ–Š        | 411/2224 [36:54<2:49:12,  5.60s/it]
0:  19%|â–ˆâ–Š        | 412/2224 [36:59<2:43:47,  5.42s/it]
0:                                                     
0: 
0: {'loss': 0.3691, 'grad_norm': 1.0234029063329058, 'learning_rate': 9.381954327400711e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–Š        | 412/2224 [36:59<2:43:47,  5.42s/it]
0:  19%|â–ˆâ–Š        | 413/2224 [37:04<2:39:30,  5.28s/it]
0:                                                     
0: 
0: {'loss': 0.4, 'grad_norm': 1.0283208055287645, 'learning_rate': 9.378442512143384e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–Š        | 413/2224 [37:04<2:39:30,  5.28s/it]
0:  19%|â–ˆâ–Š        | 414/2224 [37:09<2:37:01,  5.21s/it]
0:                                                     
0: 
0: {'loss': 0.3836, 'grad_norm': 0.9625218770929452, 'learning_rate': 9.37492140895399e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–Š        | 414/2224 [37:09<2:37:01,  5.21s/it]
0:  19%|â–ˆâ–Š        | 415/2224 [37:14<2:34:08,  5.11s/it]
0:                                                     
0: 
0: {'loss': 0.3582, 'grad_norm': 0.9693392769117692, 'learning_rate': 9.371391025301798e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–Š        | 415/2224 [37:14<2:34:08,  5.11s/it]
0:  19%|â–ˆâ–Š        | 416/2224 [37:19<2:32:28,  5.06s/it]
0:                                                     
0: 
0: {'loss': 0.4077, 'grad_norm': 1.0004635108897164, 'learning_rate': 9.367851368675762e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–Š        | 416/2224 [37:19<2:32:28,  5.06s/it]
0:  19%|â–ˆâ–‰        | 417/2224 [37:24<2:31:57,  5.05s/it]
0:                                                     
0: 
0: {'loss': 0.3816, 'grad_norm': 1.0277132391711827, 'learning_rate': 9.36430244658451e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 417/2224 [37:24<2:31:57,  5.05s/it]
0:  19%|â–ˆâ–‰        | 418/2224 [37:29<2:31:40,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3672, 'grad_norm': 0.9337692103521993, 'learning_rate': 9.360744266556326e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 418/2224 [37:29<2:31:40,  5.04s/it]
0:  19%|â–ˆâ–‰        | 419/2224 [37:34<2:30:40,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3551, 'grad_norm': 0.9799387082655602, 'learning_rate': 9.357176836139123e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 419/2224 [37:34<2:30:40,  5.01s/it]
0:  19%|â–ˆâ–‰        | 420/2224 [37:39<2:29:35,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3956, 'grad_norm': 0.9908057470323751, 'learning_rate': 9.353600162900453e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 420/2224 [37:39<2:29:35,  4.98s/it]
0:  19%|â–ˆâ–‰        | 421/2224 [37:44<2:29:47,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3482, 'grad_norm': 0.9360820933498298, 'learning_rate': 9.350014254427459e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 421/2224 [37:44<2:29:47,  4.98s/it]
0:  19%|â–ˆâ–‰        | 422/2224 [37:49<2:28:54,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3711, 'grad_norm': 0.9436806866998181, 'learning_rate': 9.346419118326884e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 422/2224 [37:49<2:28:54,  4.96s/it]
0:  19%|â–ˆâ–‰        | 423/2224 [37:53<2:28:11,  4.94s/it]
0:                                                     
0: 
0: {'loss': 0.3654, 'grad_norm': 0.9244456181218472, 'learning_rate': 9.34281476222504e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 423/2224 [37:53<2:28:11,  4.94s/it]
0:  19%|â–ˆâ–‰        | 424/2224 [37:58<2:28:44,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3537, 'grad_norm': 0.9430092809575723, 'learning_rate': 9.339201193767804e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 424/2224 [37:58<2:28:44,  4.96s/it]
0:  19%|â–ˆâ–‰        | 425/2224 [38:03<2:29:00,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3725, 'grad_norm': 0.9696349104150984, 'learning_rate': 9.335578420620587e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 425/2224 [38:03<2:29:00,  4.97s/it]
0:  19%|â–ˆâ–‰        | 426/2224 [38:09<2:29:50,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3824, 'grad_norm': 0.9548105832382938, 'learning_rate': 9.331946450468332e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 426/2224 [38:09<2:29:50,  5.00s/it]
0:  19%|â–ˆâ–‰        | 427/2224 [38:14<2:29:59,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3481, 'grad_norm': 0.9794792807807398, 'learning_rate': 9.328305291015488e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 427/2224 [38:14<2:29:59,  5.01s/it]
0:  19%|â–ˆâ–‰        | 428/2224 [38:19<2:29:52,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3421, 'grad_norm': 0.9719328412521627, 'learning_rate': 9.324654949985999e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 428/2224 [38:19<2:29:52,  5.01s/it]
0:  19%|â–ˆâ–‰        | 429/2224 [38:24<2:29:54,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.371, 'grad_norm': 0.9723876183967696, 'learning_rate': 9.320995435123284e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 429/2224 [38:24<2:29:54,  5.01s/it]
0:  19%|â–ˆâ–‰        | 430/2224 [38:29<2:29:55,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3967, 'grad_norm': 0.9667547470460581, 'learning_rate': 9.317326754190224e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 430/2224 [38:29<2:29:55,  5.01s/it]
0:  19%|â–ˆâ–‰        | 431/2224 [38:34<2:28:49,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3423, 'grad_norm': 0.9583650680306577, 'learning_rate': 9.313648914969141e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 431/2224 [38:34<2:28:49,  4.98s/it]
0:  19%|â–ˆâ–‰        | 432/2224 [38:39<2:29:25,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3869, 'grad_norm': 0.9577777198856573, 'learning_rate': 9.309961925261792e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 432/2224 [38:39<2:29:25,  5.00s/it]
0:  19%|â–ˆâ–‰        | 433/2224 [38:44<2:29:23,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3822, 'grad_norm': 1.2920638286608639, 'learning_rate': 9.306265792889334e-06, 'epoch': 0.19}
0: 
0:  19%|â–ˆâ–‰        | 433/2224 [38:44<2:29:23,  5.00s/it]
0:  20%|â–ˆâ–‰        | 434/2224 [38:49<2:28:51,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3598, 'grad_norm': 0.9843300576829177, 'learning_rate': 9.302560525692329e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 434/2224 [38:49<2:28:51,  4.99s/it]
0:  20%|â–ˆâ–‰        | 435/2224 [38:54<2:29:01,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3905, 'grad_norm': 0.995156569532058, 'learning_rate': 9.298846131530705e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 435/2224 [38:54<2:29:01,  5.00s/it]
0:  20%|â–ˆâ–‰        | 436/2224 [38:59<2:29:07,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3591, 'grad_norm': 0.9941031242784484, 'learning_rate': 9.29512261828376e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 436/2224 [38:59<2:29:07,  5.00s/it]
0:  20%|â–ˆâ–‰        | 437/2224 [39:04<2:29:18,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3594, 'grad_norm': 1.017677779430739, 'learning_rate': 9.291389993850136e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 437/2224 [39:04<2:29:18,  5.01s/it]
0:  20%|â–ˆâ–‰        | 438/2224 [39:09<2:29:21,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3788, 'grad_norm': 0.9983177392916488, 'learning_rate': 9.287648266147795e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 438/2224 [39:09<2:29:21,  5.02s/it]
0:  20%|â–ˆâ–‰        | 439/2224 [39:14<2:29:33,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3628, 'grad_norm': 1.020012385913744, 'learning_rate': 9.283897443114017e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 439/2224 [39:14<2:29:33,  5.03s/it]
0:  20%|â–ˆâ–‰        | 440/2224 [39:19<2:29:33,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3836, 'grad_norm': 1.026131822552044, 'learning_rate': 9.280137532705372e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 440/2224 [39:19<2:29:33,  5.03s/it]
0:  20%|â–ˆâ–‰        | 441/2224 [39:24<2:28:47,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3849, 'grad_norm': 1.0141018203213799, 'learning_rate': 9.27636854289771e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 441/2224 [39:24<2:28:47,  5.01s/it]
0:  20%|â–ˆâ–‰        | 442/2224 [39:29<2:28:43,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3836, 'grad_norm': 1.0249105408952903, 'learning_rate': 9.272590481686137e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 442/2224 [39:29<2:28:43,  5.01s/it]
0:  20%|â–ˆâ–‰        | 443/2224 [39:34<2:28:30,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3858, 'grad_norm': 1.0114188082714086, 'learning_rate': 9.268803357085006e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 443/2224 [39:34<2:28:30,  5.00s/it]
0:  20%|â–ˆâ–‰        | 444/2224 [39:39<2:28:37,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.36, 'grad_norm': 0.9694671023431956, 'learning_rate': 9.265007177127894e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–‰        | 444/2224 [39:39<2:28:37,  5.01s/it]
0:  20%|â–ˆâ–ˆ        | 445/2224 [39:44<2:28:31,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3877, 'grad_norm': 1.0716335510696473, 'learning_rate': 9.261201949867585e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 445/2224 [39:44<2:28:31,  5.01s/it]
0:  20%|â–ˆâ–ˆ        | 446/2224 [39:49<2:28:08,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3481, 'grad_norm': 1.027907173652226, 'learning_rate': 9.25738768337606e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 446/2224 [39:49<2:28:08,  5.00s/it]
0:  20%|â–ˆâ–ˆ        | 447/2224 [39:54<2:27:26,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3424, 'grad_norm': 0.9571040205120219, 'learning_rate': 9.25356438574447e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 447/2224 [39:54<2:27:26,  4.98s/it]
0:  20%|â–ˆâ–ˆ        | 448/2224 [39:58<2:26:21,  4.94s/it]
0:                                                     
0: 
0: {'loss': 0.3725, 'grad_norm': 0.9565984207745639, 'learning_rate': 9.249732065083125e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 448/2224 [39:58<2:26:21,  4.94s/it]
0:  20%|â–ˆâ–ˆ        | 449/2224 [40:03<2:25:47,  4.93s/it]
0:                                                     
0: 
0: {'loss': 0.3794, 'grad_norm': 1.0430896268631724, 'learning_rate': 9.245890729521478e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 449/2224 [40:03<2:25:47,  4.93s/it]
0:  20%|â–ˆâ–ˆ        | 450/2224 [40:08<2:26:38,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3931, 'grad_norm': 0.9879490439370605, 'learning_rate': 9.242040387208101e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 450/2224 [40:08<2:26:38,  4.96s/it]
0:  20%|â–ˆâ–ˆ        | 451/2224 [40:13<2:26:58,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3705, 'grad_norm': 1.0089729479387495, 'learning_rate': 9.23818104631068e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 451/2224 [40:13<2:26:58,  4.97s/it]
0:  20%|â–ˆâ–ˆ        | 452/2224 [40:18<2:27:39,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3657, 'grad_norm': 0.9739565966594462, 'learning_rate': 9.234312715015976e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 452/2224 [40:18<2:27:39,  5.00s/it]
0:  20%|â–ˆâ–ˆ        | 453/2224 [40:23<2:27:30,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.4029, 'grad_norm': 0.9917658050286953, 'learning_rate': 9.230435401529834e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 453/2224 [40:23<2:27:30,  5.00s/it]
0:  20%|â–ˆâ–ˆ        | 454/2224 [40:28<2:27:37,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3698, 'grad_norm': 0.9952765434470788, 'learning_rate': 9.226549114077143e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 454/2224 [40:28<2:27:37,  5.00s/it]
0:  20%|â–ˆâ–ˆ        | 455/2224 [40:33<2:27:43,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3824, 'grad_norm': 0.9792384454810711, 'learning_rate': 9.22265386090184e-06, 'epoch': 0.2}
0: 
0:  20%|â–ˆâ–ˆ        | 455/2224 [40:33<2:27:43,  5.01s/it]
0:  21%|â–ˆâ–ˆ        | 456/2224 [40:38<2:27:11,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.391, 'grad_norm': 1.0204339811184522, 'learning_rate': 9.218749650266868e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 456/2224 [40:38<2:27:11,  5.00s/it]
0:  21%|â–ˆâ–ˆ        | 457/2224 [40:43<2:27:24,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3546, 'grad_norm': 0.9708473084063312, 'learning_rate': 9.214836490454178e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 457/2224 [40:43<2:27:24,  5.01s/it]
0:  21%|â–ˆâ–ˆ        | 458/2224 [40:48<2:26:03,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3635, 'grad_norm': 1.0423277615846844, 'learning_rate': 9.210914389764706e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 458/2224 [40:48<2:26:03,  4.96s/it]
0:  21%|â–ˆâ–ˆ        | 459/2224 [40:53<2:26:15,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.388, 'grad_norm': 1.0530775275133282, 'learning_rate': 9.206983356518351e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 459/2224 [40:53<2:26:15,  4.97s/it]
0:  21%|â–ˆâ–ˆ        | 460/2224 [40:58<2:26:21,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3849, 'grad_norm': 0.9611606643523884, 'learning_rate': 9.203043399053963e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 460/2224 [40:58<2:26:21,  4.98s/it]
0:  21%|â–ˆâ–ˆ        | 461/2224 [41:03<2:26:25,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3956, 'grad_norm': 0.9708831546507507, 'learning_rate': 9.199094525729322e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 461/2224 [41:03<2:26:25,  4.98s/it]
0:  21%|â–ˆâ–ˆ        | 462/2224 [41:08<2:26:23,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3914, 'grad_norm': 1.0044907899760158, 'learning_rate': 9.195136744921118e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 462/2224 [41:08<2:26:23,  4.98s/it]
0:  21%|â–ˆâ–ˆ        | 463/2224 [41:13<2:26:25,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3794, 'grad_norm': 1.0137904865383958, 'learning_rate': 9.19117006502494e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 463/2224 [41:13<2:26:25,  4.99s/it]
0:  21%|â–ˆâ–ˆ        | 464/2224 [41:18<2:26:36,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3541, 'grad_norm': 0.9786041973158519, 'learning_rate': 9.187194494455254e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 464/2224 [41:18<2:26:36,  5.00s/it]
0:  21%|â–ˆâ–ˆ        | 465/2224 [41:23<2:25:29,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3395, 'grad_norm': 0.9458922847498302, 'learning_rate': 9.183210041645386e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 465/2224 [41:23<2:25:29,  4.96s/it]
0:  21%|â–ˆâ–ˆ        | 466/2224 [41:28<2:26:05,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3649, 'grad_norm': 0.9719262687114117, 'learning_rate': 9.179216715047498e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 466/2224 [41:28<2:26:05,  4.99s/it]
0:  21%|â–ˆâ–ˆ        | 467/2224 [41:33<2:26:18,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3403, 'grad_norm': 0.9885785001878822, 'learning_rate': 9.175214523132588e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 467/2224 [41:33<2:26:18,  5.00s/it]
0:  21%|â–ˆâ–ˆ        | 468/2224 [41:38<2:26:06,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.385, 'grad_norm': 0.9689406891153294, 'learning_rate': 9.17120347439045e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 468/2224 [41:38<2:26:06,  4.99s/it]
0:  21%|â–ˆâ–ˆ        | 469/2224 [41:43<2:24:10,  4.93s/it]
0:                                                     
0: 
0: {'loss': 0.3832, 'grad_norm': 1.0053961306624684, 'learning_rate': 9.167183577329667e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 469/2224 [41:43<2:24:10,  4.93s/it]
0:  21%|â–ˆâ–ˆ        | 470/2224 [41:48<2:25:05,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3863, 'grad_norm': 1.0558981365244238, 'learning_rate': 9.163154840477593e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 470/2224 [41:48<2:25:05,  4.96s/it]
0:  21%|â–ˆâ–ˆ        | 471/2224 [41:53<2:25:37,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3608, 'grad_norm': 0.9782745652479289, 'learning_rate': 9.159117272380337e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 471/2224 [41:53<2:25:37,  4.98s/it]
0:  21%|â–ˆâ–ˆ        | 472/2224 [41:58<2:25:42,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3589, 'grad_norm': 0.985958827980789, 'learning_rate': 9.155070881602736e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆ        | 472/2224 [41:58<2:25:42,  4.99s/it]
0:  21%|â–ˆâ–ˆâ–       | 473/2224 [42:03<2:26:12,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3733, 'grad_norm': 0.9817342369130367, 'learning_rate': 9.151015676728347e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 473/2224 [42:03<2:26:12,  5.01s/it]
0:  21%|â–ˆâ–ˆâ–       | 474/2224 [42:08<2:26:12,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3705, 'grad_norm': 0.9776427709364848, 'learning_rate': 9.14695166635942e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 474/2224 [42:08<2:26:12,  5.01s/it]
0:  21%|â–ˆâ–ˆâ–       | 475/2224 [42:13<2:25:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3778, 'grad_norm': 0.9811233880997292, 'learning_rate': 9.14287885911689e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 475/2224 [42:13<2:25:48,  5.00s/it]
0:  21%|â–ˆâ–ˆâ–       | 476/2224 [42:18<2:26:03,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3892, 'grad_norm': 0.9952045320847595, 'learning_rate': 9.138797263640346e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 476/2224 [42:18<2:26:03,  5.01s/it]
0:  21%|â–ˆâ–ˆâ–       | 477/2224 [42:23<2:25:47,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.344, 'grad_norm': 0.9828657154324266, 'learning_rate': 9.134706888588023e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 477/2224 [42:23<2:25:47,  5.01s/it]
0:  21%|â–ˆâ–ˆâ–       | 478/2224 [42:28<2:25:46,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.362, 'grad_norm': 0.9911790148584821, 'learning_rate': 9.13060774263678e-06, 'epoch': 0.21}
0: 
0:  21%|â–ˆâ–ˆâ–       | 478/2224 [42:28<2:25:46,  5.01s/it]
0:  22%|â–ˆâ–ˆâ–       | 479/2224 [42:33<2:25:16,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3765, 'grad_norm': 1.008344229177706, 'learning_rate': 9.12649983448208e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 479/2224 [42:33<2:25:16,  5.00s/it]
0:  22%|â–ˆâ–ˆâ–       | 480/2224 [42:38<2:25:00,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3765, 'grad_norm': 0.9890897489292624, 'learning_rate': 9.122383172837974e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 480/2224 [42:38<2:25:00,  4.99s/it]
0:  22%|â–ˆâ–ˆâ–       | 481/2224 [42:43<2:25:12,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3635, 'grad_norm': 0.9675289217405697, 'learning_rate': 9.118257766437082e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 481/2224 [42:43<2:25:12,  5.00s/it]
0:  22%|â–ˆâ–ˆâ–       | 482/2224 [42:48<2:24:04,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3517, 'grad_norm': 0.9299994142572316, 'learning_rate': 9.114123624030578e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 482/2224 [42:48<2:24:04,  4.96s/it]
0:  22%|â–ˆâ–ˆâ–       | 483/2224 [42:53<2:24:08,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3673, 'grad_norm': 1.0220421947895924, 'learning_rate': 9.109980754388157e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 483/2224 [42:53<2:24:08,  4.97s/it]
0:  22%|â–ˆâ–ˆâ–       | 484/2224 [42:58<2:25:02,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3692, 'grad_norm': 0.9257651206739235, 'learning_rate': 9.105829166298037e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 484/2224 [42:58<2:25:02,  5.00s/it]
0:  22%|â–ˆâ–ˆâ–       | 485/2224 [43:03<2:24:06,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3754, 'grad_norm': 0.9924755526468177, 'learning_rate': 9.101668868566926e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 485/2224 [43:03<2:24:06,  4.97s/it]
0:  22%|â–ˆâ–ˆâ–       | 486/2224 [43:08<2:24:10,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3595, 'grad_norm': 0.9807152141818125, 'learning_rate': 9.097499870020008e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 486/2224 [43:08<2:24:10,  4.98s/it]
0:  22%|â–ˆâ–ˆâ–       | 487/2224 [43:13<2:24:25,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3604, 'grad_norm': 0.9811715105746597, 'learning_rate': 9.093322179500925e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 487/2224 [43:13<2:24:25,  4.99s/it]
0:  22%|â–ˆâ–ˆâ–       | 488/2224 [43:18<2:24:40,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3711, 'grad_norm': 0.9798325448645158, 'learning_rate': 9.089135805871759e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 488/2224 [43:18<2:24:40,  5.00s/it]
0:  22%|â–ˆâ–ˆâ–       | 489/2224 [43:23<2:25:22,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3746, 'grad_norm': 1.0657095985956224, 'learning_rate': 9.084940758013003e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 489/2224 [43:23<2:25:22,  5.03s/it]
0:  22%|â–ˆâ–ˆâ–       | 490/2224 [43:28<2:25:04,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3782, 'grad_norm': 0.9921488060455286, 'learning_rate': 9.080737044823563e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 490/2224 [43:28<2:25:04,  5.02s/it]
0:  22%|â–ˆâ–ˆâ–       | 491/2224 [43:33<2:24:55,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3496, 'grad_norm': 0.9701926588739688, 'learning_rate': 9.076524675220714e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 491/2224 [43:33<2:24:55,  5.02s/it]
0:  22%|â–ˆâ–ˆâ–       | 492/2224 [43:38<2:22:57,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3717, 'grad_norm': 0.9626996773491302, 'learning_rate': 9.072303658140105e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 492/2224 [43:38<2:22:57,  4.95s/it]
0:  22%|â–ˆâ–ˆâ–       | 493/2224 [43:43<2:23:29,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3421, 'grad_norm': 0.96576631467599, 'learning_rate': 9.06807400253572e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 493/2224 [43:43<2:23:29,  4.97s/it]
0:  22%|â–ˆâ–ˆâ–       | 494/2224 [43:48<2:23:53,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3314, 'grad_norm': 1.029514634537707, 'learning_rate': 9.063835717379872e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 494/2224 [43:48<2:23:53,  4.99s/it]
0:  22%|â–ˆâ–ˆâ–       | 495/2224 [43:53<2:23:35,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3699, 'grad_norm': 0.990558500438573, 'learning_rate': 9.05958881166318e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 495/2224 [43:53<2:23:35,  4.98s/it]
0:  22%|â–ˆâ–ˆâ–       | 496/2224 [43:58<2:23:37,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3954, 'grad_norm': 0.9666935551131797, 'learning_rate': 9.055333294394549e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 496/2224 [43:58<2:23:37,  4.99s/it]
0:  22%|â–ˆâ–ˆâ–       | 497/2224 [44:03<2:24:16,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3511, 'grad_norm': 0.9865890269875882, 'learning_rate': 9.051069174601152e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 497/2224 [44:03<2:24:16,  5.01s/it]
0:  22%|â–ˆâ–ˆâ–       | 498/2224 [44:08<2:24:20,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3575, 'grad_norm': 0.9903995601193586, 'learning_rate': 9.046796461328406e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 498/2224 [44:08<2:24:20,  5.02s/it]
0:  22%|â–ˆâ–ˆâ–       | 499/2224 [44:13<2:23:49,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3646, 'grad_norm': 0.9502385533971714, 'learning_rate': 9.042515163639967e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 499/2224 [44:13<2:23:49,  5.00s/it]
0:  22%|â–ˆâ–ˆâ–       | 500/2224 [44:18<2:23:59,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3806, 'grad_norm': 0.9545354776037787, 'learning_rate': 9.03822529061769e-06, 'epoch': 0.22}
0: 
0:  22%|â–ˆâ–ˆâ–       | 500/2224 [44:18<2:23:59,  5.01s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 501/2224 [44:23<2:24:09,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3663, 'grad_norm': 0.987934786843594, 'learning_rate': 9.033926851361627e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 501/2224 [44:23<2:24:09,  5.02s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 502/2224 [44:28<2:23:57,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3475, 'grad_norm': 0.9244140523052012, 'learning_rate': 9.029619854990001e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 502/2224 [44:28<2:23:57,  5.02s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 503/2224 [44:33<2:23:25,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3423, 'grad_norm': 0.9968574463765852, 'learning_rate': 9.025304310639188e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 503/2224 [44:33<2:23:25,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 504/2224 [44:38<2:22:37,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3664, 'grad_norm': 0.9340104613933761, 'learning_rate': 9.020980227463692e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 504/2224 [44:38<2:22:37,  4.98s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 505/2224 [44:43<2:23:00,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3285, 'grad_norm': 0.9629337167876482, 'learning_rate': 9.016647614636136e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 505/2224 [44:43<2:23:00,  4.99s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 506/2224 [44:48<2:23:12,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3604, 'grad_norm': 0.966383539112008, 'learning_rate': 9.012306481347231e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 506/2224 [44:48<2:23:12,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 507/2224 [44:53<2:22:45,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3717, 'grad_norm': 0.9708464783827534, 'learning_rate': 9.007956836805767e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 507/2224 [44:53<2:22:45,  4.99s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 508/2224 [44:58<2:22:09,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.348, 'grad_norm': 0.9772633863778419, 'learning_rate': 9.003598690238586e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 508/2224 [44:58<2:22:09,  4.97s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 509/2224 [45:03<2:22:08,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3604, 'grad_norm': 0.9910165456480472, 'learning_rate': 8.999232050890572e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 509/2224 [45:03<2:22:08,  4.97s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 510/2224 [45:08<2:22:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3508, 'grad_norm': 1.0114577277851367, 'learning_rate': 8.994856928024612e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 510/2224 [45:08<2:22:48,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 511/2224 [45:13<2:22:46,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3834, 'grad_norm': 0.9771256514631486, 'learning_rate': 8.9904733309216e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 511/2224 [45:13<2:22:46,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 512/2224 [45:18<2:23:01,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3415, 'grad_norm': 0.973298107478736, 'learning_rate': 8.986081268880402e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 512/2224 [45:18<2:23:01,  5.01s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 513/2224 [45:23<2:22:42,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3774, 'grad_norm': 0.9709644525624105, 'learning_rate': 8.981680751217842e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 513/2224 [45:23<2:22:42,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 514/2224 [45:28<2:22:30,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3555, 'grad_norm': 0.9632329839467857, 'learning_rate': 8.977271787268678e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 514/2224 [45:28<2:22:30,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 515/2224 [45:33<2:22:32,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.383, 'grad_norm': 0.9686041307428087, 'learning_rate': 8.972854386385586e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 515/2224 [45:33<2:22:32,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 516/2224 [45:38<2:22:38,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3358, 'grad_norm': 0.9640569135569924, 'learning_rate': 8.968428557939142e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 516/2224 [45:38<2:22:38,  5.01s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 517/2224 [45:43<2:22:43,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.349, 'grad_norm': 0.944379352272392, 'learning_rate': 8.963994311317796e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 517/2224 [45:43<2:22:43,  5.02s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 518/2224 [45:48<2:22:37,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3777, 'grad_norm': 0.9738473965862517, 'learning_rate': 8.959551655927858e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 518/2224 [45:48<2:22:37,  5.02s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 519/2224 [45:53<2:22:23,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3723, 'grad_norm': 0.9740181880049062, 'learning_rate': 8.955100601193474e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 519/2224 [45:53<2:22:23,  5.01s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 520/2224 [45:58<2:22:10,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3402, 'grad_norm': 0.984538529606941, 'learning_rate': 8.950641156556606e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 520/2224 [45:58<2:22:10,  5.01s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 521/2224 [46:03<2:22:00,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3726, 'grad_norm': 0.985412330731313, 'learning_rate': 8.946173331477013e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 521/2224 [46:03<2:22:00,  5.00s/it]
0:  23%|â–ˆâ–ˆâ–Ž       | 522/2224 [46:08<2:21:09,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3506, 'grad_norm': 1.0108842909948919, 'learning_rate': 8.941697135432238e-06, 'epoch': 0.23}
0: 
0:  23%|â–ˆâ–ˆâ–Ž       | 522/2224 [46:08<2:21:09,  4.98s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 523/2224 [46:13<2:22:00,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3678, 'grad_norm': 0.9847938490269565, 'learning_rate': 8.937212577917574e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 523/2224 [46:13<2:22:00,  5.01s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 524/2224 [46:18<2:21:43,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.381, 'grad_norm': 0.9698045834047205, 'learning_rate': 8.93271966844605e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 524/2224 [46:18<2:21:43,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 525/2224 [46:23<2:21:33,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3731, 'grad_norm': 1.0098359487798587, 'learning_rate': 8.928218416548423e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 525/2224 [46:23<2:21:33,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 526/2224 [46:28<2:21:19,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3516, 'grad_norm': 0.93410476752215, 'learning_rate': 8.923708831773131e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 526/2224 [46:28<2:21:19,  4.99s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 527/2224 [46:33<2:21:36,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3579, 'grad_norm': 0.9829793985793338, 'learning_rate': 8.9191909236863e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 527/2224 [46:33<2:21:36,  5.01s/it]
0:  24%|â–ˆâ–ˆâ–Ž       | 528/2224 [46:38<2:21:27,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3398, 'grad_norm': 0.9731621759338313, 'learning_rate': 8.914664701871711e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–Ž       | 528/2224 [46:38<2:21:27,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–       | 529/2224 [46:43<2:21:15,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3624, 'grad_norm': 0.9949538732791817, 'learning_rate': 8.910130175930773e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 529/2224 [46:43<2:21:15,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–       | 530/2224 [46:48<2:19:16,  4.93s/it]
0:                                                     
0: 
0: {'loss': 0.3463, 'grad_norm': 0.9483822332210696, 'learning_rate': 8.90558735548252e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 530/2224 [46:48<2:19:16,  4.93s/it]
0:  24%|â–ˆâ–ˆâ–       | 531/2224 [46:53<2:18:52,  4.92s/it]
0:                                                     
0: 
0: {'loss': 0.3433, 'grad_norm': 0.9219926892430028, 'learning_rate': 8.901036250163576e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 531/2224 [46:53<2:18:52,  4.92s/it]
0:  24%|â–ˆâ–ˆâ–       | 532/2224 [46:58<2:19:27,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3824, 'grad_norm': 1.006754400177768, 'learning_rate': 8.896476869628139e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 532/2224 [46:58<2:19:27,  4.95s/it]
0:  24%|â–ˆâ–ˆâ–       | 533/2224 [47:03<2:19:55,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3405, 'grad_norm': 0.9778040082889087, 'learning_rate': 8.891909223547963e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 533/2224 [47:03<2:19:55,  4.96s/it]
0:  24%|â–ˆâ–ˆâ–       | 534/2224 [47:08<2:20:40,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3389, 'grad_norm': 0.9323938794237373, 'learning_rate': 8.887333321612338e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 534/2224 [47:08<2:20:40,  4.99s/it]
0:  24%|â–ˆâ–ˆâ–       | 535/2224 [47:13<2:19:50,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3797, 'grad_norm': 0.9606845263772129, 'learning_rate': 8.882749173528058e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 535/2224 [47:13<2:19:50,  4.97s/it]
0:  24%|â–ˆâ–ˆâ–       | 536/2224 [47:18<2:20:30,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3467, 'grad_norm': 0.9597963502315126, 'learning_rate': 8.87815678901942e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 536/2224 [47:18<2:20:30,  4.99s/it]
0:  24%|â–ˆâ–ˆâ–       | 537/2224 [47:23<2:19:51,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3535, 'grad_norm': 0.9526167790629323, 'learning_rate': 8.873556177828188e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 537/2224 [47:23<2:19:51,  4.97s/it]
0:  24%|â–ˆâ–ˆâ–       | 538/2224 [47:28<2:19:05,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3527, 'grad_norm': 0.9361328909946989, 'learning_rate': 8.868947349713577e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 538/2224 [47:28<2:19:05,  4.95s/it]
0:  24%|â–ˆâ–ˆâ–       | 539/2224 [47:33<2:18:55,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3539, 'grad_norm': 0.9415925271850504, 'learning_rate': 8.864330314452231e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 539/2224 [47:33<2:18:55,  4.95s/it]
0:  24%|â–ˆâ–ˆâ–       | 540/2224 [47:38<2:19:47,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3624, 'grad_norm': 0.9353395372745484, 'learning_rate': 8.85970508183821e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 540/2224 [47:38<2:19:47,  4.98s/it]
0:  24%|â–ˆâ–ˆâ–       | 541/2224 [47:43<2:20:17,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3679, 'grad_norm': 0.9634039882067865, 'learning_rate': 8.855071661682956e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 541/2224 [47:43<2:20:17,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–       | 542/2224 [47:48<2:20:13,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3431, 'grad_norm': 0.9132636709382687, 'learning_rate': 8.850430063815282e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 542/2224 [47:48<2:20:13,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–       | 543/2224 [47:53<2:20:00,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.356, 'grad_norm': 0.9652232985298899, 'learning_rate': 8.845780298081348e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 543/2224 [47:53<2:20:00,  5.00s/it]
0:  24%|â–ˆâ–ˆâ–       | 544/2224 [47:57<2:18:04,  4.93s/it]
0:                                                     
0: 
0: {'loss': 0.3419, 'grad_norm': 0.9921502326045621, 'learning_rate': 8.841122374344642e-06, 'epoch': 0.24}
0: 
0:  24%|â–ˆâ–ˆâ–       | 544/2224 [47:57<2:18:04,  4.93s/it]
0:  25%|â–ˆâ–ˆâ–       | 545/2224 [48:02<2:17:47,  4.92s/it]
0:                                                     
0: 
0: {'loss': 0.3222, 'grad_norm': 0.8918010007739327, 'learning_rate': 8.836456302485952e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 545/2224 [48:02<2:17:47,  4.92s/it]
0:  25%|â–ˆâ–ˆâ–       | 546/2224 [48:07<2:18:25,  4.95s/it]
0:                                                     
0: 
0: {'loss': 0.3492, 'grad_norm': 0.9591137287455302, 'learning_rate': 8.831782092403358e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 546/2224 [48:07<2:18:25,  4.95s/it]
0:  25%|â–ˆâ–ˆâ–       | 547/2224 [48:12<2:19:11,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.356, 'grad_norm': 0.9658096289064284, 'learning_rate': 8.827099754012198e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 547/2224 [48:12<2:19:11,  4.98s/it]
0:  25%|â–ˆâ–ˆâ–       | 548/2224 [48:17<2:19:35,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3682, 'grad_norm': 0.9640679319352561, 'learning_rate': 8.822409297245056e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 548/2224 [48:17<2:19:35,  5.00s/it]
0:  25%|â–ˆâ–ˆâ–       | 549/2224 [48:22<2:18:36,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3417, 'grad_norm': 0.9360731805568465, 'learning_rate': 8.817710732051734e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 549/2224 [48:22<2:18:36,  4.96s/it]
0:  25%|â–ˆâ–ˆâ–       | 550/2224 [48:27<2:19:00,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3417, 'grad_norm': 0.9574611692092453, 'learning_rate': 8.813004068399238e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 550/2224 [48:27<2:19:00,  4.98s/it]
0:  25%|â–ˆâ–ˆâ–       | 551/2224 [48:32<2:19:04,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3678, 'grad_norm': 0.9134346957496051, 'learning_rate': 8.80828931627175e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 551/2224 [48:32<2:19:04,  4.99s/it]
0:  25%|â–ˆâ–ˆâ–       | 552/2224 [48:37<2:19:30,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3495, 'grad_norm': 0.9250319157113657, 'learning_rate': 8.803566485670611e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 552/2224 [48:37<2:19:30,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–       | 553/2224 [48:42<2:19:35,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3191, 'grad_norm': 0.9295043433626011, 'learning_rate': 8.798835586614298e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 553/2224 [48:42<2:19:35,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–       | 554/2224 [48:47<2:19:09,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3466, 'grad_norm': 0.9420346865116673, 'learning_rate': 8.794096629138407e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 554/2224 [48:47<2:19:09,  5.00s/it]
0:  25%|â–ˆâ–ˆâ–       | 555/2224 [48:52<2:19:13,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3632, 'grad_norm': 0.930105209221894, 'learning_rate': 8.789349623295625e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–       | 555/2224 [48:52<2:19:13,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 556/2224 [48:57<2:18:53,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3819, 'grad_norm': 0.9644193262807473, 'learning_rate': 8.784594579155712e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 556/2224 [48:57<2:18:53,  5.00s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 557/2224 [49:02<2:19:11,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3663, 'grad_norm': 0.957565851791122, 'learning_rate': 8.779831506805479e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 557/2224 [49:02<2:19:11,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 558/2224 [49:07<2:18:56,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3787, 'grad_norm': 0.9565489315650084, 'learning_rate': 8.775060416348771e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 558/2224 [49:07<2:18:56,  5.00s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 559/2224 [49:12<2:19:07,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3352, 'grad_norm': 0.9057679516836045, 'learning_rate': 8.770281317906435e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 559/2224 [49:12<2:19:07,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 560/2224 [49:17<2:19:04,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3326, 'grad_norm': 0.961903663367652, 'learning_rate': 8.765494221616312e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 560/2224 [49:17<2:19:04,  5.01s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 561/2224 [49:22<2:19:14,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3561, 'grad_norm': 0.9775233172251804, 'learning_rate': 8.76069913763321e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 561/2224 [49:22<2:19:14,  5.02s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 562/2224 [49:28<2:19:15,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3539, 'grad_norm': 0.9707933654915337, 'learning_rate': 8.755896076128868e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 562/2224 [49:28<2:19:15,  5.03s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 563/2224 [49:33<2:19:32,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3693, 'grad_norm': 0.9325184866592254, 'learning_rate': 8.751085047291961e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 563/2224 [49:33<2:19:32,  5.04s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 564/2224 [49:38<2:19:37,  5.05s/it]
0:                                                     
0: 
0: {'loss': 0.3903, 'grad_norm': 0.980585995004844, 'learning_rate': 8.746266061328062e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 564/2224 [49:38<2:19:37,  5.05s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 565/2224 [49:43<2:18:59,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3395, 'grad_norm': 0.9375849474877326, 'learning_rate': 8.741439128459622e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 565/2224 [49:43<2:18:59,  5.03s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 566/2224 [49:48<2:18:49,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3567, 'grad_norm': 0.9186661320419298, 'learning_rate': 8.736604258925947e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 566/2224 [49:48<2:18:49,  5.02s/it]
0:  25%|â–ˆâ–ˆâ–Œ       | 567/2224 [49:53<2:17:46,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.364, 'grad_norm': 0.9531996937277689, 'learning_rate': 8.731761462983184e-06, 'epoch': 0.25}
0: 
0:  25%|â–ˆâ–ˆâ–Œ       | 567/2224 [49:53<2:17:46,  4.99s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 568/2224 [49:58<2:18:15,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3469, 'grad_norm': 0.8873080575802775, 'learning_rate': 8.726910750904291e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 568/2224 [49:58<2:18:15,  5.01s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 569/2224 [50:03<2:17:18,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3234, 'grad_norm': 1.0015603576381409, 'learning_rate': 8.72205213297902e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 569/2224 [50:03<2:17:18,  4.98s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 570/2224 [50:07<2:16:44,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3397, 'grad_norm': 0.925114102122122, 'learning_rate': 8.717185619513891e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 570/2224 [50:07<2:16:44,  4.96s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 571/2224 [50:12<2:17:04,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3697, 'grad_norm': 0.9348727633855725, 'learning_rate': 8.712311220832177e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 571/2224 [50:12<2:17:04,  4.98s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 572/2224 [50:17<2:17:25,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3528, 'grad_norm': 0.9091323432753592, 'learning_rate': 8.707428947273874e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 572/2224 [50:17<2:17:25,  4.99s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 573/2224 [50:22<2:17:24,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3652, 'grad_norm': 0.9223265014157892, 'learning_rate': 8.702538809195685e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 573/2224 [50:22<2:17:24,  4.99s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 574/2224 [50:28<2:17:37,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3561, 'grad_norm': 0.9569304397254265, 'learning_rate': 8.697640816970993e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 574/2224 [50:28<2:17:37,  5.00s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 575/2224 [50:33<2:17:46,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3488, 'grad_norm': 0.964674645192752, 'learning_rate': 8.692734980989846e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 575/2224 [50:33<2:17:46,  5.01s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 576/2224 [50:38<2:17:32,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3651, 'grad_norm': 0.9636420300352213, 'learning_rate': 8.687821311658928e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 576/2224 [50:38<2:17:32,  5.01s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 577/2224 [50:43<2:17:40,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3485, 'grad_norm': 0.9174945734722738, 'learning_rate': 8.68289981940154e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 577/2224 [50:43<2:17:40,  5.02s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 578/2224 [50:48<2:17:19,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.342, 'grad_norm': 0.9883784832767359, 'learning_rate': 8.67797051465758e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 578/2224 [50:48<2:17:19,  5.01s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 579/2224 [50:53<2:17:01,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3514, 'grad_norm': 0.9959576818771253, 'learning_rate': 8.673033407883517e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 579/2224 [50:53<2:17:01,  5.00s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 580/2224 [50:58<2:17:02,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3183, 'grad_norm': 0.9237491599793741, 'learning_rate': 8.668088509552368e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 580/2224 [50:58<2:17:02,  5.00s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 581/2224 [51:03<2:17:11,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.365, 'grad_norm': 0.9163477360439704, 'learning_rate': 8.66313583015368e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 581/2224 [51:03<2:17:11,  5.01s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 582/2224 [51:07<2:16:05,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3258, 'grad_norm': 0.9040162898813786, 'learning_rate': 8.65817538019351e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 582/2224 [51:07<2:16:05,  4.97s/it]
0:  26%|â–ˆâ–ˆâ–Œ       | 583/2224 [51:12<2:16:11,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3258, 'grad_norm': 0.9249107560611898, 'learning_rate': 8.65320717019439e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–Œ       | 583/2224 [51:12<2:16:11,  4.98s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 584/2224 [51:17<2:16:35,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3144, 'grad_norm': 0.9337102917730542, 'learning_rate': 8.648231210695323e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 584/2224 [51:17<2:16:35,  5.00s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 585/2224 [51:22<2:15:37,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3563, 'grad_norm': 0.9402719928897043, 'learning_rate': 8.643247512251743e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 585/2224 [51:22<2:15:37,  4.97s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 586/2224 [51:27<2:15:54,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3614, 'grad_norm': 0.9401444168122575, 'learning_rate': 8.638256085435502e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 586/2224 [51:27<2:15:54,  4.98s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 587/2224 [51:32<2:16:18,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3625, 'grad_norm': 0.9195300712354232, 'learning_rate': 8.633256940834848e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 587/2224 [51:32<2:16:18,  5.00s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 588/2224 [51:37<2:15:28,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3749, 'grad_norm': 0.9556717996020982, 'learning_rate': 8.628250089054405e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 588/2224 [51:37<2:15:28,  4.97s/it]
0:  26%|â–ˆâ–ˆâ–‹       | 589/2224 [51:42<2:15:17,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3118, 'grad_norm': 0.9468385828970444, 'learning_rate': 8.623235540715135e-06, 'epoch': 0.26}
0: 
0:  26%|â–ˆâ–ˆâ–‹       | 589/2224 [51:42<2:15:17,  4.97s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 590/2224 [51:47<2:15:15,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3705, 'grad_norm': 0.9449014897558986, 'learning_rate': 8.618213306454337e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 590/2224 [51:47<2:15:15,  4.97s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 591/2224 [51:52<2:14:59,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3713, 'grad_norm': 0.9905332124392413, 'learning_rate': 8.613183396925608e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 591/2224 [51:52<2:14:59,  4.96s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 592/2224 [51:57<2:15:16,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3649, 'grad_norm': 0.9379215033141316, 'learning_rate': 8.60814582279883e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 592/2224 [51:57<2:15:16,  4.97s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 593/2224 [52:02<2:15:44,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3384, 'grad_norm': 0.9486005472199893, 'learning_rate': 8.603100594760141e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 593/2224 [52:02<2:15:44,  4.99s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 594/2224 [52:07<2:15:41,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3525, 'grad_norm': 0.9479329450721213, 'learning_rate': 8.598047723511916e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 594/2224 [52:07<2:15:41,  4.99s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 595/2224 [52:12<2:15:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3564, 'grad_norm': 0.9124075809389737, 'learning_rate': 8.592987219772746e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 595/2224 [52:12<2:15:48,  5.00s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 596/2224 [52:17<2:15:20,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3247, 'grad_norm': 0.9480835197414659, 'learning_rate': 8.587919094277408e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 596/2224 [52:17<2:15:20,  4.99s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 597/2224 [52:22<2:15:36,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3929, 'grad_norm': 0.9707527367395584, 'learning_rate': 8.58284335777685e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 597/2224 [52:22<2:15:36,  5.00s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 598/2224 [52:27<2:15:16,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3409, 'grad_norm': 0.988839812896159, 'learning_rate': 8.577760021038166e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 598/2224 [52:27<2:15:16,  4.99s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 599/2224 [52:32<2:15:38,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3556, 'grad_norm': 0.92244035242878, 'learning_rate': 8.572669094844568e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 599/2224 [52:32<2:15:38,  5.01s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 600/2224 [52:37<2:15:46,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3707, 'grad_norm': 0.9350868090724651, 'learning_rate': 8.567570589995374e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 600/2224 [52:37<2:15:46,  5.02s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  27%|â–ˆâ–ˆâ–‹       | 601/2224 [53:51<11:35:29, 25.71s/it]
0:                                                      
0: 
0: {'loss': 0.3301, 'grad_norm': 0.9498671726771811, 'learning_rate': 8.562464517305972e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 601/2224 [53:51<11:35:29, 25.71s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 602/2224 [53:56<8:46:57, 19.49s/it] 
0:                                                     
0: 
0: {'loss': 0.3415, 'grad_norm': 0.9324637346442297, 'learning_rate': 8.557350887607808e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 602/2224 [53:56<8:46:57, 19.49s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 603/2224 [54:01<6:49:21, 15.15s/it]
0:                                                     
0: 
0: {'loss': 0.3329, 'grad_norm': 0.9580348832890734, 'learning_rate': 8.552229711748354e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 603/2224 [54:01<6:49:21, 15.15s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 604/2224 [54:06<5:27:08, 12.12s/it]
0:                                                     
0: 
0: {'loss': 0.3734, 'grad_norm': 1.0333202647055453, 'learning_rate': 8.547101000591096e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 604/2224 [54:06<5:27:08, 12.12s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 605/2224 [54:11<4:29:11,  9.98s/it]
0:                                                     
0: 
0: {'loss': 0.3241, 'grad_norm': 0.9713162282890042, 'learning_rate': 8.5419647650155e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 605/2224 [54:11<4:29:11,  9.98s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 606/2224 [54:16<3:49:00,  8.49s/it]
0:                                                     
0: 
0: {'loss': 0.351, 'grad_norm': 0.9329037257045603, 'learning_rate': 8.536821015916994e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 606/2224 [54:16<3:49:00,  8.49s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 607/2224 [54:21<3:20:37,  7.44s/it]
0:                                                     
0: 
0: {'loss': 0.3358, 'grad_norm': 0.9466245861818731, 'learning_rate': 8.531669764206945e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 607/2224 [54:21<3:20:37,  7.44s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 608/2224 [54:26<3:01:00,  6.72s/it]
0:                                                     
0: 
0: {'loss': 0.3349, 'grad_norm': 0.9451852713937334, 'learning_rate': 8.526511020812633e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 608/2224 [54:26<3:01:00,  6.72s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 609/2224 [54:31<2:47:19,  6.22s/it]
0:                                                     
0: 
0: {'loss': 0.3403, 'grad_norm': 0.9451430301467271, 'learning_rate': 8.521344796677236e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 609/2224 [54:31<2:47:19,  6.22s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 610/2224 [54:36<2:36:33,  5.82s/it]
0:                                                     
0: 
0: {'loss': 0.3471, 'grad_norm': 0.901156307061305, 'learning_rate': 8.516171102759793e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 610/2224 [54:36<2:36:33,  5.82s/it]
0:  27%|â–ˆâ–ˆâ–‹       | 611/2224 [54:41<2:29:43,  5.57s/it]
0:                                                     
0: 
0: {'loss': 0.375, 'grad_norm': 0.9578320449496126, 'learning_rate': 8.510989950035193e-06, 'epoch': 0.27}
0: 
0:  27%|â–ˆâ–ˆâ–‹       | 611/2224 [54:41<2:29:43,  5.57s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 612/2224 [54:46<2:23:27,  5.34s/it]
0:                                                     
0: 
0: {'loss': 0.3651, 'grad_norm': 0.9297874760899848, 'learning_rate': 8.50580134949415e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 612/2224 [54:46<2:23:27,  5.34s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 613/2224 [54:51<2:20:24,  5.23s/it]
0:                                                     
0: 
0: {'loss': 0.3292, 'grad_norm': 0.9329748637070983, 'learning_rate': 8.500605312143169e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 613/2224 [54:51<2:20:24,  5.23s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 614/2224 [54:56<2:18:25,  5.16s/it]
0:                                                     
0: 
0: {'loss': 0.3361, 'grad_norm': 0.9200210969508732, 'learning_rate': 8.49540184900454e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 614/2224 [54:56<2:18:25,  5.16s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 615/2224 [55:01<2:16:50,  5.10s/it]
0:                                                     
0: 
0: {'loss': 0.3463, 'grad_norm': 0.9326218178460867, 'learning_rate': 8.490190971116294e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 615/2224 [55:01<2:16:50,  5.10s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 616/2224 [55:06<2:15:55,  5.07s/it]
0:                                                     
0: 
0: {'loss': 0.3431, 'grad_norm': 0.9206509467132589, 'learning_rate': 8.484972689532201e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 616/2224 [55:06<2:15:55,  5.07s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 617/2224 [55:11<2:16:24,  5.09s/it]
0:                                                     
0: 
0: {'loss': 0.3702, 'grad_norm': 0.970185401828679, 'learning_rate': 8.479747015321735e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 617/2224 [55:11<2:16:24,  5.09s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 618/2224 [55:16<2:15:25,  5.06s/it]
0:                                                     
0: 
0: {'loss': 0.3506, 'grad_norm': 0.9694871922942647, 'learning_rate': 8.474513959570045e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 618/2224 [55:16<2:15:25,  5.06s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 619/2224 [55:21<2:13:54,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3478, 'grad_norm': 0.9133442883726857, 'learning_rate': 8.469273533377944e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 619/2224 [55:21<2:13:54,  5.01s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 620/2224 [55:26<2:13:33,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3428, 'grad_norm': 0.9361040938046118, 'learning_rate': 8.464025747861882e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 620/2224 [55:26<2:13:33,  5.00s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 621/2224 [55:31<2:12:53,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3296, 'grad_norm': 0.9405527326201737, 'learning_rate': 8.458770614153912e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 621/2224 [55:31<2:12:53,  4.97s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 622/2224 [55:36<2:13:18,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3751, 'grad_norm': 0.9175147973020876, 'learning_rate': 8.453508143401682e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 622/2224 [55:36<2:13:18,  4.99s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 623/2224 [55:41<2:13:14,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3566, 'grad_norm': 0.9933634165856972, 'learning_rate': 8.448238346768401e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 623/2224 [55:41<2:13:14,  4.99s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 624/2224 [55:46<2:13:32,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3354, 'grad_norm': 0.9354212800519398, 'learning_rate': 8.442961235432818e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 624/2224 [55:46<2:13:32,  5.01s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 625/2224 [55:51<2:13:40,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3568, 'grad_norm': 0.9329827533873635, 'learning_rate': 8.437676820589202e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 625/2224 [55:51<2:13:40,  5.02s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 626/2224 [55:56<2:14:05,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3615, 'grad_norm': 0.9253221682579683, 'learning_rate': 8.432385113447308e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 626/2224 [55:56<2:14:05,  5.03s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 627/2224 [56:01<2:14:08,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3376, 'grad_norm': 0.9182696550471579, 'learning_rate': 8.427086125232367e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 627/2224 [56:01<2:14:08,  5.04s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 628/2224 [56:06<2:13:07,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3349, 'grad_norm': 0.9280778324853565, 'learning_rate': 8.421779867185054e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 628/2224 [56:06<2:13:07,  5.00s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 629/2224 [56:11<2:13:26,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3575, 'grad_norm': 0.9383090585102826, 'learning_rate': 8.416466350561458e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 629/2224 [56:11<2:13:26,  5.02s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 630/2224 [56:16<2:12:38,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.382, 'grad_norm': 0.9241195381705821, 'learning_rate': 8.411145586633079e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 630/2224 [56:16<2:12:38,  4.99s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 631/2224 [56:21<2:12:52,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.381, 'grad_norm': 0.9795843278808921, 'learning_rate': 8.405817586686778e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 631/2224 [56:21<2:12:52,  5.00s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 632/2224 [56:26<2:13:10,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3449, 'grad_norm': 0.9356353192097159, 'learning_rate': 8.400482362024771e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 632/2224 [56:26<2:13:10,  5.02s/it]
0:  28%|â–ˆâ–ˆâ–Š       | 633/2224 [56:31<2:13:01,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3447, 'grad_norm': 0.9413671432691539, 'learning_rate': 8.3951399239646e-06, 'epoch': 0.28}
0: 
0:  28%|â–ˆâ–ˆâ–Š       | 633/2224 [56:31<2:13:01,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 634/2224 [56:36<2:13:11,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3452, 'grad_norm': 0.9450200441656631, 'learning_rate': 8.389790283839109e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 634/2224 [56:36<2:13:11,  5.03s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 635/2224 [56:41<2:13:24,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.365, 'grad_norm': 0.9546668007168346, 'learning_rate': 8.38443345299642e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 635/2224 [56:41<2:13:24,  5.04s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 636/2224 [56:46<2:13:23,  5.04s/it]
0:                                                     
0: 
0: {'loss': 0.3481, 'grad_norm': 0.9456733565310206, 'learning_rate': 8.379069442799901e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 636/2224 [56:46<2:13:23,  5.04s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 637/2224 [56:51<2:12:53,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3562, 'grad_norm': 0.9525978598230203, 'learning_rate': 8.373698264628163e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 637/2224 [56:51<2:12:53,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 638/2224 [56:56<2:11:46,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3504, 'grad_norm': 0.9258008201243267, 'learning_rate': 8.36831992987501e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 638/2224 [56:56<2:11:46,  4.99s/it]
0:  29%|â–ˆâ–ˆâ–Š       | 639/2224 [57:01<2:11:47,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3543, 'grad_norm': 0.9220852879178029, 'learning_rate': 8.362934449949437e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–Š       | 639/2224 [57:01<2:11:47,  4.99s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 640/2224 [57:06<2:12:33,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3596, 'grad_norm': 0.9211671527211549, 'learning_rate': 8.357541836275586e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 640/2224 [57:06<2:12:33,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 641/2224 [57:12<2:14:10,  5.09s/it]
0:                                                     
0: 
0: {'loss': 0.3581, 'grad_norm': 0.9525173264541766, 'learning_rate': 8.352142100292739e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 641/2224 [57:12<2:14:10,  5.09s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 642/2224 [57:17<2:13:22,  5.06s/it]
0:                                                     
0: 
0: {'loss': 0.3633, 'grad_norm': 0.9389308892401268, 'learning_rate': 8.346735253455287e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 642/2224 [57:17<2:13:22,  5.06s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 643/2224 [57:22<2:12:33,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3294, 'grad_norm': 0.9518071918889626, 'learning_rate': 8.341321307232699e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 643/2224 [57:22<2:12:33,  5.03s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 644/2224 [57:27<2:12:33,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3573, 'grad_norm': 0.9228534210151128, 'learning_rate': 8.33590027310951e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 644/2224 [57:27<2:12:33,  5.03s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 645/2224 [57:32<2:12:11,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.374, 'grad_norm': 0.9610466792265501, 'learning_rate': 8.330472162585283e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 645/2224 [57:32<2:12:11,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 646/2224 [57:37<2:12:10,  5.03s/it]
0:                                                     
0: 
0: {'loss': 0.3436, 'grad_norm': 0.9565786642964725, 'learning_rate': 8.325036987174603e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 646/2224 [57:37<2:12:10,  5.03s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 647/2224 [57:42<2:11:58,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3523, 'grad_norm': 0.9521108853242761, 'learning_rate': 8.319594758407032e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 647/2224 [57:42<2:11:58,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 648/2224 [57:47<2:11:46,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3829, 'grad_norm': 0.9613366769744407, 'learning_rate': 8.314145487827102e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 648/2224 [57:47<2:11:46,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 649/2224 [57:52<2:10:54,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3071, 'grad_norm': 0.8825264195034117, 'learning_rate': 8.308689186994272e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 649/2224 [57:52<2:10:54,  4.99s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 650/2224 [57:57<2:11:11,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3628, 'grad_norm': 0.9907877682654784, 'learning_rate': 8.303225867482922e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 650/2224 [57:57<2:11:11,  5.00s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 651/2224 [58:02<2:11:06,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3157, 'grad_norm': 0.8968626447316256, 'learning_rate': 8.297755540882323e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 651/2224 [58:02<2:11:06,  5.00s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 652/2224 [58:07<2:11:12,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3763, 'grad_norm': 0.9332611288836944, 'learning_rate': 8.292278218796606e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 652/2224 [58:07<2:11:12,  5.01s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 653/2224 [58:12<2:11:22,  5.02s/it]
0:                                                     
0: 
0: {'loss': 0.3334, 'grad_norm': 0.9442771345394343, 'learning_rate': 8.28679391284474e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 653/2224 [58:12<2:11:22,  5.02s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 654/2224 [58:17<2:10:24,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3272, 'grad_norm': 0.986702669596761, 'learning_rate': 8.281302634660509e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 654/2224 [58:17<2:10:24,  4.98s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 655/2224 [58:22<2:10:54,  5.01s/it]
0:                                                     
0: 
0: {'loss': 0.3551, 'grad_norm': 0.9271641743253195, 'learning_rate': 8.27580439589249e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 655/2224 [58:22<2:10:54,  5.01s/it]
0:  29%|â–ˆâ–ˆâ–‰       | 656/2224 [58:27<2:10:40,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3347, 'grad_norm': 0.93983556645507, 'learning_rate': 8.270299208204024e-06, 'epoch': 0.29}
0: 
0:  29%|â–ˆâ–ˆâ–‰       | 656/2224 [58:27<2:10:40,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 657/2224 [58:31<2:09:35,  4.96s/it]
0:                                                     
0: 
0: {'loss': 0.3521, 'grad_norm': 0.9036814361246681, 'learning_rate': 8.264787083273193e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 657/2224 [58:31<2:09:35,  4.96s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 658/2224 [58:36<2:09:00,  4.94s/it]
0:                                                     
0: 
0: {'loss': 0.3391, 'grad_norm': 0.9386452230189871, 'learning_rate': 8.259268032792796e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 658/2224 [58:36<2:09:00,  4.94s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 659/2224 [58:41<2:09:39,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3594, 'grad_norm': 0.9440843674525091, 'learning_rate': 8.25374206847032e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 659/2224 [58:41<2:09:39,  4.97s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 660/2224 [58:46<2:09:50,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.33, 'grad_norm': 0.9452355464829604, 'learning_rate': 8.248209202027918e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 660/2224 [58:46<2:09:50,  4.98s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 661/2224 [58:51<2:09:52,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3389, 'grad_norm': 0.9211845408213322, 'learning_rate': 8.24266944520239e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 661/2224 [58:51<2:09:52,  4.99s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 662/2224 [58:56<2:09:54,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3825, 'grad_norm': 0.9275282047349008, 'learning_rate': 8.237122809745149e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 662/2224 [58:56<2:09:54,  4.99s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 663/2224 [59:01<2:09:17,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3489, 'grad_norm': 0.9316658558323259, 'learning_rate': 8.231569307422193e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 663/2224 [59:01<2:09:17,  4.97s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 664/2224 [59:06<2:09:28,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.3792, 'grad_norm': 0.9522003028058762, 'learning_rate': 8.226008950014099e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 664/2224 [59:06<2:09:28,  4.98s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 665/2224 [59:11<2:09:24,  4.98s/it]
0:                                                     
0: 
0: {'loss': 0.338, 'grad_norm': 0.9427370438133361, 'learning_rate': 8.220441749315978e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 665/2224 [59:11<2:09:24,  4.98s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 666/2224 [59:16<2:09:48,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3365, 'grad_norm': 0.9125583725567993, 'learning_rate': 8.214867717137458e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 666/2224 [59:16<2:09:48,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–‰       | 667/2224 [59:21<2:09:40,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3516, 'grad_norm': 0.9718724711999598, 'learning_rate': 8.209286865302661e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–‰       | 667/2224 [59:21<2:09:40,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 668/2224 [59:26<2:09:27,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3275, 'grad_norm': 0.944078630061954, 'learning_rate': 8.203699205650173e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 668/2224 [59:26<2:09:27,  4.99s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 669/2224 [59:31<2:09:20,  4.99s/it]
0:                                                     
0: 
0: {'loss': 0.3413, 'grad_norm': 0.913413037271598, 'learning_rate': 8.198104750033022e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 669/2224 [59:31<2:09:20,  4.99s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 670/2224 [59:36<2:09:32,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3071, 'grad_norm': 0.9186632146355979, 'learning_rate': 8.192503510318655e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 670/2224 [59:36<2:09:32,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 671/2224 [59:41<2:09:22,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3725, 'grad_norm': 0.9615652296055112, 'learning_rate': 8.186895498388903e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 671/2224 [59:41<2:09:22,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 672/2224 [59:46<2:09:19,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3092, 'grad_norm': 0.8953100424275267, 'learning_rate': 8.181280726139971e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 672/2224 [59:46<2:09:19,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 673/2224 [59:51<2:09:10,  5.00s/it]
0:                                                     
0: 
0: {'loss': 0.3304, 'grad_norm': 0.9095299498898219, 'learning_rate': 8.175659205482402e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 673/2224 [59:51<2:09:10,  5.00s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 674/2224 [59:56<2:08:26,  4.97s/it]
0:                                                     
0: 
0: {'loss': 0.3652, 'grad_norm': 0.9038572744475755, 'learning_rate': 8.170030948341049e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 674/2224 [59:56<2:08:26,  4.97s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 675/2224 [1:00:01<2:08:37,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3386, 'grad_norm': 0.8938548066815586, 'learning_rate': 8.164395966655062e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 675/2224 [1:00:01<2:08:37,  4.98s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 676/2224 [1:00:06<2:08:27,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3211, 'grad_norm': 0.9337311439846205, 'learning_rate': 8.158754272377854e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 676/2224 [1:00:06<2:08:27,  4.98s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 677/2224 [1:00:11<2:08:32,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3547, 'grad_norm': 0.9442194307161422, 'learning_rate': 8.153105877477075e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 677/2224 [1:00:11<2:08:32,  4.99s/it]
0:  30%|â–ˆâ–ˆâ–ˆ       | 678/2224 [1:00:16<2:08:31,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3639, 'grad_norm': 0.9354989545012977, 'learning_rate': 8.147450793934587e-06, 'epoch': 0.3}
0: 
0:  30%|â–ˆâ–ˆâ–ˆ       | 678/2224 [1:00:16<2:08:31,  4.99s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 679/2224 [1:00:21<2:08:14,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3578, 'grad_norm': 0.935488032686069, 'learning_rate': 8.14178903374645e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 679/2224 [1:00:21<2:08:14,  4.98s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 680/2224 [1:00:26<2:08:31,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3486, 'grad_norm': 0.9253300653672787, 'learning_rate': 8.136120608922876e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 680/2224 [1:00:26<2:08:31,  4.99s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 681/2224 [1:00:31<2:08:33,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3497, 'grad_norm': 0.9434293679459959, 'learning_rate': 8.130445531488223e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 681/2224 [1:00:31<2:08:33,  5.00s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 682/2224 [1:00:36<2:08:28,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3521, 'grad_norm': 0.9556606685300527, 'learning_rate': 8.124763813480953e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 682/2224 [1:00:36<2:08:28,  5.00s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 683/2224 [1:00:41<2:08:21,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3576, 'grad_norm': 0.9088876942770195, 'learning_rate': 8.119075466953626e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 683/2224 [1:00:41<2:08:21,  5.00s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 684/2224 [1:00:46<2:07:41,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3648, 'grad_norm': 0.9136279523442743, 'learning_rate': 8.113380503972848e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 684/2224 [1:00:46<2:07:41,  4.98s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 685/2224 [1:00:51<2:06:59,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.3305, 'grad_norm': 0.9193444649843169, 'learning_rate': 8.107678936619274e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 685/2224 [1:00:51<2:06:59,  4.95s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 686/2224 [1:00:56<2:07:11,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3571, 'grad_norm': 0.9592031927676337, 'learning_rate': 8.10197077698756e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 686/2224 [1:00:56<2:07:11,  4.96s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 687/2224 [1:01:01<2:07:41,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3543, 'grad_norm': 0.9245196712993193, 'learning_rate': 8.096256037186354e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 687/2224 [1:01:01<2:07:41,  4.98s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 688/2224 [1:01:06<2:07:48,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3471, 'grad_norm': 0.9132310853294155, 'learning_rate': 8.090534729338252e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 688/2224 [1:01:06<2:07:48,  4.99s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 689/2224 [1:01:11<2:07:59,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3233, 'grad_norm': 0.9526683183547354, 'learning_rate': 8.084806865579793e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 689/2224 [1:01:11<2:07:59,  5.00s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 690/2224 [1:01:16<2:06:49,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3342, 'grad_norm': 0.9483521175961642, 'learning_rate': 8.079072458061417e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 690/2224 [1:01:16<2:06:49,  4.96s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 691/2224 [1:01:21<2:06:56,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3788, 'grad_norm': 0.967282519997161, 'learning_rate': 8.073331518947445e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 691/2224 [1:01:21<2:06:56,  4.97s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 692/2224 [1:01:26<2:06:57,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3153, 'grad_norm': 0.9123358334850484, 'learning_rate': 8.067584060416058e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 692/2224 [1:01:26<2:06:57,  4.97s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 693/2224 [1:01:31<2:06:23,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.2992, 'grad_norm': 0.8500677531048443, 'learning_rate': 8.061830094659262e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 693/2224 [1:01:31<2:06:23,  4.95s/it]
0:  31%|â–ˆâ–ˆâ–ˆ       | 694/2224 [1:01:36<2:06:45,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3184, 'grad_norm': 0.8574624802909604, 'learning_rate': 8.056069633882868e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆ       | 694/2224 [1:01:36<2:06:45,  4.97s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 695/2224 [1:01:41<2:07:23,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3376, 'grad_norm': 0.944475581672855, 'learning_rate': 8.050302690306463e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 695/2224 [1:01:41<2:07:23,  5.00s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 696/2224 [1:01:46<2:07:43,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3445, 'grad_norm': 0.9340392190877873, 'learning_rate': 8.044529276163395e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 696/2224 [1:01:46<2:07:43,  5.02s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 697/2224 [1:01:51<2:07:26,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3367, 'grad_norm': 0.8986819640008284, 'learning_rate': 8.038749403700724e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 697/2224 [1:01:51<2:07:26,  5.01s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 698/2224 [1:01:56<2:08:04,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.3302, 'grad_norm': 0.8961730765985658, 'learning_rate': 8.03296308517922e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 698/2224 [1:01:56<2:08:04,  5.04s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 699/2224 [1:02:01<2:07:36,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.374, 'grad_norm': 0.9730012760761784, 'learning_rate': 8.027170332873323e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 699/2224 [1:02:01<2:07:36,  5.02s/it]
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 700/2224 [1:02:06<2:07:35,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3551, 'grad_norm': 0.949156540184693, 'learning_rate': 8.02137115907112e-06, 'epoch': 0.31}
0: 
0:  31%|â–ˆâ–ˆâ–ˆâ–      | 700/2224 [1:02:06<2:07:35,  5.02s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 701/2224 [1:02:11<2:06:25,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3284, 'grad_norm': 0.9328264616231975, 'learning_rate': 8.015565576074325e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 701/2224 [1:02:11<2:06:25,  4.98s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 702/2224 [1:02:16<2:06:25,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3269, 'grad_norm': 0.9364419305357865, 'learning_rate': 8.00975359619824e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 702/2224 [1:02:16<2:06:25,  4.98s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 703/2224 [1:02:21<2:07:15,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3417, 'grad_norm': 0.8925838866258199, 'learning_rate': 8.003935231771747e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 703/2224 [1:02:21<2:07:15,  5.02s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 704/2224 [1:02:26<2:06:58,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3114, 'grad_norm': 0.8870238462539163, 'learning_rate': 7.99811049513726e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 704/2224 [1:02:26<2:06:58,  5.01s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 705/2224 [1:02:31<2:07:06,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3421, 'grad_norm': 0.9258052619714137, 'learning_rate': 7.992279398650716e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 705/2224 [1:02:31<2:07:06,  5.02s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 706/2224 [1:02:36<2:06:32,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3482, 'grad_norm': 0.9578697371190729, 'learning_rate': 7.986441954681547e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 706/2224 [1:02:36<2:06:32,  5.00s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 707/2224 [1:02:41<2:04:26,  4.92s/it]
0:                                                       
0: 
0: {'loss': 0.3167, 'grad_norm': 0.9171570352552548, 'learning_rate': 7.98059817561264e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 707/2224 [1:02:41<2:04:26,  4.92s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 708/2224 [1:02:46<2:05:10,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.3281, 'grad_norm': 0.8900059409494957, 'learning_rate': 7.974748073840329e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 708/2224 [1:02:46<2:05:10,  4.95s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 709/2224 [1:02:51<2:05:35,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3252, 'grad_norm': 0.9585139689519925, 'learning_rate': 7.968891661774358e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 709/2224 [1:02:51<2:05:35,  4.97s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 710/2224 [1:02:56<2:05:51,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3334, 'grad_norm': 0.9327538395366383, 'learning_rate': 7.963028951837855e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 710/2224 [1:02:56<2:05:51,  4.99s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 711/2224 [1:03:01<2:06:06,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3681, 'grad_norm': 1.1398143004053969, 'learning_rate': 7.957159956467309e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 711/2224 [1:03:01<2:06:06,  5.00s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 712/2224 [1:03:06<2:05:24,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.352, 'grad_norm': 0.9196589294546756, 'learning_rate': 7.951284688112542e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 712/2224 [1:03:06<2:05:24,  4.98s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 713/2224 [1:03:11<2:05:21,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3779, 'grad_norm': 0.9811056336480448, 'learning_rate': 7.945403159236682e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 713/2224 [1:03:11<2:05:21,  4.98s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 714/2224 [1:03:16<2:03:44,  4.92s/it]
0:                                                       
0: 
0: {'loss': 0.357, 'grad_norm': 0.9147759339337097, 'learning_rate': 7.93951538231614e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 714/2224 [1:03:16<2:03:44,  4.92s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 715/2224 [1:03:21<2:04:37,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3623, 'grad_norm': 0.9063981010982356, 'learning_rate': 7.93362136984058e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 715/2224 [1:03:21<2:04:37,  4.96s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 716/2224 [1:03:26<2:05:18,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3463, 'grad_norm': 0.8912561910276605, 'learning_rate': 7.92772113431289e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 716/2224 [1:03:26<2:05:18,  4.99s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 717/2224 [1:03:31<2:05:24,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3294, 'grad_norm': 0.888716312048789, 'learning_rate': 7.921814688249162e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 717/2224 [1:03:31<2:05:24,  4.99s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 718/2224 [1:03:36<2:05:31,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3431, 'grad_norm': 0.9059533254976655, 'learning_rate': 7.915902044178664e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 718/2224 [1:03:36<2:05:31,  5.00s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 719/2224 [1:03:41<2:05:32,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3754, 'grad_norm': 0.905617088704475, 'learning_rate': 7.909983214643806e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 719/2224 [1:03:41<2:05:32,  5.01s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 720/2224 [1:03:46<2:05:27,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3437, 'grad_norm': 0.8984320256985159, 'learning_rate': 7.904058212200125e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 720/2224 [1:03:46<2:05:27,  5.00s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 721/2224 [1:03:51<2:05:42,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3472, 'grad_norm': 0.9283013985304019, 'learning_rate': 7.89812704941625e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 721/2224 [1:03:51<2:05:42,  5.02s/it]
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 722/2224 [1:03:56<2:05:52,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3537, 'grad_norm': 0.9594935023483412, 'learning_rate': 7.892189738873877e-06, 'epoch': 0.32}
0: 
0:  32%|â–ˆâ–ˆâ–ˆâ–      | 722/2224 [1:03:56<2:05:52,  5.03s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2224 [1:04:01<2:05:09,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3429, 'grad_norm': 0.9103732433456736, 'learning_rate': 7.886246293167744e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 723/2224 [1:04:01<2:05:09,  5.00s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2224 [1:04:06<2:05:46,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3703, 'grad_norm': 0.9695200755545665, 'learning_rate': 7.880296724905605e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 724/2224 [1:04:06<2:05:46,  5.03s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2224 [1:04:11<2:05:10,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.347, 'grad_norm': 0.9340732654595176, 'learning_rate': 7.8743410467082e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 725/2224 [1:04:11<2:05:10,  5.01s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2224 [1:04:16<2:04:40,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.366, 'grad_norm': 0.9229823212402479, 'learning_rate': 7.86837927120923e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 726/2224 [1:04:16<2:04:40,  4.99s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2224 [1:04:21<2:03:56,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.321, 'grad_norm': 0.9329912734425635, 'learning_rate': 7.862411411055328e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 727/2224 [1:04:21<2:03:56,  4.97s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2224 [1:04:26<2:03:51,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3162, 'grad_norm': 0.9293244878995595, 'learning_rate': 7.856437478906041e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 728/2224 [1:04:26<2:03:51,  4.97s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2224 [1:04:31<2:04:12,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.342, 'grad_norm': 0.9152906402030078, 'learning_rate': 7.85045748743379e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 729/2224 [1:04:31<2:04:12,  4.99s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2224 [1:04:36<2:04:37,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3143, 'grad_norm': 0.8822757772111643, 'learning_rate': 7.84447144932385e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 730/2224 [1:04:36<2:04:37,  5.01s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2224 [1:04:41<2:04:34,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3102, 'grad_norm': 0.8624886855236095, 'learning_rate': 7.838479377274326e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 731/2224 [1:04:41<2:04:34,  5.01s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2224 [1:04:46<2:04:46,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3315, 'grad_norm': 1.155612323402234, 'learning_rate': 7.832481283996122e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 732/2224 [1:04:46<2:04:46,  5.02s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2224 [1:04:51<2:04:58,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3598, 'grad_norm': 0.9672457909902409, 'learning_rate': 7.826477182212912e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 733/2224 [1:04:51<2:04:58,  5.03s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2224 [1:04:56<2:03:49,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3419, 'grad_norm': 0.9064506169768333, 'learning_rate': 7.820467084661118e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2224 [1:04:56<2:03:49,  4.99s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2224 [1:05:01<2:04:12,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3333, 'grad_norm': 0.9437787485317906, 'learning_rate': 7.81445100408988e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2224 [1:05:01<2:04:12,  5.01s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 736/2224 [1:05:06<2:04:29,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.326, 'grad_norm': 0.8894176065010599, 'learning_rate': 7.808428953261031e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 736/2224 [1:05:06<2:04:29,  5.02s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 737/2224 [1:05:11<2:03:37,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3483, 'grad_norm': 0.92442713644057, 'learning_rate': 7.802400944949068e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 737/2224 [1:05:11<2:03:37,  4.99s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 738/2224 [1:05:16<2:04:19,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3517, 'grad_norm': 0.9131735339950962, 'learning_rate': 7.796366991941124e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 738/2224 [1:05:16<2:04:19,  5.02s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 739/2224 [1:05:21<2:02:55,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.32, 'grad_norm': 0.9220725222775585, 'learning_rate': 7.790327107036942e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 739/2224 [1:05:21<2:02:55,  4.97s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2224 [1:05:26<2:02:57,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3177, 'grad_norm': 0.9100687414204056, 'learning_rate': 7.784281303048853e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2224 [1:05:26<2:02:57,  4.97s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 741/2224 [1:05:31<2:03:14,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3495, 'grad_norm': 0.869572491095401, 'learning_rate': 7.778229592801738e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 741/2224 [1:05:31<2:03:14,  4.99s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 742/2224 [1:05:36<2:03:24,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3211, 'grad_norm': 0.8970652241228098, 'learning_rate': 7.772171989133013e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 742/2224 [1:05:36<2:03:24,  5.00s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 743/2224 [1:05:41<2:02:53,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3483, 'grad_norm': 0.8934723656217249, 'learning_rate': 7.76610850489259e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 743/2224 [1:05:41<2:02:53,  4.98s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 744/2224 [1:05:46<2:04:58,  5.07s/it]
0:                                                       
0: 
0: {'loss': 0.3663, 'grad_norm': 0.9560500381509656, 'learning_rate': 7.760039152942856e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 744/2224 [1:05:46<2:04:58,  5.07s/it]
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 745/2224 [1:05:51<2:04:31,  5.05s/it]
0:                                                       
0: 
0: {'loss': 0.3322, 'grad_norm': 0.8908912108306727, 'learning_rate': 7.753963946158651e-06, 'epoch': 0.33}
0: 
0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 745/2224 [1:05:51<2:04:31,  5.05s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 746/2224 [1:05:56<2:04:03,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.3398, 'grad_norm': 0.9392077889397038, 'learning_rate': 7.747882897427228e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 746/2224 [1:05:56<2:04:03,  5.04s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 747/2224 [1:06:01<2:07:26,  5.18s/it]
0:                                                       
0: 
0: {'loss': 0.3385, 'grad_norm': 0.9025127478961, 'learning_rate': 7.741796019648237e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 747/2224 [1:06:01<2:07:26,  5.18s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 748/2224 [1:06:06<2:05:25,  5.10s/it]
0:                                                       
0: 
0: {'loss': 0.3418, 'grad_norm': 0.9477634069864989, 'learning_rate': 7.735703325733688e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 748/2224 [1:06:06<2:05:25,  5.10s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 749/2224 [1:06:12<2:07:06,  5.17s/it]
0:                                                       
0: 
0: {'loss': 0.3313, 'grad_norm': 0.904049389669683, 'learning_rate': 7.729604828607935e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 749/2224 [1:06:12<2:07:06,  5.17s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2224 [1:06:17<2:07:22,  5.19s/it]
0:                                                       
0: 
0: {'loss': 0.3194, 'grad_norm': 0.9228271673000548, 'learning_rate': 7.723500541207637e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2224 [1:06:17<2:07:22,  5.19s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 751/2224 [1:06:22<2:05:51,  5.13s/it]
0:                                                       
0: 
0: {'loss': 0.32, 'grad_norm': 0.9453441509336828, 'learning_rate': 7.717390476481741e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 751/2224 [1:06:22<2:05:51,  5.13s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 752/2224 [1:06:27<2:05:00,  5.10s/it]
0:                                                       
0: 
0: {'loss': 0.3424, 'grad_norm': 0.9374647470812618, 'learning_rate': 7.711274647391443e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 752/2224 [1:06:27<2:05:00,  5.10s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 753/2224 [1:06:32<2:04:30,  5.08s/it]
0:                                                       
0: 
0: {'loss': 0.3402, 'grad_norm': 0.9089435484917064, 'learning_rate': 7.705153066910172e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 753/2224 [1:06:32<2:04:30,  5.08s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 754/2224 [1:06:37<2:06:46,  5.17s/it]
0:                                                       
0: 
0: {'loss': 0.3176, 'grad_norm': 0.9299510924536996, 'learning_rate': 7.699025748023553e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 754/2224 [1:06:37<2:06:46,  5.17s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 755/2224 [1:06:42<2:05:41,  5.13s/it]
0:                                                       
0: 
0: {'loss': 0.3448, 'grad_norm': 0.9421062253741173, 'learning_rate': 7.69289270372939e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 755/2224 [1:06:42<2:05:41,  5.13s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 756/2224 [1:06:47<2:03:48,  5.06s/it]
0:                                                       
0: 
0: {'loss': 0.2884, 'grad_norm': 0.8668605020537431, 'learning_rate': 7.686753947037623e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 756/2224 [1:06:47<2:03:48,  5.06s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 757/2224 [1:06:53<2:07:40,  5.22s/it]
0:                                                       
0: 
0: {'loss': 0.3441, 'grad_norm': 0.9386822240994855, 'learning_rate': 7.680609490970324e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 757/2224 [1:06:53<2:07:40,  5.22s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 758/2224 [1:06:58<2:05:54,  5.15s/it]
0:                                                       
0: 
0: {'loss': 0.3627, 'grad_norm': 0.9572048865950742, 'learning_rate': 7.67445934856164e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 758/2224 [1:06:58<2:05:54,  5.15s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 759/2224 [1:07:03<2:04:48,  5.11s/it]
0:                                                       
0: 
0: {'loss': 0.3454, 'grad_norm': 0.9492056166949568, 'learning_rate': 7.668303532857786e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 759/2224 [1:07:03<2:04:48,  5.11s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 760/2224 [1:07:08<2:02:23,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3241, 'grad_norm': 0.9456766627944692, 'learning_rate': 7.662142056917016e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 760/2224 [1:07:08<2:02:23,  5.02s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 761/2224 [1:07:13<2:02:05,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3298, 'grad_norm': 1.3831767540154167, 'learning_rate': 7.655974933809585e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 761/2224 [1:07:13<2:02:05,  5.01s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 762/2224 [1:07:18<2:01:19,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3295, 'grad_norm': 0.9016556458219904, 'learning_rate': 7.64980217661773e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 762/2224 [1:07:18<2:01:19,  4.98s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 763/2224 [1:07:23<2:01:21,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3197, 'grad_norm': 0.8790082213444088, 'learning_rate': 7.643623798435637e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 763/2224 [1:07:23<2:01:21,  4.98s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 764/2224 [1:07:28<2:01:22,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3081, 'grad_norm': 0.8915161399435746, 'learning_rate': 7.63743981236942e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 764/2224 [1:07:28<2:01:22,  4.99s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 765/2224 [1:07:33<2:01:26,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3726, 'grad_norm': 0.9496179920613103, 'learning_rate': 7.631250231537089e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 765/2224 [1:07:33<2:01:26,  4.99s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 766/2224 [1:07:37<2:00:37,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3226, 'grad_norm': 0.9027877789534363, 'learning_rate': 7.6250550690685136e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 766/2224 [1:07:37<2:00:37,  4.96s/it]
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 767/2224 [1:07:42<2:00:50,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3445, 'grad_norm': 0.9203339923002165, 'learning_rate': 7.618854338105415e-06, 'epoch': 0.34}
0: 
0:  34%|â–ˆâ–ˆâ–ˆâ–      | 767/2224 [1:07:42<2:00:50,  4.98s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 768/2224 [1:07:47<2:01:03,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3378, 'grad_norm': 0.8916276712130634, 'learning_rate': 7.61264805180132e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 768/2224 [1:07:47<2:01:03,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 769/2224 [1:07:52<2:01:07,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.2974, 'grad_norm': 0.8703915091545907, 'learning_rate': 7.606436223321541e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 769/2224 [1:07:52<2:01:07,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 770/2224 [1:07:57<2:01:13,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3717, 'grad_norm': 0.9391425829998212, 'learning_rate': 7.600218865843147e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 770/2224 [1:07:57<2:01:13,  5.00s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 771/2224 [1:08:03<2:01:20,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3369, 'grad_norm': 0.8769631985684291, 'learning_rate': 7.5939959925549365e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 771/2224 [1:08:03<2:01:20,  5.01s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 772/2224 [1:08:08<2:01:18,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3253, 'grad_norm': 0.8564152105055657, 'learning_rate': 7.587767616657411e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 772/2224 [1:08:08<2:01:18,  5.01s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 773/2224 [1:08:13<2:01:16,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3176, 'grad_norm': 0.905735075442425, 'learning_rate': 7.581533751362737e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 773/2224 [1:08:13<2:01:16,  5.02s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 774/2224 [1:08:18<2:01:00,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3367, 'grad_norm': 0.9418697142464778, 'learning_rate': 7.575294409894733e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 774/2224 [1:08:18<2:01:00,  5.01s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 775/2224 [1:08:23<2:01:12,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3467, 'grad_norm': 0.8876189443622828, 'learning_rate': 7.569049605488832e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 775/2224 [1:08:23<2:01:12,  5.02s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 776/2224 [1:08:27<1:59:41,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.33, 'grad_norm': 0.8459514878506488, 'learning_rate': 7.5627993513920514e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 776/2224 [1:08:27<1:59:41,  4.96s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 777/2224 [1:08:32<1:59:58,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3277, 'grad_norm': 0.9311679667690401, 'learning_rate': 7.556543660862976e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 777/2224 [1:08:32<1:59:58,  4.97s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 778/2224 [1:08:37<1:59:20,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.3126, 'grad_norm': 0.9417506324296632, 'learning_rate': 7.550282547171719e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–      | 778/2224 [1:08:37<1:59:20,  4.95s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2224 [1:08:42<1:59:49,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.2945, 'grad_norm': 0.8836473019625795, 'learning_rate': 7.544016023599894e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 779/2224 [1:08:42<1:59:49,  4.98s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2224 [1:08:47<2:00:11,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3329, 'grad_norm': 0.9691417434855107, 'learning_rate': 7.5377441034406e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 780/2224 [1:08:47<2:00:11,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2224 [1:08:52<2:00:15,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3272, 'grad_norm': 0.9135083271122559, 'learning_rate': 7.531466799998373e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 781/2224 [1:08:52<2:00:15,  5.00s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2224 [1:08:57<1:59:28,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3673, 'grad_norm': 0.8957501435195405, 'learning_rate': 7.525184126589176e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 782/2224 [1:08:57<1:59:28,  4.97s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2224 [1:09:02<1:59:45,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3672, 'grad_norm': 0.9288307777732326, 'learning_rate': 7.5188960965403625e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 783/2224 [1:09:02<1:59:45,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2224 [1:09:07<1:59:48,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3478, 'grad_norm': 0.8564181393200019, 'learning_rate': 7.512602723190643e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 784/2224 [1:09:07<1:59:48,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2224 [1:09:12<1:59:54,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3299, 'grad_norm': 0.9310085771384241, 'learning_rate': 7.5063040198900714e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 785/2224 [1:09:12<1:59:54,  5.00s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2224 [1:09:17<2:00:04,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3314, 'grad_norm': 0.880673412701831, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 786/2224 [1:09:17<2:00:04,  5.01s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2224 [1:09:22<1:59:27,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3444, 'grad_norm': 0.9161060493284349, 'learning_rate': 7.493690676893066e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 787/2224 [1:09:22<1:59:27,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2224 [1:09:27<1:59:31,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.318, 'grad_norm': 0.9121782239127504, 'learning_rate': 7.48737606395315e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 788/2224 [1:09:27<1:59:31,  4.99s/it]
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2224 [1:09:32<1:59:20,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3107, 'grad_norm': 0.8965768964282376, 'learning_rate': 7.481056174575357e-06, 'epoch': 0.35}
0: 
0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 789/2224 [1:09:32<1:59:20,  4.99s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2224 [1:09:37<1:59:24,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3428, 'grad_norm': 0.9085582840732827, 'learning_rate': 7.474731022165986e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2224 [1:09:37<1:59:24,  5.00s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 791/2224 [1:09:42<1:58:57,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3565, 'grad_norm': 0.9279759772398929, 'learning_rate': 7.468400620142499e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 791/2224 [1:09:42<1:58:57,  4.98s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 792/2224 [1:09:47<1:58:57,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3299, 'grad_norm': 0.921036144805153, 'learning_rate': 7.462064981933492e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 792/2224 [1:09:47<1:58:57,  4.98s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 793/2224 [1:09:52<1:59:07,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3363, 'grad_norm': 0.9826482946464948, 'learning_rate': 7.455724120978674e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 793/2224 [1:09:52<1:59:07,  4.99s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 794/2224 [1:09:57<1:59:17,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3341, 'grad_norm': 0.9368062649680083, 'learning_rate': 7.449378050728826e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 794/2224 [1:09:57<1:59:17,  5.01s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 795/2224 [1:10:02<1:59:08,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3446, 'grad_norm': 0.9046032534706752, 'learning_rate': 7.443026784645782e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 795/2224 [1:10:02<1:59:08,  5.00s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 796/2224 [1:10:07<1:58:04,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3028, 'grad_norm': 0.8757467058646705, 'learning_rate': 7.436670336202401e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 796/2224 [1:10:07<1:58:04,  4.96s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 797/2224 [1:10:12<1:58:22,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3407, 'grad_norm': 0.9353519298538823, 'learning_rate': 7.430308718882531e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 797/2224 [1:10:12<1:58:22,  4.98s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 798/2224 [1:10:17<1:58:15,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3314, 'grad_norm': 0.9039311415111905, 'learning_rate': 7.4239419461809894e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 798/2224 [1:10:17<1:58:15,  4.98s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 799/2224 [1:10:22<1:58:38,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3507, 'grad_norm': 0.8826586778315006, 'learning_rate': 7.417570031603525e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 799/2224 [1:10:22<1:58:38,  5.00s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2224 [1:10:27<1:57:59,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3293, 'grad_norm': 0.9126391358826571, 'learning_rate': 7.4111929886667935e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2224 [1:10:27<1:57:59,  4.97s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 801/2224 [1:11:32<9:03:57, 22.94s/it]
0:                                                       
0: 
0: {'loss': 0.347, 'grad_norm': 0.8770722040966582, 'learning_rate': 7.4048108308983355e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 801/2224 [1:11:32<9:03:57, 22.94s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 802/2224 [1:11:37<6:56:28, 17.57s/it]
0:                                                       
0: 
0: {'loss': 0.3214, 'grad_norm': 0.856141575317327, 'learning_rate': 7.398423571836535e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 802/2224 [1:11:37<6:56:28, 17.57s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 803/2224 [1:11:42<5:26:40, 13.79s/it]
0:                                                       
0: 
0: {'loss': 0.358, 'grad_norm': 0.8577406578013003, 'learning_rate': 7.392031225030601e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 803/2224 [1:11:42<5:26:40, 13.79s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 804/2224 [1:11:47<4:23:28, 11.13s/it]
0:                                                       
0: 
0: {'loss': 0.2894, 'grad_norm': 0.8650875658793117, 'learning_rate': 7.385633804040534e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 804/2224 [1:11:47<4:23:28, 11.13s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 805/2224 [1:11:52<3:38:44,  9.25s/it]
0:                                                       
0: 
0: {'loss': 0.3445, 'grad_norm': 0.8829847375755339, 'learning_rate': 7.379231322437098e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 805/2224 [1:11:52<3:38:44,  9.25s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 806/2224 [1:11:57<3:08:38,  7.98s/it]
0:                                                       
0: 
0: {'loss': 0.3363, 'grad_norm': 0.8835268931270666, 'learning_rate': 7.372823793801793e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 806/2224 [1:11:57<3:08:38,  7.98s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2224 [1:12:02<2:47:06,  7.08s/it]
0:                                                       
0: 
0: {'loss': 0.3052, 'grad_norm': 0.8712492450341311, 'learning_rate': 7.366411231726825e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 807/2224 [1:12:02<2:47:06,  7.08s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2224 [1:12:07<2:32:22,  6.46s/it]
0:                                                       
0: 
0: {'loss': 0.3236, 'grad_norm': 0.9301712931366158, 'learning_rate': 7.3599936498150735e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 808/2224 [1:12:07<2:32:22,  6.46s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2224 [1:12:12<2:22:19,  6.03s/it]
0:                                                       
0: 
0: {'loss': 0.3339, 'grad_norm': 0.9056124183106719, 'learning_rate': 7.3535710616800744e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 809/2224 [1:12:12<2:22:19,  6.03s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2224 [1:12:17<2:14:56,  5.73s/it]
0:                                                       
0: 
0: {'loss': 0.3521, 'grad_norm': 0.87881561761136, 'learning_rate': 7.347143480945977e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 810/2224 [1:12:17<2:14:56,  5.73s/it]
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2224 [1:12:22<2:10:07,  5.53s/it]
0:                                                       
0: 
0: {'loss': 0.3369, 'grad_norm': 0.9029029440565502, 'learning_rate': 7.3407109212475225e-06, 'epoch': 0.36}
0: 
0:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 811/2224 [1:12:22<2:10:07,  5.53s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2224 [1:12:27<2:05:44,  5.34s/it]
0:                                                       
0: 
0: {'loss': 0.3074, 'grad_norm': 0.8751484932272057, 'learning_rate': 7.334273396230016e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 812/2224 [1:12:27<2:05:44,  5.34s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2224 [1:12:32<2:02:41,  5.22s/it]
0:                                                       
0: 
0: {'loss': 0.3391, 'grad_norm': 0.8704905195856345, 'learning_rate': 7.327830919549291e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 813/2224 [1:12:32<2:02:41,  5.22s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2224 [1:12:37<2:00:54,  5.15s/it]
0:                                                       
0: 
0: {'loss': 0.3457, 'grad_norm': 0.8956211217975278, 'learning_rate': 7.321383504871692e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 814/2224 [1:12:37<2:00:54,  5.15s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2224 [1:12:42<1:59:46,  5.10s/it]
0:                                                       
0: 
0: {'loss': 0.3149, 'grad_norm': 0.8936283443157432, 'learning_rate': 7.314931165874031e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 815/2224 [1:12:42<1:59:46,  5.10s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2224 [1:12:47<1:58:51,  5.06s/it]
0:                                                       
0: 
0: {'loss': 0.3428, 'grad_norm': 0.906901129514872, 'learning_rate': 7.308473916243572e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 816/2224 [1:12:47<1:58:51,  5.06s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2224 [1:12:52<1:57:23,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3357, 'grad_norm': 0.8836797454277426, 'learning_rate': 7.30201176967799e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 817/2224 [1:12:52<1:57:23,  5.01s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 818/2224 [1:12:57<1:57:04,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3362, 'grad_norm': 0.916373590692198, 'learning_rate': 7.295544739885353e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 818/2224 [1:12:57<1:57:04,  5.00s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 819/2224 [1:13:02<1:57:14,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3774, 'grad_norm': 0.9096328159324663, 'learning_rate': 7.289072840584085e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 819/2224 [1:13:02<1:57:14,  5.01s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2224 [1:13:06<1:56:17,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3153, 'grad_norm': 0.8856701290968708, 'learning_rate': 7.282596085502941e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2224 [1:13:06<1:56:17,  4.97s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 821/2224 [1:13:11<1:56:42,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3221, 'grad_norm': 0.8440678137769925, 'learning_rate': 7.276114488380973e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 821/2224 [1:13:11<1:56:42,  4.99s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 822/2224 [1:13:16<1:56:25,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3269, 'grad_norm': 0.8701341540164751, 'learning_rate': 7.269628062967511e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 822/2224 [1:13:16<1:56:25,  4.98s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 823/2224 [1:13:21<1:56:37,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3205, 'grad_norm': 0.925331851573425, 'learning_rate': 7.26313682302212e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 823/2224 [1:13:21<1:56:37,  4.99s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 824/2224 [1:13:26<1:56:31,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3403, 'grad_norm': 0.9357579397428428, 'learning_rate': 7.256640782314581e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 824/2224 [1:13:26<1:56:31,  4.99s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 825/2224 [1:13:31<1:56:09,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3344, 'grad_norm': 1.0364881914844994, 'learning_rate': 7.250139954624864e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 825/2224 [1:13:31<1:56:09,  4.98s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 826/2224 [1:13:36<1:55:37,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3296, 'grad_norm': 0.9076102566137796, 'learning_rate': 7.243634353743083e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 826/2224 [1:13:36<1:55:37,  4.96s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 827/2224 [1:13:41<1:56:00,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3316, 'grad_norm': 0.9535643095198149, 'learning_rate': 7.237123993469486e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 827/2224 [1:13:41<1:56:00,  4.98s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 828/2224 [1:13:46<1:56:07,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3407, 'grad_norm': 0.9372097848785137, 'learning_rate': 7.230608887614411e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 828/2224 [1:13:46<1:56:07,  4.99s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 829/2224 [1:13:51<1:56:32,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3366, 'grad_norm': 0.9079560406461583, 'learning_rate': 7.224089049998269e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 829/2224 [1:13:51<1:56:32,  5.01s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2224 [1:13:56<1:56:31,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3123, 'grad_norm': 0.9075314378360341, 'learning_rate': 7.217564494451505e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2224 [1:13:56<1:56:31,  5.02s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 831/2224 [1:14:01<1:56:22,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3131, 'grad_norm': 0.8906537054534235, 'learning_rate': 7.211035234814569e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 831/2224 [1:14:01<1:56:22,  5.01s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 832/2224 [1:14:06<1:56:12,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3138, 'grad_norm': 0.8938176177464564, 'learning_rate': 7.204501284937897e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 832/2224 [1:14:06<1:56:12,  5.01s/it]
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 833/2224 [1:14:12<1:56:35,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3241, 'grad_norm': 0.8761375113876729, 'learning_rate': 7.197962658681866e-06, 'epoch': 0.37}
0: 
0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 833/2224 [1:14:12<1:56:35,  5.03s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2224 [1:14:17<1:56:13,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3325, 'grad_norm': 0.9689983517166852, 'learning_rate': 7.191419369916781e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 834/2224 [1:14:17<1:56:13,  5.02s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2224 [1:14:22<1:56:14,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3481, 'grad_norm': 0.9361599389656083, 'learning_rate': 7.184871432522833e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 835/2224 [1:14:22<1:56:14,  5.02s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2224 [1:14:26<1:55:12,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.2914, 'grad_norm': 0.8718152568026417, 'learning_rate': 7.178318860390072e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 836/2224 [1:14:26<1:55:12,  4.98s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2224 [1:14:31<1:55:42,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.2993, 'grad_norm': 0.9185729770027463, 'learning_rate': 7.171761667418386e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 837/2224 [1:14:32<1:55:42,  5.01s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2224 [1:14:36<1:54:38,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3463, 'grad_norm': 0.9420407287305073, 'learning_rate': 7.1651998675174584e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 838/2224 [1:14:36<1:54:38,  4.96s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2224 [1:14:41<1:55:00,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3613, 'grad_norm': 0.8976826495725133, 'learning_rate': 7.15863347460675e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2224 [1:14:41<1:55:00,  4.98s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2224 [1:14:46<1:55:08,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3426, 'grad_norm': 0.9523842038351503, 'learning_rate': 7.1520625026154645e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 840/2224 [1:14:46<1:55:08,  4.99s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2224 [1:14:51<1:55:35,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3798, 'grad_norm': 0.9584303294266567, 'learning_rate': 7.1454869654825145e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 841/2224 [1:14:51<1:55:35,  5.02s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2224 [1:14:57<1:55:47,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3357, 'grad_norm': 0.8908908598771696, 'learning_rate': 7.138906877156503e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 842/2224 [1:14:57<1:55:47,  5.03s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2224 [1:15:01<1:54:51,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3644, 'grad_norm': 0.9694875017111684, 'learning_rate': 7.1323222515956825e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 843/2224 [1:15:01<1:54:51,  4.99s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2224 [1:15:06<1:55:01,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3529, 'grad_norm': 0.9131109709914876, 'learning_rate': 7.12573310276793e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 844/2224 [1:15:06<1:55:01,  5.00s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 845/2224 [1:15:11<1:54:52,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.32, 'grad_norm': 0.942296742339114, 'learning_rate': 7.1191394446507226e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 845/2224 [1:15:11<1:54:52,  5.00s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 846/2224 [1:15:16<1:54:17,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3508, 'grad_norm': 0.9229950676285253, 'learning_rate': 7.112541291231099e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 846/2224 [1:15:16<1:54:17,  4.98s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 847/2224 [1:15:21<1:54:00,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3217, 'grad_norm': 0.8673618375138743, 'learning_rate': 7.105938656505634e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 847/2224 [1:15:21<1:54:00,  4.97s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 848/2224 [1:15:26<1:53:21,  4.94s/it]
0:                                                       
0: 
0: {'loss': 0.3572, 'grad_norm': 0.9648866931357235, 'learning_rate': 7.099331554480411e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 848/2224 [1:15:26<1:53:21,  4.94s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 849/2224 [1:15:31<1:54:03,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3353, 'grad_norm': 0.9626310776513289, 'learning_rate': 7.092719999170985e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 849/2224 [1:15:31<1:54:03,  4.98s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2224 [1:15:36<1:54:26,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3332, 'grad_norm': 0.915265492723519, 'learning_rate': 7.086104004602364e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2224 [1:15:36<1:54:26,  5.00s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 851/2224 [1:15:41<1:54:45,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3267, 'grad_norm': 0.86648658565832, 'learning_rate': 7.079483584808965e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 851/2224 [1:15:41<1:54:45,  5.01s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 852/2224 [1:15:46<1:55:11,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.35, 'grad_norm': 0.922869352241587, 'learning_rate': 7.0728587538346005e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 852/2224 [1:15:46<1:55:11,  5.04s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 853/2224 [1:15:51<1:54:46,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3446, 'grad_norm': 0.9662065988289953, 'learning_rate': 7.066229525732437e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 853/2224 [1:15:51<1:54:46,  5.02s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 854/2224 [1:15:56<1:54:23,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3319, 'grad_norm': 0.9153200634208448, 'learning_rate': 7.059595914564965e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 854/2224 [1:15:56<1:54:23,  5.01s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 855/2224 [1:16:01<1:54:23,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3263, 'grad_norm': 0.8867484117409985, 'learning_rate': 7.052957934403979e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 855/2224 [1:16:01<1:54:23,  5.01s/it]
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 856/2224 [1:16:06<1:54:11,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.2875, 'grad_norm': 0.8790748574489882, 'learning_rate': 7.046315599330538e-06, 'epoch': 0.38}
0: 
0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 856/2224 [1:16:06<1:54:11,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 857/2224 [1:16:11<1:53:55,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3392, 'grad_norm': 0.9538500190271056, 'learning_rate': 7.039668923434937e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 857/2224 [1:16:11<1:53:55,  5.00s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 858/2224 [1:16:16<1:53:57,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3183, 'grad_norm': 0.9165650213806679, 'learning_rate': 7.033017920816684e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 858/2224 [1:16:16<1:53:57,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 859/2224 [1:16:21<1:53:52,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3319, 'grad_norm': 0.8926979256097486, 'learning_rate': 7.02636260558446e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 859/2224 [1:16:21<1:53:52,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2224 [1:16:26<1:53:44,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3202, 'grad_norm': 0.91117616383553, 'learning_rate': 7.019702991856099e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2224 [1:16:26<1:53:44,  5.00s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 861/2224 [1:16:31<1:52:53,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.32, 'grad_norm': 0.9389832751465492, 'learning_rate': 7.013039093758553e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 861/2224 [1:16:31<1:52:53,  4.97s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2224 [1:16:36<1:53:18,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.327, 'grad_norm': 0.8982282176399147, 'learning_rate': 7.0063709254278565e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 862/2224 [1:16:36<1:53:18,  4.99s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2224 [1:16:41<1:53:33,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3297, 'grad_norm': 0.9377762980034923, 'learning_rate': 6.99969850100911e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 863/2224 [1:16:41<1:53:33,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2224 [1:16:46<1:53:29,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3045, 'grad_norm': 0.8967787234355543, 'learning_rate': 6.993021834656437e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 864/2224 [1:16:46<1:53:29,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2224 [1:16:51<1:53:38,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3434, 'grad_norm': 0.9440368046502146, 'learning_rate': 6.986340940532963e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 865/2224 [1:16:51<1:53:38,  5.02s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2224 [1:16:56<1:53:31,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3212, 'grad_norm': 0.908894581272112, 'learning_rate': 6.9796558328107775e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 866/2224 [1:16:56<1:53:31,  5.02s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2224 [1:17:01<1:53:14,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3308, 'grad_norm': 0.945089374518896, 'learning_rate': 6.972966525670915e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 867/2224 [1:17:01<1:53:14,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2224 [1:17:07<1:53:37,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3184, 'grad_norm': 0.8721073893323652, 'learning_rate': 6.96627303330331e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 868/2224 [1:17:07<1:53:37,  5.03s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2224 [1:17:11<1:52:45,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.32, 'grad_norm': 0.8359003388796838, 'learning_rate': 6.959575369906783e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 869/2224 [1:17:11<1:52:45,  4.99s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2224 [1:17:16<1:52:50,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3345, 'grad_norm': 0.900822997708135, 'learning_rate': 6.952873549688994e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 870/2224 [1:17:16<1:52:50,  5.00s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2224 [1:17:21<1:52:09,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.2968, 'grad_norm': 0.900846836562053, 'learning_rate': 6.9461675868664305e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 871/2224 [1:17:21<1:52:09,  4.97s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 872/2224 [1:17:26<1:52:22,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.31, 'grad_norm': 0.8660864323090038, 'learning_rate': 6.939457495664357e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 872/2224 [1:17:26<1:52:22,  4.99s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 873/2224 [1:17:31<1:52:23,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3259, 'grad_norm': 0.9356601362031453, 'learning_rate': 6.9327432903168046e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 873/2224 [1:17:31<1:52:23,  4.99s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 874/2224 [1:17:36<1:52:36,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3323, 'grad_norm': 0.8965199869533047, 'learning_rate': 6.92602498506653e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 874/2224 [1:17:36<1:52:36,  5.01s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 875/2224 [1:17:41<1:52:24,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3149, 'grad_norm': 0.9141290379094195, 'learning_rate': 6.919302594164979e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 875/2224 [1:17:41<1:52:24,  5.00s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 876/2224 [1:17:46<1:51:16,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.3249, 'grad_norm': 0.8753313869357987, 'learning_rate': 6.9125761318722775e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 876/2224 [1:17:46<1:51:16,  4.95s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 877/2224 [1:17:51<1:51:34,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3478, 'grad_norm': 0.9040509838591773, 'learning_rate': 6.905845612457174e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 877/2224 [1:17:51<1:51:34,  4.97s/it]
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 878/2224 [1:17:56<1:52:03,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3185, 'grad_norm': 0.8785431232471519, 'learning_rate': 6.899111050197036e-06, 'epoch': 0.39}
0: 
0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 878/2224 [1:17:56<1:52:03,  5.00s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 879/2224 [1:18:01<1:52:07,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3077, 'grad_norm': 0.886698778702311, 'learning_rate': 6.892372459377797e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 879/2224 [1:18:01<1:52:07,  5.00s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2224 [1:18:06<1:52:12,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3063, 'grad_norm': 0.9016838615379024, 'learning_rate': 6.885629854293941e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2224 [1:18:06<1:52:12,  5.01s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 881/2224 [1:18:11<1:51:56,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3213, 'grad_norm': 0.8768697943808413, 'learning_rate': 6.8788832492484695e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 881/2224 [1:18:11<1:51:56,  5.00s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 882/2224 [1:18:16<1:51:44,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3535, 'grad_norm': 0.9241945324055566, 'learning_rate': 6.872132658552864e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 882/2224 [1:18:16<1:51:44,  5.00s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 883/2224 [1:18:21<1:51:52,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3405, 'grad_norm': 0.9415134617123542, 'learning_rate': 6.865378096527063e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 883/2224 [1:18:21<1:51:52,  5.01s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 884/2224 [1:18:26<1:52:11,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.2986, 'grad_norm': 0.857792197638323, 'learning_rate': 6.858619577499431e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 884/2224 [1:18:26<1:52:11,  5.02s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 885/2224 [1:18:31<1:51:59,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3182, 'grad_norm': 0.8577498208570371, 'learning_rate': 6.851857115806723e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 885/2224 [1:18:31<1:51:59,  5.02s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 886/2224 [1:18:36<1:51:58,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3306, 'grad_norm': 0.8682508782925714, 'learning_rate': 6.845090725794061e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 886/2224 [1:18:36<1:51:58,  5.02s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 887/2224 [1:18:41<1:51:49,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3508, 'grad_norm': 0.9621598559780927, 'learning_rate': 6.838320421814897e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 887/2224 [1:18:41<1:51:49,  5.02s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 888/2224 [1:18:46<1:51:38,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.374, 'grad_norm': 0.9161113484955477, 'learning_rate': 6.831546218230987e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 888/2224 [1:18:46<1:51:38,  5.01s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 889/2224 [1:18:51<1:50:53,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3087, 'grad_norm': 0.880336672495005, 'learning_rate': 6.824768129412361e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 889/2224 [1:18:51<1:50:53,  4.98s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2224 [1:18:56<1:50:05,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.3387, 'grad_norm': 0.9267838840375211, 'learning_rate': 6.817986169737287e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 890/2224 [1:18:56<1:50:05,  4.95s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2224 [1:19:01<1:50:21,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3158, 'grad_norm': 0.8708756073308447, 'learning_rate': 6.811200353592249e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 891/2224 [1:19:01<1:50:21,  4.97s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2224 [1:19:06<1:50:37,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3312, 'grad_norm': 0.8915910715388194, 'learning_rate': 6.804410695371906e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 892/2224 [1:19:06<1:50:37,  4.98s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2224 [1:19:11<1:50:50,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3437, 'grad_norm': 0.917024633550839, 'learning_rate': 6.797617209479071e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 893/2224 [1:19:11<1:50:50,  5.00s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2224 [1:19:16<1:50:05,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3368, 'grad_norm': 0.8839719925748301, 'learning_rate': 6.790819910324675e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 894/2224 [1:19:16<1:50:05,  4.97s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2224 [1:19:21<1:50:10,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.317, 'grad_norm': 0.9039016281293235, 'learning_rate': 6.784018812327738e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 895/2224 [1:19:21<1:50:10,  4.97s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2224 [1:19:26<1:50:15,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.339, 'grad_norm': 0.8780583538173866, 'learning_rate': 6.777213929915343e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 896/2224 [1:19:26<1:50:15,  4.98s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2224 [1:19:31<1:50:08,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3331, 'grad_norm': 0.90045620230861, 'learning_rate': 6.770405277522594e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 897/2224 [1:19:31<1:50:08,  4.98s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2224 [1:19:36<1:50:37,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3424, 'grad_norm': 0.9009670216061452, 'learning_rate': 6.763592869592594e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 898/2224 [1:19:36<1:50:37,  5.01s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2224 [1:19:41<1:50:51,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3068, 'grad_norm': 0.8922811121749242, 'learning_rate': 6.756776720576415e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 899/2224 [1:19:41<1:50:51,  5.02s/it]
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 900/2224 [1:19:46<1:50:33,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3148, 'grad_norm': 0.8791736476962188, 'learning_rate': 6.749956844933063e-06, 'epoch': 0.4}
0: 
0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 900/2224 [1:19:46<1:50:33,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 901/2224 [1:19:51<1:50:24,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3097, 'grad_norm': 0.9043991414746492, 'learning_rate': 6.743133257129449e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 901/2224 [1:19:51<1:50:24,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 902/2224 [1:19:56<1:50:25,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3016, 'grad_norm': 0.8823135812685329, 'learning_rate': 6.736305971640364e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 902/2224 [1:19:56<1:50:25,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 903/2224 [1:20:01<1:50:21,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3275, 'grad_norm': 0.9128598259320339, 'learning_rate': 6.7294750029484315e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 903/2224 [1:20:01<1:50:21,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 904/2224 [1:20:06<1:50:29,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3449, 'grad_norm': 0.956990557758598, 'learning_rate': 6.7226403655441e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 904/2224 [1:20:06<1:50:29,  5.02s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 905/2224 [1:20:11<1:50:42,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.3245, 'grad_norm': 0.8972075656214181, 'learning_rate': 6.715802073925592e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 905/2224 [1:20:11<1:50:42,  5.04s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 906/2224 [1:20:16<1:50:29,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3249, 'grad_norm': 0.9060270288629423, 'learning_rate': 6.708960142598888e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 906/2224 [1:20:16<1:50:29,  5.03s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 907/2224 [1:20:21<1:50:23,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3656, 'grad_norm': 0.9162282308577759, 'learning_rate': 6.702114586077683e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 907/2224 [1:20:22<1:50:23,  5.03s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 908/2224 [1:20:27<1:50:27,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.3113, 'grad_norm': 0.9267278815501936, 'learning_rate': 6.695265418883366e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 908/2224 [1:20:27<1:50:27,  5.04s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 909/2224 [1:20:32<1:50:06,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3149, 'grad_norm': 0.9044993537875329, 'learning_rate': 6.688412655544987e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 909/2224 [1:20:32<1:50:06,  5.02s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2224 [1:20:36<1:49:24,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3293, 'grad_norm': 0.8690668095328604, 'learning_rate': 6.6815563105992206e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2224 [1:20:36<1:49:24,  5.00s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 911/2224 [1:20:41<1:48:37,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.3355, 'grad_norm': 0.9173617921145997, 'learning_rate': 6.674696398590339e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 911/2224 [1:20:41<1:48:37,  4.96s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 912/2224 [1:20:46<1:48:48,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3146, 'grad_norm': 0.9229400697132424, 'learning_rate': 6.667832934070185e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 912/2224 [1:20:46<1:48:48,  4.98s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 913/2224 [1:20:51<1:48:47,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3192, 'grad_norm': 0.8377218152947663, 'learning_rate': 6.6609659315981345e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 913/2224 [1:20:51<1:48:47,  4.98s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 914/2224 [1:20:56<1:48:59,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3292, 'grad_norm': 0.8890659474791746, 'learning_rate': 6.654095405741067e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 914/2224 [1:20:56<1:48:59,  4.99s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 915/2224 [1:21:01<1:48:44,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3167, 'grad_norm': 0.8859008382198216, 'learning_rate': 6.647221371073338e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 915/2224 [1:21:01<1:48:44,  4.98s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 916/2224 [1:21:06<1:48:49,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3247, 'grad_norm': 0.8637809350912703, 'learning_rate': 6.640343842176747e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 916/2224 [1:21:06<1:48:49,  4.99s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 917/2224 [1:21:11<1:49:11,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3105, 'grad_norm': 0.8765717142501359, 'learning_rate': 6.633462833640507e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 917/2224 [1:21:11<1:49:11,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2224 [1:21:16<1:49:16,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3179, 'grad_norm': 0.8813541453120962, 'learning_rate': 6.626578360061209e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 918/2224 [1:21:16<1:49:16,  5.02s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2224 [1:21:21<1:49:12,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3123, 'grad_norm': 0.898619159978904, 'learning_rate': 6.619690436042793e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 919/2224 [1:21:21<1:49:12,  5.02s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2224 [1:21:26<1:48:54,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3239, 'grad_norm': 0.933063338225766, 'learning_rate': 6.6127990761965236e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 920/2224 [1:21:26<1:48:54,  5.01s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2224 [1:21:31<1:48:12,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3233, 'grad_norm': 0.9200282033632456, 'learning_rate': 6.605904295140949e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 921/2224 [1:21:31<1:48:12,  4.98s/it]
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2224 [1:21:36<1:48:37,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.2996, 'grad_norm': 0.8799559945700131, 'learning_rate': 6.59900610750188e-06, 'epoch': 0.41}
0: 
0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 922/2224 [1:21:36<1:48:37,  5.01s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2224 [1:21:41<1:48:51,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3837, 'grad_norm': 0.9683054230084299, 'learning_rate': 6.592104527912348e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 923/2224 [1:21:41<1:48:51,  5.02s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2224 [1:21:47<1:48:43,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3652, 'grad_norm': 0.9250453509756595, 'learning_rate': 6.585199571012581e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 924/2224 [1:21:47<1:48:43,  5.02s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2224 [1:21:51<1:47:52,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3312, 'grad_norm': 0.8907707414215299, 'learning_rate': 6.578291251449978e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2224 [1:21:51<1:47:52,  4.98s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2224 [1:21:56<1:47:34,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3337, 'grad_norm': 0.9240473369541882, 'learning_rate': 6.5713795838790606e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 926/2224 [1:21:56<1:47:34,  4.97s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 927/2224 [1:22:01<1:47:38,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.2986, 'grad_norm': 0.8858656806686694, 'learning_rate': 6.56446458296146e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 927/2224 [1:22:01<1:47:38,  4.98s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 928/2224 [1:22:06<1:48:29,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3197, 'grad_norm': 0.8739200743132084, 'learning_rate': 6.557546263365875e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 928/2224 [1:22:06<1:48:29,  5.02s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 929/2224 [1:22:12<1:48:27,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3177, 'grad_norm': 0.8870849517121037, 'learning_rate': 6.550624639768044e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 929/2224 [1:22:12<1:48:27,  5.03s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 930/2224 [1:22:17<1:48:33,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3218, 'grad_norm': 0.8980244842419145, 'learning_rate': 6.54369972685072e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 930/2224 [1:22:17<1:48:33,  5.03s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 931/2224 [1:22:22<1:49:22,  5.08s/it]
0:                                                       
0: 
0: {'loss': 0.3192, 'grad_norm': 0.9218110108115728, 'learning_rate': 6.536771539303625e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 931/2224 [1:22:22<1:49:22,  5.08s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 932/2224 [1:22:27<1:48:43,  5.05s/it]
0:                                                       
0: 
0: {'loss': 0.3162, 'grad_norm': 0.8831646498364429, 'learning_rate': 6.529840091823432e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 932/2224 [1:22:27<1:48:43,  5.05s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 933/2224 [1:22:32<1:48:33,  5.05s/it]
0:                                                       
0: 
0: {'loss': 0.3389, 'grad_norm': 0.8990018788559639, 'learning_rate': 6.522905399113729e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 933/2224 [1:22:32<1:48:33,  5.05s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 934/2224 [1:22:37<1:48:27,  5.04s/it]
0:                                                       
0: 
0: {'loss': 0.3252, 'grad_norm': 0.8906134193455437, 'learning_rate': 6.5159674758849875e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 934/2224 [1:22:37<1:48:27,  5.04s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 935/2224 [1:22:42<1:48:04,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3258, 'grad_norm': 0.8772732013611966, 'learning_rate': 6.509026336854531e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 935/2224 [1:22:42<1:48:04,  5.03s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 936/2224 [1:22:47<1:47:35,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3236, 'grad_norm': 0.8976826749813461, 'learning_rate': 6.50208199674651e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 936/2224 [1:22:47<1:47:35,  5.01s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 937/2224 [1:22:52<1:47:13,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3323, 'grad_norm': 0.9096109878410201, 'learning_rate': 6.495134470291858e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 937/2224 [1:22:52<1:47:13,  5.00s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 938/2224 [1:22:57<1:46:55,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3326, 'grad_norm': 0.9054948601486371, 'learning_rate': 6.4881837722282694e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 938/2224 [1:22:57<1:46:55,  4.99s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 939/2224 [1:23:02<1:47:00,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3453, 'grad_norm': 0.8962033012767099, 'learning_rate': 6.4812299173001706e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 939/2224 [1:23:02<1:47:00,  5.00s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2224 [1:23:07<1:46:56,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3481, 'grad_norm': 0.8818432403828961, 'learning_rate': 6.4742729202586805e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2224 [1:23:07<1:46:56,  5.00s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 941/2224 [1:23:12<1:46:50,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3285, 'grad_norm': 0.9313469525718144, 'learning_rate': 6.467312795861586e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 941/2224 [1:23:12<1:46:50,  5.00s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 942/2224 [1:23:17<1:46:45,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3121, 'grad_norm': 0.8802109498477612, 'learning_rate': 6.460349558873302e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 942/2224 [1:23:17<1:46:45,  5.00s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 943/2224 [1:23:22<1:46:08,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3177, 'grad_norm': 0.9152246142087339, 'learning_rate': 6.453383224064858e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 943/2224 [1:23:22<1:46:08,  4.97s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 944/2224 [1:23:27<1:46:09,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3051, 'grad_norm': 0.8832797935400856, 'learning_rate': 6.446413806213845e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 944/2224 [1:23:27<1:46:09,  4.98s/it]
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 945/2224 [1:23:32<1:46:24,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3396, 'grad_norm': 0.9019523380099481, 'learning_rate': 6.439441320104394e-06, 'epoch': 0.42}
0: 
0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 945/2224 [1:23:32<1:46:24,  4.99s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2224 [1:23:37<1:46:21,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3133, 'grad_norm': 0.9320901614581119, 'learning_rate': 6.432465780527151e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 946/2224 [1:23:37<1:46:21,  4.99s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2224 [1:23:42<1:46:08,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3264, 'grad_norm': 0.95019742946777, 'learning_rate': 6.425487202279232e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 947/2224 [1:23:42<1:46:08,  4.99s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2224 [1:23:47<1:46:44,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3248, 'grad_norm': 0.8700199028272161, 'learning_rate': 6.418505600164205e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 948/2224 [1:23:47<1:46:44,  5.02s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2224 [1:23:52<1:45:46,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3111, 'grad_norm': 0.8820789026239204, 'learning_rate': 6.411520988992051e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 949/2224 [1:23:52<1:45:46,  4.98s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2224 [1:23:57<1:45:46,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3183, 'grad_norm': 0.8800473462454605, 'learning_rate': 6.404533383579129e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 950/2224 [1:23:57<1:45:46,  4.98s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2224 [1:24:02<1:45:49,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3011, 'grad_norm': 0.9018087145995246, 'learning_rate': 6.3975427987481586e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 951/2224 [1:24:02<1:45:49,  4.99s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2224 [1:24:07<1:46:06,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3401, 'grad_norm': 0.9122101728237989, 'learning_rate': 6.390549249328171e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 952/2224 [1:24:07<1:46:06,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2224 [1:24:12<1:46:34,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3355, 'grad_norm': 0.912629510655761, 'learning_rate': 6.383552750154494e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 953/2224 [1:24:12<1:46:34,  5.03s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 954/2224 [1:24:17<1:46:07,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3054, 'grad_norm': 0.9527254720127021, 'learning_rate': 6.376553316068705e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 954/2224 [1:24:17<1:46:07,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 955/2224 [1:24:22<1:46:01,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3223, 'grad_norm': 0.8909325438603627, 'learning_rate': 6.369550961918612e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 955/2224 [1:24:22<1:46:01,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 956/2224 [1:24:27<1:46:08,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3257, 'grad_norm': 0.8845386543686699, 'learning_rate': 6.362545702558214e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 956/2224 [1:24:27<1:46:08,  5.02s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 957/2224 [1:24:32<1:45:58,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3126, 'grad_norm': 0.9184444098921234, 'learning_rate': 6.355537552847677e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 957/2224 [1:24:32<1:45:58,  5.02s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 958/2224 [1:24:37<1:45:38,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3273, 'grad_norm': 0.9021111172226135, 'learning_rate': 6.348526527653298e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 958/2224 [1:24:37<1:45:38,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 959/2224 [1:24:42<1:45:33,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3289, 'grad_norm': 0.9134142730251203, 'learning_rate': 6.341512641847468e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 959/2224 [1:24:42<1:45:33,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2224 [1:24:47<1:45:23,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3369, 'grad_norm': 0.8884978862810103, 'learning_rate': 6.334495910308651e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2224 [1:24:47<1:45:23,  5.00s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 961/2224 [1:24:52<1:45:14,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.353, 'grad_norm': 0.8980322171212439, 'learning_rate': 6.327476347921349e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 961/2224 [1:24:52<1:45:14,  5.00s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 962/2224 [1:24:57<1:45:04,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3344, 'grad_norm': 0.9300103213249511, 'learning_rate': 6.320453969576064e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 962/2224 [1:24:57<1:45:04,  5.00s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 963/2224 [1:25:02<1:45:20,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3588, 'grad_norm': 0.9008619434558264, 'learning_rate': 6.313428790169274e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 963/2224 [1:25:02<1:45:20,  5.01s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 964/2224 [1:25:07<1:45:25,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.331, 'grad_norm': 0.8899437507684198, 'learning_rate': 6.3064008246034e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 964/2224 [1:25:07<1:45:25,  5.02s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 965/2224 [1:25:12<1:44:33,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3035, 'grad_norm': 0.8747154302455973, 'learning_rate': 6.299370087786774e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 965/2224 [1:25:12<1:44:33,  4.98s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 966/2224 [1:25:17<1:44:00,  4.96s/it]
0:                                                       
0: 
0: {'loss': 0.2976, 'grad_norm': 0.8966713997070274, 'learning_rate': 6.292336594633601e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 966/2224 [1:25:17<1:44:00,  4.96s/it]
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 967/2224 [1:25:22<1:44:22,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3258, 'grad_norm': 0.8923363030758006, 'learning_rate': 6.28530036006394e-06, 'epoch': 0.43}
0: 
0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 967/2224 [1:25:22<1:44:22,  4.98s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 968/2224 [1:25:27<1:44:38,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3151, 'grad_norm': 0.8745740551758271, 'learning_rate': 6.27826139900366e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 968/2224 [1:25:27<1:44:38,  5.00s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 969/2224 [1:25:32<1:43:51,  4.97s/it]
0:                                                       
0: 
0: {'loss': 0.3006, 'grad_norm': 0.8645153009304332, 'learning_rate': 6.2712197263844166e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 969/2224 [1:25:32<1:43:51,  4.97s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2224 [1:25:36<1:43:17,  4.94s/it]
0:                                                       
0: 
0: {'loss': 0.3169, 'grad_norm': 0.9183820610777054, 'learning_rate': 6.264175357143616e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2224 [1:25:36<1:43:17,  4.94s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 971/2224 [1:25:41<1:43:23,  4.95s/it]
0:                                                       
0: 
0: {'loss': 0.2998, 'grad_norm': 1.5832045033280278, 'learning_rate': 6.257128306224388e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 971/2224 [1:25:41<1:43:23,  4.95s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 972/2224 [1:25:46<1:43:51,  4.98s/it]
0:                                                       
0: 
0: {'loss': 0.3122, 'grad_norm': 0.8799912290726578, 'learning_rate': 6.250078588575546e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 972/2224 [1:25:46<1:43:51,  4.98s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2224 [1:25:51<1:44:07,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3105, 'grad_norm': 0.8912391195078941, 'learning_rate': 6.243026219151562e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 973/2224 [1:25:51<1:44:07,  4.99s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2224 [1:25:57<1:44:15,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3234, 'grad_norm': 0.9090694758989039, 'learning_rate': 6.235971212912535e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 974/2224 [1:25:57<1:44:15,  5.00s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2224 [1:26:02<1:44:28,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3247, 'grad_norm': 0.9218024130095902, 'learning_rate': 6.228913584824157e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 975/2224 [1:26:02<1:44:28,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2224 [1:26:07<1:44:25,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.2856, 'grad_norm': 0.820768501323515, 'learning_rate': 6.221853349857677e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 976/2224 [1:26:07<1:44:25,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2224 [1:26:12<1:44:17,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3012, 'grad_norm': 0.8878334867311131, 'learning_rate': 6.214790522989882e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 977/2224 [1:26:12<1:44:17,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2224 [1:26:17<1:44:10,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3464, 'grad_norm': 0.9390229523055603, 'learning_rate': 6.207725119203052e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 978/2224 [1:26:17<1:44:10,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2224 [1:26:22<1:44:06,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3334, 'grad_norm': 0.9529545954541174, 'learning_rate': 6.2006571534849345e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 979/2224 [1:26:22<1:44:06,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2224 [1:26:27<1:44:14,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3055, 'grad_norm': 0.8603157570576063, 'learning_rate': 6.193586640828712e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 980/2224 [1:26:27<1:44:14,  5.03s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 981/2224 [1:26:32<1:44:03,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.324, 'grad_norm': 0.9102790622906962, 'learning_rate': 6.186513596232966e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 981/2224 [1:26:32<1:44:03,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 982/2224 [1:26:37<1:43:41,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.2946, 'grad_norm': 0.9068240472975762, 'learning_rate': 6.179438034701656e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 982/2224 [1:26:37<1:43:41,  5.01s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 983/2224 [1:26:42<1:43:43,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.345, 'grad_norm': 0.9242557673428669, 'learning_rate': 6.172359971244075e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 983/2224 [1:26:42<1:43:43,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 984/2224 [1:26:47<1:43:52,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3221, 'grad_norm': 0.908416137508511, 'learning_rate': 6.1652794208748245e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 984/2224 [1:26:47<1:43:52,  5.03s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 985/2224 [1:26:52<1:43:36,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3007, 'grad_norm': 0.8999455772305409, 'learning_rate': 6.158196398613786e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 985/2224 [1:26:52<1:43:36,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 986/2224 [1:26:57<1:43:46,  5.03s/it]
0:                                                       
0: 
0: {'loss': 0.3243, 'grad_norm': 0.8755496958404082, 'learning_rate': 6.151110919486076e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 986/2224 [1:26:57<1:43:46,  5.03s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 987/2224 [1:27:02<1:43:35,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3257, 'grad_norm': 0.9170415965013392, 'learning_rate': 6.144022998522031e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 987/2224 [1:27:02<1:43:35,  5.02s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 988/2224 [1:27:07<1:43:15,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3365, 'grad_norm': 0.8978697421600872, 'learning_rate': 6.136932650757161e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 988/2224 [1:27:07<1:43:15,  5.01s/it]
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 989/2224 [1:27:12<1:42:45,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.3406, 'grad_norm': 0.9460170367830919, 'learning_rate': 6.12983989123213e-06, 'epoch': 0.44}
0: 
0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 989/2224 [1:27:12<1:42:45,  4.99s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2224 [1:27:17<1:42:53,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.301, 'grad_norm': 0.8479694032970163, 'learning_rate': 6.122744734992711e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2224 [1:27:17<1:42:53,  5.00s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 991/2224 [1:27:22<1:42:38,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3125, 'grad_norm': 0.8604802134920626, 'learning_rate': 6.115647197089768e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 991/2224 [1:27:22<1:42:38,  5.00s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 992/2224 [1:27:27<1:43:08,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3108, 'grad_norm': 0.8892421638728139, 'learning_rate': 6.108547292579213e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 992/2224 [1:27:27<1:43:08,  5.02s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 993/2224 [1:27:32<1:42:55,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3112, 'grad_norm': 0.9246129886926966, 'learning_rate': 6.10144503652198e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 993/2224 [1:27:32<1:42:55,  5.02s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 994/2224 [1:27:37<1:42:56,  5.02s/it]
0:                                                       
0: 
0: {'loss': 0.3373, 'grad_norm': 0.9568306942296159, 'learning_rate': 6.0943404439839885e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 994/2224 [1:27:37<1:42:56,  5.02s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 995/2224 [1:27:42<1:42:13,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.341, 'grad_norm': 0.9194304278571583, 'learning_rate': 6.0872335300361186e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 995/2224 [1:27:42<1:42:13,  4.99s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 996/2224 [1:27:47<1:42:25,  5.00s/it]
0:                                                       
0: 
0: {'loss': 0.3126, 'grad_norm': 0.9321228497591983, 'learning_rate': 6.08012430975417e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 996/2224 [1:27:47<1:42:25,  5.00s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 997/2224 [1:27:52<1:42:07,  4.99s/it]
0:                                                       
0: 
0: {'loss': 0.2983, 'grad_norm': 0.922340285451695, 'learning_rate': 6.0730127982188405e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 997/2224 [1:27:52<1:42:07,  4.99s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 998/2224 [1:27:57<1:42:19,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3056, 'grad_norm': 0.8926024592748727, 'learning_rate': 6.065899010515685e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 998/2224 [1:27:57<1:42:19,  5.01s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 999/2224 [1:28:02<1:42:16,  5.01s/it]
0:                                                       
0: 
0: {'loss': 0.3103, 'grad_norm': 0.8896857965849226, 'learning_rate': 6.058782961735085e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 999/2224 [1:28:02<1:42:16,  5.01s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2224 [1:28:07<1:42:10,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3318, 'grad_norm': 0.9505451900000914, 'learning_rate': 6.051664666972221e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2224 [1:28:07<1:42:10,  5.01s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2224 [1:29:12<7:50:36, 23.09s/it]
0:                                                        
0: 
0: {'loss': 0.3162, 'grad_norm': 0.9054144372408914, 'learning_rate': 6.044544141327039e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1001/2224 [1:29:12<7:50:36, 23.09s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2224 [1:29:17<5:59:33, 17.65s/it]
0:                                                        
0: 
0: {'loss': 0.317, 'grad_norm': 0.9446078504328754, 'learning_rate': 6.0374213999042155e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1002/2224 [1:29:17<5:59:33, 17.65s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2224 [1:29:22<4:41:52, 13.85s/it]
0:                                                        
0: 
0: {'loss': 0.3434, 'grad_norm': 0.9625176336185979, 'learning_rate': 6.030296457813126e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1003/2224 [1:29:22<4:41:52, 13.85s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2224 [1:29:27<3:47:20, 11.18s/it]
0:                                                        
0: 
0: {'loss': 0.3324, 'grad_norm': 0.9189821487725062, 'learning_rate': 6.023169330167815e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1004/2224 [1:29:27<3:47:20, 11.18s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2224 [1:29:32<3:09:20,  9.32s/it]
0:                                                        
0: 
0: {'loss': 0.2791, 'grad_norm': 0.8631017511896587, 'learning_rate': 6.016040032086966e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1005/2224 [1:29:32<3:09:20,  9.32s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2224 [1:29:37<2:42:22,  8.00s/it]
0:                                                        
0: 
0: {'loss': 0.3405, 'grad_norm': 0.8743132217153471, 'learning_rate': 6.008908578693865e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1006/2224 [1:29:37<2:42:22,  8.00s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2224 [1:29:42<2:23:55,  7.10s/it]
0:                                                        
0: 
0: {'loss': 0.3252, 'grad_norm': 0.9068360368078225, 'learning_rate': 6.001774985116366e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1007/2224 [1:29:42<2:23:55,  7.10s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2224 [1:29:47<2:10:32,  6.44s/it]
0:                                                        
0: 
0: {'loss': 0.3454, 'grad_norm': 0.8864274259656175, 'learning_rate': 5.994639266486871e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1008/2224 [1:29:47<2:10:32,  6.44s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1009/2224 [1:29:52<2:01:55,  6.02s/it]
0:                                                        
0: 
0: {'loss': 0.3529, 'grad_norm': 0.8951070994775465, 'learning_rate': 5.9875014379422825e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1009/2224 [1:29:52<2:01:55,  6.02s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1010/2224 [1:29:57<1:55:37,  5.71s/it]
0:                                                        
0: 
0: {'loss': 0.3415, 'grad_norm': 0.9171722076519722, 'learning_rate': 5.980361514623982e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1010/2224 [1:29:57<1:55:37,  5.71s/it]
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1011/2224 [1:30:02<1:51:15,  5.50s/it]
0:                                                        
0: 
0: {'loss': 0.3039, 'grad_norm': 0.8805382407213399, 'learning_rate': 5.973219511677795e-06, 'epoch': 0.45}
0: 
0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1011/2224 [1:30:02<1:51:15,  5.50s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1012/2224 [1:30:07<1:48:17,  5.36s/it]
0:                                                        
0: 
0: {'loss': 0.3379, 'grad_norm': 0.920392434845408, 'learning_rate': 5.9660754442539555e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1012/2224 [1:30:07<1:48:17,  5.36s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1013/2224 [1:30:12<1:46:12,  5.26s/it]
0:                                                        
0: 
0: {'loss': 0.3139, 'grad_norm': 0.9473571475148738, 'learning_rate': 5.958929327507082e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1013/2224 [1:30:12<1:46:12,  5.26s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1014/2224 [1:30:17<1:43:57,  5.15s/it]
0:                                                        
0: 
0: {'loss': 0.3287, 'grad_norm': 0.897311801703449, 'learning_rate': 5.9517811765961365e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1014/2224 [1:30:17<1:43:57,  5.15s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1015/2224 [1:30:22<1:42:58,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.3385, 'grad_norm': 0.9013815473295714, 'learning_rate': 5.944631006684396e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1015/2224 [1:30:22<1:42:58,  5.11s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1016/2224 [1:30:27<1:42:12,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3178, 'grad_norm': 0.9400615235456525, 'learning_rate': 5.937478832939422e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1016/2224 [1:30:27<1:42:12,  5.08s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1017/2224 [1:30:32<1:41:28,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3126, 'grad_norm': 0.8999658293985854, 'learning_rate': 5.930324670533023e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1017/2224 [1:30:32<1:41:28,  5.04s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1018/2224 [1:30:37<1:41:19,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.328, 'grad_norm': 0.9087529893260246, 'learning_rate': 5.923168534641232e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1018/2224 [1:30:37<1:41:19,  5.04s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1019/2224 [1:30:42<1:40:54,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2996, 'grad_norm': 0.9243160261261019, 'learning_rate': 5.916010440444264e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1019/2224 [1:30:42<1:40:54,  5.02s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2224 [1:30:47<1:40:54,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3176, 'grad_norm': 0.912783122736945, 'learning_rate': 5.908850403126492e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2224 [1:30:47<1:40:54,  5.03s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1021/2224 [1:30:52<1:40:52,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2911, 'grad_norm': 0.9648036431387658, 'learning_rate': 5.901688437876404e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1021/2224 [1:30:52<1:40:52,  5.03s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1022/2224 [1:30:57<1:40:43,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.316, 'grad_norm': 0.8785129312717095, 'learning_rate': 5.894524559886584e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1022/2224 [1:30:57<1:40:43,  5.03s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1023/2224 [1:31:02<1:39:51,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.319, 'grad_norm': 0.8823882564547691, 'learning_rate': 5.887358784353672e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1023/2224 [1:31:02<1:39:51,  4.99s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1024/2224 [1:31:07<1:39:09,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.3025, 'grad_norm': 0.9135847919657126, 'learning_rate': 5.880191126478332e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1024/2224 [1:31:07<1:39:09,  4.96s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1025/2224 [1:31:12<1:39:26,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3181, 'grad_norm': 0.915349553658094, 'learning_rate': 5.8730216014652195e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1025/2224 [1:31:12<1:39:26,  4.98s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1026/2224 [1:31:17<1:39:42,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3172, 'grad_norm': 0.9554925687782884, 'learning_rate': 5.865850224522957e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1026/2224 [1:31:17<1:39:42,  4.99s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1027/2224 [1:31:22<1:38:58,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.2979, 'grad_norm': 0.8949814622526853, 'learning_rate': 5.85867701086409e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1027/2224 [1:31:22<1:38:58,  4.96s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1028/2224 [1:31:27<1:38:54,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.2771, 'grad_norm': 0.8944148603367311, 'learning_rate': 5.851501975705061e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1028/2224 [1:31:27<1:38:54,  4.96s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2224 [1:31:32<1:39:12,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.321, 'grad_norm': 0.930803122498638, 'learning_rate': 5.844325134266177e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1029/2224 [1:31:32<1:39:12,  4.98s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2224 [1:31:37<1:38:28,  4.95s/it]
0:                                                        
0: 
0: {'loss': 0.3304, 'grad_norm': 0.9179037556950747, 'learning_rate': 5.837146501771575e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1030/2224 [1:31:37<1:38:28,  4.95s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2224 [1:31:42<1:38:41,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.3327, 'grad_norm': 0.9386722101520117, 'learning_rate': 5.829966093449195e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1031/2224 [1:31:42<1:38:41,  4.96s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2224 [1:31:47<1:39:10,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3362, 'grad_norm': 0.9017619008449327, 'learning_rate': 5.822783924530742e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1032/2224 [1:31:47<1:39:10,  4.99s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2224 [1:31:51<1:38:20,  4.95s/it]
0:                                                        
0: 
0: {'loss': 0.3073, 'grad_norm': 0.8610675205455042, 'learning_rate': 5.815600010251654e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1033/2224 [1:31:51<1:38:20,  4.95s/it]
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2224 [1:31:56<1:38:48,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3319, 'grad_norm': 0.879210746869194, 'learning_rate': 5.808414365851076e-06, 'epoch': 0.46}
0: 
0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1034/2224 [1:31:57<1:38:48,  4.98s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2224 [1:32:02<1:38:59,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2836, 'grad_norm': 0.853082338531239, 'learning_rate': 5.801227006571818e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1035/2224 [1:32:02<1:38:59,  5.00s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1036/2224 [1:32:07<1:39:03,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2965, 'grad_norm': 0.8836056562511122, 'learning_rate': 5.794037947660331e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1036/2224 [1:32:07<1:39:03,  5.00s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1037/2224 [1:32:12<1:39:31,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3305, 'grad_norm': 0.8820273966096362, 'learning_rate': 5.78684720436667e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1037/2224 [1:32:12<1:39:31,  5.03s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1038/2224 [1:32:17<1:39:11,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3263, 'grad_norm': 0.9746927791666589, 'learning_rate': 5.779654791944464e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1038/2224 [1:32:17<1:39:11,  5.02s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1039/2224 [1:32:22<1:39:16,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3558, 'grad_norm': 0.9541008437529084, 'learning_rate': 5.7724607256508795e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1039/2224 [1:32:22<1:39:16,  5.03s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1040/2224 [1:32:27<1:39:27,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.344, 'grad_norm': 0.9260750305742579, 'learning_rate': 5.765265020746596e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1040/2224 [1:32:27<1:39:27,  5.04s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1041/2224 [1:32:32<1:38:37,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3367, 'grad_norm': 0.9217710447039609, 'learning_rate': 5.75806769249577e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1041/2224 [1:32:32<1:38:37,  5.00s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1042/2224 [1:32:37<1:38:49,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3308, 'grad_norm': 0.9346628532061599, 'learning_rate': 5.750868756165994e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1042/2224 [1:32:37<1:38:49,  5.02s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1043/2224 [1:32:42<1:38:35,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3472, 'grad_norm': 0.8963825548543748, 'learning_rate': 5.743668227028277e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1043/2224 [1:32:42<1:38:35,  5.01s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1044/2224 [1:32:47<1:38:33,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3147, 'grad_norm': 0.9222439447806176, 'learning_rate': 5.736466120357008e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1044/2224 [1:32:47<1:38:33,  5.01s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1045/2224 [1:32:52<1:38:20,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3282, 'grad_norm': 0.9048997641776614, 'learning_rate': 5.729262451429918e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1045/2224 [1:32:52<1:38:20,  5.00s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1046/2224 [1:32:57<1:37:42,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3191, 'grad_norm': 0.9072971836485858, 'learning_rate': 5.722057235528055e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1046/2224 [1:32:57<1:37:42,  4.98s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1047/2224 [1:33:02<1:37:15,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.3192, 'grad_norm': 0.9145811803788277, 'learning_rate': 5.71485048793575e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1047/2224 [1:33:02<1:37:15,  4.96s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1048/2224 [1:33:07<1:37:20,  4.97s/it]
0:                                                        
0: 
0: {'loss': 0.299, 'grad_norm': 0.8725842569398701, 'learning_rate': 5.707642223940579e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1048/2224 [1:33:07<1:37:20,  4.97s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1049/2224 [1:33:12<1:37:53,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3352, 'grad_norm': 0.9021296082582294, 'learning_rate': 5.700432458833339e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1049/2224 [1:33:12<1:37:53,  5.00s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2224 [1:33:17<1:37:56,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2779, 'grad_norm': 0.9145133421365863, 'learning_rate': 5.693221207908006e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2224 [1:33:17<1:37:56,  5.01s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1051/2224 [1:33:22<1:37:57,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3034, 'grad_norm': 0.8576195128442945, 'learning_rate': 5.686008486461713e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1051/2224 [1:33:22<1:37:57,  5.01s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1052/2224 [1:33:27<1:37:58,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3178, 'grad_norm': 0.8710740303233347, 'learning_rate': 5.678794309794712e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1052/2224 [1:33:27<1:37:58,  5.02s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1053/2224 [1:33:32<1:37:57,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3175, 'grad_norm': 0.8515912474235212, 'learning_rate': 5.671578693210337e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1053/2224 [1:33:32<1:37:57,  5.02s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1054/2224 [1:33:37<1:38:12,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3482, 'grad_norm': 0.9101051826871723, 'learning_rate': 5.664361652014981e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1054/2224 [1:33:37<1:38:12,  5.04s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1055/2224 [1:33:42<1:38:09,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3019, 'grad_norm': 0.9027564504978266, 'learning_rate': 5.65714320151806e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1055/2224 [1:33:42<1:38:09,  5.04s/it]
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1056/2224 [1:33:47<1:37:11,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3021, 'grad_norm': 0.8845313888301461, 'learning_rate': 5.649923357031974e-06, 'epoch': 0.47}
0: 
0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1056/2224 [1:33:47<1:37:11,  4.99s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2224 [1:33:52<1:37:50,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.333, 'grad_norm': 0.8681237859798515, 'learning_rate': 5.642702133872086e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1057/2224 [1:33:52<1:37:50,  5.03s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2224 [1:33:57<1:37:41,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3127, 'grad_norm': 0.8630582213692212, 'learning_rate': 5.635479547356678e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1058/2224 [1:33:57<1:37:41,  5.03s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2224 [1:34:02<1:37:11,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3101, 'grad_norm': 0.8885570734817592, 'learning_rate': 5.628255612806931e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1059/2224 [1:34:02<1:37:11,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2224 [1:34:07<1:37:16,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.305, 'grad_norm': 0.8652134088108553, 'learning_rate': 5.621030345546879e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1060/2224 [1:34:07<1:37:16,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2224 [1:34:12<1:36:55,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3224, 'grad_norm': 0.893314568094457, 'learning_rate': 5.613803760903385e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1061/2224 [1:34:12<1:36:55,  5.00s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2224 [1:34:17<1:36:13,  4.97s/it]
0:                                                        
0: 
0: {'loss': 0.3035, 'grad_norm': 0.8536953036670282, 'learning_rate': 5.60657587420611e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1062/2224 [1:34:17<1:36:13,  4.97s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1063/2224 [1:34:22<1:36:09,  4.97s/it]
0:                                                        
0: 
0: {'loss': 0.3191, 'grad_norm': 0.9808541581059128, 'learning_rate': 5.5993467007874725e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1063/2224 [1:34:22<1:36:09,  4.97s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1064/2224 [1:34:27<1:36:27,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3274, 'grad_norm': 0.9124671305899791, 'learning_rate': 5.592116255982622e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1064/2224 [1:34:27<1:36:27,  4.99s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1065/2224 [1:34:32<1:36:34,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3054, 'grad_norm': 0.9038542167992646, 'learning_rate': 5.584884555129408e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1065/2224 [1:34:32<1:36:34,  5.00s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1066/2224 [1:34:37<1:36:38,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3107, 'grad_norm': 0.8454780712645965, 'learning_rate': 5.577651613568336e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1066/2224 [1:34:37<1:36:38,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1067/2224 [1:34:42<1:36:33,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.32, 'grad_norm': 0.9049698364815333, 'learning_rate': 5.570417446642553e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1067/2224 [1:34:42<1:36:33,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1068/2224 [1:34:47<1:36:28,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.29, 'grad_norm': 0.8858939609843981, 'learning_rate': 5.563182069697803e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1068/2224 [1:34:47<1:36:28,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1069/2224 [1:34:52<1:36:34,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3284, 'grad_norm': 0.8915882626045343, 'learning_rate': 5.555945498082391e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1069/2224 [1:34:52<1:36:34,  5.02s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1070/2224 [1:34:57<1:36:37,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3122, 'grad_norm': 0.8924213754705778, 'learning_rate': 5.548707747147164e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1070/2224 [1:34:57<1:36:37,  5.02s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1071/2224 [1:35:02<1:36:30,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3246, 'grad_norm': 0.9045753049962308, 'learning_rate': 5.541468832245465e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1071/2224 [1:35:02<1:36:30,  5.02s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1072/2224 [1:35:07<1:35:50,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3268, 'grad_norm': 0.8972066098719771, 'learning_rate': 5.53422876873311e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1072/2224 [1:35:07<1:35:50,  4.99s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1073/2224 [1:35:12<1:36:07,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3058, 'grad_norm': 0.9142503838014644, 'learning_rate': 5.526987571968347e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1073/2224 [1:35:12<1:36:07,  5.01s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1074/2224 [1:35:17<1:36:15,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3286, 'grad_norm': 0.9614735942432237, 'learning_rate': 5.519745257311833e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1074/2224 [1:35:17<1:36:15,  5.02s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1075/2224 [1:35:22<1:35:36,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.296, 'grad_norm': 0.8695769266908252, 'learning_rate': 5.512501840126594e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1075/2224 [1:35:22<1:35:36,  4.99s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1076/2224 [1:35:27<1:35:59,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3247, 'grad_norm': 0.8674335217940976, 'learning_rate': 5.505257335777993e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1076/2224 [1:35:27<1:35:59,  5.02s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1077/2224 [1:35:32<1:35:00,  4.97s/it]
0:                                                        
0: 
0: {'loss': 0.3451, 'grad_norm': 0.9428642883142831, 'learning_rate': 5.498011759633705e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1077/2224 [1:35:32<1:35:00,  4.97s/it]
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1078/2224 [1:35:37<1:34:38,  4.95s/it]
0:                                                        
0: 
0: {'loss': 0.2911, 'grad_norm': 0.8451543030083292, 'learning_rate': 5.490765127063671e-06, 'epoch': 0.48}
0: 
0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1078/2224 [1:35:37<1:34:38,  4.95s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1079/2224 [1:35:42<1:35:05,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.282, 'grad_norm': 0.8939755525107712, 'learning_rate': 5.483517453440076e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1079/2224 [1:35:42<1:35:05,  4.98s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2224 [1:35:47<1:35:21,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3147, 'grad_norm': 0.8735167233700057, 'learning_rate': 5.476268754137316e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2224 [1:35:47<1:35:21,  5.00s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1081/2224 [1:35:52<1:35:26,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3261, 'grad_norm': 0.8795863264687369, 'learning_rate': 5.469019044531961e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1081/2224 [1:35:52<1:35:26,  5.01s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1082/2224 [1:35:57<1:35:18,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3203, 'grad_norm': 0.9145554714157105, 'learning_rate': 5.461768340002725e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1082/2224 [1:35:57<1:35:18,  5.01s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1083/2224 [1:36:02<1:35:27,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3215, 'grad_norm': 0.8650960274216594, 'learning_rate': 5.454516655930428e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1083/2224 [1:36:02<1:35:27,  5.02s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1084/2224 [1:36:07<1:34:55,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3202, 'grad_norm': 0.8904073745695047, 'learning_rate': 5.447264007697973e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1084/2224 [1:36:07<1:34:55,  5.00s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2224 [1:36:12<1:34:48,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3191, 'grad_norm': 0.9095329824982543, 'learning_rate': 5.440010410690307e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1085/2224 [1:36:12<1:34:48,  4.99s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2224 [1:36:17<1:34:57,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3241, 'grad_norm': 0.9027484314866788, 'learning_rate': 5.4327558802943905e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1086/2224 [1:36:17<1:34:57,  5.01s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2224 [1:36:22<1:35:11,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3426, 'grad_norm': 0.8814874057724358, 'learning_rate': 5.425500431899156e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1087/2224 [1:36:22<1:35:11,  5.02s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2224 [1:36:27<1:35:12,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.321, 'grad_norm': 0.8791501161682146, 'learning_rate': 5.418244080895499e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1088/2224 [1:36:27<1:35:12,  5.03s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2224 [1:36:32<1:35:22,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3439, 'grad_norm': 0.8974407421653251, 'learning_rate': 5.410986842676216e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1089/2224 [1:36:32<1:35:22,  5.04s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1090/2224 [1:36:37<1:35:08,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3084, 'grad_norm': 0.8970351904795953, 'learning_rate': 5.403728732635992e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1090/2224 [1:36:37<1:35:08,  5.03s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1091/2224 [1:36:42<1:35:21,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3101, 'grad_norm': 0.8486858860603043, 'learning_rate': 5.396469766171359e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1091/2224 [1:36:42<1:35:21,  5.05s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1092/2224 [1:36:47<1:34:43,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.282, 'grad_norm': 0.8593632106705863, 'learning_rate': 5.389209958680662e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1092/2224 [1:36:47<1:34:43,  5.02s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1093/2224 [1:36:52<1:34:54,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3103, 'grad_norm': 0.8658375073623659, 'learning_rate': 5.381949325564042e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1093/2224 [1:36:52<1:34:54,  5.04s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1094/2224 [1:36:57<1:35:02,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2675, 'grad_norm': 0.886239312277576, 'learning_rate': 5.374687882223378e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1094/2224 [1:36:57<1:35:02,  5.05s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1095/2224 [1:37:02<1:35:42,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3157, 'grad_norm': 0.8856091460002717, 'learning_rate': 5.367425644062276e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1095/2224 [1:37:02<1:35:42,  5.09s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1096/2224 [1:37:07<1:35:31,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3232, 'grad_norm': 0.8803543787460878, 'learning_rate': 5.360162626486026e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1096/2224 [1:37:07<1:35:31,  5.08s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1097/2224 [1:37:13<1:35:28,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2942, 'grad_norm': 0.8658940014857932, 'learning_rate': 5.35289884490157e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1097/2224 [1:37:13<1:35:28,  5.08s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1098/2224 [1:37:18<1:34:59,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3012, 'grad_norm': 0.916655691747812, 'learning_rate': 5.3456343147174715e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1098/2224 [1:37:18<1:34:59,  5.06s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1099/2224 [1:37:23<1:34:33,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3288, 'grad_norm': 0.8982047536320885, 'learning_rate': 5.338369051343884e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1099/2224 [1:37:23<1:34:33,  5.04s/it]
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2224 [1:37:27<1:33:56,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3029, 'grad_norm': 0.8690078903671996, 'learning_rate': 5.331103070192509e-06, 'epoch': 0.49}
0: 
0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2224 [1:37:27<1:33:56,  5.02s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1101/2224 [1:37:33<1:34:10,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3422, 'grad_norm': 0.888754182522447, 'learning_rate': 5.323836386676582e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1101/2224 [1:37:33<1:34:10,  5.03s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1102/2224 [1:37:38<1:36:45,  5.17s/it]
0:                                                        
0: 
0: {'loss': 0.3293, 'grad_norm': 0.9141704636867899, 'learning_rate': 5.31656901621082e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1102/2224 [1:37:38<1:36:45,  5.17s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1103/2224 [1:37:43<1:35:55,  5.13s/it]
0:                                                        
0: 
0: {'loss': 0.3238, 'grad_norm': 0.8927877832128625, 'learning_rate': 5.3093009742114e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1103/2224 [1:37:43<1:35:55,  5.13s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1104/2224 [1:37:48<1:35:38,  5.12s/it]
0:                                                        
0: 
0: {'loss': 0.3288, 'grad_norm': 0.8942650392675163, 'learning_rate': 5.302032276095923e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1104/2224 [1:37:48<1:35:38,  5.12s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1105/2224 [1:37:53<1:35:08,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.3122, 'grad_norm': 0.9495153070950615, 'learning_rate': 5.294762937283381e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1105/2224 [1:37:53<1:35:08,  5.10s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1106/2224 [1:37:58<1:34:59,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.3048, 'grad_norm': 0.884035552572482, 'learning_rate': 5.2874929731941285e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1106/2224 [1:37:58<1:34:59,  5.10s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1107/2224 [1:38:03<1:34:35,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3125, 'grad_norm': 0.9134926158636181, 'learning_rate': 5.28022239924984e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1107/2224 [1:38:03<1:34:35,  5.08s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1108/2224 [1:38:08<1:34:43,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3152, 'grad_norm': 0.8536742130567403, 'learning_rate': 5.272951230873492e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1108/2224 [1:38:08<1:34:43,  5.09s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1109/2224 [1:38:13<1:33:44,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3134, 'grad_norm': 0.876219553242751, 'learning_rate': 5.265679483489315e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1109/2224 [1:38:13<1:33:44,  5.04s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2224 [1:38:18<1:33:36,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2902, 'grad_norm': 0.8501294228341918, 'learning_rate': 5.2584071725227715e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2224 [1:38:18<1:33:36,  5.04s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1111/2224 [1:38:24<1:33:53,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3155, 'grad_norm': 0.8882346753032426, 'learning_rate': 5.2511343134005185e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1111/2224 [1:38:24<1:33:53,  5.06s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2224 [1:38:29<1:33:38,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3024, 'grad_norm': 0.9092814206504671, 'learning_rate': 5.243860921550373e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1112/2224 [1:38:29<1:33:38,  5.05s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2224 [1:38:34<1:33:45,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3053, 'grad_norm': 0.853667032349621, 'learning_rate': 5.236587012401289e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1113/2224 [1:38:34<1:33:45,  5.06s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2224 [1:38:39<1:33:43,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3367, 'grad_norm': 0.9068178839053667, 'learning_rate': 5.229312601383311e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1114/2224 [1:38:39<1:33:43,  5.07s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2224 [1:38:44<1:33:18,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3191, 'grad_norm': 0.9207158866784144, 'learning_rate': 5.22203770392755e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1115/2224 [1:38:44<1:33:18,  5.05s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2224 [1:38:49<1:33:17,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3092, 'grad_norm': 0.9027903511984947, 'learning_rate': 5.214762335466152e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1116/2224 [1:38:49<1:33:17,  5.05s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2224 [1:38:54<1:33:09,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2949, 'grad_norm': 0.8762097198526925, 'learning_rate': 5.2074865114322574e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1117/2224 [1:38:54<1:33:09,  5.05s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1118/2224 [1:38:59<1:33:18,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3194, 'grad_norm': 0.8869882858827643, 'learning_rate': 5.200210247259976e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1118/2224 [1:38:59<1:33:18,  5.06s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1119/2224 [1:39:04<1:33:01,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3352, 'grad_norm': 0.8794879551114501, 'learning_rate': 5.192933558384351e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1119/2224 [1:39:04<1:33:01,  5.05s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1120/2224 [1:39:09<1:32:44,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3062, 'grad_norm': 0.866601802544612, 'learning_rate': 5.185656460241326e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1120/2224 [1:39:09<1:32:44,  5.04s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1121/2224 [1:39:14<1:32:24,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3266, 'grad_norm': 0.8936304109045935, 'learning_rate': 5.178378968267712e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1121/2224 [1:39:14<1:32:24,  5.03s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1122/2224 [1:39:19<1:32:24,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3386, 'grad_norm': 0.8710597520354108, 'learning_rate': 5.171101097901158e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1122/2224 [1:39:19<1:32:24,  5.03s/it]
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1123/2224 [1:39:24<1:31:35,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.298, 'grad_norm': 0.8752277963807693, 'learning_rate': 5.163822864580111e-06, 'epoch': 0.5}
0: 
0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1123/2224 [1:39:24<1:31:35,  4.99s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1124/2224 [1:39:29<1:31:52,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3347, 'grad_norm': 0.9110970662186965, 'learning_rate': 5.156544283743794e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1124/2224 [1:39:29<1:31:52,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1125/2224 [1:39:34<1:31:52,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3415, 'grad_norm': 0.8747932466384654, 'learning_rate': 5.149265370832163e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1125/2224 [1:39:34<1:31:52,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1126/2224 [1:39:39<1:31:54,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3002, 'grad_norm': 0.913774495155581, 'learning_rate': 5.141986141285878e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1126/2224 [1:39:39<1:31:54,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1127/2224 [1:39:44<1:31:46,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2951, 'grad_norm': 0.8662935431823586, 'learning_rate': 5.134706610546276e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1127/2224 [1:39:44<1:31:46,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1128/2224 [1:39:49<1:31:51,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3076, 'grad_norm': 0.879185165085065, 'learning_rate': 5.127426794055324e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1128/2224 [1:39:49<1:31:51,  5.03s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1129/2224 [1:39:54<1:31:52,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3127, 'grad_norm': 0.8488103364898442, 'learning_rate': 5.120146707255604e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1129/2224 [1:39:54<1:31:52,  5.03s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2224 [1:39:59<1:31:49,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2888, 'grad_norm': 0.8828944018423719, 'learning_rate': 5.1128663655902656e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2224 [1:39:59<1:31:49,  5.04s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1131/2224 [1:40:04<1:31:16,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3088, 'grad_norm': 0.9419401309913442, 'learning_rate': 5.105585784503001e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1131/2224 [1:40:04<1:31:16,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1132/2224 [1:40:09<1:31:20,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.308, 'grad_norm': 0.8680579930419315, 'learning_rate': 5.098304979438014e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1132/2224 [1:40:09<1:31:20,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1133/2224 [1:40:14<1:31:18,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3299, 'grad_norm': 0.9072199346173563, 'learning_rate': 5.091023965839974e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1133/2224 [1:40:14<1:31:18,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1134/2224 [1:40:19<1:30:54,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2973, 'grad_norm': 0.8446936574055997, 'learning_rate': 5.083742759154003e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1134/2224 [1:40:19<1:30:54,  5.00s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1135/2224 [1:40:24<1:31:00,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.302, 'grad_norm': 0.8906378466885696, 'learning_rate': 5.076461374825626e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1135/2224 [1:40:24<1:31:00,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1136/2224 [1:40:29<1:30:55,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3014, 'grad_norm': 0.9226045768993253, 'learning_rate': 5.06917982830075e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1136/2224 [1:40:29<1:30:55,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1137/2224 [1:40:34<1:30:48,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3443, 'grad_norm': 0.9225653797670662, 'learning_rate': 5.0618981350256205e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1137/2224 [1:40:34<1:30:48,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1138/2224 [1:40:39<1:30:19,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.332, 'grad_norm': 0.899848999685061, 'learning_rate': 5.054616310446796e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1138/2224 [1:40:39<1:30:19,  4.99s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1139/2224 [1:40:44<1:30:21,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.9368373563016059, 'learning_rate': 5.047334370011118e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1139/2224 [1:40:44<1:30:21,  5.00s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2224 [1:40:49<1:30:33,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3085, 'grad_norm': 0.9073431151263845, 'learning_rate': 5.040052329165667e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1140/2224 [1:40:49<1:30:33,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2224 [1:40:54<1:30:35,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2981, 'grad_norm': 0.8994653145586088, 'learning_rate': 5.032770203357742e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1141/2224 [1:40:54<1:30:35,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2224 [1:40:59<1:30:39,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3153, 'grad_norm': 0.8884746751914341, 'learning_rate': 5.025488008034818e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1142/2224 [1:40:59<1:30:39,  5.03s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2224 [1:41:04<1:30:30,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3024, 'grad_norm': 0.8725604956214189, 'learning_rate': 5.018205758644521e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1143/2224 [1:41:04<1:30:30,  5.02s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2224 [1:41:09<1:30:08,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3145, 'grad_norm': 0.8674111035588851, 'learning_rate': 5.01092347063459e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1144/2224 [1:41:09<1:30:08,  5.01s/it]
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1145/2224 [1:41:14<1:30:19,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.321, 'grad_norm': 0.8863888292700907, 'learning_rate': 5.003641159452846e-06, 'epoch': 0.51}
0: 
0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1145/2224 [1:41:14<1:30:19,  5.02s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1146/2224 [1:41:19<1:30:18,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2931, 'grad_norm': 0.8895304838881628, 'learning_rate': 4.996358840547158e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1146/2224 [1:41:19<1:30:18,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1147/2224 [1:41:24<1:30:20,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3207, 'grad_norm': 1.035297585632861, 'learning_rate': 4.989076529365412e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1147/2224 [1:41:24<1:30:20,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1148/2224 [1:41:29<1:29:27,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3131, 'grad_norm': 0.9024483600482002, 'learning_rate': 4.9817942413554795e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1148/2224 [1:41:29<1:29:27,  4.99s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1149/2224 [1:41:34<1:29:44,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3246, 'grad_norm': 0.9326241599314814, 'learning_rate': 4.9745119919651826e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1149/2224 [1:41:34<1:29:44,  5.01s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1150/2224 [1:41:39<1:29:52,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.8772989450388008, 'learning_rate': 4.967229796642259e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1150/2224 [1:41:39<1:29:52,  5.02s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1151/2224 [1:41:44<1:29:43,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2897, 'grad_norm': 0.9207776802047752, 'learning_rate': 4.9599476708343334e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1151/2224 [1:41:44<1:29:43,  5.02s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1152/2224 [1:41:49<1:29:39,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3508, 'grad_norm': 0.9178963455134613, 'learning_rate': 4.952665629988882e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1152/2224 [1:41:49<1:29:39,  5.02s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1153/2224 [1:41:55<1:29:47,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3438, 'grad_norm': 0.8861704180435396, 'learning_rate': 4.945383689553205e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1153/2224 [1:41:55<1:29:47,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1154/2224 [1:42:00<1:29:54,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3226, 'grad_norm': 0.9254111917517569, 'learning_rate': 4.938101864974381e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1154/2224 [1:42:00<1:29:54,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1155/2224 [1:42:05<1:29:48,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3386, 'grad_norm': 0.9231630525658712, 'learning_rate': 4.930820171699251e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1155/2224 [1:42:05<1:29:48,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1156/2224 [1:42:10<1:29:47,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3156, 'grad_norm': 0.9006475674697146, 'learning_rate': 4.923538625174375e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1156/2224 [1:42:10<1:29:47,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1157/2224 [1:42:15<1:29:45,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.316, 'grad_norm': 0.8732349217953655, 'learning_rate': 4.916257240845998e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1157/2224 [1:42:15<1:29:45,  5.05s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1158/2224 [1:42:20<1:29:25,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3011, 'grad_norm': 0.8496363101565475, 'learning_rate': 4.9089760341600266e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1158/2224 [1:42:20<1:29:25,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1159/2224 [1:42:25<1:29:04,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3042, 'grad_norm': 0.9018857942518321, 'learning_rate': 4.90169502056199e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1159/2224 [1:42:25<1:29:04,  5.02s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2224 [1:42:30<1:29:15,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3259, 'grad_norm': 0.910696556537951, 'learning_rate': 4.8944142154970005e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2224 [1:42:30<1:29:15,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1161/2224 [1:42:35<1:29:12,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2892, 'grad_norm': 0.8758464889854229, 'learning_rate': 4.887133634409736e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1161/2224 [1:42:35<1:29:12,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1162/2224 [1:42:40<1:29:02,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.305, 'grad_norm': 0.8707218225485833, 'learning_rate': 4.879853292744399e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1162/2224 [1:42:40<1:29:02,  5.03s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1163/2224 [1:42:45<1:29:05,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3243, 'grad_norm': 0.9134663519896847, 'learning_rate': 4.872573205944677e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1163/2224 [1:42:45<1:29:05,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1164/2224 [1:42:50<1:29:02,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.8871141345600638, 'learning_rate': 4.865293389453725e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1164/2224 [1:42:50<1:29:02,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1165/2224 [1:42:55<1:28:52,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2825, 'grad_norm': 0.8673911274807804, 'learning_rate': 4.858013858714122e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1165/2224 [1:42:55<1:28:52,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1166/2224 [1:43:00<1:28:51,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3184, 'grad_norm': 0.9113013302459498, 'learning_rate': 4.850734629167839e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1166/2224 [1:43:00<1:28:51,  5.04s/it]
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1167/2224 [1:43:05<1:28:35,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2917, 'grad_norm': 0.8811495471946952, 'learning_rate': 4.8434557162562065e-06, 'epoch': 0.52}
0: 
0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1167/2224 [1:43:05<1:28:35,  5.03s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2224 [1:43:10<1:28:02,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3246, 'grad_norm': 0.8964744081123669, 'learning_rate': 4.83617713541989e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1168/2224 [1:43:10<1:28:02,  5.00s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2224 [1:43:15<1:28:22,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3159, 'grad_norm': 0.8894667943781845, 'learning_rate': 4.828898902098844e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1169/2224 [1:43:15<1:28:22,  5.03s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2224 [1:43:20<1:28:29,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3135, 'grad_norm': 0.8978876083667935, 'learning_rate': 4.821621031732289e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1170/2224 [1:43:20<1:28:29,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2224 [1:43:25<1:28:38,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2992, 'grad_norm': 0.8944478607697448, 'learning_rate': 4.814343539758675e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1171/2224 [1:43:25<1:28:38,  5.05s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1172/2224 [1:43:30<1:28:33,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3165, 'grad_norm': 0.8911257805222539, 'learning_rate': 4.807066441615651e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1172/2224 [1:43:30<1:28:33,  5.05s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1173/2224 [1:43:35<1:28:51,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3274, 'grad_norm': 0.8795561016694622, 'learning_rate': 4.799789752740026e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1173/2224 [1:43:35<1:28:51,  5.07s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1174/2224 [1:43:40<1:28:45,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3053, 'grad_norm': 0.8830108577302788, 'learning_rate': 4.792513488567743e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1174/2224 [1:43:40<1:28:45,  5.07s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1175/2224 [1:43:45<1:28:07,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3268, 'grad_norm': 0.945293547281872, 'learning_rate': 4.785237664533851e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1175/2224 [1:43:45<1:28:07,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1176/2224 [1:43:50<1:28:07,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3364, 'grad_norm': 0.8659257723544892, 'learning_rate': 4.777962296072452e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1176/2224 [1:43:50<1:28:07,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1177/2224 [1:43:55<1:27:39,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3143, 'grad_norm': 0.9134317265875695, 'learning_rate': 4.770687398616691e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1177/2224 [1:43:55<1:27:39,  5.02s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1178/2224 [1:44:00<1:27:34,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3094, 'grad_norm': 0.8983317414259567, 'learning_rate': 4.763412987598711e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1178/2224 [1:44:00<1:27:34,  5.02s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1179/2224 [1:44:05<1:27:23,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2929, 'grad_norm': 0.8686267834030601, 'learning_rate': 4.756139078449628e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1179/2224 [1:44:05<1:27:23,  5.02s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1180/2224 [1:44:11<1:27:36,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.321, 'grad_norm': 0.8802221596499564, 'learning_rate': 4.748865686599483e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1180/2224 [1:44:11<1:27:36,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1181/2224 [1:44:16<1:27:27,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.307, 'grad_norm': 0.8612013638541818, 'learning_rate': 4.7415928274772285e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1181/2224 [1:44:16<1:27:27,  5.03s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1182/2224 [1:44:21<1:27:19,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3253, 'grad_norm': 0.9020971283411116, 'learning_rate': 4.734320516510687e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1182/2224 [1:44:21<1:27:19,  5.03s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1183/2224 [1:44:26<1:26:48,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3141, 'grad_norm': 0.8712645791226091, 'learning_rate': 4.72704876912651e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1183/2224 [1:44:26<1:26:48,  5.00s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1184/2224 [1:44:30<1:26:21,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3106, 'grad_norm': 0.8803952693384087, 'learning_rate': 4.7197776007501605e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1184/2224 [1:44:30<1:26:21,  4.98s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1185/2224 [1:44:36<1:26:39,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3153, 'grad_norm': 0.9094320497610858, 'learning_rate': 4.712507026805874e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1185/2224 [1:44:36<1:26:39,  5.00s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1186/2224 [1:44:41<1:27:02,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3035, 'grad_norm': 0.8597259353575716, 'learning_rate': 4.705237062716621e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1186/2224 [1:44:41<1:27:02,  5.03s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1187/2224 [1:44:46<1:27:08,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3122, 'grad_norm': 0.8735150597545966, 'learning_rate': 4.697967723904079e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1187/2224 [1:44:46<1:27:08,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1188/2224 [1:44:51<1:26:56,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3205, 'grad_norm': 0.8872806411186253, 'learning_rate': 4.690699025788603e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1188/2224 [1:44:51<1:26:56,  5.04s/it]
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1189/2224 [1:44:56<1:27:06,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2945, 'grad_norm': 0.8846473032238317, 'learning_rate': 4.6834309837891825e-06, 'epoch': 0.53}
0: 
0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1189/2224 [1:44:56<1:27:06,  5.05s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2224 [1:45:01<1:26:53,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3097, 'grad_norm': 0.8733023071832082, 'learning_rate': 4.6761636133234195e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2224 [1:45:01<1:26:53,  5.04s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1191/2224 [1:45:06<1:27:02,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3087, 'grad_norm': 0.8608633864643447, 'learning_rate': 4.668896929807491e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1191/2224 [1:45:06<1:27:02,  5.06s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1192/2224 [1:45:11<1:26:59,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.316, 'grad_norm': 0.8915266303512878, 'learning_rate': 4.661630948656119e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1192/2224 [1:45:11<1:26:59,  5.06s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1193/2224 [1:45:16<1:26:46,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3058, 'grad_norm': 0.8581402709035454, 'learning_rate': 4.654365685282529e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1193/2224 [1:45:16<1:26:46,  5.05s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1194/2224 [1:45:21<1:26:44,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2965, 'grad_norm': 0.8951882425583517, 'learning_rate': 4.64710115509843e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1194/2224 [1:45:21<1:26:44,  5.05s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1195/2224 [1:45:26<1:26:20,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2937, 'grad_norm': 0.8534199314856569, 'learning_rate': 4.639837373513975e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1195/2224 [1:45:26<1:26:20,  5.03s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2224 [1:45:31<1:26:24,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.34, 'grad_norm': 0.9472601267504535, 'learning_rate': 4.632574355937725e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1196/2224 [1:45:31<1:26:24,  5.04s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2224 [1:45:36<1:25:44,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3075, 'grad_norm': 0.8932334150949608, 'learning_rate': 4.625312117776623e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1197/2224 [1:45:36<1:25:44,  5.01s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2224 [1:45:41<1:26:10,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3217, 'grad_norm': 0.8914552380651244, 'learning_rate': 4.6180506744359605e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1198/2224 [1:45:41<1:26:10,  5.04s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1199/2224 [1:45:46<1:26:14,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3035, 'grad_norm': 0.8927724640743772, 'learning_rate': 4.610790041319339e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1199/2224 [1:45:46<1:26:14,  5.05s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1200/2224 [1:45:51<1:26:22,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3122, 'grad_norm': 0.9117777960273216, 'learning_rate': 4.603530233828644e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1200/2224 [1:45:51<1:26:22,  5.06s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1201/2224 [1:46:47<5:43:08, 20.13s/it]
0:                                                        
0: 
0: {'loss': 0.2917, 'grad_norm': 0.8955372145306604, 'learning_rate': 4.596271267364011e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1201/2224 [1:46:47<5:43:08, 20.13s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1202/2224 [1:46:52<4:25:43, 15.60s/it]
0:                                                        
0: 
0: {'loss': 0.3065, 'grad_norm': 0.8411491409254321, 'learning_rate': 4.589013157323785e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1202/2224 [1:46:52<4:25:43, 15.60s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1203/2224 [1:46:57<3:31:27, 12.43s/it]
0:                                                        
0: 
0: {'loss': 0.279, 'grad_norm': 0.8550111246769574, 'learning_rate': 4.581755919104502e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1203/2224 [1:46:57<3:31:27, 12.43s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1204/2224 [1:47:02<2:53:33, 10.21s/it]
0:                                                        
0: 
0: {'loss': 0.259, 'grad_norm': 0.858430744724144, 'learning_rate': 4.574499568100843e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1204/2224 [1:47:02<2:53:33, 10.21s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1205/2224 [1:47:07<2:27:19,  8.68s/it]
0:                                                        
0: 
0: {'loss': 0.312, 'grad_norm': 0.8807966042172205, 'learning_rate': 4.567244119705613e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1205/2224 [1:47:07<2:27:19,  8.68s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1206/2224 [1:47:12<2:08:43,  7.59s/it]
0:                                                        
0: 
0: {'loss': 0.3114, 'grad_norm': 0.8886416963412541, 'learning_rate': 4.559989589309694e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1206/2224 [1:47:12<2:08:43,  7.59s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1207/2224 [1:47:17<1:55:52,  6.84s/it]
0:                                                        
0: 
0: {'loss': 0.3065, 'grad_norm': 0.839217794710575, 'learning_rate': 4.552735992302028e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1207/2224 [1:47:17<1:55:52,  6.84s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1208/2224 [1:47:22<1:46:41,  6.30s/it]
0:                                                        
0: 
0: {'loss': 0.296, 'grad_norm': 0.8366741684934095, 'learning_rate': 4.545483344069574e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1208/2224 [1:47:22<1:46:41,  6.30s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1209/2224 [1:47:27<1:40:04,  5.92s/it]
0:                                                        
0: 
0: {'loss': 0.2936, 'grad_norm': 0.8626836208343246, 'learning_rate': 4.538231659997277e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1209/2224 [1:47:27<1:40:04,  5.92s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1210/2224 [1:47:32<1:34:50,  5.61s/it]
0:                                                        
0: 
0: {'loss': 0.2877, 'grad_norm': 0.8854529813512005, 'learning_rate': 4.530980955468039e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1210/2224 [1:47:32<1:34:50,  5.61s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1211/2224 [1:47:37<1:31:47,  5.44s/it]
0:                                                        
0: 
0: {'loss': 0.2904, 'grad_norm': 0.8927413308290783, 'learning_rate': 4.5237312458626846e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1211/2224 [1:47:37<1:31:47,  5.44s/it]
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1212/2224 [1:47:42<1:29:41,  5.32s/it]
0:                                                        
0: 
0: {'loss': 0.3009, 'grad_norm': 0.8754000568070239, 'learning_rate': 4.516482546559925e-06, 'epoch': 0.54}
0: 
0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1212/2224 [1:47:42<1:29:41,  5.32s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1213/2224 [1:47:47<1:28:10,  5.23s/it]
0:                                                        
0: 
0: {'loss': 0.3012, 'grad_norm': 0.8779873844367436, 'learning_rate': 4.509234872936331e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1213/2224 [1:47:47<1:28:10,  5.23s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1214/2224 [1:47:52<1:27:18,  5.19s/it]
0:                                                        
0: 
0: {'loss': 0.319, 'grad_norm': 0.908454600503461, 'learning_rate': 4.501988240366296e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1214/2224 [1:47:52<1:27:18,  5.19s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1215/2224 [1:47:57<1:26:29,  5.14s/it]
0:                                                        
0: 
0: {'loss': 0.3065, 'grad_norm': 0.8893753258776317, 'learning_rate': 4.494742664222008e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1215/2224 [1:47:57<1:26:29,  5.14s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1216/2224 [1:48:02<1:25:56,  5.12s/it]
0:                                                        
0: 
0: {'loss': 0.3062, 'grad_norm': 0.9143045334282883, 'learning_rate': 4.4874981598734074e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1216/2224 [1:48:02<1:25:56,  5.12s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1217/2224 [1:48:07<1:24:53,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.313, 'grad_norm': 0.9162703282426735, 'learning_rate': 4.480254742688167e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1217/2224 [1:48:07<1:24:53,  5.06s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1218/2224 [1:48:12<1:24:32,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.29, 'grad_norm': 0.8667162932241459, 'learning_rate': 4.4730124280316544e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1218/2224 [1:48:12<1:24:32,  5.04s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1219/2224 [1:48:17<1:24:18,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2954, 'grad_norm': 0.853838305062382, 'learning_rate': 4.465771231266892e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1219/2224 [1:48:17<1:24:18,  5.03s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2224 [1:48:22<1:24:15,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3053, 'grad_norm': 0.9017246685177289, 'learning_rate': 4.458531167754535e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2224 [1:48:22<1:24:15,  5.04s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1221/2224 [1:48:27<1:24:19,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3027, 'grad_norm': 0.8824076634866339, 'learning_rate': 4.451292252852838e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1221/2224 [1:48:27<1:24:19,  5.04s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1222/2224 [1:48:32<1:23:30,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3067, 'grad_norm': 0.86377765776718, 'learning_rate': 4.44405450191761e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1222/2224 [1:48:32<1:23:30,  5.00s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1223/2224 [1:48:37<1:23:36,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2881, 'grad_norm': 0.8784088662045051, 'learning_rate': 4.4368179303021985e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1223/2224 [1:48:37<1:23:36,  5.01s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2224 [1:48:42<1:23:49,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3136, 'grad_norm': 0.8967302929063987, 'learning_rate': 4.4295825533574475e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1224/2224 [1:48:42<1:23:49,  5.03s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2224 [1:48:47<1:23:47,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3092, 'grad_norm': 0.8577611373599751, 'learning_rate': 4.422348386431665e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1225/2224 [1:48:47<1:23:47,  5.03s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2224 [1:48:52<1:23:46,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3132, 'grad_norm': 0.9069233572916058, 'learning_rate': 4.415115444870594e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1226/2224 [1:48:52<1:23:46,  5.04s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1227/2224 [1:48:57<1:23:32,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2724, 'grad_norm': 0.8516482536690569, 'learning_rate': 4.407883744017378e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1227/2224 [1:48:57<1:23:32,  5.03s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1228/2224 [1:49:02<1:23:21,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3127, 'grad_norm': 0.9287616270407506, 'learning_rate': 4.400653299212529e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1228/2224 [1:49:02<1:23:21,  5.02s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1229/2224 [1:49:07<1:23:27,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3148, 'grad_norm': 0.8926417272448801, 'learning_rate': 4.393424125793891e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1229/2224 [1:49:07<1:23:27,  5.03s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1230/2224 [1:49:12<1:23:06,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2666, 'grad_norm': 0.9478570594417056, 'learning_rate': 4.386196239096615e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1230/2224 [1:49:12<1:23:06,  5.02s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1231/2224 [1:49:17<1:23:05,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3078, 'grad_norm': 0.8623291966609395, 'learning_rate': 4.378969654453124e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1231/2224 [1:49:17<1:23:05,  5.02s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1232/2224 [1:49:22<1:23:17,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3175, 'grad_norm': 0.8772662807890919, 'learning_rate': 4.37174438719307e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1232/2224 [1:49:22<1:23:17,  5.04s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1233/2224 [1:49:27<1:22:26,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.2994, 'grad_norm': 0.867672674920383, 'learning_rate': 4.364520452643322e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1233/2224 [1:49:27<1:22:26,  4.99s/it]
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1234/2224 [1:49:32<1:22:30,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3008, 'grad_norm': 0.8534168657773478, 'learning_rate': 4.357297866127917e-06, 'epoch': 0.55}
0: 
0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1234/2224 [1:49:32<1:22:30,  5.00s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1235/2224 [1:49:37<1:22:24,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3331, 'grad_norm': 0.8873786527621149, 'learning_rate': 4.350076642968027e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1235/2224 [1:49:37<1:22:24,  5.00s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1236/2224 [1:49:42<1:22:46,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2804, 'grad_norm': 0.8572121255118101, 'learning_rate': 4.342856798481941e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1236/2224 [1:49:42<1:22:46,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1237/2224 [1:49:47<1:22:41,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2914, 'grad_norm': 0.8820434922613598, 'learning_rate': 4.335638347985021e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1237/2224 [1:49:47<1:22:41,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1238/2224 [1:49:53<1:22:41,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8922192518871929, 'learning_rate': 4.328421306789665e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1238/2224 [1:49:53<1:22:41,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1239/2224 [1:49:58<1:22:33,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3329, 'grad_norm': 0.9182541728035701, 'learning_rate': 4.32120569020529e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1239/2224 [1:49:58<1:22:33,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1240/2224 [1:50:03<1:22:24,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3201, 'grad_norm': 0.8943935177963919, 'learning_rate': 4.3139915135382875e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1240/2224 [1:50:03<1:22:24,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1241/2224 [1:50:08<1:22:42,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3324, 'grad_norm': 0.8969525709216065, 'learning_rate': 4.306778792091995e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1241/2224 [1:50:08<1:22:42,  5.05s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1242/2224 [1:50:13<1:22:33,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3221, 'grad_norm': 0.9019181535104142, 'learning_rate': 4.299567541166662e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1242/2224 [1:50:13<1:22:33,  5.04s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1243/2224 [1:50:18<1:22:15,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8721325955106414, 'learning_rate': 4.292357776059421e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1243/2224 [1:50:18<1:22:15,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1244/2224 [1:50:23<1:22:07,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3366, 'grad_norm': 0.8729195220798531, 'learning_rate': 4.285149512064252e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1244/2224 [1:50:23<1:22:07,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1245/2224 [1:50:28<1:22:02,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2949, 'grad_norm': 0.852108930795861, 'learning_rate': 4.277942764471946e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1245/2224 [1:50:28<1:22:02,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1246/2224 [1:50:33<1:22:13,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2986, 'grad_norm': 0.8747495742116332, 'learning_rate': 4.2707375485700835e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1246/2224 [1:50:33<1:22:13,  5.04s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1247/2224 [1:50:38<1:21:27,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3157, 'grad_norm': 0.8915614033933907, 'learning_rate': 4.263533879642995e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1247/2224 [1:50:38<1:21:27,  5.00s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1248/2224 [1:50:43<1:21:31,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3061, 'grad_norm': 0.8808340775924542, 'learning_rate': 4.2563317729717245e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1248/2224 [1:50:43<1:21:31,  5.01s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1249/2224 [1:50:48<1:21:42,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2732, 'grad_norm': 0.8495741884094806, 'learning_rate': 4.249131243834008e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1249/2224 [1:50:48<1:21:42,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2224 [1:50:53<1:21:34,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2919, 'grad_norm': 0.8763695813338613, 'learning_rate': 4.241932307504233e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2224 [1:50:53<1:21:34,  5.03s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2224 [1:50:58<1:21:20,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3291, 'grad_norm': 0.897635161119075, 'learning_rate': 4.2347349792534045e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1251/2224 [1:50:58<1:21:20,  5.02s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2224 [1:51:03<1:23:41,  5.17s/it]
0:                                                        
0: 
0: {'loss': 0.3029, 'grad_norm': 0.9140101033858391, 'learning_rate': 4.227539274349121e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1252/2224 [1:51:03<1:23:41,  5.17s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2224 [1:51:08<1:22:26,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.2974, 'grad_norm': 0.8884925930959621, 'learning_rate': 4.220345208055539e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1253/2224 [1:51:08<1:22:26,  5.09s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1254/2224 [1:51:13<1:22:14,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3096, 'grad_norm': 0.9390368064420012, 'learning_rate': 4.213152795633332e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1254/2224 [1:51:13<1:22:14,  5.09s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1255/2224 [1:51:18<1:21:57,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3229, 'grad_norm': 0.952714970138443, 'learning_rate': 4.2059620523396696e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1255/2224 [1:51:18<1:21:57,  5.07s/it]
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1256/2224 [1:51:24<1:23:28,  5.17s/it]
0:                                                        
0: 
0: {'loss': 0.305, 'grad_norm': 0.8917155226641745, 'learning_rate': 4.198772993428182e-06, 'epoch': 0.56}
0: 
0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1256/2224 [1:51:24<1:23:28,  5.17s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1257/2224 [1:51:29<1:24:42,  5.26s/it]
0:                                                        
0: 
0: {'loss': 0.3112, 'grad_norm': 0.9002843914669708, 'learning_rate': 4.191585634148926e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1257/2224 [1:51:29<1:24:42,  5.26s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1258/2224 [1:51:34<1:23:38,  5.20s/it]
0:                                                        
0: 
0: {'loss': 0.2925, 'grad_norm': 0.8616261287412685, 'learning_rate': 4.184399989748347e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1258/2224 [1:51:34<1:23:38,  5.20s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1259/2224 [1:51:39<1:22:15,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.3034, 'grad_norm': 0.8869697663243101, 'learning_rate': 4.17721607546926e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1259/2224 [1:51:39<1:22:15,  5.11s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1260/2224 [1:51:44<1:21:44,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3428, 'grad_norm': 0.9220322957519882, 'learning_rate': 4.170033906550808e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1260/2224 [1:51:44<1:21:44,  5.09s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1261/2224 [1:51:50<1:22:41,  5.15s/it]
0:                                                        
0: 
0: {'loss': 0.3113, 'grad_norm': 0.8717446332572177, 'learning_rate': 4.162853498228427e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1261/2224 [1:51:50<1:22:41,  5.15s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1262/2224 [1:51:55<1:23:29,  5.21s/it]
0:                                                        
0: 
0: {'loss': 0.3184, 'grad_norm': 0.8729103225745645, 'learning_rate': 4.155674865733825e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1262/2224 [1:51:55<1:23:29,  5.21s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1263/2224 [1:52:00<1:22:44,  5.17s/it]
0:                                                        
0: 
0: {'loss': 0.3161, 'grad_norm': 0.8751905322959865, 'learning_rate': 4.148498024294942e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1263/2224 [1:52:00<1:22:44,  5.17s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1264/2224 [1:52:05<1:23:15,  5.20s/it]
0:                                                        
0: 
0: {'loss': 0.2895, 'grad_norm': 0.8460453666824129, 'learning_rate': 4.141322989135912e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1264/2224 [1:52:05<1:23:15,  5.20s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1265/2224 [1:52:10<1:22:29,  5.16s/it]
0:                                                        
0: 
0: {'loss': 0.321, 'grad_norm': 0.950366950674365, 'learning_rate': 4.134149775477044e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1265/2224 [1:52:10<1:22:29,  5.16s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1266/2224 [1:52:15<1:21:34,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.3266, 'grad_norm': 0.9147356131000728, 'learning_rate': 4.12697839853478e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1266/2224 [1:52:15<1:21:34,  5.11s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1267/2224 [1:52:20<1:20:58,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2913, 'grad_norm': 0.8994041407771338, 'learning_rate': 4.11980887352167e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1267/2224 [1:52:20<1:20:58,  5.08s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1268/2224 [1:52:26<1:21:50,  5.14s/it]
0:                                                        
0: 
0: {'loss': 0.2825, 'grad_norm': 0.8479090035194005, 'learning_rate': 4.112641215646329e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1268/2224 [1:52:26<1:21:50,  5.14s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1269/2224 [1:52:31<1:21:13,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.2883, 'grad_norm': 0.8583569943176799, 'learning_rate': 4.105475440113417e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1269/2224 [1:52:31<1:21:13,  5.10s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2224 [1:52:36<1:20:30,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2836, 'grad_norm': 0.8934062955045902, 'learning_rate': 4.098311562123599e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2224 [1:52:36<1:20:30,  5.06s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1271/2224 [1:52:41<1:20:34,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3111, 'grad_norm': 0.9156812442837887, 'learning_rate': 4.091149596873511e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1271/2224 [1:52:41<1:20:34,  5.07s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1272/2224 [1:52:46<1:20:32,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3197, 'grad_norm': 0.8699372110325089, 'learning_rate': 4.083989559555736e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1272/2224 [1:52:46<1:20:32,  5.08s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1273/2224 [1:52:51<1:19:11,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3108, 'grad_norm': 0.8798982844405697, 'learning_rate': 4.076831465358769e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1273/2224 [1:52:51<1:19:11,  5.00s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1274/2224 [1:52:56<1:19:19,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8567428037504693, 'learning_rate': 4.0696753294669785e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1274/2224 [1:52:56<1:19:19,  5.01s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1275/2224 [1:53:01<1:19:30,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2896, 'grad_norm': 0.8545691446772639, 'learning_rate': 4.0625211670605805e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1275/2224 [1:53:01<1:19:30,  5.03s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1276/2224 [1:53:06<1:19:33,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2812, 'grad_norm': 0.856157276136558, 'learning_rate': 4.055368993315605e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1276/2224 [1:53:06<1:19:33,  5.04s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1277/2224 [1:53:11<1:19:22,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3133, 'grad_norm': 0.8816981854455929, 'learning_rate': 4.048218823403865e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1277/2224 [1:53:11<1:19:22,  5.03s/it]
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1278/2224 [1:53:16<1:19:21,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.298, 'grad_norm': 0.8544349590689039, 'learning_rate': 4.041070672492919e-06, 'epoch': 0.57}
0: 
0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1278/2224 [1:53:16<1:19:21,  5.03s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2224 [1:53:21<1:19:27,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3068, 'grad_norm': 0.8825082787175743, 'learning_rate': 4.033924555746045e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1279/2224 [1:53:21<1:19:27,  5.04s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2224 [1:53:26<1:19:30,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.318, 'grad_norm': 0.8850624135307107, 'learning_rate': 4.026780488322208e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1280/2224 [1:53:26<1:19:30,  5.05s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1281/2224 [1:53:31<1:19:22,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3001, 'grad_norm': 0.8739333871829965, 'learning_rate': 4.019638485376019e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1281/2224 [1:53:31<1:19:22,  5.05s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1282/2224 [1:53:36<1:18:57,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3209, 'grad_norm': 0.8650711824970285, 'learning_rate': 4.012498562057718e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1282/2224 [1:53:36<1:18:57,  5.03s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1283/2224 [1:53:41<1:19:12,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2739, 'grad_norm': 0.832490230954052, 'learning_rate': 4.005360733513131e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1283/2224 [1:53:41<1:19:12,  5.05s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1284/2224 [1:53:46<1:19:01,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3048, 'grad_norm': 0.8543711114758313, 'learning_rate': 3.9982250148836345e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1284/2224 [1:53:46<1:19:01,  5.04s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1285/2224 [1:53:51<1:18:54,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3074, 'grad_norm': 0.8565403325383151, 'learning_rate': 3.991091421306137e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1285/2224 [1:53:51<1:18:54,  5.04s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1286/2224 [1:53:56<1:18:40,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3349, 'grad_norm': 0.9168086951697894, 'learning_rate': 3.983959967913035e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1286/2224 [1:53:56<1:18:40,  5.03s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1287/2224 [1:54:01<1:18:30,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3059, 'grad_norm': 0.8906392432573922, 'learning_rate': 3.976830669832186e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1287/2224 [1:54:01<1:18:30,  5.03s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1288/2224 [1:54:06<1:18:33,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2627, 'grad_norm': 0.8391605118412567, 'learning_rate': 3.969703542186876e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1288/2224 [1:54:06<1:18:33,  5.04s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1289/2224 [1:54:11<1:18:37,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3191, 'grad_norm': 0.8918207132227681, 'learning_rate': 3.962578600095785e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1289/2224 [1:54:11<1:18:37,  5.05s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1290/2224 [1:54:16<1:18:45,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.306, 'grad_norm': 0.844513827117535, 'learning_rate': 3.955455858672963e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1290/2224 [1:54:16<1:18:45,  5.06s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1291/2224 [1:54:22<1:19:06,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3197, 'grad_norm': 0.9130359280939419, 'learning_rate': 3.948335333027779e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1291/2224 [1:54:22<1:19:06,  5.09s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1292/2224 [1:54:27<1:19:19,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.3211, 'grad_norm': 0.9097935107703742, 'learning_rate': 3.941217038264916e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1292/2224 [1:54:27<1:19:19,  5.11s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1293/2224 [1:54:32<1:19:29,  5.12s/it]
0:                                                        
0: 
0: {'loss': 0.2942, 'grad_norm': 0.8975059669113741, 'learning_rate': 3.9341009894843184e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1293/2224 [1:54:32<1:19:29,  5.12s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1294/2224 [1:54:37<1:19:17,  5.12s/it]
0:                                                        
0: 
0: {'loss': 0.3146, 'grad_norm': 0.8886962073604038, 'learning_rate': 3.926987201781162e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1294/2224 [1:54:37<1:19:17,  5.12s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1295/2224 [1:54:42<1:18:52,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.2894, 'grad_norm': 0.8780762435563709, 'learning_rate': 3.919875690245831e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1295/2224 [1:54:42<1:18:52,  5.09s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1296/2224 [1:54:47<1:18:52,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.2938, 'grad_norm': 0.907647511109085, 'learning_rate': 3.912766469963885e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1296/2224 [1:54:47<1:18:52,  5.10s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1297/2224 [1:54:52<1:18:40,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.3056, 'grad_norm': 0.9086890849020772, 'learning_rate': 3.905659556016014e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1297/2224 [1:54:52<1:18:40,  5.09s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1298/2224 [1:54:57<1:18:38,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.3168, 'grad_norm': 0.8761234915201829, 'learning_rate': 3.898554963478022e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1298/2224 [1:54:57<1:18:38,  5.10s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1299/2224 [1:55:02<1:18:44,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.2763, 'grad_norm': 0.8525957779433976, 'learning_rate': 3.8914527074207895e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1299/2224 [1:55:02<1:18:44,  5.11s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2224 [1:55:07<1:18:17,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3305, 'grad_norm': 0.8609948776641425, 'learning_rate': 3.884352802910233e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2224 [1:55:07<1:18:17,  5.08s/it]
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1301/2224 [1:55:13<1:18:34,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.2922, 'grad_norm': 0.9085975962591638, 'learning_rate': 3.8772552650072895e-06, 'epoch': 0.58}
0: 
0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1301/2224 [1:55:13<1:18:34,  5.11s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1302/2224 [1:55:18<1:18:14,  5.09s/it]
0:                                                        
0: 
0: {'loss': 0.2776, 'grad_norm': 0.8651389705203459, 'learning_rate': 3.8701601087678705e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1302/2224 [1:55:18<1:18:14,  5.09s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1303/2224 [1:55:23<1:18:17,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.3192, 'grad_norm': 0.8579576243726548, 'learning_rate': 3.86306734924284e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1303/2224 [1:55:23<1:18:17,  5.10s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1304/2224 [1:55:28<1:18:10,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.2956, 'grad_norm': 0.9037313277616256, 'learning_rate': 3.85597700147797e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1304/2224 [1:55:28<1:18:10,  5.10s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1305/2224 [1:55:33<1:18:04,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.2751, 'grad_norm': 0.8346043797120044, 'learning_rate': 3.848889080513924e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1305/2224 [1:55:33<1:18:04,  5.10s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1306/2224 [1:55:38<1:17:37,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.2884, 'grad_norm': 0.8633180816590398, 'learning_rate': 3.8418036013862166e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1306/2224 [1:55:38<1:17:37,  5.07s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2224 [1:55:43<1:17:17,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.308, 'grad_norm': 0.9069755976852137, 'learning_rate': 3.834720579125176e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1307/2224 [1:55:43<1:17:17,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1308/2224 [1:55:48<1:17:05,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2944, 'grad_norm': 0.9149801484971654, 'learning_rate': 3.8276400287559264e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1308/2224 [1:55:48<1:17:05,  5.05s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1309/2224 [1:55:53<1:17:09,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3148, 'grad_norm': 0.8778197878051726, 'learning_rate': 3.8205619652983465e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1309/2224 [1:55:53<1:17:09,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1310/2224 [1:55:58<1:17:04,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2788, 'grad_norm': 0.8877764185734747, 'learning_rate': 3.813486403767036e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1310/2224 [1:55:58<1:17:04,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1311/2224 [1:56:03<1:17:08,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.2998, 'grad_norm': 0.8696843036304898, 'learning_rate': 3.8064133591712903e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1311/2224 [1:56:03<1:17:08,  5.07s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1312/2224 [1:56:08<1:17:04,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3154, 'grad_norm': 0.9211865810213782, 'learning_rate': 3.7993428465150684e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1312/2224 [1:56:08<1:17:04,  5.07s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1313/2224 [1:56:13<1:16:53,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3056, 'grad_norm': 0.8931263220153175, 'learning_rate': 3.79227488079695e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1313/2224 [1:56:13<1:16:53,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1314/2224 [1:56:18<1:16:48,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.363, 'grad_norm': 0.8956878556249998, 'learning_rate': 3.785209477010119e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1314/2224 [1:56:18<1:16:48,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1315/2224 [1:56:24<1:16:50,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3055, 'grad_norm': 0.9319146964099779, 'learning_rate': 3.7781466501423235e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1315/2224 [1:56:24<1:16:50,  5.07s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1316/2224 [1:56:29<1:16:44,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3173, 'grad_norm': 0.8765378425498498, 'learning_rate': 3.7710864151758463e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1316/2224 [1:56:29<1:16:44,  5.07s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1317/2224 [1:56:34<1:16:24,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3061, 'grad_norm': 0.8520667335557168, 'learning_rate': 3.7640287870874665e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1317/2224 [1:56:34<1:16:24,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1318/2224 [1:56:39<1:16:04,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3208, 'grad_norm': 0.8921464827431232, 'learning_rate': 3.7569737808484386e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1318/2224 [1:56:39<1:16:04,  5.04s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1319/2224 [1:56:44<1:16:20,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2994, 'grad_norm': 0.8767088017538982, 'learning_rate': 3.7499214114244564e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1319/2224 [1:56:44<1:16:20,  5.06s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1320/2224 [1:56:49<1:15:55,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2872, 'grad_norm': 0.8705537806398835, 'learning_rate': 3.742871693775613e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1320/2224 [1:56:49<1:15:55,  5.04s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1321/2224 [1:56:54<1:15:56,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3075, 'grad_norm': 0.8859408891992855, 'learning_rate': 3.735824642856384e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1321/2224 [1:56:54<1:15:56,  5.05s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1322/2224 [1:56:59<1:15:58,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2944, 'grad_norm': 0.9166316829528953, 'learning_rate': 3.7287802736155847e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1322/2224 [1:56:59<1:15:58,  5.05s/it]
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1323/2224 [1:57:04<1:15:46,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.33, 'grad_norm': 0.8927388298480183, 'learning_rate': 3.7217386009963413e-06, 'epoch': 0.59}
0: 
0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1323/2224 [1:57:04<1:15:46,  5.05s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1324/2224 [1:57:09<1:15:37,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3042, 'grad_norm': 0.88576257177539, 'learning_rate': 3.7146996399360615e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1324/2224 [1:57:09<1:15:37,  5.04s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1325/2224 [1:57:14<1:15:23,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3187, 'grad_norm': 0.8762583893220328, 'learning_rate': 3.7076634053664013e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1325/2224 [1:57:14<1:15:23,  5.03s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1326/2224 [1:57:19<1:15:35,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3277, 'grad_norm': 0.8860538622807524, 'learning_rate': 3.700629912213228e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1326/2224 [1:57:19<1:15:35,  5.05s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1327/2224 [1:57:24<1:15:26,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2817, 'grad_norm': 0.8862465978677592, 'learning_rate': 3.6935991753966005e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1327/2224 [1:57:24<1:15:26,  5.05s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1328/2224 [1:57:29<1:15:15,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3061, 'grad_norm': 0.8693638785571496, 'learning_rate': 3.686571209830727e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1328/2224 [1:57:29<1:15:15,  5.04s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1329/2224 [1:57:34<1:15:31,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2914, 'grad_norm': 0.883281143583172, 'learning_rate': 3.6795460304239383e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1329/2224 [1:57:34<1:15:31,  5.06s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2224 [1:57:39<1:15:23,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3056, 'grad_norm': 0.8877091490470199, 'learning_rate': 3.6725236520786523e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2224 [1:57:39<1:15:23,  5.06s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1331/2224 [1:57:44<1:14:51,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3099, 'grad_norm': 0.9016482414616223, 'learning_rate': 3.6655040896913485e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1331/2224 [1:57:44<1:14:51,  5.03s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1332/2224 [1:57:49<1:14:40,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2909, 'grad_norm': 0.8916545154945046, 'learning_rate': 3.6584873581525344e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1332/2224 [1:57:49<1:14:40,  5.02s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1333/2224 [1:57:54<1:14:40,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2972, 'grad_norm': 0.895642770965918, 'learning_rate': 3.6514734723467037e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1333/2224 [1:57:54<1:14:40,  5.03s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1334/2224 [1:57:59<1:14:23,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2941, 'grad_norm': 0.8960889678440331, 'learning_rate': 3.644462447152323e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1334/2224 [1:57:59<1:14:23,  5.02s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2224 [1:58:04<1:14:20,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3025, 'grad_norm': 0.8933296409186626, 'learning_rate': 3.6374542974417874e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1335/2224 [1:58:04<1:14:20,  5.02s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1336/2224 [1:58:09<1:14:21,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2989, 'grad_norm': 0.8697783282558724, 'learning_rate': 3.6304490380813908e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1336/2224 [1:58:09<1:14:21,  5.02s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1337/2224 [1:58:14<1:14:27,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3034, 'grad_norm': 0.8603571227146378, 'learning_rate': 3.6234466839312966e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1337/2224 [1:58:14<1:14:27,  5.04s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1338/2224 [1:58:19<1:14:25,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3617, 'grad_norm': 0.933808933862129, 'learning_rate': 3.616447249845506e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1338/2224 [1:58:19<1:14:25,  5.04s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1339/2224 [1:58:24<1:14:09,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2865, 'grad_norm': 0.8646900823812269, 'learning_rate': 3.609450750671829e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1339/2224 [1:58:24<1:14:09,  5.03s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1340/2224 [1:58:29<1:14:08,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3051, 'grad_norm': 0.9073070708911776, 'learning_rate': 3.6024572012518423e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1340/2224 [1:58:29<1:14:08,  5.03s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1341/2224 [1:58:35<1:14:16,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.292, 'grad_norm': 0.8676697823541459, 'learning_rate': 3.5954666164208705e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1341/2224 [1:58:35<1:14:16,  5.05s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1342/2224 [1:58:40<1:14:20,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2708, 'grad_norm': 0.8405646531821472, 'learning_rate': 3.5884790110079513e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1342/2224 [1:58:40<1:14:20,  5.06s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1343/2224 [1:58:45<1:14:00,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2816, 'grad_norm': 0.866130469083102, 'learning_rate': 3.581494399835796e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1343/2224 [1:58:45<1:14:00,  5.04s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1344/2224 [1:58:50<1:14:02,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3135, 'grad_norm': 0.8930891477122374, 'learning_rate': 3.5745127977207687e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1344/2224 [1:58:50<1:14:02,  5.05s/it]
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1345/2224 [1:58:55<1:13:53,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.8652097336949445, 'learning_rate': 3.567534219472852e-06, 'epoch': 0.6}
0: 
0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1345/2224 [1:58:55<1:13:53,  5.04s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1346/2224 [1:59:00<1:13:50,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2676, 'grad_norm': 0.8560055301173604, 'learning_rate': 3.5605586798956077e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1346/2224 [1:59:00<1:13:50,  5.05s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1347/2224 [1:59:05<1:13:50,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.33, 'grad_norm': 0.8761700605177344, 'learning_rate': 3.553586193786157e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1347/2224 [1:59:05<1:13:50,  5.05s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1348/2224 [1:59:10<1:13:21,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3165, 'grad_norm': 0.9031112078854022, 'learning_rate': 3.546616775935144e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1348/2224 [1:59:10<1:13:21,  5.02s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1349/2224 [1:59:15<1:12:53,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2864, 'grad_norm': 0.8488835470093162, 'learning_rate': 3.5396504411266986e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1349/2224 [1:59:15<1:12:53,  5.00s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1350/2224 [1:59:20<1:12:55,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3031, 'grad_norm': 0.8726828314430999, 'learning_rate': 3.532687204138416e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1350/2224 [1:59:20<1:12:55,  5.01s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1351/2224 [1:59:25<1:12:33,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.3206, 'grad_norm': 0.8895764871932786, 'learning_rate': 3.5257270797413203e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1351/2224 [1:59:25<1:12:33,  4.99s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1352/2224 [1:59:30<1:12:35,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2788, 'grad_norm': 0.8714674879946892, 'learning_rate': 3.5187700826998315e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1352/2224 [1:59:30<1:12:35,  5.00s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1353/2224 [1:59:35<1:12:49,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2922, 'grad_norm': 0.8845637689839682, 'learning_rate': 3.5118162277717314e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1353/2224 [1:59:35<1:12:49,  5.02s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1354/2224 [1:59:40<1:12:40,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2987, 'grad_norm': 0.9028889382804334, 'learning_rate': 3.5048655297081436e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1354/2224 [1:59:40<1:12:40,  5.01s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1355/2224 [1:59:45<1:12:08,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3082, 'grad_norm': 0.8732540123102663, 'learning_rate': 3.497918003253492e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1355/2224 [1:59:45<1:12:08,  4.98s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1356/2224 [1:59:50<1:11:59,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.2828, 'grad_norm': 0.8693060907641328, 'learning_rate': 3.4909736631454694e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1356/2224 [1:59:50<1:11:59,  4.98s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1357/2224 [1:59:55<1:12:15,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3298, 'grad_norm': 0.9151423214492298, 'learning_rate': 3.484032524115014e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1357/2224 [1:59:55<1:12:15,  5.00s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1358/2224 [2:00:00<1:12:21,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3034, 'grad_norm': 0.9129691694485621, 'learning_rate': 3.477094600886274e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1358/2224 [2:00:00<1:12:21,  5.01s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1359/2224 [2:00:05<1:12:24,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2943, 'grad_norm': 0.9020723835482397, 'learning_rate': 3.47015990817657e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1359/2224 [2:00:05<1:12:24,  5.02s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2224 [2:00:10<1:12:11,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3089, 'grad_norm': 0.891867043496302, 'learning_rate': 3.4632284606963768e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2224 [2:00:10<1:12:11,  5.01s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1361/2224 [2:00:15<1:12:18,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2956, 'grad_norm': 0.8783548349948105, 'learning_rate': 3.4563002731492827e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1361/2224 [2:00:15<1:12:18,  5.03s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1362/2224 [2:00:20<1:12:09,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2869, 'grad_norm': 0.8874524019844122, 'learning_rate': 3.449375360231957e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1362/2224 [2:00:20<1:12:09,  5.02s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1363/2224 [2:00:25<1:12:09,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2739, 'grad_norm': 0.8350609861581995, 'learning_rate': 3.442453736634127e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1363/2224 [2:00:25<1:12:09,  5.03s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1364/2224 [2:00:30<1:12:10,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2812, 'grad_norm': 0.85885579122006, 'learning_rate': 3.4355354170385413e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1364/2224 [2:00:30<1:12:10,  5.04s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1365/2224 [2:00:35<1:12:28,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2982, 'grad_norm': 0.8736016640927291, 'learning_rate': 3.4286204161209415e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1365/2224 [2:00:35<1:12:28,  5.06s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1366/2224 [2:00:40<1:12:21,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2791, 'grad_norm': 0.8585168704710997, 'learning_rate': 3.421708748550023e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1366/2224 [2:00:40<1:12:21,  5.06s/it]
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1367/2224 [2:00:45<1:12:02,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.308, 'grad_norm': 0.9054335632199177, 'learning_rate': 3.4148004289874183e-06, 'epoch': 0.61}
0: 
0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1367/2224 [2:00:45<1:12:02,  5.04s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1368/2224 [2:00:50<1:11:47,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2833, 'grad_norm': 0.8463085965846084, 'learning_rate': 3.407895472087655e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1368/2224 [2:00:50<1:11:47,  5.03s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1369/2224 [2:00:55<1:11:58,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2975, 'grad_norm': 0.8546788903882698, 'learning_rate': 3.4009938924981223e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1369/2224 [2:00:55<1:11:58,  5.05s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1370/2224 [2:01:00<1:11:24,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2921, 'grad_norm': 0.879446487904371, 'learning_rate': 3.394095704859052e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1370/2224 [2:01:00<1:11:24,  5.02s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1371/2224 [2:01:05<1:11:27,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3126, 'grad_norm': 0.8964494335975074, 'learning_rate': 3.3872009238034785e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1371/2224 [2:01:05<1:11:27,  5.03s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1372/2224 [2:01:10<1:11:26,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2994, 'grad_norm': 0.8618480687853232, 'learning_rate': 3.3803095639572088e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1372/2224 [2:01:10<1:11:26,  5.03s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1373/2224 [2:01:15<1:11:43,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2778, 'grad_norm': 0.854911787483978, 'learning_rate': 3.3734216399387925e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1373/2224 [2:01:15<1:11:43,  5.06s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1374/2224 [2:01:20<1:10:51,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2897, 'grad_norm': 0.9124071588619023, 'learning_rate': 3.366537166359495e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1374/2224 [2:01:20<1:10:51,  5.00s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1375/2224 [2:01:25<1:10:47,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.275, 'grad_norm': 0.8631329437627749, 'learning_rate': 3.3596561578232532e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1375/2224 [2:01:25<1:10:47,  5.00s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1376/2224 [2:01:30<1:10:35,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.2871, 'grad_norm': 0.8800103012476431, 'learning_rate': 3.3527786289266632e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1376/2224 [2:01:30<1:10:35,  4.99s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1377/2224 [2:01:35<1:10:17,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3089, 'grad_norm': 0.867054482634613, 'learning_rate': 3.3459045942589354e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1377/2224 [2:01:35<1:10:17,  4.98s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1378/2224 [2:01:40<1:10:32,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3249, 'grad_norm': 0.8649089283027579, 'learning_rate': 3.339034068401868e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1378/2224 [2:01:40<1:10:32,  5.00s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1379/2224 [2:01:45<1:10:36,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3196, 'grad_norm': 0.8724085799637132, 'learning_rate': 3.3321670659298155e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1379/2224 [2:01:45<1:10:36,  5.01s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1380/2224 [2:01:50<1:10:36,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3136, 'grad_norm': 0.9101034675433038, 'learning_rate': 3.3253036014096607e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1380/2224 [2:01:50<1:10:36,  5.02s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1381/2224 [2:01:55<1:10:34,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3121, 'grad_norm': 0.8639280088676459, 'learning_rate': 3.3184436894007815e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1381/2224 [2:01:55<1:10:34,  5.02s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1382/2224 [2:02:00<1:10:40,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3038, 'grad_norm': 0.8514181050678656, 'learning_rate': 3.311587344455014e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1382/2224 [2:02:00<1:10:40,  5.04s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1383/2224 [2:02:05<1:10:42,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.325, 'grad_norm': 0.8870001360244348, 'learning_rate': 3.304734581116634e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1383/2224 [2:02:05<1:10:42,  5.04s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1384/2224 [2:02:10<1:10:03,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2907, 'grad_norm': 0.8858594209753125, 'learning_rate': 3.2978854139223186e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1384/2224 [2:02:10<1:10:03,  5.00s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1385/2224 [2:02:15<1:09:58,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2969, 'grad_norm': 0.8616752553005016, 'learning_rate': 3.2910398574011137e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1385/2224 [2:02:15<1:09:58,  5.00s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1386/2224 [2:02:20<1:10:12,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3066, 'grad_norm': 0.8843942424909319, 'learning_rate': 3.284197926074408e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1386/2224 [2:02:20<1:10:12,  5.03s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1387/2224 [2:02:26<1:10:24,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3075, 'grad_norm': 0.8601227222915131, 'learning_rate': 3.2773596344559024e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1387/2224 [2:02:26<1:10:24,  5.05s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1388/2224 [2:02:30<1:09:47,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2888, 'grad_norm': 0.9088706390862001, 'learning_rate': 3.27052499705157e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1388/2224 [2:02:31<1:09:47,  5.01s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1389/2224 [2:02:36<1:10:03,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3065, 'grad_norm': 0.880197388531286, 'learning_rate': 3.2636940283596385e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1389/2224 [2:02:36<1:10:03,  5.03s/it]
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1390/2224 [2:02:41<1:09:57,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2792, 'grad_norm': 0.8788691096620389, 'learning_rate': 3.2568667428705508e-06, 'epoch': 0.62}
0: 
0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1390/2224 [2:02:41<1:09:57,  5.03s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1391/2224 [2:02:46<1:10:09,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2886, 'grad_norm': 0.8368104673829445, 'learning_rate': 3.2500431550669386e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1391/2224 [2:02:46<1:10:09,  5.05s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1392/2224 [2:02:51<1:09:58,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3163, 'grad_norm': 0.8872254804711076, 'learning_rate': 3.2432232794235863e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1392/2224 [2:02:51<1:09:58,  5.05s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1393/2224 [2:02:56<1:10:00,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3049, 'grad_norm': 0.8731676893615727, 'learning_rate': 3.2364071304074075e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1393/2224 [2:02:56<1:10:00,  5.05s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1394/2224 [2:03:01<1:09:47,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3136, 'grad_norm': 0.8654884927974182, 'learning_rate': 3.2295947224774086e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1394/2224 [2:03:01<1:09:47,  5.05s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1395/2224 [2:03:06<1:09:46,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3104, 'grad_norm': 0.9199211366875933, 'learning_rate': 3.222786070084658e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1395/2224 [2:03:06<1:09:46,  5.05s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1396/2224 [2:03:11<1:09:05,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3067, 'grad_norm': 0.8930298464415839, 'learning_rate': 3.2159811876722615e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1396/2224 [2:03:11<1:09:05,  5.01s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1397/2224 [2:03:16<1:09:14,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.8686516506636272, 'learning_rate': 3.209180089675327e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1397/2224 [2:03:16<1:09:14,  5.02s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1398/2224 [2:03:21<1:09:16,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2971, 'grad_norm': 0.9190654602252035, 'learning_rate': 3.2023827905209304e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1398/2224 [2:03:21<1:09:16,  5.03s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1399/2224 [2:03:26<1:09:14,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2992, 'grad_norm': 0.8784332394841308, 'learning_rate': 3.195589304628095e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1399/2224 [2:03:26<1:09:14,  5.04s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1400/2224 [2:03:31<1:08:44,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3175, 'grad_norm': 0.8788816054192942, 'learning_rate': 3.188799646407752e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1400/2224 [2:03:31<1:08:44,  5.01s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1401/2224 [2:04:23<4:21:45, 19.08s/it]
0:                                                        
0: 
0: {'loss': 0.2801, 'grad_norm': 0.852314590679014, 'learning_rate': 3.1820138302627142e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1401/2224 [2:04:23<4:21:45, 19.08s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1402/2224 [2:04:28<3:23:33, 14.86s/it]
0:                                                        
0: 
0: {'loss': 0.278, 'grad_norm': 0.853032562572302, 'learning_rate': 3.1752318705876404e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1402/2224 [2:04:28<3:23:33, 14.86s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1403/2224 [2:04:33<2:42:51, 11.90s/it]
0:                                                        
0: 
0: {'loss': 0.2962, 'grad_norm': 0.8735712628220366, 'learning_rate': 3.1684537817690138e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1403/2224 [2:04:33<2:42:51, 11.90s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1404/2224 [2:04:38<2:14:02,  9.81s/it]
0:                                                        
0: 
0: {'loss': 0.2823, 'grad_norm': 0.8468881876453216, 'learning_rate': 3.161679578185105e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1404/2224 [2:04:38<2:14:02,  9.81s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1405/2224 [2:04:43<1:54:13,  8.37s/it]
0:                                                        
0: 
0: {'loss': 0.3094, 'grad_norm': 0.8911936862051473, 'learning_rate': 3.1549092742059404e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1405/2224 [2:04:43<1:54:13,  8.37s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1406/2224 [2:04:48<1:40:22,  7.36s/it]
0:                                                        
0: 
0: {'loss': 0.2963, 'grad_norm': 0.91599396564174, 'learning_rate': 3.1481428841932778e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1406/2224 [2:04:48<1:40:22,  7.36s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1407/2224 [2:04:53<1:30:49,  6.67s/it]
0:                                                        
0: 
0: {'loss': 0.2784, 'grad_norm': 0.8527669686997913, 'learning_rate': 3.141380422500572e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1407/2224 [2:04:53<1:30:49,  6.67s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1408/2224 [2:04:58<1:23:55,  6.17s/it]
0:                                                        
0: 
0: {'loss': 0.3249, 'grad_norm': 0.8695340646285282, 'learning_rate': 3.1346219034729387e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1408/2224 [2:04:58<1:23:55,  6.17s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1409/2224 [2:05:03<1:18:58,  5.81s/it]
0:                                                        
0: 
0: {'loss': 0.2994, 'grad_norm': 0.8631351689955373, 'learning_rate': 3.1278673414471374e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1409/2224 [2:05:03<1:18:58,  5.81s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2224 [2:05:08<1:15:51,  5.59s/it]
0:                                                        
0: 
0: {'loss': 0.275, 'grad_norm': 0.8549258714775796, 'learning_rate': 3.1211167507515326e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2224 [2:05:08<1:15:51,  5.59s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1411/2224 [2:05:13<1:13:29,  5.42s/it]
0:                                                        
0: 
0: {'loss': 0.2795, 'grad_norm': 0.8435899709543567, 'learning_rate': 3.1143701457060606e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1411/2224 [2:05:13<1:13:29,  5.42s/it]
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1412/2224 [2:05:18<1:11:41,  5.30s/it]
0:                                                        
0: 
0: {'loss': 0.3175, 'grad_norm': 0.8872569217902757, 'learning_rate': 3.107627540622205e-06, 'epoch': 0.63}
0: 
0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1412/2224 [2:05:18<1:11:41,  5.30s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1413/2224 [2:05:23<1:10:44,  5.23s/it]
0:                                                        
0: 
0: {'loss': 0.2881, 'grad_norm': 0.8477960374916971, 'learning_rate': 3.100888949802966e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1413/2224 [2:05:23<1:10:44,  5.23s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1414/2224 [2:05:28<1:09:34,  5.15s/it]
0:                                                        
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8491104473905328, 'learning_rate': 3.094154387542827e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1414/2224 [2:05:28<1:09:34,  5.15s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1415/2224 [2:05:33<1:08:54,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.3026, 'grad_norm': 0.9134552795255427, 'learning_rate': 3.0874238681277246e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1415/2224 [2:05:33<1:08:54,  5.11s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1416/2224 [2:05:38<1:08:40,  5.10s/it]
0:                                                        
0: 
0: {'loss': 0.3163, 'grad_norm': 0.892035905657156, 'learning_rate': 3.080697405835021e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1416/2224 [2:05:38<1:08:40,  5.10s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1417/2224 [2:05:43<1:08:43,  5.11s/it]
0:                                                        
0: 
0: {'loss': 0.2869, 'grad_norm': 0.8460698987010589, 'learning_rate': 3.0739750149334734e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1417/2224 [2:05:43<1:08:43,  5.11s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1418/2224 [2:05:48<1:08:16,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2989, 'grad_norm': 0.8916304204640516, 'learning_rate': 3.0672567096831963e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1418/2224 [2:05:48<1:08:16,  5.08s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1419/2224 [2:05:53<1:08:11,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3134, 'grad_norm': 0.8632304976890334, 'learning_rate': 3.060542504335644e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1419/2224 [2:05:53<1:08:11,  5.08s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1420/2224 [2:05:58<1:07:55,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3083, 'grad_norm': 0.8645700401300518, 'learning_rate': 3.053832413133573e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1420/2224 [2:05:58<1:07:55,  5.07s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1421/2224 [2:06:03<1:08:01,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.3118, 'grad_norm': 0.8581209969398418, 'learning_rate': 3.0471264503110077e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1421/2224 [2:06:03<1:08:01,  5.08s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1422/2224 [2:06:08<1:07:06,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.285, 'grad_norm': 0.8372996155639648, 'learning_rate': 3.040424630093219e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1422/2224 [2:06:08<1:07:06,  5.02s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1423/2224 [2:06:13<1:06:12,  4.96s/it]
0:                                                        
0: 
0: {'loss': 0.3359, 'grad_norm': 0.8994770094705787, 'learning_rate': 3.033726966696692e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1423/2224 [2:06:13<1:06:12,  4.96s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1424/2224 [2:06:18<1:06:20,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.295, 'grad_norm': 0.8797638122388175, 'learning_rate': 3.0270334743290876e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1424/2224 [2:06:18<1:06:20,  4.98s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1425/2224 [2:06:23<1:06:25,  4.99s/it]
0:                                                        
0: 
0: {'loss': 0.273, 'grad_norm': 0.8738140592997642, 'learning_rate': 3.0203441671892233e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1425/2224 [2:06:23<1:06:25,  4.99s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1426/2224 [2:06:28<1:06:29,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2925, 'grad_norm': 0.8824517920216264, 'learning_rate': 3.013659059467039e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1426/2224 [2:06:28<1:06:29,  5.00s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1427/2224 [2:06:33<1:06:05,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.303, 'grad_norm': 0.9143655278865478, 'learning_rate': 3.0069781653435647e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1427/2224 [2:06:33<1:06:05,  4.98s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1428/2224 [2:06:38<1:06:20,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3066, 'grad_norm': 0.8542366771524876, 'learning_rate': 3.0003014989908918e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1428/2224 [2:06:38<1:06:20,  5.00s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1429/2224 [2:06:43<1:06:29,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2917, 'grad_norm': 0.8742392117912856, 'learning_rate': 2.9936290745721443e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1429/2224 [2:06:43<1:06:29,  5.02s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1430/2224 [2:06:48<1:06:27,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2858, 'grad_norm': 0.8733277326794424, 'learning_rate': 2.9869609062414508e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1430/2224 [2:06:48<1:06:27,  5.02s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1431/2224 [2:06:53<1:06:34,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2834, 'grad_norm': 0.8886214895301654, 'learning_rate': 2.980297008143902e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1431/2224 [2:06:53<1:06:34,  5.04s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1432/2224 [2:06:58<1:05:58,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3047, 'grad_norm': 0.8692672006581571, 'learning_rate': 2.9736373944155418e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1432/2224 [2:06:58<1:05:58,  5.00s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1433/2224 [2:07:03<1:05:38,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.3097, 'grad_norm': 0.8773631157987776, 'learning_rate': 2.966982079183319e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1433/2224 [2:07:03<1:05:38,  4.98s/it]
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1434/2224 [2:07:08<1:05:30,  4.98s/it]
0:                                                        
0: 
0: {'loss': 0.329, 'grad_norm': 0.8825103121168955, 'learning_rate': 2.960331076565065e-06, 'epoch': 0.64}
0: 
0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1434/2224 [2:07:08<1:05:30,  4.98s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1435/2224 [2:07:13<1:05:54,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.3124, 'grad_norm': 0.8546463691856626, 'learning_rate': 2.953684400669464e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1435/2224 [2:07:13<1:05:54,  5.01s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1436/2224 [2:07:18<1:05:53,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2997, 'grad_norm': 0.9071770236890382, 'learning_rate': 2.947042065596023e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1436/2224 [2:07:18<1:05:53,  5.02s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1437/2224 [2:07:23<1:06:05,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2656, 'grad_norm': 0.8379786845449085, 'learning_rate': 2.9404040854350363e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1437/2224 [2:07:23<1:06:05,  5.04s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1438/2224 [2:07:28<1:06:11,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2908, 'grad_norm': 0.8665307950438652, 'learning_rate': 2.933770474267565e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1438/2224 [2:07:28<1:06:11,  5.05s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1439/2224 [2:07:33<1:05:47,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3037, 'grad_norm': 0.8725002493846445, 'learning_rate': 2.9271412461654e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1439/2224 [2:07:33<1:05:47,  5.03s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2224 [2:07:39<1:05:54,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.277, 'grad_norm': 0.8776257006315249, 'learning_rate': 2.9205164151910358e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2224 [2:07:39<1:05:54,  5.04s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1441/2224 [2:07:44<1:05:42,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2621, 'grad_norm': 0.8354791120435453, 'learning_rate': 2.9138959953976393e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1441/2224 [2:07:44<1:05:42,  5.04s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1442/2224 [2:07:49<1:05:38,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3288, 'grad_norm': 0.8920612365309202, 'learning_rate': 2.9072800008290165e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1442/2224 [2:07:49<1:05:38,  5.04s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1443/2224 [2:07:54<1:05:43,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3228, 'grad_norm': 0.9305432183370306, 'learning_rate': 2.9006684455195906e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1443/2224 [2:07:54<1:05:43,  5.05s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1444/2224 [2:07:59<1:05:36,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2909, 'grad_norm': 0.8653344255355824, 'learning_rate': 2.894061343494366e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1444/2224 [2:07:59<1:05:36,  5.05s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1445/2224 [2:08:04<1:05:21,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3244, 'grad_norm': 0.9384154526379779, 'learning_rate': 2.887458708768901e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1445/2224 [2:08:04<1:05:21,  5.03s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1446/2224 [2:08:09<1:05:11,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3347, 'grad_norm': 0.9269103048984807, 'learning_rate': 2.8808605553492787e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1446/2224 [2:08:09<1:05:11,  5.03s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1447/2224 [2:08:14<1:05:03,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3038, 'grad_norm': 0.8738588187203252, 'learning_rate': 2.8742668972320708e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1447/2224 [2:08:14<1:05:03,  5.02s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1448/2224 [2:08:19<1:04:50,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.291, 'grad_norm': 0.8852548620399824, 'learning_rate': 2.8676777484043196e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1448/2224 [2:08:19<1:04:50,  5.01s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1449/2224 [2:08:24<1:04:46,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2945, 'grad_norm': 0.888706549100002, 'learning_rate': 2.8610931228435e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1449/2224 [2:08:24<1:04:46,  5.02s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1450/2224 [2:08:29<1:04:49,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2818, 'grad_norm': 0.8919561940450038, 'learning_rate': 2.8545130345174863e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1450/2224 [2:08:29<1:04:49,  5.03s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1451/2224 [2:08:34<1:04:47,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2802, 'grad_norm': 0.8578414692506192, 'learning_rate': 2.8479374973845375e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1451/2224 [2:08:34<1:04:47,  5.03s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1452/2224 [2:08:39<1:04:23,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3003, 'grad_norm': 0.8823928469765628, 'learning_rate': 2.84136652539325e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1452/2224 [2:08:39<1:04:23,  5.00s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1453/2224 [2:08:44<1:03:51,  4.97s/it]
0:                                                        
0: 
0: {'loss': 0.3048, 'grad_norm': 0.8770591187610234, 'learning_rate': 2.834800132482544e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1453/2224 [2:08:44<1:03:51,  4.97s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1454/2224 [2:08:49<1:04:09,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.3182, 'grad_norm': 0.8968855310062493, 'learning_rate': 2.8282383325816165e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1454/2224 [2:08:49<1:04:09,  5.00s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1455/2224 [2:08:54<1:04:20,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2958, 'grad_norm': 0.8761701738270569, 'learning_rate': 2.8216811396099297e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1455/2224 [2:08:54<1:04:20,  5.02s/it]
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1456/2224 [2:08:59<1:04:01,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2858, 'grad_norm': 0.8691420033794885, 'learning_rate': 2.8151285674771687e-06, 'epoch': 0.65}
0: 
0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1456/2224 [2:08:59<1:04:01,  5.00s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1457/2224 [2:09:04<1:04:10,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.291, 'grad_norm': 0.8980121400098919, 'learning_rate': 2.8085806300832192e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1457/2224 [2:09:04<1:04:10,  5.02s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1458/2224 [2:09:09<1:04:09,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8614320637657535, 'learning_rate': 2.8020373413181336e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1458/2224 [2:09:09<1:04:09,  5.03s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1459/2224 [2:09:14<1:04:21,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3056, 'grad_norm': 0.8480081078759358, 'learning_rate': 2.7954987150621047e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1459/2224 [2:09:14<1:04:21,  5.05s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1460/2224 [2:09:19<1:04:28,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.291, 'grad_norm': 0.8684237015110815, 'learning_rate': 2.788964765185431e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1460/2224 [2:09:19<1:04:28,  5.06s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1461/2224 [2:09:24<1:04:26,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3013, 'grad_norm': 0.8603434226777374, 'learning_rate': 2.7824355055484964e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1461/2224 [2:09:24<1:04:26,  5.07s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1462/2224 [2:09:29<1:04:03,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2845, 'grad_norm': 0.8572206940760578, 'learning_rate': 2.7759109500017306e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1462/2224 [2:09:29<1:04:03,  5.04s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1463/2224 [2:09:34<1:04:04,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.308, 'grad_norm': 0.8824801035713691, 'learning_rate': 2.7693911123855898e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1463/2224 [2:09:34<1:04:04,  5.05s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1464/2224 [2:09:39<1:03:59,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2861, 'grad_norm': 0.8599374796752717, 'learning_rate': 2.762876006530516e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1464/2224 [2:09:39<1:03:59,  5.05s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1465/2224 [2:09:44<1:03:48,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2702, 'grad_norm': 0.8832100098274983, 'learning_rate': 2.756365646256918e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1465/2224 [2:09:44<1:03:48,  5.04s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1466/2224 [2:09:49<1:03:41,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2881, 'grad_norm': 0.8904682934510219, 'learning_rate': 2.749860045375139e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1466/2224 [2:09:49<1:03:41,  5.04s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1467/2224 [2:09:54<1:03:41,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.285, 'grad_norm': 0.8838057473009197, 'learning_rate': 2.7433592176854195e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1467/2224 [2:09:54<1:03:41,  5.05s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1468/2224 [2:09:59<1:03:07,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2756, 'grad_norm': 0.8584109082848456, 'learning_rate': 2.736863176977882e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1468/2224 [2:09:59<1:03:07,  5.01s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1469/2224 [2:10:04<1:03:10,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2825, 'grad_norm': 0.8623830511752545, 'learning_rate': 2.730371937032492e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1469/2224 [2:10:04<1:03:10,  5.02s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2224 [2:10:09<1:02:46,  5.00s/it]
0:                                                        
0: 
0: {'loss': 0.2969, 'grad_norm': 0.8779016444796529, 'learning_rate': 2.723885511619029e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2224 [2:10:09<1:02:46,  5.00s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1471/2224 [2:10:14<1:02:50,  5.01s/it]
0:                                                        
0: 
0: {'loss': 0.2935, 'grad_norm': 0.915151524108368, 'learning_rate': 2.7174039144970614e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1471/2224 [2:10:14<1:02:50,  5.01s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1472/2224 [2:10:19<1:02:58,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8757259501002281, 'learning_rate': 2.7109271594159157e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1472/2224 [2:10:19<1:02:58,  5.03s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1473/2224 [2:10:24<1:03:03,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2946, 'grad_norm': 0.8918390609854248, 'learning_rate': 2.704455260114648e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1473/2224 [2:10:24<1:03:03,  5.04s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1474/2224 [2:10:29<1:02:47,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3069, 'grad_norm': 0.8570009906034608, 'learning_rate': 2.69798823032201e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1474/2224 [2:10:29<1:02:47,  5.02s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1475/2224 [2:10:34<1:02:44,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2922, 'grad_norm': 0.8726236882612962, 'learning_rate': 2.691526083756428e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1475/2224 [2:10:34<1:02:44,  5.03s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1476/2224 [2:10:40<1:02:48,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3144, 'grad_norm': 0.8552203754908924, 'learning_rate': 2.68506883412597e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1476/2224 [2:10:40<1:02:48,  5.04s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1477/2224 [2:10:45<1:02:39,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2893, 'grad_norm': 0.8342589529792277, 'learning_rate': 2.678616495128309e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1477/2224 [2:10:45<1:02:39,  5.03s/it]
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1478/2224 [2:10:50<1:02:35,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2739, 'grad_norm': 0.8459928973119561, 'learning_rate': 2.672169080450709e-06, 'epoch': 0.66}
0: 
0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1478/2224 [2:10:50<1:02:35,  5.03s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1479/2224 [2:10:55<1:02:46,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.298, 'grad_norm': 0.8751878102948458, 'learning_rate': 2.665726603769987e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1479/2224 [2:10:55<1:02:46,  5.06s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1480/2224 [2:11:00<1:02:55,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2937, 'grad_norm': 0.8718547533503743, 'learning_rate': 2.6592890787524796e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1480/2224 [2:11:00<1:02:55,  5.08s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1481/2224 [2:11:05<1:02:39,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3054, 'grad_norm': 0.8965541025043352, 'learning_rate': 2.6528565190540246e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1481/2224 [2:11:05<1:02:39,  5.06s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1482/2224 [2:11:10<1:02:32,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2803, 'grad_norm': 0.8575390324161624, 'learning_rate': 2.646428938319928e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1482/2224 [2:11:10<1:02:32,  5.06s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1483/2224 [2:11:15<1:02:34,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.2726, 'grad_norm': 0.8509924967374759, 'learning_rate': 2.640006350184928e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1483/2224 [2:11:15<1:02:34,  5.07s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1484/2224 [2:11:20<1:02:35,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2838, 'grad_norm': 0.8588982329921759, 'learning_rate': 2.6335887682731774e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1484/2224 [2:11:20<1:02:35,  5.08s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1485/2224 [2:11:25<1:02:26,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.2889, 'grad_norm': 0.8947502910959585, 'learning_rate': 2.6271762061982086e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1485/2224 [2:11:25<1:02:26,  5.07s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1486/2224 [2:11:30<1:02:25,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.3024, 'grad_norm': 0.8541344140217957, 'learning_rate': 2.6207686775629027e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1486/2224 [2:11:30<1:02:25,  5.07s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1487/2224 [2:11:35<1:02:25,  5.08s/it]
0:                                                        
0: 
0: {'loss': 0.2899, 'grad_norm': 0.8915334625719357, 'learning_rate': 2.614366195959466e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1487/2224 [2:11:35<1:02:25,  5.08s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1488/2224 [2:11:40<1:02:03,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.291, 'grad_norm': 0.8745222602756032, 'learning_rate': 2.607968774969399e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1488/2224 [2:11:40<1:02:03,  5.06s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1489/2224 [2:11:45<1:01:46,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3166, 'grad_norm': 0.9145435305100003, 'learning_rate': 2.6015764281634666e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1489/2224 [2:11:45<1:01:46,  5.04s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1490/2224 [2:11:50<1:01:42,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2873, 'grad_norm': 0.8700282305301706, 'learning_rate': 2.5951891691016657e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1490/2224 [2:11:50<1:01:42,  5.04s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1491/2224 [2:11:55<1:01:20,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3024, 'grad_norm': 0.9186250049004573, 'learning_rate': 2.5888070113332065e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1491/2224 [2:11:55<1:01:20,  5.02s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1492/2224 [2:12:00<1:01:22,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2716, 'grad_norm': 0.8364167227271725, 'learning_rate': 2.582429968396478e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1492/2224 [2:12:00<1:01:22,  5.03s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1493/2224 [2:12:05<1:01:31,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.3055, 'grad_norm': 0.8884695248096299, 'learning_rate': 2.576058053819012e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1493/2224 [2:12:05<1:01:31,  5.05s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1494/2224 [2:12:11<1:01:21,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3028, 'grad_norm': 0.9471968371503466, 'learning_rate': 2.569691281117469e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1494/2224 [2:12:11<1:01:21,  5.04s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1495/2224 [2:12:16<1:01:18,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2714, 'grad_norm': 0.8663994530084074, 'learning_rate': 2.563329663797602e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1495/2224 [2:12:16<1:01:18,  5.05s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1496/2224 [2:12:21<1:01:06,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2884, 'grad_norm': 0.8803589416795636, 'learning_rate': 2.5569732153542205e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1496/2224 [2:12:21<1:01:06,  5.04s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1497/2224 [2:12:26<1:01:12,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2984, 'grad_norm': 0.8557265800195684, 'learning_rate': 2.550621949271177e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1497/2224 [2:12:26<1:01:12,  5.05s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1498/2224 [2:12:31<1:01:01,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.319, 'grad_norm': 0.9023367077285447, 'learning_rate': 2.5442758790213272e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1498/2224 [2:12:31<1:01:01,  5.04s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1499/2224 [2:12:36<1:01:12,  5.07s/it]
0:                                                        
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8467845950413014, 'learning_rate': 2.5379350180665074e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1499/2224 [2:12:36<1:01:12,  5.07s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2224 [2:12:41<1:01:04,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.3028, 'grad_norm': 0.8780700856062303, 'learning_rate': 2.531599379857501e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2224 [2:12:41<1:01:04,  5.06s/it]
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1501/2224 [2:12:46<1:00:57,  5.06s/it]
0:                                                        
0: 
0: {'loss': 0.2961, 'grad_norm': 0.8802860409186949, 'learning_rate': 2.5252689778340134e-06, 'epoch': 0.67}
0: 
0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1501/2224 [2:12:46<1:00:57,  5.06s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1502/2224 [2:12:51<1:00:44,  5.05s/it]
0:                                                        
0: 
0: {'loss': 0.2997, 'grad_norm': 0.9328453723305208, 'learning_rate': 2.5189438254246436e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1502/2224 [2:12:51<1:00:44,  5.05s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1503/2224 [2:12:56<1:00:25,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.3005, 'grad_norm': 0.852285031480286, 'learning_rate': 2.5126239360468514e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1503/2224 [2:12:56<1:00:25,  5.03s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1504/2224 [2:13:01<1:00:13,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.2818, 'grad_norm': 0.8821851900945697, 'learning_rate': 2.5063093231069346e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1504/2224 [2:13:01<1:00:13,  5.02s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1505/2224 [2:13:06<1:00:09,  5.02s/it]
0:                                                        
0: 
0: {'loss': 0.3092, 'grad_norm': 0.8936841391367407, 'learning_rate': 2.5000000000000015e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1505/2224 [2:13:06<1:00:09,  5.02s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1506/2224 [2:13:11<1:00:08,  5.03s/it]
0:                                                        
0: 
0: {'loss': 0.2941, 'grad_norm': 0.8738010803060094, 'learning_rate': 2.4936959801099302e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1506/2224 [2:13:11<1:00:08,  5.03s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1507/2224 [2:13:16<1:00:16,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.3103, 'grad_norm': 0.8682875557961965, 'learning_rate': 2.4873972768093573e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1507/2224 [2:13:16<1:00:16,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1508/2224 [2:13:21<1:00:08,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2877, 'grad_norm': 0.8489665318278288, 'learning_rate': 2.481103903459641e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1508/2224 [2:13:21<1:00:08,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1509/2224 [2:13:26<1:00:04,  5.04s/it]
0:                                                        
0: 
0: {'loss': 0.2869, 'grad_norm': 0.8801770433969216, 'learning_rate': 2.4748158734108253e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1509/2224 [2:13:26<1:00:04,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1510/2224 [2:13:31<59:53,  5.03s/it]  
0:                                                      
0: 
0: {'loss': 0.2951, 'grad_norm': 0.9063164206097416, 'learning_rate': 2.4685332000016283e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1510/2224 [2:13:31<59:53,  5.03s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1511/2224 [2:13:36<59:52,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2628, 'grad_norm': 0.8799708673775073, 'learning_rate': 2.462255896559402e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1511/2224 [2:13:36<59:52,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1512/2224 [2:13:41<59:49,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3178, 'grad_norm': 0.8711190600656961, 'learning_rate': 2.455983976400106e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1512/2224 [2:13:41<59:49,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1513/2224 [2:13:46<59:53,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3061, 'grad_norm': 0.8874591779711333, 'learning_rate': 2.449717452828282e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1513/2224 [2:13:46<59:53,  5.05s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1514/2224 [2:13:51<59:39,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2952, 'grad_norm': 0.8741304232755379, 'learning_rate': 2.443456339137023e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1514/2224 [2:13:51<59:39,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1515/2224 [2:13:56<59:48,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3023, 'grad_norm': 0.9080061336074714, 'learning_rate': 2.437200648607949e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1515/2224 [2:13:56<59:48,  5.06s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1516/2224 [2:14:01<59:33,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2789, 'grad_norm': 0.8720558984956019, 'learning_rate': 2.4309503945111695e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1516/2224 [2:14:01<59:33,  5.05s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1517/2224 [2:14:07<59:52,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2875, 'grad_norm': 0.8395484562974076, 'learning_rate': 2.4247055901052667e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1517/2224 [2:14:07<59:52,  5.08s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1518/2224 [2:14:12<59:35,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2955, 'grad_norm': 0.9497993158827588, 'learning_rate': 2.4184662486372646e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1518/2224 [2:14:12<59:35,  5.06s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1519/2224 [2:14:17<59:31,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.332, 'grad_norm': 0.8998307440068034, 'learning_rate': 2.4122323833425907e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1519/2224 [2:14:17<59:31,  5.07s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1520/2224 [2:14:22<59:00,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3128, 'grad_norm': 0.8453657652426704, 'learning_rate': 2.406004007445063e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1520/2224 [2:14:22<59:00,  5.03s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1521/2224 [2:14:27<59:03,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.288, 'grad_norm': 0.8767424830201421, 'learning_rate': 2.399781134156855e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1521/2224 [2:14:27<59:03,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1522/2224 [2:14:32<58:56,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3211, 'grad_norm': 0.917665742156773, 'learning_rate': 2.3935637766784613e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1522/2224 [2:14:32<58:56,  5.04s/it]
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1523/2224 [2:14:37<58:37,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2844, 'grad_norm': 0.8515036998977558, 'learning_rate': 2.387351948198682e-06, 'epoch': 0.68}
0: 
0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1523/2224 [2:14:37<58:37,  5.02s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1524/2224 [2:14:42<58:37,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3193, 'grad_norm': 0.8960267812057677, 'learning_rate': 2.381145661894586e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1524/2224 [2:14:42<58:37,  5.02s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1525/2224 [2:14:47<59:20,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.277, 'grad_norm': 0.8526432506300298, 'learning_rate': 2.3749449309314885e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1525/2224 [2:14:47<59:20,  5.09s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1526/2224 [2:14:52<59:07,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2992, 'grad_norm': 0.8538298644178741, 'learning_rate': 2.3687497684629145e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1526/2224 [2:14:52<59:07,  5.08s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1527/2224 [2:14:57<58:56,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2887, 'grad_norm': 0.8481187429933972, 'learning_rate': 2.3625601876305793e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1527/2224 [2:14:57<58:56,  5.07s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1528/2224 [2:15:02<58:44,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2956, 'grad_norm': 0.8791530351576556, 'learning_rate': 2.3563762015643645e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1528/2224 [2:15:02<58:44,  5.06s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1529/2224 [2:15:07<58:12,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2786, 'grad_norm': 0.8535692119900367, 'learning_rate': 2.350197823382272e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1529/2224 [2:15:07<58:12,  5.03s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1530/2224 [2:15:12<58:22,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.286, 'grad_norm': 0.8486961707752555, 'learning_rate': 2.3440250661904164e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1530/2224 [2:15:12<58:22,  5.05s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1531/2224 [2:15:17<58:19,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2892, 'grad_norm': 0.8769299716287747, 'learning_rate': 2.3378579430829856e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1531/2224 [2:15:17<58:19,  5.05s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1532/2224 [2:15:22<57:30,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.3206, 'grad_norm': 0.8692305637919938, 'learning_rate': 2.3316964671422148e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1532/2224 [2:15:22<57:30,  4.99s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1533/2224 [2:15:27<57:34,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2921, 'grad_norm': 0.859605320812476, 'learning_rate': 2.325540651438361e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1533/2224 [2:15:27<57:34,  5.00s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1534/2224 [2:15:32<57:30,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2969, 'grad_norm': 0.8684123125119828, 'learning_rate': 2.3193905090296783e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1534/2224 [2:15:32<57:30,  5.00s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1535/2224 [2:15:37<57:37,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.308, 'grad_norm': 0.8949622662411086, 'learning_rate': 2.3132460529623767e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1535/2224 [2:15:37<57:37,  5.02s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1536/2224 [2:15:42<57:38,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2919, 'grad_norm': 0.8812161588561525, 'learning_rate': 2.3071072962706127e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1536/2224 [2:15:42<57:38,  5.03s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1537/2224 [2:15:47<57:47,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3006, 'grad_norm': 0.8764174987188701, 'learning_rate': 2.300974251976448e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1537/2224 [2:15:47<57:47,  5.05s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1538/2224 [2:15:52<57:48,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3091, 'grad_norm': 0.8936216452305858, 'learning_rate': 2.2948469330898316e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1538/2224 [2:15:52<57:48,  5.06s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1539/2224 [2:15:57<57:18,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3022, 'grad_norm': 0.8464107214833075, 'learning_rate': 2.288725352608559e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1539/2224 [2:15:57<57:18,  5.02s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1540/2224 [2:16:02<57:18,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2644, 'grad_norm': 0.897110406120948, 'learning_rate': 2.2826095235182604e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1540/2224 [2:16:02<57:18,  5.03s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1541/2224 [2:16:07<57:13,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.8878816514504547, 'learning_rate': 2.276499458792363e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1541/2224 [2:16:07<57:13,  5.03s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1542/2224 [2:16:13<57:19,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.288, 'grad_norm': 0.8527089948997483, 'learning_rate': 2.270395171392065e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1542/2224 [2:16:13<57:19,  5.04s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1543/2224 [2:16:18<57:23,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2621, 'grad_norm': 0.8758937024860088, 'learning_rate': 2.2642966742663115e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1543/2224 [2:16:18<57:23,  5.06s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1544/2224 [2:16:23<57:21,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2784, 'grad_norm': 0.9081711019819633, 'learning_rate': 2.2582039803517646e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1544/2224 [2:16:23<57:21,  5.06s/it]
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1545/2224 [2:16:28<57:30,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2693, 'grad_norm': 0.9106403313437881, 'learning_rate': 2.252117102572773e-06, 'epoch': 0.69}
0: 
0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1545/2224 [2:16:28<57:30,  5.08s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1546/2224 [2:16:33<57:17,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2992, 'grad_norm': 0.8713850717232068, 'learning_rate': 2.2460360538413493e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1546/2224 [2:16:33<57:17,  5.07s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1547/2224 [2:16:38<57:01,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2949, 'grad_norm': 0.8982322577860817, 'learning_rate': 2.2399608470571456e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1547/2224 [2:16:38<57:01,  5.05s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1548/2224 [2:16:43<57:01,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2919, 'grad_norm': 0.9030842527264392, 'learning_rate': 2.233891495107413e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1548/2224 [2:16:43<57:01,  5.06s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1549/2224 [2:16:48<56:45,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3399, 'grad_norm': 0.9158790834751983, 'learning_rate': 2.2278280108669885e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1549/2224 [2:16:48<56:45,  5.04s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1550/2224 [2:16:53<56:38,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.314, 'grad_norm': 0.8939598963970449, 'learning_rate': 2.221770407198262e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1550/2224 [2:16:53<56:38,  5.04s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1551/2224 [2:16:58<56:26,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3067, 'grad_norm': 0.8949030277689678, 'learning_rate': 2.2157186969511497e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1551/2224 [2:16:58<56:26,  5.03s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1552/2224 [2:17:03<56:27,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.29, 'grad_norm': 0.8618121976847132, 'learning_rate': 2.2096728929630596e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1552/2224 [2:17:03<56:27,  5.04s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1553/2224 [2:17:08<56:36,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2655, 'grad_norm': 0.8351014110073304, 'learning_rate': 2.203633008058878e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1553/2224 [2:17:08<56:36,  5.06s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1554/2224 [2:17:13<56:36,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.3056, 'grad_norm': 0.8688113002359932, 'learning_rate': 2.197599055050933e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1554/2224 [2:17:13<56:36,  5.07s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1555/2224 [2:17:18<56:28,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2798, 'grad_norm': 0.8480711562447868, 'learning_rate': 2.191571046738969e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1555/2224 [2:17:18<56:28,  5.07s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1556/2224 [2:17:23<56:33,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2579, 'grad_norm': 0.8233085651423033, 'learning_rate': 2.1855489959101196e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1556/2224 [2:17:23<56:33,  5.08s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1557/2224 [2:17:28<55:55,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2949, 'grad_norm': 0.8970789641695758, 'learning_rate': 2.179532915338883e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1557/2224 [2:17:28<55:55,  5.03s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1558/2224 [2:17:33<55:56,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3049, 'grad_norm': 0.8794109982655967, 'learning_rate': 2.1735228177870894e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1558/2224 [2:17:33<55:56,  5.04s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1559/2224 [2:17:38<55:55,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2642, 'grad_norm': 0.8382303466631406, 'learning_rate': 2.167518716003879e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1559/2224 [2:17:38<55:55,  5.05s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1560/2224 [2:17:44<55:52,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.27, 'grad_norm': 0.8536192587983349, 'learning_rate': 2.1615206227256756e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1560/2224 [2:17:44<55:52,  5.05s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1561/2224 [2:17:48<55:26,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2641, 'grad_norm': 0.8636115773221071, 'learning_rate': 2.155528550676152e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1561/2224 [2:17:48<55:26,  5.02s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1562/2224 [2:17:53<55:05,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2775, 'grad_norm': 0.8870454022430204, 'learning_rate': 2.1495425125662123e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1562/2224 [2:17:53<55:05,  4.99s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1563/2224 [2:17:58<55:08,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2914, 'grad_norm': 0.8768099539786852, 'learning_rate': 2.1435625210939597e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1563/2224 [2:17:58<55:08,  5.01s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1564/2224 [2:18:03<55:17,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2899, 'grad_norm': 0.8760806972078602, 'learning_rate': 2.1375885889446736e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1564/2224 [2:18:04<55:17,  5.03s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1565/2224 [2:18:09<55:28,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2867, 'grad_norm': 0.8713043897772066, 'learning_rate': 2.1316207287907725e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1565/2224 [2:18:09<55:28,  5.05s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1566/2224 [2:18:14<55:33,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2966, 'grad_norm': 0.8656703906628255, 'learning_rate': 2.1256589532918015e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1566/2224 [2:18:14<55:33,  5.07s/it]
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1567/2224 [2:18:19<55:22,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2979, 'grad_norm': 0.8920591157137171, 'learning_rate': 2.119703275094396e-06, 'epoch': 0.7}
0: 
0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1567/2224 [2:18:19<55:22,  5.06s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1568/2224 [2:18:24<55:10,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2935, 'grad_norm': 0.8748661133918736, 'learning_rate': 2.113753706832257e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1568/2224 [2:18:24<55:10,  5.05s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1569/2224 [2:18:29<55:07,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8864119760549108, 'learning_rate': 2.1078102611261234e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1569/2224 [2:18:29<55:07,  5.05s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1570/2224 [2:18:34<54:46,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2835, 'grad_norm': 0.8944745341812267, 'learning_rate': 2.101872950583752e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1570/2224 [2:18:34<54:46,  5.02s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1571/2224 [2:18:39<54:53,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2736, 'grad_norm': 0.8368390488631015, 'learning_rate': 2.0959417877998763e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1571/2224 [2:18:39<54:53,  5.04s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1572/2224 [2:18:44<54:19,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2908, 'grad_norm': 0.8657840563838816, 'learning_rate': 2.090016785356195e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1572/2224 [2:18:44<54:19,  5.00s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1573/2224 [2:18:49<54:48,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2969, 'grad_norm': 0.8894020915672726, 'learning_rate': 2.0840979558213386e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1573/2224 [2:18:49<54:48,  5.05s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1574/2224 [2:18:54<54:51,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.29, 'grad_norm': 0.8911831549011258, 'learning_rate': 2.078185311750839e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1574/2224 [2:18:54<54:51,  5.06s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1575/2224 [2:18:59<54:52,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2925, 'grad_norm': 0.8644676653389982, 'learning_rate': 2.0722788656871114e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1575/2224 [2:18:59<54:52,  5.07s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1576/2224 [2:19:04<54:53,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.3245, 'grad_norm': 0.9168084021924586, 'learning_rate': 2.066378630159421e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1576/2224 [2:19:04<54:53,  5.08s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1577/2224 [2:19:09<54:44,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.3129, 'grad_norm': 0.8696103228942956, 'learning_rate': 2.060484617683862e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1577/2224 [2:19:09<54:44,  5.08s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1578/2224 [2:19:14<54:34,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2957, 'grad_norm': 0.8674790576230679, 'learning_rate': 2.054596840763319e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1578/2224 [2:19:14<54:34,  5.07s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1579/2224 [2:19:19<54:32,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.3331, 'grad_norm': 0.8711959225332928, 'learning_rate': 2.0487153118874603e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1579/2224 [2:19:19<54:32,  5.07s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2224 [2:19:25<54:36,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.3058, 'grad_norm': 0.9085286738225646, 'learning_rate': 2.0428400435326946e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2224 [2:19:25<54:36,  5.09s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1581/2224 [2:19:30<54:22,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.3254, 'grad_norm': 0.8930763414916554, 'learning_rate': 2.0369710481621464e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1581/2224 [2:19:30<54:22,  5.07s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1582/2224 [2:19:35<54:20,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.3314, 'grad_norm': 0.9185253203743431, 'learning_rate': 2.0311083382256423e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1582/2224 [2:19:35<54:20,  5.08s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1583/2224 [2:19:40<54:10,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2828, 'grad_norm': 0.8781684353803536, 'learning_rate': 2.0252519261596727e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1583/2224 [2:19:40<54:10,  5.07s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1584/2224 [2:19:45<53:36,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2785, 'grad_norm': 0.8797214932825871, 'learning_rate': 2.0194018243873612e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1584/2224 [2:19:45<53:36,  5.03s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1585/2224 [2:19:50<53:31,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2807, 'grad_norm': 0.8590781452709495, 'learning_rate': 2.0135580453184546e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1585/2224 [2:19:50<53:31,  5.03s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1586/2224 [2:19:55<53:32,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2872, 'grad_norm': 0.8609609843569732, 'learning_rate': 2.0077206013492833e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1586/2224 [2:19:55<53:32,  5.04s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1587/2224 [2:20:00<53:28,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2886, 'grad_norm': 0.8685459690877517, 'learning_rate': 2.0018895048627423e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1587/2224 [2:20:00<53:28,  5.04s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1588/2224 [2:20:05<53:07,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2872, 'grad_norm': 0.8797744202071767, 'learning_rate': 1.9960647682282546e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1588/2224 [2:20:05<53:07,  5.01s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1589/2224 [2:20:10<53:16,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3097, 'grad_norm': 0.9046052632930219, 'learning_rate': 1.9902464038017594e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1589/2224 [2:20:10<53:16,  5.03s/it]
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1590/2224 [2:20:15<53:07,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3343, 'grad_norm': 0.9031310403205566, 'learning_rate': 1.984434423925678e-06, 'epoch': 0.71}
0: 
0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1590/2224 [2:20:15<53:07,  5.03s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1591/2224 [2:20:20<53:01,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2836, 'grad_norm': 0.8411707471517622, 'learning_rate': 1.9786288409288817e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1591/2224 [2:20:20<53:01,  5.03s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1592/2224 [2:20:25<52:57,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2962, 'grad_norm': 0.8858870594378948, 'learning_rate': 1.9728296671266794e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1592/2224 [2:20:25<52:57,  5.03s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1593/2224 [2:20:30<53:03,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3036, 'grad_norm': 0.8638938205604215, 'learning_rate': 1.9670369148207837e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1593/2224 [2:20:30<53:03,  5.04s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1594/2224 [2:20:35<52:54,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3114, 'grad_norm': 0.9055911266410096, 'learning_rate': 1.9612505962992785e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1594/2224 [2:20:35<52:54,  5.04s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1595/2224 [2:20:40<52:56,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3104, 'grad_norm': 0.9030398470089067, 'learning_rate': 1.955470723836607e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1595/2224 [2:20:40<52:56,  5.05s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1596/2224 [2:20:45<52:55,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2637, 'grad_norm': 0.8571397740758696, 'learning_rate': 1.9496973096935366e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1596/2224 [2:20:45<52:55,  5.06s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1597/2224 [2:20:50<52:48,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.322, 'grad_norm': 0.917037037994818, 'learning_rate': 1.9439303661171336e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1597/2224 [2:20:50<52:48,  5.05s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1598/2224 [2:20:55<52:42,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.306, 'grad_norm': 0.8691785795057082, 'learning_rate': 1.938169905340739e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1598/2224 [2:20:55<52:42,  5.05s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1599/2224 [2:21:00<52:35,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2764, 'grad_norm': 0.8315160586309238, 'learning_rate': 1.9324159395839422e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1599/2224 [2:21:00<52:35,  5.05s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1600/2224 [2:21:05<52:48,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2888, 'grad_norm': 0.8635698277090487, 'learning_rate': 1.926668481052556e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1600/2224 [2:21:05<52:48,  5.08s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1601/2224 [2:22:08<3:52:51, 22.43s/it]
0:                                                        
0: 
0: {'loss': 0.3019, 'grad_norm': 0.8879576275650088, 'learning_rate': 1.920927541938584e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1601/2224 [2:22:08<3:52:51, 22.43s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1602/2224 [2:22:13<2:58:20, 17.20s/it]
0:                                                        
0: 
0: {'loss': 0.3169, 'grad_norm': 0.8875274839707998, 'learning_rate': 1.915193134420207e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1602/2224 [2:22:13<2:58:20, 17.20s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1603/2224 [2:22:18<2:20:17, 13.56s/it]
0:                                                        
0: 
0: {'loss': 0.3233, 'grad_norm': 0.8768927168417158, 'learning_rate': 1.909465270661749e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1603/2224 [2:22:18<2:20:17, 13.56s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1604/2224 [2:22:23<1:53:46, 11.01s/it]
0:                                                        
0: 
0: {'loss': 0.3053, 'grad_norm': 0.8952172122214183, 'learning_rate': 1.9037439628136479e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1604/2224 [2:22:23<1:53:46, 11.01s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1605/2224 [2:22:29<1:35:14,  9.23s/it]
0:                                                        
0: 
0: {'loss': 0.3026, 'grad_norm': 0.8886763791251918, 'learning_rate': 1.8980292230124398e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1605/2224 [2:22:29<1:35:14,  9.23s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1606/2224 [2:22:34<1:22:04,  7.97s/it]
0:                                                        
0: 
0: {'loss': 0.2922, 'grad_norm': 0.8537146265645691, 'learning_rate': 1.8923210633807287e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1606/2224 [2:22:34<1:22:04,  7.97s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1607/2224 [2:22:39<1:12:35,  7.06s/it]
0:                                                        
0: 
0: {'loss': 0.293, 'grad_norm': 0.8663417431036033, 'learning_rate': 1.8866194960271538e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1607/2224 [2:22:39<1:12:35,  7.06s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2224 [2:22:44<1:06:13,  6.45s/it]
0:                                                        
0: 
0: {'loss': 0.2948, 'grad_norm': 0.8812384955473331, 'learning_rate': 1.8809245330463766e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2224 [2:22:44<1:06:13,  6.45s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2224 [2:22:48<1:01:32,  6.00s/it]
0:                                                        
0: 
0: {'loss': 0.2975, 'grad_norm': 0.8669263394288156, 'learning_rate': 1.8752361865190466e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2224 [2:22:49<1:01:32,  6.00s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2224 [2:22:53<58:16,  5.69s/it]  
0:                                                      
0: 
0: {'loss': 0.2984, 'grad_norm': 0.8686380122882951, 'learning_rate': 1.8695544685117778e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2224 [2:22:53<58:16,  5.69s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2224 [2:22:59<56:19,  5.51s/it]
0:                                                      
0: 
0: {'loss': 0.2743, 'grad_norm': 0.8587770626246924, 'learning_rate': 1.8638793910771236e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2224 [2:22:59<56:19,  5.51s/it]
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2224 [2:23:04<54:52,  5.38s/it]
0:                                                      
0: 
0: {'loss': 0.2692, 'grad_norm': 0.8611158094849349, 'learning_rate': 1.8582109662535496e-06, 'epoch': 0.72}
0: 
0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2224 [2:23:04<54:52,  5.38s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1613/2224 [2:23:09<53:48,  5.28s/it]
0:                                                      
0: 
0: {'loss': 0.2849, 'grad_norm': 0.8503475369098902, 'learning_rate': 1.8525492060654132e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1613/2224 [2:23:09<53:48,  5.28s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1614/2224 [2:23:14<52:37,  5.18s/it]
0:                                                      
0: 
0: {'loss': 0.2786, 'grad_norm': 0.8766079377778604, 'learning_rate': 1.846894122522927e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1614/2224 [2:23:14<52:37,  5.18s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1615/2224 [2:23:19<52:05,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.2925, 'grad_norm': 0.874670253376191, 'learning_rate': 1.8412457276221468e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1615/2224 [2:23:19<52:05,  5.13s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1616/2224 [2:23:24<51:46,  5.11s/it]
0:                                                      
0: 
0: {'loss': 0.2976, 'grad_norm': 0.8697090810259389, 'learning_rate': 1.8356040333449393e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1616/2224 [2:23:24<51:46,  5.11s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1617/2224 [2:23:29<51:37,  5.10s/it]
0:                                                      
0: 
0: {'loss': 0.3044, 'grad_norm': 0.8511841300284888, 'learning_rate': 1.8299690516589524e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1617/2224 [2:23:29<51:37,  5.10s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1618/2224 [2:23:34<51:24,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.2928, 'grad_norm': 0.8621812666451842, 'learning_rate': 1.8243407945176e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1618/2224 [2:23:34<51:24,  5.09s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1619/2224 [2:23:39<51:10,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2799, 'grad_norm': 0.8795491288891965, 'learning_rate': 1.8187192738600306e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1619/2224 [2:23:39<51:10,  5.08s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1620/2224 [2:23:44<50:51,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2762, 'grad_norm': 0.8461734738756611, 'learning_rate': 1.8131045016110987e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1620/2224 [2:23:44<50:51,  5.05s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1621/2224 [2:23:49<50:44,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2653, 'grad_norm': 0.8304045573418681, 'learning_rate': 1.807496489681348e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1621/2224 [2:23:49<50:44,  5.05s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1622/2224 [2:23:54<50:54,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2771, 'grad_norm': 0.8589165445322946, 'learning_rate': 1.801895249966979e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1622/2224 [2:23:54<50:54,  5.07s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1623/2224 [2:23:59<50:21,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2988, 'grad_norm': 0.9171256860293997, 'learning_rate': 1.7963007943498284e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1623/2224 [2:23:59<50:21,  5.03s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1624/2224 [2:24:04<50:26,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2889, 'grad_norm': 0.8747767158006313, 'learning_rate': 1.7907131346973394e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1624/2224 [2:24:04<50:26,  5.04s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1625/2224 [2:24:09<50:26,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2985, 'grad_norm': 0.8476283162766568, 'learning_rate': 1.7851322828625418e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1625/2224 [2:24:09<50:26,  5.05s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1626/2224 [2:24:14<50:12,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2891, 'grad_norm': 0.915623228985968, 'learning_rate': 1.7795582506840236e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1626/2224 [2:24:14<50:12,  5.04s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1627/2224 [2:24:19<49:43,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2867, 'grad_norm': 0.8845271774574599, 'learning_rate': 1.773991049985902e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1627/2224 [2:24:19<49:43,  5.00s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1628/2224 [2:24:24<49:56,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3027, 'grad_norm': 0.8695497957849087, 'learning_rate': 1.7684306925778072e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1628/2224 [2:24:24<49:56,  5.03s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1629/2224 [2:24:29<49:38,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2638, 'grad_norm': 0.8436851375624195, 'learning_rate': 1.7628771902548548e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1629/2224 [2:24:29<49:38,  5.01s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1630/2224 [2:24:34<49:42,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2776, 'grad_norm': 0.8469583663089463, 'learning_rate': 1.757330554797611e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1630/2224 [2:24:34<49:42,  5.02s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1631/2224 [2:24:39<49:42,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.276, 'grad_norm': 0.8938218894764091, 'learning_rate': 1.7517907979720822e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1631/2224 [2:24:39<49:42,  5.03s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1632/2224 [2:24:44<49:39,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3235, 'grad_norm': 0.9142756579608922, 'learning_rate': 1.7462579315296828e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1632/2224 [2:24:44<49:39,  5.03s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1633/2224 [2:24:49<49:31,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2587, 'grad_norm': 0.8262900476757217, 'learning_rate': 1.740731967207206e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1633/2224 [2:24:49<49:31,  5.03s/it]
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1634/2224 [2:24:54<49:26,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.286, 'grad_norm': 0.8792302765654654, 'learning_rate': 1.7352129167268078e-06, 'epoch': 0.73}
0: 
0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1634/2224 [2:24:54<49:26,  5.03s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1635/2224 [2:24:59<49:26,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2861, 'grad_norm': 0.8345079628565498, 'learning_rate': 1.729700791795977e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1635/2224 [2:24:59<49:26,  5.04s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1636/2224 [2:25:04<49:17,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2841, 'grad_norm': 0.8651808484715561, 'learning_rate': 1.7241956041075114e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1636/2224 [2:25:04<49:17,  5.03s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1637/2224 [2:25:09<49:07,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.294, 'grad_norm': 0.8839691269943234, 'learning_rate': 1.7186973653394923e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1637/2224 [2:25:09<49:07,  5.02s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1638/2224 [2:25:14<48:41,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2768, 'grad_norm': 0.8549693498378281, 'learning_rate': 1.7132060871552615e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1638/2224 [2:25:14<48:41,  4.99s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1639/2224 [2:25:19<48:46,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2945, 'grad_norm': 0.8439234528994619, 'learning_rate': 1.7077217812033954e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1639/2224 [2:25:19<48:46,  5.00s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2224 [2:25:24<48:19,  4.96s/it]
0:                                                      
0: 
0: {'loss': 0.2946, 'grad_norm': 0.8798690677481447, 'learning_rate': 1.7022444591176762e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2224 [2:25:24<48:19,  4.96s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1641/2224 [2:25:29<48:31,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2495, 'grad_norm': 0.8525328323912098, 'learning_rate': 1.6967741325170772e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1641/2224 [2:25:29<48:31,  4.99s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1642/2224 [2:25:34<48:29,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2772, 'grad_norm': 0.9152994395343446, 'learning_rate': 1.6913108130057303e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1642/2224 [2:25:34<48:29,  5.00s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1643/2224 [2:25:39<48:28,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2805, 'grad_norm': 0.8846762231394181, 'learning_rate': 1.6858545121729004e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1643/2224 [2:25:39<48:28,  5.01s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1644/2224 [2:25:44<48:24,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2614, 'grad_norm': 0.8420279654470357, 'learning_rate': 1.6804052415929672e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1644/2224 [2:25:44<48:24,  5.01s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1645/2224 [2:25:49<48:22,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2764, 'grad_norm': 0.8674671178509097, 'learning_rate': 1.674963012825398e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1645/2224 [2:25:49<48:22,  5.01s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1646/2224 [2:25:54<48:20,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.282, 'grad_norm': 0.8766949882730115, 'learning_rate': 1.6695278374147172e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1646/2224 [2:25:54<48:20,  5.02s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1647/2224 [2:25:59<48:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8816205808857966, 'learning_rate': 1.6640997268904925e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1647/2224 [2:25:59<48:23,  5.03s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1648/2224 [2:26:04<48:23,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3089, 'grad_norm': 0.8998824027497176, 'learning_rate': 1.658678692767302e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1648/2224 [2:26:04<48:23,  5.04s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1649/2224 [2:26:09<48:16,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3073, 'grad_norm': 0.8902219469297727, 'learning_rate': 1.6532647465447154e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1649/2224 [2:26:10<48:16,  5.04s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1650/2224 [2:26:15<48:19,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2969, 'grad_norm': 0.8888969633012985, 'learning_rate': 1.647857899707262e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1650/2224 [2:26:15<48:19,  5.05s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1651/2224 [2:26:20<48:17,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2629, 'grad_norm': 0.8402677796026743, 'learning_rate': 1.6424581637244157e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1651/2224 [2:26:20<48:17,  5.06s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1652/2224 [2:26:25<48:03,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2763, 'grad_norm': 0.9177752406855028, 'learning_rate': 1.637065550050565e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1652/2224 [2:26:25<48:03,  5.04s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1653/2224 [2:26:30<48:01,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2865, 'grad_norm': 0.884471475867584, 'learning_rate': 1.6316800701249902e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1653/2224 [2:26:30<48:01,  5.05s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1654/2224 [2:26:35<47:55,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2898, 'grad_norm': 0.9138793190674683, 'learning_rate': 1.626301735371838e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1654/2224 [2:26:35<47:55,  5.04s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1655/2224 [2:26:40<47:32,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2747, 'grad_norm': 1.1132459650015196, 'learning_rate': 1.6209305572001005e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1655/2224 [2:26:40<47:32,  5.01s/it]
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1656/2224 [2:26:45<47:41,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2747, 'grad_norm': 0.8331338626538463, 'learning_rate': 1.615566547003583e-06, 'epoch': 0.74}
0: 
0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1656/2224 [2:26:45<47:41,  5.04s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1657/2224 [2:26:50<47:41,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2968, 'grad_norm': 0.8594356646387605, 'learning_rate': 1.6102097161608915e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1657/2224 [2:26:50<47:41,  5.05s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1658/2224 [2:26:55<47:40,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2968, 'grad_norm': 0.9015956848329246, 'learning_rate': 1.6048600760354023e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1658/2224 [2:26:55<47:40,  5.05s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1659/2224 [2:27:00<47:33,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2809, 'grad_norm': 0.895051629654568, 'learning_rate': 1.5995176379752314e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1659/2224 [2:27:00<47:33,  5.05s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1660/2224 [2:27:05<47:25,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2867, 'grad_norm': 0.8673787566431151, 'learning_rate': 1.5941824133132245e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1660/2224 [2:27:05<47:25,  5.05s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1661/2224 [2:27:10<46:58,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2848, 'grad_norm': 0.9047404606782735, 'learning_rate': 1.5888544133669227e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1661/2224 [2:27:10<46:58,  5.01s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1662/2224 [2:27:15<47:06,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3015, 'grad_norm': 0.8615213156626305, 'learning_rate': 1.5835336494385434e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1662/2224 [2:27:15<47:06,  5.03s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1663/2224 [2:27:20<46:54,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2802, 'grad_norm': 0.8792791286009842, 'learning_rate': 1.5782201328149494e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1663/2224 [2:27:20<46:54,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1664/2224 [2:27:25<46:40,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2823, 'grad_norm': 0.8697997222197853, 'learning_rate': 1.5729138747676343e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1664/2224 [2:27:25<46:40,  5.00s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1665/2224 [2:27:30<46:53,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2927, 'grad_norm': 0.8489965177027371, 'learning_rate': 1.567614886552693e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1665/2224 [2:27:30<46:53,  5.03s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1666/2224 [2:27:35<46:25,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.3009, 'grad_norm': 0.9263870686336237, 'learning_rate': 1.5623231794107995e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1666/2224 [2:27:35<46:25,  4.99s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1667/2224 [2:27:40<46:30,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3007, 'grad_norm': 0.9079631756167179, 'learning_rate': 1.557038764567182e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1667/2224 [2:27:40<46:30,  5.01s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1668/2224 [2:27:45<46:25,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2839, 'grad_norm': 0.8753387964613023, 'learning_rate': 1.551761653231601e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1668/2224 [2:27:45<46:25,  5.01s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1669/2224 [2:27:50<46:28,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2849, 'grad_norm': 0.8901221846884241, 'learning_rate': 1.5464918565983195e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1669/2224 [2:27:50<46:28,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1670/2224 [2:27:55<46:29,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2778, 'grad_norm': 0.8838392956109247, 'learning_rate': 1.541229385846089e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1670/2224 [2:27:55<46:29,  5.04s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1671/2224 [2:28:00<46:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2872, 'grad_norm': 0.898765126884594, 'learning_rate': 1.5359742521381206e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1671/2224 [2:28:00<46:23,  5.03s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1672/2224 [2:28:05<46:01,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.3124, 'grad_norm': 0.9174284176416625, 'learning_rate': 1.5307264666220562e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1672/2224 [2:28:05<46:01,  5.00s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1673/2224 [2:28:10<46:06,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2619, 'grad_norm': 0.8468878937418728, 'learning_rate': 1.525486040429956e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1673/2224 [2:28:10<46:06,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1674/2224 [2:28:15<45:53,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.302, 'grad_norm': 0.9074750079832125, 'learning_rate': 1.520252984678266e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1674/2224 [2:28:15<45:53,  5.01s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1675/2224 [2:28:20<45:55,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2791, 'grad_norm': 0.8524413613653273, 'learning_rate': 1.5150273104677998e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1675/2224 [2:28:20<45:55,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1676/2224 [2:28:25<45:51,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3028, 'grad_norm': 0.9304916137516617, 'learning_rate': 1.509809028883708e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1676/2224 [2:28:25<45:51,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1677/2224 [2:28:30<45:35,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2928, 'grad_norm': 0.9086678140403687, 'learning_rate': 1.5045981509954632e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1677/2224 [2:28:30<45:35,  5.00s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1678/2224 [2:28:35<45:39,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3133, 'grad_norm': 0.9014668554769911, 'learning_rate': 1.4993946878568315e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1678/2224 [2:28:35<45:39,  5.02s/it]
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1679/2224 [2:28:40<45:43,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2669, 'grad_norm': 0.8680978279040135, 'learning_rate': 1.4941986505058508e-06, 'epoch': 0.75}
0: 
0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1679/2224 [2:28:40<45:43,  5.03s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1680/2224 [2:28:45<45:39,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2538, 'grad_norm': 0.8521228473897604, 'learning_rate': 1.4890100499648063e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1680/2224 [2:28:45<45:39,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1681/2224 [2:28:50<45:35,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.281, 'grad_norm': 0.8789695434869434, 'learning_rate': 1.4838288972402088e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1681/2224 [2:28:50<45:35,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1682/2224 [2:28:55<45:33,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2748, 'grad_norm': 0.8998080500486674, 'learning_rate': 1.4786552033227659e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1682/2224 [2:28:55<45:33,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1683/2224 [2:29:01<45:36,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3123, 'grad_norm': 0.8812025420679471, 'learning_rate': 1.4734889791873674e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1683/2224 [2:29:01<45:36,  5.06s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1684/2224 [2:29:05<45:14,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3084, 'grad_norm': 0.8752873183745742, 'learning_rate': 1.468330235793058e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1684/2224 [2:29:05<45:14,  5.03s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1685/2224 [2:29:10<45:07,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2664, 'grad_norm': 0.8460660836941157, 'learning_rate': 1.463178984083008e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1685/2224 [2:29:10<45:07,  5.02s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1686/2224 [2:29:15<44:42,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2925, 'grad_norm': 0.9014199892707012, 'learning_rate': 1.458035234984501e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1686/2224 [2:29:15<44:42,  4.99s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1687/2224 [2:29:21<45:01,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3018, 'grad_norm': 0.8558535814464838, 'learning_rate': 1.4528989994089043e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1687/2224 [2:29:21<45:01,  5.03s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1688/2224 [2:29:26<45:02,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2764, 'grad_norm': 0.8649149752457134, 'learning_rate': 1.4477702882516475e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1688/2224 [2:29:26<45:02,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1689/2224 [2:29:31<45:08,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.286, 'grad_norm': 0.842831276291988, 'learning_rate': 1.4426491123921943e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1689/2224 [2:29:31<45:08,  5.06s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1690/2224 [2:29:36<44:47,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2605, 'grad_norm': 0.8423050213007655, 'learning_rate': 1.4375354826940297e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1690/2224 [2:29:36<44:47,  5.03s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1691/2224 [2:29:41<44:41,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2809, 'grad_norm': 0.8633794298808025, 'learning_rate': 1.4324294100046271e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1691/2224 [2:29:41<44:41,  5.03s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1692/2224 [2:29:46<44:32,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2962, 'grad_norm': 0.9089530186529201, 'learning_rate': 1.4273309051554323e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1692/2224 [2:29:46<44:32,  5.02s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1693/2224 [2:29:51<44:27,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2731, 'grad_norm': 0.8566062297437107, 'learning_rate': 1.4222399789618352e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1693/2224 [2:29:51<44:27,  5.02s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1694/2224 [2:29:56<44:32,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2962, 'grad_norm': 0.8413533915543809, 'learning_rate': 1.4171566422231515e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1694/2224 [2:29:56<44:32,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1695/2224 [2:30:01<44:30,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2914, 'grad_norm': 0.881133645241578, 'learning_rate': 1.4120809057225937e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1695/2224 [2:30:01<44:30,  5.05s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1696/2224 [2:30:06<44:22,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2785, 'grad_norm': 0.8639711781327576, 'learning_rate': 1.4070127802272554e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1696/2224 [2:30:06<44:22,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1697/2224 [2:30:11<44:18,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2844, 'grad_norm': 0.9157849731432306, 'learning_rate': 1.4019522764880856e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1697/2224 [2:30:11<44:18,  5.04s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1698/2224 [2:30:16<44:14,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3429, 'grad_norm': 0.9064656627577476, 'learning_rate': 1.3968994052398604e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1698/2224 [2:30:16<44:14,  5.05s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1699/2224 [2:30:21<43:44,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2984, 'grad_norm': 0.9044872285886669, 'learning_rate': 1.3918541772011712e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1699/2224 [2:30:21<43:44,  5.00s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1700/2224 [2:30:26<43:50,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2635, 'grad_norm': 0.8719445202330753, 'learning_rate': 1.3868166030743923e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1700/2224 [2:30:26<43:50,  5.02s/it]
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1701/2224 [2:30:31<43:56,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.281, 'grad_norm': 0.8771268254099868, 'learning_rate': 1.3817866935456647e-06, 'epoch': 0.76}
0: 
0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1701/2224 [2:30:31<43:56,  5.04s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1702/2224 [2:30:36<44:02,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2762, 'grad_norm': 0.8645325094828019, 'learning_rate': 1.376764459284866e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1702/2224 [2:30:36<44:02,  5.06s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1703/2224 [2:30:41<43:37,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.289, 'grad_norm': 0.8543222025723397, 'learning_rate': 1.3717499109455968e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1703/2224 [2:30:41<43:37,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1704/2224 [2:30:46<43:42,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2704, 'grad_norm': 0.8722466921437235, 'learning_rate': 1.3667430591651532e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1704/2224 [2:30:46<43:42,  5.04s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1705/2224 [2:30:51<43:27,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2808, 'grad_norm': 0.869396676852502, 'learning_rate': 1.3617439145645005e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1705/2224 [2:30:51<43:27,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1706/2224 [2:30:56<43:24,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2765, 'grad_norm': 0.8396082469523939, 'learning_rate': 1.3567524877482602e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1706/2224 [2:30:56<43:24,  5.03s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1707/2224 [2:31:01<43:16,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2445, 'grad_norm': 0.8306435763260268, 'learning_rate': 1.351768789304679e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1707/2224 [2:31:01<43:16,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1708/2224 [2:31:06<43:15,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2987, 'grad_norm': 0.8774923608526417, 'learning_rate': 1.34679282980561e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1708/2224 [2:31:06<43:15,  5.03s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1709/2224 [2:31:11<43:06,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2827, 'grad_norm': 0.8660194490544197, 'learning_rate': 1.341824619806491e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1709/2224 [2:31:11<43:06,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1710/2224 [2:31:16<43:04,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2689, 'grad_norm': 0.8846752826839759, 'learning_rate': 1.3368641698463197e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1710/2224 [2:31:16<43:04,  5.03s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1711/2224 [2:31:21<42:58,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.306, 'grad_norm': 0.8929840298663836, 'learning_rate': 1.3319114904476343e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1711/2224 [2:31:21<42:58,  5.03s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1712/2224 [2:31:26<42:52,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2953, 'grad_norm': 0.8830889196935151, 'learning_rate': 1.3269665921164847e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1712/2224 [2:31:26<42:52,  5.03s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1713/2224 [2:31:31<42:31,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2738, 'grad_norm': 0.8469659932392284, 'learning_rate': 1.3220294853424204e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1713/2224 [2:31:31<42:31,  4.99s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1714/2224 [2:31:36<42:35,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3047, 'grad_norm': 0.889202920324153, 'learning_rate': 1.3171001805984613e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1714/2224 [2:31:36<42:35,  5.01s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1715/2224 [2:31:41<42:32,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3018, 'grad_norm': 0.8868146445808176, 'learning_rate': 1.3121786883410742e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1715/2224 [2:31:41<42:32,  5.01s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1716/2224 [2:31:46<42:31,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3012, 'grad_norm': 0.8832787243249353, 'learning_rate': 1.3072650190101555e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1716/2224 [2:31:46<42:31,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1717/2224 [2:31:51<42:11,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2881, 'grad_norm': 0.8892253444354569, 'learning_rate': 1.30235918302901e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1717/2224 [2:31:51<42:11,  4.99s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1718/2224 [2:31:56<42:18,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3036, 'grad_norm': 0.8536258284903602, 'learning_rate': 1.2974611908043178e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1718/2224 [2:31:56<42:18,  5.02s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1719/2224 [2:32:02<42:32,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3034, 'grad_norm': 0.9036340663341828, 'learning_rate': 1.2925710527261276e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1719/2224 [2:32:02<42:32,  5.05s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1720/2224 [2:32:07<42:34,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.295, 'grad_norm': 0.8498132935880202, 'learning_rate': 1.2876887791678244e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1720/2224 [2:32:07<42:34,  5.07s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1721/2224 [2:32:12<42:19,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2711, 'grad_norm': 0.8501963254465177, 'learning_rate': 1.2828143804861093e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1721/2224 [2:32:12<42:19,  5.05s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1722/2224 [2:32:17<42:23,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8511862408707891, 'learning_rate': 1.277947867020981e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1722/2224 [2:32:17<42:23,  5.07s/it]
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1723/2224 [2:32:22<42:09,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2638, 'grad_norm': 0.8708496794189697, 'learning_rate': 1.2730892490957096e-06, 'epoch': 0.77}
0: 
0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1723/2224 [2:32:22<42:09,  5.05s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1724/2224 [2:32:27<42:03,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2988, 'grad_norm': 0.8626737215479753, 'learning_rate': 1.2682385370168182e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1724/2224 [2:32:27<42:03,  5.05s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1725/2224 [2:32:32<41:39,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2844, 'grad_norm': 0.8665305362799742, 'learning_rate': 1.263395741074055e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1725/2224 [2:32:32<41:39,  5.01s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1726/2224 [2:32:37<41:38,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2999, 'grad_norm': 0.8362971212624097, 'learning_rate': 1.2585608715403803e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1726/2224 [2:32:37<41:38,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1727/2224 [2:32:42<41:19,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.3028, 'grad_norm': 0.8606738084849773, 'learning_rate': 1.2537339386719398e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1727/2224 [2:32:42<41:19,  4.99s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1728/2224 [2:32:47<41:22,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.822063788062905, 'learning_rate': 1.2489149527080408e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1728/2224 [2:32:47<41:22,  5.01s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1729/2224 [2:32:52<41:28,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2908, 'grad_norm': 0.8326620963784381, 'learning_rate': 1.2441039238711344e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1729/2224 [2:32:52<41:28,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1730/2224 [2:32:57<41:24,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2777, 'grad_norm': 0.8430282534602449, 'learning_rate': 1.2393008623667946e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1730/2224 [2:32:57<41:24,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1731/2224 [2:33:02<41:25,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2921, 'grad_norm': 0.8578858847036805, 'learning_rate': 1.2345057783836883e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1731/2224 [2:33:02<41:25,  5.04s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1732/2224 [2:33:07<41:12,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.8641566712451542, 'learning_rate': 1.2297186820935659e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1732/2224 [2:33:07<41:12,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1733/2224 [2:33:12<41:14,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2957, 'grad_norm': 0.8932971272183956, 'learning_rate': 1.2249395836512307e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1733/2224 [2:33:12<41:14,  5.04s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1734/2224 [2:33:17<41:12,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2834, 'grad_norm': 0.8666568704963986, 'learning_rate': 1.2201684931945213e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1734/2224 [2:33:17<41:12,  5.05s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1735/2224 [2:33:22<41:08,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2891, 'grad_norm': 0.8502519598768098, 'learning_rate': 1.215405420844289e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1735/2224 [2:33:22<41:08,  5.05s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1736/2224 [2:33:27<40:41,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2866, 'grad_norm': 0.9016857243163524, 'learning_rate': 1.2106503767043747e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1736/2224 [2:33:27<40:41,  5.00s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1737/2224 [2:33:32<40:43,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2766, 'grad_norm': 0.847024561148721, 'learning_rate': 1.2059033708615936e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1737/2224 [2:33:32<40:43,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1738/2224 [2:33:37<40:41,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2803, 'grad_norm': 0.844901427334481, 'learning_rate': 1.201164413385702e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1738/2224 [2:33:37<40:41,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1739/2224 [2:33:42<40:33,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.262, 'grad_norm': 0.8591280427244248, 'learning_rate': 1.1964335143293898e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1739/2224 [2:33:42<40:33,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1740/2224 [2:33:47<40:27,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3194, 'grad_norm': 0.8764855855652435, 'learning_rate': 1.191710683728252e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1740/2224 [2:33:47<40:27,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1741/2224 [2:33:52<40:24,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.296, 'grad_norm': 0.8715268966634308, 'learning_rate': 1.1869959316007634e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1741/2224 [2:33:52<40:24,  5.02s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1742/2224 [2:33:57<40:26,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2804, 'grad_norm': 0.883496305705304, 'learning_rate': 1.1822892679482661e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1742/2224 [2:33:57<40:26,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1743/2224 [2:34:02<40:19,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2938, 'grad_norm': 0.8824615312379754, 'learning_rate': 1.177590702754946e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1743/2224 [2:34:02<40:19,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1744/2224 [2:34:07<40:16,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2573, 'grad_norm': 0.8777626967048809, 'learning_rate': 1.172900245987803e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1744/2224 [2:34:07<40:16,  5.03s/it]
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1745/2224 [2:34:12<40:10,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2684, 'grad_norm': 0.8790440249624651, 'learning_rate': 1.1682179075966432e-06, 'epoch': 0.78}
0: 
0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1745/2224 [2:34:12<40:10,  5.03s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1746/2224 [2:34:17<39:49,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2835, 'grad_norm': 0.8872664412517853, 'learning_rate': 1.163543697514049e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1746/2224 [2:34:17<39:49,  5.00s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1747/2224 [2:34:22<39:51,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2982, 'grad_norm': 0.8553259705927243, 'learning_rate': 1.1588776256553596e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1747/2224 [2:34:22<39:51,  5.01s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1748/2224 [2:34:27<39:47,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2824, 'grad_norm': 0.8608793883655027, 'learning_rate': 1.1542197019186518e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1748/2224 [2:34:27<39:47,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1749/2224 [2:34:32<39:46,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2827, 'grad_norm': 0.8948009921894878, 'learning_rate': 1.1495699361847173e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1749/2224 [2:34:32<39:46,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2224 [2:34:37<39:55,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3108, 'grad_norm': 0.8515296161937012, 'learning_rate': 1.1449283383170446e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2224 [2:34:37<39:55,  5.05s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1751/2224 [2:34:42<39:52,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2766, 'grad_norm': 0.8540575916671034, 'learning_rate': 1.14029491816179e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1751/2224 [2:34:42<39:52,  5.06s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1752/2224 [2:34:47<39:41,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.282, 'grad_norm': 0.898351166197724, 'learning_rate': 1.1356696855477678e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1752/2224 [2:34:47<39:41,  5.05s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1753/2224 [2:34:53<39:38,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2892, 'grad_norm': 0.8995133429079064, 'learning_rate': 1.1310526502864249e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1753/2224 [2:34:53<39:38,  5.05s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1754/2224 [2:34:58<39:31,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2967, 'grad_norm': 0.8832027787130703, 'learning_rate': 1.1264438221718133e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1754/2224 [2:34:58<39:31,  5.05s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1755/2224 [2:35:03<39:21,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3151, 'grad_norm': 0.8854798352419805, 'learning_rate': 1.1218432109805804e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1755/2224 [2:35:03<39:21,  5.04s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1756/2224 [2:35:08<39:10,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2751, 'grad_norm': 0.8786935032205797, 'learning_rate': 1.117250826471944e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1756/2224 [2:35:08<39:10,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1757/2224 [2:35:13<39:05,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2745, 'grad_norm': 0.8523008205405226, 'learning_rate': 1.112666678387665e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1757/2224 [2:35:13<39:05,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1758/2224 [2:35:18<39:03,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2579, 'grad_norm': 0.8548137478639424, 'learning_rate': 1.1080907764520372e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1758/2224 [2:35:18<39:03,  5.03s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1759/2224 [2:35:23<38:56,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2853, 'grad_norm': 0.8732989997092973, 'learning_rate': 1.103523130371863e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1759/2224 [2:35:23<38:56,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1760/2224 [2:35:28<38:47,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3054, 'grad_norm': 0.8756737401477483, 'learning_rate': 1.0989637498364259e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1760/2224 [2:35:28<38:47,  5.02s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1761/2224 [2:35:33<39:09,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2909, 'grad_norm': 0.8833504629573323, 'learning_rate': 1.0944126445174808e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1761/2224 [2:35:33<39:09,  5.07s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1762/2224 [2:35:38<39:02,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2779, 'grad_norm': 0.825215482517897, 'learning_rate': 1.0898698240692273e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1762/2224 [2:35:38<39:02,  5.07s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1763/2224 [2:35:43<39:21,  5.12s/it]
0:                                                      
0: 
0: {'loss': 0.2769, 'grad_norm': 0.8694839058226613, 'learning_rate': 1.0853352981282905e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1763/2224 [2:35:43<39:21,  5.12s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1764/2224 [2:35:48<39:38,  5.17s/it]
0:                                                      
0: 
0: {'loss': 0.2849, 'grad_norm': 0.8844085122095874, 'learning_rate': 1.0808090763136997e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1764/2224 [2:35:48<39:38,  5.17s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1765/2224 [2:35:54<39:19,  5.14s/it]
0:                                                      
0: 
0: {'loss': 0.2974, 'grad_norm': 0.8792061910603611, 'learning_rate': 1.0762911682268694e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1765/2224 [2:35:54<39:19,  5.14s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1766/2224 [2:35:59<39:03,  5.12s/it]
0:                                                      
0: 
0: {'loss': 0.3029, 'grad_norm': 0.8875823545758896, 'learning_rate': 1.07178158345158e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1766/2224 [2:35:59<39:03,  5.12s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1767/2224 [2:36:04<38:46,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.2796, 'grad_norm': 0.8597833926764968, 'learning_rate': 1.0672803315539499e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1767/2224 [2:36:04<38:46,  5.09s/it]
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1768/2224 [2:36:09<40:04,  5.27s/it]
0:                                                      
0: 
0: {'loss': 0.3115, 'grad_norm': 1.029877932677609, 'learning_rate': 1.062787422082428e-06, 'epoch': 0.79}
0: 
0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1768/2224 [2:36:09<40:04,  5.27s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1769/2224 [2:36:14<39:25,  5.20s/it]
0:                                                      
0: 
0: {'loss': 0.2884, 'grad_norm': 0.8651081889297458, 'learning_rate': 1.058302864567764e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1769/2224 [2:36:14<39:25,  5.20s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1770/2224 [2:36:19<38:44,  5.12s/it]
0:                                                      
0: 
0: {'loss': 0.2811, 'grad_norm': 0.8327953034972594, 'learning_rate': 1.0538266685229881e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1770/2224 [2:36:19<38:44,  5.12s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1771/2224 [2:36:25<39:01,  5.17s/it]
0:                                                      
0: 
0: {'loss': 0.2734, 'grad_norm': 0.8627004940217833, 'learning_rate': 1.0493588434433965e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1771/2224 [2:36:25<39:01,  5.17s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1772/2224 [2:36:30<38:36,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.3, 'grad_norm': 0.9134030199608827, 'learning_rate': 1.0448993988065275e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1772/2224 [2:36:30<38:36,  5.13s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1773/2224 [2:36:35<39:20,  5.23s/it]
0:                                                      
0: 
0: {'loss': 0.3057, 'grad_norm': 0.8949270389513182, 'learning_rate': 1.0404483440721435e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1773/2224 [2:36:35<39:20,  5.23s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1774/2224 [2:36:40<38:57,  5.19s/it]
0:                                                      
0: 
0: {'loss': 0.2993, 'grad_norm': 0.9554009943458878, 'learning_rate': 1.036005688682205e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1774/2224 [2:36:40<38:57,  5.19s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1775/2224 [2:36:45<38:35,  5.16s/it]
0:                                                      
0: 
0: {'loss': 0.2935, 'grad_norm': 0.8773258202987302, 'learning_rate': 1.031571442060859e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1775/2224 [2:36:45<38:35,  5.16s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1776/2224 [2:36:51<39:39,  5.31s/it]
0:                                                      
0: 
0: {'loss': 0.2902, 'grad_norm': 0.8901133941592778, 'learning_rate': 1.0271456136144153e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1776/2224 [2:36:51<39:39,  5.31s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1777/2224 [2:36:56<39:11,  5.26s/it]
0:                                                      
0: 
0: {'loss': 0.2826, 'grad_norm': 0.8585192878577077, 'learning_rate': 1.0227282127313238e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1777/2224 [2:36:56<39:11,  5.26s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1778/2224 [2:37:01<38:31,  5.18s/it]
0:                                                      
0: 
0: {'loss': 0.2915, 'grad_norm': 0.8714934107917969, 'learning_rate': 1.0183192487821591e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1778/2224 [2:37:01<38:31,  5.18s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1779/2224 [2:37:06<38:06,  5.14s/it]
0:                                                      
0: 
0: {'loss': 0.2647, 'grad_norm': 0.862705877197925, 'learning_rate': 1.0139187311195992e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1779/2224 [2:37:06<38:06,  5.14s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1780/2224 [2:37:11<37:56,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.2867, 'grad_norm': 0.8473771123812633, 'learning_rate': 1.0095266690784006e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1780/2224 [2:37:11<37:56,  5.13s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1781/2224 [2:37:16<37:52,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.3032, 'grad_norm': 0.859345094787055, 'learning_rate': 1.0051430719753891e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1781/2224 [2:37:16<37:52,  5.13s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1782/2224 [2:37:22<38:04,  5.17s/it]
0:                                                      
0: 
0: {'loss': 0.2906, 'grad_norm': 0.8695546977195775, 'learning_rate': 1.0007679491094308e-06, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1782/2224 [2:37:22<38:04,  5.17s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1783/2224 [2:37:27<38:19,  5.22s/it]
0:                                                      
0: 
0: {'loss': 0.3135, 'grad_norm': 0.8767283526288846, 'learning_rate': 9.964013097614145e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1783/2224 [2:37:27<38:19,  5.22s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1784/2224 [2:37:32<37:55,  5.17s/it]
0:                                                      
0: 
0: {'loss': 0.2883, 'grad_norm': 0.8720322328092815, 'learning_rate': 9.92043163194235e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1784/2224 [2:37:32<37:55,  5.17s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1785/2224 [2:37:37<37:33,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.3158, 'grad_norm': 0.8901522280058011, 'learning_rate': 9.876935186527709e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1785/2224 [2:37:37<37:33,  5.13s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1786/2224 [2:37:42<37:08,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.268, 'grad_norm': 0.8935015396034848, 'learning_rate': 9.83352385363867e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1786/2224 [2:37:42<37:08,  5.09s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1787/2224 [2:37:47<37:00,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.29, 'grad_norm': 0.877157019885537, 'learning_rate': 9.790197725363093e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1787/2224 [2:37:47<37:00,  5.08s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1788/2224 [2:37:52<36:49,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2649, 'grad_norm': 0.8441133032347057, 'learning_rate': 9.746956893608128e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1788/2224 [2:37:52<36:49,  5.07s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1789/2224 [2:37:57<36:36,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2583, 'grad_norm': 0.8631489368172589, 'learning_rate': 9.703801450099987e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1789/2224 [2:37:57<36:36,  5.05s/it]
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1790/2224 [2:38:02<36:16,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2948, 'grad_norm': 0.9153679366634836, 'learning_rate': 9.66073148638373e-07, 'epoch': 0.8}
0: 
0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1790/2224 [2:38:02<36:16,  5.01s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1791/2224 [2:38:07<36:21,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2709, 'grad_norm': 0.8524535178200014, 'learning_rate': 9.617747093823103e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1791/2224 [2:38:07<36:21,  5.04s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1792/2224 [2:38:12<36:13,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2757, 'grad_norm': 0.8587003050485712, 'learning_rate': 9.574848363600347e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1792/2224 [2:38:12<36:13,  5.03s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1793/2224 [2:38:17<36:07,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.298, 'grad_norm': 0.8617613432839138, 'learning_rate': 9.532035386715937e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1793/2224 [2:38:17<36:07,  5.03s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1794/2224 [2:38:22<36:04,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.301, 'grad_norm': 0.8467340279517914, 'learning_rate': 9.489308253988494e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1794/2224 [2:38:22<36:04,  5.03s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1795/2224 [2:38:27<36:05,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3165, 'grad_norm': 0.8660346211733198, 'learning_rate': 9.446667056054525e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1795/2224 [2:38:27<36:05,  5.05s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1796/2224 [2:38:32<36:05,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2534, 'grad_norm': 0.8313878148972611, 'learning_rate': 9.404111883368211e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1796/2224 [2:38:32<36:05,  5.06s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1797/2224 [2:38:37<35:48,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3032, 'grad_norm': 0.8707889343592073, 'learning_rate': 9.361642826201295e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1797/2224 [2:38:37<35:48,  5.03s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1798/2224 [2:38:42<35:46,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3255, 'grad_norm': 0.9069613953325697, 'learning_rate': 9.319259974642819e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1798/2224 [2:38:42<35:46,  5.04s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1799/2224 [2:38:47<35:43,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2768, 'grad_norm': 0.8285674197358011, 'learning_rate': 9.276963418598978e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1799/2224 [2:38:47<35:43,  5.04s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1800/2224 [2:38:53<35:37,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2768, 'grad_norm': 0.846496432076088, 'learning_rate': 9.234753247792871e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1800/2224 [2:38:53<35:37,  5.04s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1801/2224 [2:39:49<2:24:16, 20.47s/it]
0:                                                        
0: 
0: {'loss': 0.2719, 'grad_norm': 0.8596351400147992, 'learning_rate': 9.192629551764393e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1801/2224 [2:39:49<2:24:16, 20.47s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1802/2224 [2:39:54<1:51:25, 15.84s/it]
0:                                                        
0: 
0: {'loss': 0.2701, 'grad_norm': 0.881804075257835, 'learning_rate': 9.150592419869969e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1802/2224 [2:39:54<1:51:25, 15.84s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1803/2224 [2:39:59<1:28:12, 12.57s/it]
0:                                                        
0: 
0: {'loss': 0.27, 'grad_norm': 0.8528433844534027, 'learning_rate': 9.108641941282425e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1803/2224 [2:39:59<1:28:12, 12.57s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1804/2224 [2:40:04<1:12:15, 10.32s/it]
0:                                                        
0: 
0: {'loss': 0.3014, 'grad_norm': 0.8880132032627895, 'learning_rate': 9.066778204990745e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1804/2224 [2:40:04<1:12:15, 10.32s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1805/2224 [2:40:09<1:00:53,  8.72s/it]
0:                                                        
0: 
0: {'loss': 0.2823, 'grad_norm': 0.8912543904626828, 'learning_rate': 9.02500129979993e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1805/2224 [2:40:09<1:00:53,  8.72s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1806/2224 [2:40:14<52:59,  7.61s/it]  
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8558269431979406, 'learning_rate': 8.98331131433075e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1806/2224 [2:40:14<52:59,  7.61s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1807/2224 [2:40:19<47:34,  6.85s/it]
0:                                                      
0: 
0: {'loss': 0.28, 'grad_norm': 0.8951018861136029, 'learning_rate': 8.941708337019639e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1807/2224 [2:40:19<47:34,  6.85s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1808/2224 [2:40:24<43:44,  6.31s/it]
0:                                                      
0: 
0: {'loss': 0.2895, 'grad_norm': 0.8639845964245005, 'learning_rate': 8.90019245611845e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1808/2224 [2:40:24<43:44,  6.31s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1809/2224 [2:40:29<41:08,  5.95s/it]
0:                                                      
0: 
0: {'loss': 0.2528, 'grad_norm': 0.8480435058872764, 'learning_rate': 8.858763759694245e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1809/2224 [2:40:29<41:08,  5.95s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1810/2224 [2:40:34<39:07,  5.67s/it]
0:                                                      
0: 
0: {'loss': 0.2798, 'grad_norm': 0.9032595340069679, 'learning_rate': 8.817422335629177e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1810/2224 [2:40:34<39:07,  5.67s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1811/2224 [2:40:39<37:53,  5.51s/it]
0:                                                      
0: 
0: {'loss': 0.3008, 'grad_norm': 0.88045204741966, 'learning_rate': 8.776168271620266e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1811/2224 [2:40:39<37:53,  5.51s/it]
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1812/2224 [2:40:44<36:41,  5.34s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.90507043530813, 'learning_rate': 8.735001655179226e-07, 'epoch': 0.81}
0: 
0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1812/2224 [2:40:44<36:41,  5.34s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1813/2224 [2:40:49<36:04,  5.27s/it]
0:                                                      
0: 
0: {'loss': 0.2926, 'grad_norm': 0.9183105576232069, 'learning_rate': 8.693922573632224e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1813/2224 [2:40:49<36:04,  5.27s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1814/2224 [2:40:55<35:32,  5.20s/it]
0:                                                      
0: 
0: {'loss': 0.2857, 'grad_norm': 0.8668311350594656, 'learning_rate': 8.652931114119789e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1814/2224 [2:40:55<35:32,  5.20s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1815/2224 [2:41:00<35:05,  5.15s/it]
0:                                                      
0: 
0: {'loss': 0.2861, 'grad_norm': 0.8868517629910394, 'learning_rate': 8.612027363596564e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1815/2224 [2:41:00<35:05,  5.15s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1816/2224 [2:41:05<34:56,  5.14s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.855428306613116, 'learning_rate': 8.57121140883112e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1816/2224 [2:41:05<34:56,  5.14s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1817/2224 [2:41:10<34:20,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.297, 'grad_norm': 0.8918531961824155, 'learning_rate': 8.530483336405793e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1817/2224 [2:41:10<34:20,  5.06s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1818/2224 [2:41:14<33:54,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2796, 'grad_norm': 0.8497880005350401, 'learning_rate': 8.489843232716544e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1818/2224 [2:41:14<33:54,  5.01s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1819/2224 [2:41:19<33:53,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.276, 'grad_norm': 0.8716043611681524, 'learning_rate': 8.449291183972646e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1819/2224 [2:41:19<33:53,  5.02s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1820/2224 [2:41:25<34:00,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3107, 'grad_norm': 0.8866719937946781, 'learning_rate': 8.408827276196635e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1820/2224 [2:41:25<34:00,  5.05s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1821/2224 [2:41:30<33:50,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2659, 'grad_norm': 0.8568998572057622, 'learning_rate': 8.368451595224081e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1821/2224 [2:41:30<33:50,  5.04s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1822/2224 [2:41:35<33:39,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3018, 'grad_norm': 0.8764714538854383, 'learning_rate': 8.328164226703344e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1822/2224 [2:41:35<33:39,  5.02s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1823/2224 [2:41:40<33:34,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2684, 'grad_norm': 0.8936462880841434, 'learning_rate': 8.287965256095515e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1823/2224 [2:41:40<33:34,  5.02s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1824/2224 [2:41:45<33:38,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.285, 'grad_norm': 0.8498708477514216, 'learning_rate': 8.247854768674119e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1824/2224 [2:41:45<33:38,  5.05s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1825/2224 [2:41:50<33:26,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2758, 'grad_norm': 0.8487576103073867, 'learning_rate': 8.207832849525022e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1825/2224 [2:41:50<33:26,  5.03s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1826/2224 [2:41:55<33:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2793, 'grad_norm': 1.3254468769436474, 'learning_rate': 8.167899583546169e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1826/2224 [2:41:55<33:23,  5.03s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1827/2224 [2:42:00<33:16,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3008, 'grad_norm': 0.8429252397667667, 'learning_rate': 8.128055055447476e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1827/2224 [2:42:00<33:16,  5.03s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1828/2224 [2:42:05<33:19,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.267, 'grad_norm': 0.8676335384577509, 'learning_rate': 8.088299349750628e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1828/2224 [2:42:05<33:19,  5.05s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1829/2224 [2:42:10<33:10,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2804, 'grad_norm': 0.8978493625361466, 'learning_rate': 8.048632550788848e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1829/2224 [2:42:10<33:10,  5.04s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2224 [2:42:15<33:14,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3011, 'grad_norm': 0.8822030445508767, 'learning_rate': 8.009054742706801e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1830/2224 [2:42:15<33:14,  5.06s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1831/2224 [2:42:20<33:04,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2754, 'grad_norm': 0.884688745899111, 'learning_rate': 7.969566009460372e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1831/2224 [2:42:20<33:04,  5.05s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1832/2224 [2:42:25<33:08,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.3002, 'grad_norm': 0.8660569290258469, 'learning_rate': 7.930166434816489e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1832/2224 [2:42:25<33:08,  5.07s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2224 [2:42:30<33:05,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.3076, 'grad_norm': 0.8910987644023916, 'learning_rate': 7.890856102352945e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2224 [2:42:30<33:05,  5.08s/it]
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2224 [2:42:35<32:55,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2864, 'grad_norm': 0.87684862679797, 'learning_rate': 7.851635095458226e-07, 'epoch': 0.82}
0: 
0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2224 [2:42:35<32:55,  5.07s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1835/2224 [2:42:40<32:44,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2988, 'grad_norm': 0.8886451149048142, 'learning_rate': 7.812503497331342e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1835/2224 [2:42:40<32:44,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1836/2224 [2:42:45<32:40,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2746, 'grad_norm': 0.862743286070073, 'learning_rate': 7.773461390981618e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1836/2224 [2:42:45<32:40,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1837/2224 [2:42:50<32:39,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2679, 'grad_norm': 0.8688530693861287, 'learning_rate': 7.734508859228562e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1837/2224 [2:42:50<32:39,  5.06s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1838/2224 [2:42:55<32:28,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2504, 'grad_norm': 0.8693787379119207, 'learning_rate': 7.695645984701683e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1838/2224 [2:42:55<32:28,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1839/2224 [2:43:00<32:23,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2701, 'grad_norm': 0.8594913707069145, 'learning_rate': 7.656872849840252e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1839/2224 [2:43:01<32:23,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1840/2224 [2:43:05<32:13,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2987, 'grad_norm': 0.8700652028372201, 'learning_rate': 7.618189536893216e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1840/2224 [2:43:06<32:13,  5.04s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1841/2224 [2:43:10<31:58,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.255, 'grad_norm': 0.8367486553781156, 'learning_rate': 7.579596127918993e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1841/2224 [2:43:10<31:58,  5.01s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1842/2224 [2:43:15<31:43,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2824, 'grad_norm': 0.9007773884129028, 'learning_rate': 7.541092704785236e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1842/2224 [2:43:15<31:43,  4.98s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1843/2224 [2:43:20<31:42,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2621, 'grad_norm': 0.9182814927885472, 'learning_rate': 7.502679349168767e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1843/2224 [2:43:20<31:42,  4.99s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1844/2224 [2:43:25<31:42,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2853, 'grad_norm': 0.8532912646961205, 'learning_rate': 7.464356142555323e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1844/2224 [2:43:25<31:42,  5.01s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1845/2224 [2:43:30<31:40,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2639, 'grad_norm': 0.8555707203411872, 'learning_rate': 7.426123166239424e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1845/2224 [2:43:30<31:40,  5.02s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1846/2224 [2:43:36<31:41,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2742, 'grad_norm': 0.8498711921235405, 'learning_rate': 7.38798050132416e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1846/2224 [2:43:36<31:41,  5.03s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1847/2224 [2:43:41<31:32,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.289, 'grad_norm': 0.884067709583771, 'learning_rate': 7.349928228721071e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1847/2224 [2:43:41<31:32,  5.02s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1848/2224 [2:43:46<31:33,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2752, 'grad_norm': 0.8227606864535835, 'learning_rate': 7.311966429149952e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1848/2224 [2:43:46<31:33,  5.04s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1849/2224 [2:43:51<31:32,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2879, 'grad_norm': 0.8781865042116603, 'learning_rate': 7.274095183138635e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1849/2224 [2:43:51<31:32,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1850/2224 [2:43:56<31:27,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2683, 'grad_norm': 0.8495921415897524, 'learning_rate': 7.236314571022907e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1850/2224 [2:43:56<31:27,  5.05s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1851/2224 [2:44:01<31:09,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2815, 'grad_norm': 0.888930746155955, 'learning_rate': 7.198624672946292e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1851/2224 [2:44:01<31:09,  5.01s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1852/2224 [2:44:06<31:06,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2777, 'grad_norm': 0.8722065347445196, 'learning_rate': 7.161025568859841e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1852/2224 [2:44:06<31:06,  5.02s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1853/2224 [2:44:11<31:06,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2746, 'grad_norm': 0.8672012035121621, 'learning_rate': 7.123517338522063e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1853/2224 [2:44:11<31:06,  5.03s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1854/2224 [2:44:16<30:54,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.283, 'grad_norm': 0.8651387222995561, 'learning_rate': 7.086100061498663e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1854/2224 [2:44:16<30:54,  5.01s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1855/2224 [2:44:21<30:54,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2954, 'grad_norm': 0.8740952320880845, 'learning_rate': 7.048773817162407e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1855/2224 [2:44:21<30:54,  5.03s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1856/2224 [2:44:26<30:40,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2704, 'grad_norm': 0.8482524092088558, 'learning_rate': 7.011538684692965e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1856/2224 [2:44:26<30:40,  5.00s/it]
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1857/2224 [2:44:31<30:28,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2913, 'grad_norm': 0.8653059983011572, 'learning_rate': 6.974394743076729e-07, 'epoch': 0.83}
0: 
0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1857/2224 [2:44:31<30:28,  4.98s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1858/2224 [2:44:36<30:26,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2939, 'grad_norm': 0.8826421108532796, 'learning_rate': 6.937342071106656e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1858/2224 [2:44:36<30:26,  4.99s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1859/2224 [2:44:41<30:14,  4.97s/it]
0:                                                      
0: 
0: {'loss': 0.258, 'grad_norm': 0.8470568527916533, 'learning_rate': 6.900380747382085e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1859/2224 [2:44:41<30:14,  4.97s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1860/2224 [2:44:46<30:18,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2751, 'grad_norm': 0.8562410683638242, 'learning_rate': 6.86351085030858e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1860/2224 [2:44:46<30:18,  5.00s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1861/2224 [2:44:51<30:14,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2768, 'grad_norm': 0.8834983504366571, 'learning_rate': 6.826732458097779e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1861/2224 [2:44:51<30:14,  5.00s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1862/2224 [2:44:56<30:20,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3121, 'grad_norm': 0.8595591495461181, 'learning_rate': 6.790045648767174e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1862/2224 [2:44:56<30:20,  5.03s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1863/2224 [2:45:01<30:13,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3, 'grad_norm': 0.9164780990645278, 'learning_rate': 6.753450500140018e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1863/2224 [2:45:01<30:13,  5.02s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1864/2224 [2:45:06<30:10,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2982, 'grad_norm': 0.895846136590216, 'learning_rate': 6.716947089845133e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1864/2224 [2:45:06<30:10,  5.03s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1865/2224 [2:45:11<29:58,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2799, 'grad_norm': 0.8587846604862115, 'learning_rate': 6.680535495316687e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1865/2224 [2:45:11<29:58,  5.01s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1866/2224 [2:45:16<29:56,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2674, 'grad_norm': 0.8804984607119636, 'learning_rate': 6.644215793794129e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1866/2224 [2:45:16<29:56,  5.02s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1867/2224 [2:45:21<29:59,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3002, 'grad_norm': 0.840911895904354, 'learning_rate': 6.607988062321974e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1867/2224 [2:45:21<29:59,  5.04s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1868/2224 [2:45:26<29:58,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2901, 'grad_norm': 0.8886281812793208, 'learning_rate': 6.571852377749599e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1868/2224 [2:45:26<29:58,  5.05s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1869/2224 [2:45:31<29:58,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.279, 'grad_norm': 0.8731860290456468, 'learning_rate': 6.535808816731176e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1869/2224 [2:45:31<29:58,  5.07s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1870/2224 [2:45:36<29:34,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2658, 'grad_norm': 0.8647133114858787, 'learning_rate': 6.499857455725416e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1870/2224 [2:45:36<29:34,  5.01s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1871/2224 [2:45:41<29:36,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2918, 'grad_norm': 0.876473905119656, 'learning_rate': 6.463998370995478e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1871/2224 [2:45:41<29:36,  5.03s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1872/2224 [2:45:46<29:35,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2583, 'grad_norm': 0.8421200187574109, 'learning_rate': 6.428231638608762e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1872/2224 [2:45:46<29:35,  5.04s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1873/2224 [2:45:51<29:32,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.8371139294440259, 'learning_rate': 6.392557334436755e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1873/2224 [2:45:51<29:32,  5.05s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1874/2224 [2:45:56<29:30,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.296, 'grad_norm': 0.8693079716511583, 'learning_rate': 6.356975534154908e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1874/2224 [2:45:56<29:30,  5.06s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1875/2224 [2:46:01<29:17,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2965, 'grad_norm': 0.8871018403991459, 'learning_rate': 6.321486313242392e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1875/2224 [2:46:01<29:17,  5.03s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1876/2224 [2:46:06<29:01,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2849, 'grad_norm': 0.8423660181927428, 'learning_rate': 6.286089746982033e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1876/2224 [2:46:06<29:01,  5.00s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1877/2224 [2:46:11<29:02,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2744, 'grad_norm': 0.8460484727302942, 'learning_rate': 6.25078591046011e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1877/2224 [2:46:11<29:02,  5.02s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1878/2224 [2:46:16<28:59,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2656, 'grad_norm': 0.8687416485175142, 'learning_rate': 6.215574878566167e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1878/2224 [2:46:16<28:59,  5.03s/it]
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1879/2224 [2:46:21<28:56,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2566, 'grad_norm': 0.8160424289551177, 'learning_rate': 6.1804567259929e-07, 'epoch': 0.84}
0: 
0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1879/2224 [2:46:21<28:56,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1880/2224 [2:46:26<28:52,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3122, 'grad_norm': 0.8907655276814102, 'learning_rate': 6.145431527236001e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1880/2224 [2:46:26<28:52,  5.04s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1881/2224 [2:46:31<28:49,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3039, 'grad_norm': 0.8855085776164738, 'learning_rate': 6.11049935659393e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1881/2224 [2:46:31<28:49,  5.04s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1882/2224 [2:46:36<28:40,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2917, 'grad_norm': 0.8867557387964635, 'learning_rate': 6.075660288167851e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1882/2224 [2:46:36<28:40,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1883/2224 [2:46:41<28:36,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2932, 'grad_norm': 0.8886586755671119, 'learning_rate': 6.04091439586143e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1883/2224 [2:46:41<28:36,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1884/2224 [2:46:46<28:32,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2685, 'grad_norm': 0.8642052995129652, 'learning_rate': 6.006261753380649e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1884/2224 [2:46:46<28:32,  5.04s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1885/2224 [2:46:51<28:13,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2842, 'grad_norm': 0.8706465150025663, 'learning_rate': 5.971702434233706e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1885/2224 [2:46:51<28:13,  5.00s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1886/2224 [2:46:56<28:16,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2736, 'grad_norm': 0.8370549147952857, 'learning_rate': 5.937236511730832e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1886/2224 [2:46:56<28:16,  5.02s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1887/2224 [2:47:02<28:13,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3095, 'grad_norm': 0.8890349261342029, 'learning_rate': 5.902864058984121e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1887/2224 [2:47:02<28:13,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1888/2224 [2:47:07<28:07,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2698, 'grad_norm': 0.8228643927363073, 'learning_rate': 5.868585148907413e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1888/2224 [2:47:07<28:07,  5.02s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1889/2224 [2:47:12<28:04,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2696, 'grad_norm': 0.8760346121043604, 'learning_rate': 5.8343998542161e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1889/2224 [2:47:12<28:04,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2224 [2:47:17<28:05,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2871, 'grad_norm': 0.912569801047638, 'learning_rate': 5.800308247427e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2224 [2:47:17<28:05,  5.05s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1891/2224 [2:47:22<28:02,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2545, 'grad_norm': 0.8290530336794748, 'learning_rate': 5.766310400858177e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1891/2224 [2:47:22<28:02,  5.05s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1892/2224 [2:47:27<27:57,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.277, 'grad_norm': 0.856584692541318, 'learning_rate': 5.732406386628814e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1892/2224 [2:47:27<27:57,  5.05s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1893/2224 [2:47:32<27:56,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2741, 'grad_norm': 0.8322946174491935, 'learning_rate': 5.698596276659063e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1893/2224 [2:47:32<27:56,  5.06s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1894/2224 [2:47:37<27:47,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3013, 'grad_norm': 0.872153028702675, 'learning_rate': 5.664880142669838e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1894/2224 [2:47:37<27:47,  5.05s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1895/2224 [2:47:42<27:41,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2407, 'grad_norm': 0.8739486898989919, 'learning_rate': 5.631258056182737e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1895/2224 [2:47:42<27:41,  5.05s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1896/2224 [2:47:47<27:40,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2894, 'grad_norm': 0.8552473994549971, 'learning_rate': 5.597730088519849e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1896/2224 [2:47:47<27:40,  5.06s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1897/2224 [2:47:52<27:35,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3077, 'grad_norm': 0.8999010270780041, 'learning_rate': 5.564296310803613e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1897/2224 [2:47:52<27:35,  5.06s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1898/2224 [2:47:57<27:23,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2854, 'grad_norm': 0.894976778013317, 'learning_rate': 5.530956793956644e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1898/2224 [2:47:57<27:23,  5.04s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1899/2224 [2:48:02<27:15,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2877, 'grad_norm': 0.8794005756096894, 'learning_rate': 5.497711608701622e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1899/2224 [2:48:02<27:15,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1900/2224 [2:48:07<27:08,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.298, 'grad_norm': 0.8958379105951714, 'learning_rate': 5.464560825561127e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1900/2224 [2:48:07<27:08,  5.03s/it]
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1901/2224 [2:48:12<27:08,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3054, 'grad_norm': 0.8583865057756817, 'learning_rate': 5.431504514857466e-07, 'epoch': 0.85}
0: 
0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1901/2224 [2:48:12<27:08,  5.04s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1902/2224 [2:48:17<26:54,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2941, 'grad_norm': 0.8706018746682469, 'learning_rate': 5.398542746712553e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1902/2224 [2:48:17<26:54,  5.01s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1903/2224 [2:48:22<26:57,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2681, 'grad_norm': 0.8322654234294774, 'learning_rate': 5.36567559104777e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1903/2224 [2:48:22<26:57,  5.04s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1904/2224 [2:48:27<26:50,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2982, 'grad_norm': 0.8734011444618435, 'learning_rate': 5.332903117583749e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1904/2224 [2:48:27<26:50,  5.03s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1905/2224 [2:48:32<26:47,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2935, 'grad_norm': 0.8812188411303126, 'learning_rate': 5.300225395840314e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1905/2224 [2:48:32<26:47,  5.04s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1906/2224 [2:48:37<26:20,  4.97s/it]
0:                                                      
0: 
0: {'loss': 0.2722, 'grad_norm': 0.8354805237323968, 'learning_rate': 5.2676424951363e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1906/2224 [2:48:37<26:20,  4.97s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1907/2224 [2:48:42<26:19,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2796, 'grad_norm': 0.8927274893703917, 'learning_rate': 5.235154484589356e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1907/2224 [2:48:42<26:19,  4.98s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1908/2224 [2:48:47<26:20,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.3117, 'grad_norm': 0.9183717856282082, 'learning_rate': 5.202761433115883e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1908/2224 [2:48:47<26:20,  5.00s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1909/2224 [2:48:52<26:17,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2746, 'grad_norm': 0.8382643656417355, 'learning_rate': 5.170463409430821e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1909/2224 [2:48:52<26:17,  5.01s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1910/2224 [2:48:57<26:03,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2851, 'grad_norm': 0.9003826928117921, 'learning_rate': 5.138260482047563e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1910/2224 [2:48:57<26:03,  4.98s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1911/2224 [2:49:02<26:06,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.8192966092426606, 'learning_rate': 5.106152719277724e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1911/2224 [2:49:02<26:06,  5.00s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1912/2224 [2:49:07<25:53,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2912, 'grad_norm': 0.8643454549411511, 'learning_rate': 5.074140189231091e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1912/2224 [2:49:07<25:53,  4.98s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1913/2224 [2:49:12<25:59,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2944, 'grad_norm': 0.8351023656973551, 'learning_rate': 5.042222959815418e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1913/2224 [2:49:12<25:59,  5.01s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1914/2224 [2:49:17<25:46,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2519, 'grad_norm': 0.8628749467048963, 'learning_rate': 5.010401098736295e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1914/2224 [2:49:17<25:46,  4.99s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1915/2224 [2:49:22<25:45,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2919, 'grad_norm': 0.8707814928659826, 'learning_rate': 4.97867467349702e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1915/2224 [2:49:22<25:45,  5.00s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1916/2224 [2:49:27<25:47,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2896, 'grad_norm': 0.8542303631720684, 'learning_rate': 4.947043751398445e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1916/2224 [2:49:27<25:47,  5.03s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1917/2224 [2:49:32<25:40,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2903, 'grad_norm': 0.8738788742611933, 'learning_rate': 4.91550839953881e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1917/2224 [2:49:32<25:40,  5.02s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1918/2224 [2:49:37<25:29,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2767, 'grad_norm': 0.8784200310032061, 'learning_rate': 4.884068684813648e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1918/2224 [2:49:37<25:29,  5.00s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1919/2224 [2:49:42<25:29,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2821, 'grad_norm': 0.8678870391528125, 'learning_rate': 4.852724673915615e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1919/2224 [2:49:42<25:29,  5.01s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1920/2224 [2:49:47<25:25,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.287, 'grad_norm': 0.8767850365089905, 'learning_rate': 4.821476433334332e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1920/2224 [2:49:47<25:25,  5.02s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1921/2224 [2:49:52<25:25,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2552, 'grad_norm': 0.863791844094216, 'learning_rate': 4.79032402935628e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1921/2224 [2:49:52<25:25,  5.03s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1922/2224 [2:49:57<25:25,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.294, 'grad_norm': 0.8411857009797534, 'learning_rate': 4.759267528064648e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1922/2224 [2:49:57<25:25,  5.05s/it]
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1923/2224 [2:50:02<25:18,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2737, 'grad_norm': 0.8784004746530841, 'learning_rate': 4.7283069953391883e-07, 'epoch': 0.86}
0: 
0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1923/2224 [2:50:02<25:18,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1924/2224 [2:50:07<25:00,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.3104, 'grad_norm': 0.9083129239570014, 'learning_rate': 4.6974424968560475e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1924/2224 [2:50:07<25:00,  5.00s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1925/2224 [2:50:12<24:58,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2584, 'grad_norm': 0.8350427338642717, 'learning_rate': 4.6666740980876866e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1925/2224 [2:50:12<24:58,  5.01s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1926/2224 [2:50:17<24:55,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2854, 'grad_norm': 0.8670037132185684, 'learning_rate': 4.6360018643027027e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1926/2224 [2:50:17<24:55,  5.02s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1927/2224 [2:50:23<24:56,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.284, 'grad_norm': 0.8732853343942211, 'learning_rate': 4.6054258605656987e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1927/2224 [2:50:23<24:56,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1928/2224 [2:50:27<24:43,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3008, 'grad_norm': 0.8516979394912633, 'learning_rate': 4.574946151737147e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1928/2224 [2:50:27<24:43,  5.01s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1929/2224 [2:50:33<24:42,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2818, 'grad_norm': 0.8370074404355374, 'learning_rate': 4.544562802473257e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1929/2224 [2:50:33<24:42,  5.03s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1930/2224 [2:50:38<24:43,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2727, 'grad_norm': 0.8807447650822297, 'learning_rate': 4.51427587722581e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1930/2224 [2:50:38<24:43,  5.05s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1931/2224 [2:50:43<24:38,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2606, 'grad_norm': 0.8464574489263041, 'learning_rate': 4.484085440242064e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1931/2224 [2:50:43<24:38,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1932/2224 [2:50:48<24:22,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.299, 'grad_norm': 0.8846585013436483, 'learning_rate': 4.4539915555646016e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1932/2224 [2:50:48<24:22,  5.01s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1933/2224 [2:50:53<24:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2839, 'grad_norm': 0.8505675746086597, 'learning_rate': 4.423994287031175e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1933/2224 [2:50:53<24:23,  5.03s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1934/2224 [2:50:58<24:09,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2892, 'grad_norm': 0.8770655718084919, 'learning_rate': 4.394093698274582e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1934/2224 [2:50:58<24:09,  5.00s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1935/2224 [2:51:03<24:19,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2852, 'grad_norm': 0.8400480623094706, 'learning_rate': 4.364289852722564e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1935/2224 [2:51:03<24:19,  5.05s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1936/2224 [2:51:08<24:14,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.274, 'grad_norm': 0.8406440839657002, 'learning_rate': 4.334582813597621e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1936/2224 [2:51:08<24:14,  5.05s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1937/2224 [2:51:13<24:07,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3035, 'grad_norm': 0.8778866947908981, 'learning_rate': 4.304972643916883e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1937/2224 [2:51:13<24:07,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1938/2224 [2:51:18<24:01,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2882, 'grad_norm': 0.827400343999193, 'learning_rate': 4.275459406492027e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1938/2224 [2:51:18<24:01,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1939/2224 [2:51:23<23:55,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2566, 'grad_norm': 0.8786785503832412, 'learning_rate': 4.246043163929092e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1939/2224 [2:51:23<23:55,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1940/2224 [2:51:28<23:53,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2801, 'grad_norm': 0.8421475459306234, 'learning_rate': 4.2167239786283567e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1940/2224 [2:51:28<23:53,  5.05s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1941/2224 [2:51:33<23:48,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2905, 'grad_norm': 0.8686255467010997, 'learning_rate': 4.1875019127842197e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1941/2224 [2:51:33<23:48,  5.05s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1942/2224 [2:51:38<23:40,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3036, 'grad_norm': 0.8842761497167856, 'learning_rate': 4.158377028385069e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1942/2224 [2:51:38<23:40,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1943/2224 [2:51:43<23:36,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2619, 'grad_norm': 0.8344395826243598, 'learning_rate': 4.129349387213127e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1943/2224 [2:51:43<23:36,  5.04s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1944/2224 [2:51:48<23:18,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2801, 'grad_norm': 0.8997239209296827, 'learning_rate': 4.1004190508443564e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1944/2224 [2:51:48<23:18,  4.99s/it]
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1945/2224 [2:51:53<23:19,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.325, 'grad_norm': 0.9073629376019416, 'learning_rate': 4.0715860806482934e-07, 'epoch': 0.87}
0: 
0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1945/2224 [2:51:53<23:19,  5.02s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1946/2224 [2:51:58<23:12,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2633, 'grad_norm': 0.8649357845589372, 'learning_rate': 4.0428505377879235e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1946/2224 [2:51:58<23:12,  5.01s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1947/2224 [2:52:03<23:00,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2759, 'grad_norm': 0.8643016535041015, 'learning_rate': 4.0142124832195815e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1947/2224 [2:52:03<23:00,  4.99s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1948/2224 [2:52:08<23:01,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2907, 'grad_norm': 0.8924237208082312, 'learning_rate': 3.9856719776927956e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1948/2224 [2:52:08<23:01,  5.01s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1949/2224 [2:52:13<23:00,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2955, 'grad_norm': 0.846585226507979, 'learning_rate': 3.9572290817501634e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1949/2224 [2:52:13<23:00,  5.02s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1950/2224 [2:52:18<22:57,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2949, 'grad_norm': 0.8675683935069073, 'learning_rate': 3.9288838557272115e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1950/2224 [2:52:18<22:57,  5.03s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1951/2224 [2:52:23<22:53,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3409, 'grad_norm': 0.8964244251802495, 'learning_rate': 3.900636359752297e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1951/2224 [2:52:23<22:53,  5.03s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1952/2224 [2:52:28<22:52,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2606, 'grad_norm': 0.8591179606129636, 'learning_rate': 3.872486653746471e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1952/2224 [2:52:28<22:52,  5.05s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1953/2224 [2:52:33<22:52,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2853, 'grad_norm': 0.841659038014575, 'learning_rate': 3.844434797423313e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1953/2224 [2:52:33<22:52,  5.06s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1954/2224 [2:52:38<22:45,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2785, 'grad_norm': 0.8352632893609556, 'learning_rate': 3.816480850288862e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1954/2224 [2:52:38<22:45,  5.06s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1955/2224 [2:52:43<22:28,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.242, 'grad_norm': 0.807191693521029, 'learning_rate': 3.788624871641455e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1955/2224 [2:52:43<22:28,  5.01s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1956/2224 [2:52:48<22:25,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2619, 'grad_norm': 0.904059283139361, 'learning_rate': 3.7608669205716153e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1956/2224 [2:52:48<22:25,  5.02s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1957/2224 [2:52:53<22:25,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2795, 'grad_norm': 0.8573749178782214, 'learning_rate': 3.7332070559619073e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1957/2224 [2:52:53<22:25,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1958/2224 [2:52:58<22:20,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2957, 'grad_norm': 0.8511539608270784, 'learning_rate': 3.705645336486846e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1958/2224 [2:52:58<22:20,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1959/2224 [2:53:03<22:06,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2943, 'grad_norm': 0.8758431803564691, 'learning_rate': 3.678181820612742e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1959/2224 [2:53:03<22:06,  5.00s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1960/2224 [2:53:08<21:54,  4.98s/it]
0:                                                      
0: 
0: {'loss': 0.2885, 'grad_norm': 0.8521842075832253, 'learning_rate': 3.6508165665975704e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1960/2224 [2:53:08<21:54,  4.98s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1961/2224 [2:53:13<21:56,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.255, 'grad_norm': 0.865100157062585, 'learning_rate': 3.623549632490897e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1961/2224 [2:53:13<21:56,  5.01s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1962/2224 [2:53:18<22:00,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.309, 'grad_norm': 0.887325848246463, 'learning_rate': 3.5963810761337147e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1962/2224 [2:53:18<22:00,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1963/2224 [2:53:24<21:55,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2517, 'grad_norm': 0.8449898395399917, 'learning_rate': 3.569310955158295e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1963/2224 [2:53:24<21:55,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1964/2224 [2:53:29<21:49,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2875, 'grad_norm': 0.883403712269344, 'learning_rate': 3.5423393269881477e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1964/2224 [2:53:29<21:49,  5.03s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1965/2224 [2:53:34<21:45,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2635, 'grad_norm': 0.8504439036428532, 'learning_rate': 3.5154662488378346e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1965/2224 [2:53:34<21:45,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1966/2224 [2:53:39<21:40,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2735, 'grad_norm': 0.8419987729918537, 'learning_rate': 3.488691777712844e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1966/2224 [2:53:39<21:40,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1967/2224 [2:53:44<21:36,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2948, 'grad_norm': 0.9038960743635417, 'learning_rate': 3.46201597040951e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1967/2224 [2:53:44<21:36,  5.04s/it]
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1968/2224 [2:53:49<21:31,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.286, 'grad_norm': 0.8959922789844902, 'learning_rate': 3.435438883514874e-07, 'epoch': 0.88}
0: 
0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1968/2224 [2:53:49<21:31,  5.04s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1969/2224 [2:53:54<21:20,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2742, 'grad_norm': 0.8409513481029552, 'learning_rate': 3.4089605734065556e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1969/2224 [2:53:54<21:20,  5.02s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1970/2224 [2:53:59<21:18,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2972, 'grad_norm': 0.8578548139284232, 'learning_rate': 3.382581096252646e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1970/2224 [2:53:59<21:18,  5.03s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1971/2224 [2:54:04<21:13,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3098, 'grad_norm': 0.9125719487076616, 'learning_rate': 3.3563005080115787e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1971/2224 [2:54:04<21:13,  5.04s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1972/2224 [2:54:09<21:07,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.29, 'grad_norm': 0.8747596857761702, 'learning_rate': 3.3301188644320194e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1972/2224 [2:54:09<21:07,  5.03s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1973/2224 [2:54:14<21:01,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2706, 'grad_norm': 0.8786207170296052, 'learning_rate': 3.3040362210527365e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1973/2224 [2:54:14<21:01,  5.02s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1974/2224 [2:54:19<20:57,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3064, 'grad_norm': 0.8806403248505305, 'learning_rate': 3.2780526332024997e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1974/2224 [2:54:19<20:57,  5.03s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1975/2224 [2:54:24<21:33,  5.20s/it]
0:                                                      
0: 
0: {'loss': 0.2879, 'grad_norm': 0.8839328326579171, 'learning_rate': 3.252168155999957e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1975/2224 [2:54:24<21:33,  5.20s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1976/2224 [2:54:29<21:12,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.3114, 'grad_norm': 0.8770052662834581, 'learning_rate': 3.2263828443534873e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1976/2224 [2:54:29<21:12,  5.13s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1977/2224 [2:54:35<21:06,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.2939, 'grad_norm': 0.8323467645203341, 'learning_rate': 3.200696752961141e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1977/2224 [2:54:35<21:06,  5.13s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1978/2224 [2:54:40<20:54,  5.10s/it]
0:                                                      
0: 
0: {'loss': 0.2934, 'grad_norm': 0.8634548202203448, 'learning_rate': 3.1751099363104944e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1978/2224 [2:54:40<20:54,  5.10s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1979/2224 [2:54:45<20:46,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.2836, 'grad_norm': 0.8884413532107728, 'learning_rate': 3.149622448678502e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1979/2224 [2:54:45<20:46,  5.09s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1980/2224 [2:54:50<20:37,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2882, 'grad_norm': 0.8630545417208979, 'learning_rate': 3.124234344131444e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1980/2224 [2:54:50<20:37,  5.07s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1981/2224 [2:54:55<20:33,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.265, 'grad_norm': 0.8316906554214631, 'learning_rate': 3.098945676524773e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1981/2224 [2:54:55<20:33,  5.08s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1982/2224 [2:55:00<20:29,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.3026, 'grad_norm': 0.857912607518658, 'learning_rate': 3.0737564995029943e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1982/2224 [2:55:00<20:29,  5.08s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1983/2224 [2:55:05<20:23,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2624, 'grad_norm': 0.8475492231700562, 'learning_rate': 3.0486668664995887e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1983/2224 [2:55:05<20:23,  5.08s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1984/2224 [2:55:10<20:14,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2734, 'grad_norm': 0.8560719370775547, 'learning_rate': 3.023676830736849e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1984/2224 [2:55:10<20:14,  5.06s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1985/2224 [2:55:15<20:09,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2892, 'grad_norm': 0.8697853912731897, 'learning_rate': 2.9987864452258266e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1985/2224 [2:55:15<20:09,  5.06s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1986/2224 [2:55:20<20:05,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2638, 'grad_norm': 0.8656858377822467, 'learning_rate': 2.9739957627661407e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1986/2224 [2:55:20<20:05,  5.06s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1987/2224 [2:55:25<19:55,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2864, 'grad_norm': 0.8655487681720595, 'learning_rate': 2.9493048359459485e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1987/2224 [2:55:25<19:55,  5.05s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1988/2224 [2:55:30<19:48,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2975, 'grad_norm': 0.8793858063731629, 'learning_rate': 2.924713717141797e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1988/2224 [2:55:30<19:48,  5.03s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1989/2224 [2:55:35<19:34,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2843, 'grad_norm': 0.8874574919923613, 'learning_rate': 2.900222458518481e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1989/2224 [2:55:35<19:34,  5.00s/it]
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1990/2224 [2:55:40<19:34,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8726456951127455, 'learning_rate': 2.8758311120289985e-07, 'epoch': 0.89}
0: 
0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1990/2224 [2:55:40<19:34,  5.02s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1991/2224 [2:55:45<19:32,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.291, 'grad_norm': 0.8825211352528681, 'learning_rate': 2.851539729414382e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1991/2224 [2:55:45<19:32,  5.03s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1992/2224 [2:55:50<19:26,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2465, 'grad_norm': 0.8643787743503007, 'learning_rate': 2.8273483622036193e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1992/2224 [2:55:50<19:26,  5.03s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1993/2224 [2:55:55<19:28,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8497276834127682, 'learning_rate': 2.8032570617135377e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1993/2224 [2:55:55<19:28,  5.06s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1994/2224 [2:56:00<19:21,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.263, 'grad_norm': 0.8609399294600594, 'learning_rate': 2.779265879048698e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1994/2224 [2:56:00<19:21,  5.05s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1995/2224 [2:56:05<19:19,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2864, 'grad_norm': 0.832997460572797, 'learning_rate': 2.7553748651012737e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1995/2224 [2:56:05<19:19,  5.06s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1996/2224 [2:56:10<19:13,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2797, 'grad_norm': 0.8944887784639893, 'learning_rate': 2.7315840705509524e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1996/2224 [2:56:10<19:13,  5.06s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1997/2224 [2:56:15<19:06,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3178, 'grad_norm': 0.9026858530870071, 'learning_rate': 2.707893545864837e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1997/2224 [2:56:15<19:06,  5.05s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1998/2224 [2:56:21<19:02,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2941, 'grad_norm': 0.8402610719140203, 'learning_rate': 2.6843033412973175e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1998/2224 [2:56:21<19:02,  5.05s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1999/2224 [2:56:26<19:00,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2811, 'grad_norm': 0.8257722721000831, 'learning_rate': 2.660813506889992e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1999/2224 [2:56:26<19:00,  5.07s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2000/2224 [2:56:31<18:52,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2856, 'grad_norm': 0.865196135184429, 'learning_rate': 2.63742409247153e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2000/2224 [2:56:31<18:52,  5.06s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2001/2224 [2:57:26<1:15:21, 20.27s/it]
0:                                                        
0: 
0: {'loss': 0.272, 'grad_norm': 0.8517434134474907, 'learning_rate': 2.614135147657587e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2001/2224 [2:57:26<1:15:21, 20.27s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2002/2224 [2:57:31<58:06, 15.70s/it]  
0:                                                      
0: 
0: {'loss': 0.2682, 'grad_norm': 0.8500816024881707, 'learning_rate': 2.59094672185069e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2002/2224 [2:57:31<58:06, 15.70s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2003/2224 [2:57:37<46:05, 12.51s/it]
0:                                                      
0: 
0: {'loss': 0.2822, 'grad_norm': 0.8752478825969087, 'learning_rate': 2.567858864240141e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2003/2224 [2:57:37<46:05, 12.51s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2004/2224 [2:57:42<37:39, 10.27s/it]
0:                                                      
0: 
0: {'loss': 0.2838, 'grad_norm': 0.8906428780783487, 'learning_rate': 2.5448716238019145e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2004/2224 [2:57:42<37:39, 10.27s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2005/2224 [2:57:47<31:41,  8.68s/it]
0:                                                      
0: 
0: {'loss': 0.2971, 'grad_norm': 0.8983860198640533, 'learning_rate': 2.52198504929852e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2005/2224 [2:57:47<31:41,  8.68s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2006/2224 [2:57:52<27:35,  7.60s/it]
0:                                                      
0: 
0: {'loss': 0.273, 'grad_norm': 0.868996292251645, 'learning_rate': 2.4991991892789614e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2006/2224 [2:57:52<27:35,  7.60s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2007/2224 [2:57:57<24:40,  6.82s/it]
0:                                                      
0: 
0: {'loss': 0.2641, 'grad_norm': 0.8483810503226162, 'learning_rate': 2.47651409207858e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2007/2224 [2:57:57<24:40,  6.82s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2008/2224 [2:58:02<22:39,  6.30s/it]
0:                                                      
0: 
0: {'loss': 0.2948, 'grad_norm': 0.8987124555060878, 'learning_rate': 2.4539298058189555e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2008/2224 [2:58:02<22:39,  6.30s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2009/2224 [2:58:07<21:15,  5.93s/it]
0:                                                      
0: 
0: {'loss': 0.2901, 'grad_norm': 0.8651716891637152, 'learning_rate': 2.4314463784078537e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2009/2224 [2:58:07<21:15,  5.93s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2010/2224 [2:58:12<20:06,  5.64s/it]
0:                                                      
0: 
0: {'loss': 0.2986, 'grad_norm': 0.8767916685351379, 'learning_rate': 2.4090638575390625e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2010/2224 [2:58:12<20:06,  5.64s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2011/2224 [2:58:17<19:20,  5.45s/it]
0:                                                      
0: 
0: {'loss': 0.2657, 'grad_norm': 0.8416107362620396, 'learning_rate': 2.38678229069233e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2011/2224 [2:58:17<19:20,  5.45s/it]
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2012/2224 [2:58:22<18:41,  5.29s/it]
0:                                                      
0: 
0: {'loss': 0.285, 'grad_norm': 0.8765061032205418, 'learning_rate': 2.3646017251332476e-07, 'epoch': 0.9}
0: 
0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2012/2224 [2:58:22<18:41,  5.29s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2013/2224 [2:58:27<18:21,  5.22s/it]
0:                                                      
0: 
0: {'loss': 0.2725, 'grad_norm': 0.8723507705836788, 'learning_rate': 2.342522207913156e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2013/2224 [2:58:27<18:21,  5.22s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2014/2224 [2:58:32<18:05,  5.17s/it]
0:                                                      
0: 
0: {'loss': 0.2933, 'grad_norm': 0.8663436699240135, 'learning_rate': 2.3205437858690449e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2014/2224 [2:58:32<18:05,  5.17s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2015/2224 [2:58:37<17:52,  5.13s/it]
0:                                                      
0: 
0: {'loss': 0.2834, 'grad_norm': 0.8366239339769145, 'learning_rate': 2.2986665056234425e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2015/2224 [2:58:37<17:52,  5.13s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2016/2224 [2:58:42<17:42,  5.11s/it]
0:                                                      
0: 
0: {'loss': 0.2776, 'grad_norm': 0.9207822764575254, 'learning_rate': 2.2768904135843317e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2016/2224 [2:58:42<17:42,  5.11s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2017/2224 [2:58:47<17:37,  5.11s/it]
0:                                                      
0: 
0: {'loss': 0.2681, 'grad_norm': 0.8582986524703177, 'learning_rate': 2.2552155559450672e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2017/2224 [2:58:47<17:37,  5.11s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2018/2224 [2:58:52<17:28,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.301, 'grad_norm': 0.8654712983143299, 'learning_rate': 2.233641978684209e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2018/2224 [2:58:52<17:28,  5.09s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2019/2224 [2:58:57<17:17,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2988, 'grad_norm': 0.9053492475418824, 'learning_rate': 2.212169727565522e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2019/2224 [2:58:57<17:17,  5.06s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2020/2224 [2:59:02<17:14,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2859, 'grad_norm': 0.863174791340394, 'learning_rate': 2.1907988481377984e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2020/2224 [2:59:02<17:14,  5.07s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2021/2224 [2:59:07<17:08,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3143, 'grad_norm': 0.8594737573289437, 'learning_rate': 2.169529385734809e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2021/2224 [2:59:07<17:08,  5.06s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2022/2224 [2:59:12<17:03,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2783, 'grad_norm': 0.8947743173157344, 'learning_rate': 2.1483613854751728e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2022/2224 [2:59:12<17:03,  5.07s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2023/2224 [2:59:17<16:49,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2894, 'grad_norm': 0.8775137244958657, 'learning_rate': 2.1272948922622881e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2023/2224 [2:59:17<16:49,  5.02s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2024/2224 [2:59:22<16:45,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3064, 'grad_norm': 0.8757185439377387, 'learning_rate': 2.1063299507842362e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2024/2224 [2:59:22<16:45,  5.03s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2025/2224 [2:59:27<16:39,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3117, 'grad_norm': 0.8812409877472039, 'learning_rate': 2.0854666055136697e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2025/2224 [2:59:27<16:39,  5.02s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2026/2224 [2:59:32<16:36,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.258, 'grad_norm': 0.8427443026359406, 'learning_rate': 2.0647049007077145e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2026/2224 [2:59:32<16:36,  5.03s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2027/2224 [2:59:37<16:22,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2679, 'grad_norm': 0.8664660271842233, 'learning_rate': 2.0440448804079195e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2027/2224 [2:59:37<16:22,  4.99s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2028/2224 [2:59:42<16:18,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.285, 'grad_norm': 0.8431058736753068, 'learning_rate': 2.0234865884400934e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2028/2224 [2:59:42<16:18,  4.99s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2029/2224 [2:59:47<16:25,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2492, 'grad_norm': 0.8522490355324926, 'learning_rate': 2.0030300684142745e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2029/2224 [2:59:47<16:25,  5.06s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2030/2224 [2:59:52<16:12,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2901, 'grad_norm': 0.8474185256301981, 'learning_rate': 1.9826753637246122e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2030/2224 [2:59:52<16:12,  5.01s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2031/2224 [2:59:57<16:08,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3048, 'grad_norm': 1.160223383154196, 'learning_rate': 1.9624225175492672e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2031/2224 [2:59:57<16:08,  5.02s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2032/2224 [3:00:02<16:04,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2786, 'grad_norm': 0.8361892002786238, 'learning_rate': 1.9422715728503294e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2032/2224 [3:00:02<16:04,  5.03s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2033/2224 [3:00:07<16:01,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2893, 'grad_norm': 0.8793958497973654, 'learning_rate': 1.9222225723737385e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2033/2224 [3:00:07<16:01,  5.03s/it]
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2034/2224 [3:00:12<15:59,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2754, 'grad_norm': 0.8541507800840837, 'learning_rate': 1.9022755586491747e-07, 'epoch': 0.91}
0: 
0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2034/2224 [3:00:13<15:59,  5.05s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2035/2224 [3:00:18<15:54,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2857, 'grad_norm': 0.8181856644776342, 'learning_rate': 1.8824305739899685e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2035/2224 [3:00:18<15:54,  5.05s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2036/2224 [3:00:23<15:50,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.291, 'grad_norm': 0.8513583441253687, 'learning_rate': 1.8626876604930177e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2036/2224 [3:00:23<15:50,  5.05s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2037/2224 [3:00:28<15:39,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2796, 'grad_norm': 0.8784032310883003, 'learning_rate': 1.8430468600387108e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2037/2224 [3:00:28<15:39,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2038/2224 [3:00:33<15:35,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2782, 'grad_norm': 0.8575927880180197, 'learning_rate': 1.82350821429082e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2038/2224 [3:00:33<15:35,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2039/2224 [3:00:38<15:30,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.266, 'grad_norm': 0.8610832925204264, 'learning_rate': 1.804071764696408e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2039/2224 [3:00:38<15:30,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2040/2224 [3:00:43<15:25,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2565, 'grad_norm': 0.8822905678780868, 'learning_rate': 1.7847375524857712e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2040/2224 [3:00:43<15:25,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2041/2224 [3:00:48<15:14,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2842, 'grad_norm': 0.8278799201712304, 'learning_rate': 1.7655056186722973e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2041/2224 [3:00:48<15:14,  5.00s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2042/2224 [3:00:53<15:08,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2634, 'grad_norm': 0.8706897802487051, 'learning_rate': 1.7463760040524469e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2042/2224 [3:00:53<15:08,  4.99s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2043/2224 [3:00:58<15:07,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2927, 'grad_norm': 0.9231372964446499, 'learning_rate': 1.7273487492056206e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2043/2224 [3:00:58<15:07,  5.01s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2044/2224 [3:01:03<15:05,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2653, 'grad_norm': 0.8800204996134102, 'learning_rate': 1.7084238944940656e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2044/2224 [3:01:03<15:05,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2045/2224 [3:01:08<15:06,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3115, 'grad_norm': 0.8857076186016395, 'learning_rate': 1.6896014800628412e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2045/2224 [3:01:08<15:06,  5.06s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2046/2224 [3:01:13<14:54,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2841, 'grad_norm': 0.8660682811454076, 'learning_rate': 1.6708815458396754e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2046/2224 [3:01:13<14:54,  5.02s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2047/2224 [3:01:18<14:52,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3206, 'grad_norm': 0.8703475243038432, 'learning_rate': 1.6522641315349252e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2047/2224 [3:01:18<14:52,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2048/2224 [3:01:23<14:46,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2947, 'grad_norm': 0.8678629737021615, 'learning_rate': 1.6337492766414609e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2048/2224 [3:01:23<14:46,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2049/2224 [3:01:28<14:41,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2636, 'grad_norm': 0.8519438462418558, 'learning_rate': 1.615337020434593e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2049/2224 [3:01:28<14:41,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2224 [3:01:33<14:36,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2957, 'grad_norm': 0.8576778778316347, 'learning_rate': 1.5970274019720067e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2050/2224 [3:01:33<14:36,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2051/2224 [3:01:38<14:32,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2886, 'grad_norm': 0.8512101109780457, 'learning_rate': 1.5788204600936497e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2051/2224 [3:01:38<14:32,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2052/2224 [3:01:43<14:25,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.305, 'grad_norm': 0.8521452463842654, 'learning_rate': 1.5607162334216608e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2052/2224 [3:01:43<14:25,  5.03s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2053/2224 [3:01:48<14:21,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2614, 'grad_norm': 0.8369178788059451, 'learning_rate': 1.5427147603602976e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2053/2224 [3:01:48<14:21,  5.04s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2054/2224 [3:01:53<14:08,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2495, 'grad_norm': 0.845821022772233, 'learning_rate': 1.5248160790958533e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2054/2224 [3:01:53<14:08,  4.99s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2055/2224 [3:01:58<14:08,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2847, 'grad_norm': 0.8671960700689015, 'learning_rate': 1.5070202275965562e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2055/2224 [3:01:58<14:08,  5.02s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2056/2224 [3:02:03<14:03,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2698, 'grad_norm': 0.8584113758117669, 'learning_rate': 1.4893272436125207e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2056/2224 [3:02:03<14:03,  5.02s/it]
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2057/2224 [3:02:08<13:58,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2737, 'grad_norm': 0.8532204404567191, 'learning_rate': 1.471737164675624e-07, 'epoch': 0.92}
0: 
0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2057/2224 [3:02:08<13:58,  5.02s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2058/2224 [3:02:13<13:48,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2635, 'grad_norm': 0.843547114721617, 'learning_rate': 1.4542500280994797e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2058/2224 [3:02:13<13:48,  4.99s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2059/2224 [3:02:18<13:45,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.28, 'grad_norm': 0.8436692519936516, 'learning_rate': 1.4368658709793092e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2059/2224 [3:02:18<13:45,  5.01s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2060/2224 [3:02:23<13:43,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.29, 'grad_norm': 0.8426619002531811, 'learning_rate': 1.419584730191903e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2060/2224 [3:02:23<13:43,  5.02s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2061/2224 [3:02:28<13:36,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2866, 'grad_norm': 0.846574754401008, 'learning_rate': 1.4024066423955097e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2061/2224 [3:02:28<13:36,  5.01s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2062/2224 [3:02:33<13:30,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2762, 'grad_norm': 0.8278205899452824, 'learning_rate': 1.385331644029786e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2062/2224 [3:02:33<13:30,  5.01s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2063/2224 [3:02:38<13:29,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2915, 'grad_norm': 0.8719234857660841, 'learning_rate': 1.368359771315697e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2063/2224 [3:02:38<13:29,  5.03s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2064/2224 [3:02:43<13:26,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3028, 'grad_norm': 0.9148041933571852, 'learning_rate': 1.3514910602554442e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2064/2224 [3:02:43<13:26,  5.04s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2065/2224 [3:02:48<13:25,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2786, 'grad_norm': 0.8271018995151074, 'learning_rate': 1.3347255466324095e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2065/2224 [3:02:48<13:25,  5.06s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2066/2224 [3:02:53<13:22,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2837, 'grad_norm': 0.8397339506498186, 'learning_rate': 1.3180632660110493e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2066/2224 [3:02:53<13:22,  5.08s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2067/2224 [3:02:58<13:12,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2597, 'grad_norm': 0.8780532898987456, 'learning_rate': 1.3015042537368406e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2067/2224 [3:02:58<13:12,  5.05s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2068/2224 [3:03:04<13:10,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2906, 'grad_norm': 0.8562196709060064, 'learning_rate': 1.285048544936196e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2068/2224 [3:03:04<13:10,  5.07s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2069/2224 [3:03:09<13:06,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2858, 'grad_norm': 0.8754931154651996, 'learning_rate': 1.268696174516404e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2069/2224 [3:03:09<13:06,  5.07s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2070/2224 [3:03:14<13:00,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2781, 'grad_norm': 0.8359032627295044, 'learning_rate': 1.252447177165511e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2070/2224 [3:03:14<13:00,  5.07s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2071/2224 [3:03:19<12:54,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2871, 'grad_norm': 0.8858005199889395, 'learning_rate': 1.236301587352312e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2071/2224 [3:03:19<12:54,  5.06s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2072/2224 [3:03:24<12:51,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2634, 'grad_norm': 0.8424535541145756, 'learning_rate': 1.2202594393262268e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2072/2224 [3:03:24<12:51,  5.07s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2073/2224 [3:03:29<12:45,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2632, 'grad_norm': 0.8977882426191633, 'learning_rate': 1.2043207671172675e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2073/2224 [3:03:29<12:45,  5.07s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2074/2224 [3:03:34<12:39,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2813, 'grad_norm': 0.8880368733801829, 'learning_rate': 1.188485604535905e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2074/2224 [3:03:34<12:39,  5.06s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2075/2224 [3:03:39<12:31,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2968, 'grad_norm': 0.9033859340759557, 'learning_rate': 1.1727539851730696e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2075/2224 [3:03:39<12:31,  5.05s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2076/2224 [3:03:44<12:26,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2987, 'grad_norm': 0.8615322018089623, 'learning_rate': 1.1571259424000448e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2076/2224 [3:03:44<12:26,  5.05s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2077/2224 [3:03:49<12:21,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2931, 'grad_norm': 0.8771972182850906, 'learning_rate': 1.1416015093683785e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2077/2224 [3:03:49<12:21,  5.04s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2078/2224 [3:03:54<12:12,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2615, 'grad_norm': 0.8608834113111015, 'learning_rate': 1.126180719009845e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2078/2224 [3:03:54<12:12,  5.02s/it]
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2079/2224 [3:03:59<12:11,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8097816199691033, 'learning_rate': 1.1108636040363718e-07, 'epoch': 0.93}
0: 
0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2079/2224 [3:03:59<12:11,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2080/2224 [3:04:04<12:07,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2776, 'grad_norm': 0.9085460168532138, 'learning_rate': 1.0956501969399457e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2080/2224 [3:04:04<12:07,  5.05s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2081/2224 [3:04:09<12:01,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2963, 'grad_norm': 0.8903591998327226, 'learning_rate': 1.0805405299925686e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2081/2224 [3:04:09<12:01,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2082/2224 [3:04:14<11:56,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3028, 'grad_norm': 0.8919291520254747, 'learning_rate': 1.0655346352461793e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2082/2224 [3:04:14<11:56,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2083/2224 [3:04:19<11:51,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2731, 'grad_norm': 0.8426551497780324, 'learning_rate': 1.050632544532576e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2083/2224 [3:04:19<11:51,  5.05s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2084/2224 [3:04:24<11:46,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2624, 'grad_norm': 0.8505443121485518, 'learning_rate': 1.0358342894633722e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2084/2224 [3:04:24<11:46,  5.05s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2085/2224 [3:04:29<11:37,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3038, 'grad_norm': 0.9122010343028776, 'learning_rate': 1.0211399014299017e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2085/2224 [3:04:29<11:37,  5.02s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2086/2224 [3:04:34<11:34,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2863, 'grad_norm': 0.8704674026702816, 'learning_rate': 1.006549411603186e-07, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2086/2224 [3:04:34<11:34,  5.03s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2087/2224 [3:04:39<11:26,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.281, 'grad_norm': 0.8446423375302581, 'learning_rate': 9.920628509338337e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2087/2224 [3:04:39<11:26,  5.01s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2088/2224 [3:04:44<11:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2847, 'grad_norm': 0.8668470052698533, 'learning_rate': 9.776802501519911e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2088/2224 [3:04:44<11:23,  5.03s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2089/2224 [3:04:49<11:15,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.3085, 'grad_norm': 0.9089001636496536, 'learning_rate': 9.634016397672807e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2089/2224 [3:04:49<11:15,  5.00s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2090/2224 [3:04:54<11:15,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2605, 'grad_norm': 0.8303601257086688, 'learning_rate': 9.492270500687296e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2090/2224 [3:04:54<11:15,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2091/2224 [3:05:00<11:11,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2878, 'grad_norm': 0.8483365874963011, 'learning_rate': 9.351565111247018e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2091/2224 [3:05:00<11:11,  5.05s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2092/2224 [3:05:05<11:05,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2897, 'grad_norm': 0.8879841846992514, 'learning_rate': 9.211900527828554e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2092/2224 [3:05:05<11:05,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2093/2224 [3:05:10<11:00,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2747, 'grad_norm': 0.8356111822583059, 'learning_rate': 9.073277046700469e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2093/2224 [3:05:10<11:00,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2094/2224 [3:05:15<10:50,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3029, 'grad_norm': 0.884081435701083, 'learning_rate': 8.935694961922869e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2094/2224 [3:05:15<10:50,  5.01s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2095/2224 [3:05:19<10:43,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2813, 'grad_norm': 0.9113389654236709, 'learning_rate': 8.799154565346913e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2095/2224 [3:05:19<10:43,  4.99s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2096/2224 [3:05:24<10:39,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2836, 'grad_norm': 0.8620654772729127, 'learning_rate': 8.663656146613964e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2096/2224 [3:05:24<10:39,  5.00s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2097/2224 [3:05:29<10:35,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2804, 'grad_norm': 0.8713644423871484, 'learning_rate': 8.529199993154824e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2097/2224 [3:05:29<10:35,  5.00s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2098/2224 [3:05:35<10:32,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2987, 'grad_norm': 0.8884407957184216, 'learning_rate': 8.39578639018973e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2098/2224 [3:05:35<10:32,  5.02s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2099/2224 [3:05:40<10:29,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2545, 'grad_norm': 0.8213302442968028, 'learning_rate': 8.263415620727134e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2099/2224 [3:05:40<10:29,  5.04s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2100/2224 [3:05:45<10:20,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2975, 'grad_norm': 0.905599500048471, 'learning_rate': 8.132087965563363e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2100/2224 [3:05:45<10:20,  5.00s/it]
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2101/2224 [3:05:50<10:18,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2821, 'grad_norm': 0.8795823580688158, 'learning_rate': 8.001803703282074e-08, 'epoch': 0.94}
0: 
0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2101/2224 [3:05:50<10:18,  5.03s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2102/2224 [3:05:55<10:11,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2689, 'grad_norm': 0.8424000847868806, 'learning_rate': 7.872563110253584e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2102/2224 [3:05:55<10:11,  5.01s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2103/2224 [3:06:00<10:05,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2714, 'grad_norm': 0.8578904005925727, 'learning_rate': 7.74436646063409e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2103/2224 [3:06:00<10:05,  5.01s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2104/2224 [3:06:05<10:03,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2936, 'grad_norm': 0.844749869375709, 'learning_rate': 7.617214026365616e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2104/2224 [3:06:05<10:03,  5.03s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2105/2224 [3:06:10<09:59,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2742, 'grad_norm': 0.8961778474785042, 'learning_rate': 7.491106077174904e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2105/2224 [3:06:10<09:59,  5.04s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2106/2224 [3:06:15<09:54,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3, 'grad_norm': 0.8757506661728522, 'learning_rate': 7.36604288057291e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2106/2224 [3:06:15<09:54,  5.04s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2107/2224 [3:06:20<09:49,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2683, 'grad_norm': 0.874961163110469, 'learning_rate': 7.242024701854755e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2107/2224 [3:06:20<09:49,  5.04s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2108/2224 [3:06:25<09:45,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2898, 'grad_norm': 0.8787902427554697, 'learning_rate': 7.119051804098442e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2108/2224 [3:06:25<09:45,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2109/2224 [3:06:30<09:41,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2902, 'grad_norm': 0.8628448090694788, 'learning_rate': 6.99712444816486e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2109/2224 [3:06:30<09:41,  5.06s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2110/2224 [3:06:35<09:29,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2659, 'grad_norm': 0.870362734253207, 'learning_rate': 6.876242892696727e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2110/2224 [3:06:35<09:29,  5.00s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2111/2224 [3:06:40<09:31,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2852, 'grad_norm': 0.8368871099218714, 'learning_rate': 6.756407394118591e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2111/2224 [3:06:40<09:31,  5.06s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2112/2224 [3:06:45<09:28,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2505, 'grad_norm': 0.8588420912205745, 'learning_rate': 6.637618206635887e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2112/2224 [3:06:45<09:28,  5.08s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2113/2224 [3:06:50<09:25,  5.09s/it]
0:                                                      
0: 
0: {'loss': 0.2933, 'grad_norm': 0.8579015799847453, 'learning_rate': 6.519875582234547e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2113/2224 [3:06:50<09:25,  5.09s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2114/2224 [3:06:55<09:16,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2583, 'grad_norm': 0.8532119510023914, 'learning_rate': 6.403179770680445e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2114/2224 [3:06:55<09:16,  5.06s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2115/2224 [3:07:00<09:12,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2572, 'grad_norm': 0.8438100837231622, 'learning_rate': 6.287531019518844e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2115/2224 [3:07:00<09:12,  5.07s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2116/2224 [3:07:05<09:03,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2624, 'grad_norm': 0.8558256515793036, 'learning_rate': 6.172929574073894e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2116/2224 [3:07:05<09:03,  5.04s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2117/2224 [3:07:10<09:00,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2601, 'grad_norm': 0.8540216759561885, 'learning_rate': 6.059375677448131e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2117/2224 [3:07:10<09:00,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2118/2224 [3:07:15<08:55,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2979, 'grad_norm': 0.8798560558438085, 'learning_rate': 5.946869570521929e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2118/2224 [3:07:15<08:55,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2119/2224 [3:07:20<08:50,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2924, 'grad_norm': 0.8644750609533007, 'learning_rate': 5.835411491953047e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2119/2224 [3:07:20<08:50,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2120/2224 [3:07:25<08:43,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.27, 'grad_norm': 0.8639547524712893, 'learning_rate': 5.7250016781759673e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2120/2224 [3:07:25<08:43,  5.03s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2121/2224 [3:07:31<08:39,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2855, 'grad_norm': 0.8636722646052019, 'learning_rate': 5.61564036340162e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2121/2224 [3:07:31<08:39,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2122/2224 [3:07:36<08:34,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2983, 'grad_norm': 0.8746771326855001, 'learning_rate': 5.507327779616656e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2122/2224 [3:07:36<08:34,  5.05s/it]
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2123/2224 [3:07:41<08:30,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2725, 'grad_norm': 0.8638658276183767, 'learning_rate': 5.400064156583118e-08, 'epoch': 0.95}
0: 
0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2123/2224 [3:07:41<08:30,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2124/2224 [3:07:46<08:26,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2597, 'grad_norm': 0.8466547629059364, 'learning_rate': 5.293849721837996e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2124/2224 [3:07:46<08:26,  5.06s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2125/2224 [3:07:51<08:20,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.25, 'grad_norm': 0.8277790145945599, 'learning_rate': 5.188684700692503e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2125/2224 [3:07:51<08:20,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2126/2224 [3:07:56<08:14,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2773, 'grad_norm': 0.8515827044148806, 'learning_rate': 5.0845693162318024e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2126/2224 [3:07:56<08:14,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2127/2224 [3:08:01<08:08,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2724, 'grad_norm': 0.8433205919911662, 'learning_rate': 4.981503789314501e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2127/2224 [3:08:01<08:08,  5.04s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2128/2224 [3:08:06<08:05,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.3016, 'grad_norm': 0.8587326403571041, 'learning_rate': 4.8794883385721023e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2128/2224 [3:08:06<08:05,  5.06s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2129/2224 [3:08:11<07:57,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2608, 'grad_norm': 0.8304412524173646, 'learning_rate': 4.7785231804086674e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2129/2224 [3:08:11<07:57,  5.03s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2130/2224 [3:08:16<07:51,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2809, 'grad_norm': 0.850840511963051, 'learning_rate': 4.678608529000206e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2130/2224 [3:08:16<07:51,  5.02s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2131/2224 [3:08:21<07:47,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2848, 'grad_norm': 0.9205416795372531, 'learning_rate': 4.5797445962943446e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2131/2224 [3:08:21<07:47,  5.02s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2132/2224 [3:08:26<07:43,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.273, 'grad_norm': 0.8443365557168562, 'learning_rate': 4.481931592009881e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2132/2224 [3:08:26<07:43,  5.04s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2133/2224 [3:08:31<07:39,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.3102, 'grad_norm': 0.8960685410432785, 'learning_rate': 4.385169723636229e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2133/2224 [3:08:31<07:39,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2134/2224 [3:08:36<07:36,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2873, 'grad_norm': 0.8479715925016671, 'learning_rate': 4.289459196433032e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2134/2224 [3:08:36<07:36,  5.07s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2135/2224 [3:08:41<07:28,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2926, 'grad_norm': 0.847818235463792, 'learning_rate': 4.194800213429773e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2135/2224 [3:08:41<07:28,  5.04s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2136/2224 [3:08:46<07:23,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.3016, 'grad_norm': 0.8806537959671836, 'learning_rate': 4.101192975425328e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2136/2224 [3:08:46<07:23,  5.03s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2137/2224 [3:08:51<07:17,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2837, 'grad_norm': 0.8447533222192006, 'learning_rate': 4.008637680987415e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2137/2224 [3:08:51<07:17,  5.03s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2138/2224 [3:08:56<07:11,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2683, 'grad_norm': 0.836705305974355, 'learning_rate': 3.917134526452482e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2138/2224 [3:08:56<07:11,  5.02s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2139/2224 [3:09:01<07:07,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2768, 'grad_norm': 0.8605068700227554, 'learning_rate': 3.826683705924816e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2139/2224 [3:09:01<07:07,  5.03s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2140/2224 [3:09:06<07:00,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2946, 'grad_norm': 0.8670750463289615, 'learning_rate': 3.737285411276659e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2140/2224 [3:09:06<07:00,  5.00s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2141/2224 [3:09:11<06:55,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2998, 'grad_norm': 0.8843632264215429, 'learning_rate': 3.648939832147369e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2141/2224 [3:09:11<06:55,  5.01s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2142/2224 [3:09:16<06:52,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2766, 'grad_norm': 0.8553255860765525, 'learning_rate': 3.561647155943204e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2142/2224 [3:09:16<06:52,  5.03s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2143/2224 [3:09:21<06:49,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2759, 'grad_norm': 0.855182692202082, 'learning_rate': 3.475407567837041e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2143/2224 [3:09:21<06:49,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2144/2224 [3:09:26<06:44,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8628479842328058, 'learning_rate': 3.390221250767767e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2144/2224 [3:09:26<06:44,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2145/2224 [3:09:31<06:38,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2864, 'grad_norm': 0.8682084249410453, 'learning_rate': 3.306088385439998e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2145/2224 [3:09:31<06:38,  5.05s/it]
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2146/2224 [3:09:36<06:30,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2594, 'grad_norm': 0.8639434071039828, 'learning_rate': 3.2230091503235864e-08, 'epoch': 0.96}
0: 
0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2146/2224 [3:09:36<06:30,  5.01s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2147/2224 [3:09:41<06:26,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2692, 'grad_norm': 0.850670775789578, 'learning_rate': 3.1409837216534456e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2147/2224 [3:09:41<06:26,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2148/2224 [3:09:46<06:21,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2795, 'grad_norm': 0.8360263681155151, 'learning_rate': 3.060012273429114e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2148/2224 [3:09:46<06:21,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2149/2224 [3:09:51<06:16,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.238, 'grad_norm': 0.8413465264495954, 'learning_rate': 2.9800949774141385e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2149/2224 [3:09:52<06:16,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2150/2224 [3:09:56<06:10,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.3, 'grad_norm': 0.8363614699860341, 'learning_rate': 2.901232003136134e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2150/2224 [3:09:56<06:10,  5.00s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2151/2224 [3:10:01<06:05,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2931, 'grad_norm': 0.8830193116608968, 'learning_rate': 2.8234235178859482e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2151/2224 [3:10:01<06:05,  5.01s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2152/2224 [3:10:07<06:02,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2699, 'grad_norm': 0.8669565398501924, 'learning_rate': 2.746669686717829e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2152/2224 [3:10:07<06:02,  5.03s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2153/2224 [3:10:11<05:54,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2828, 'grad_norm': 0.8695271140069988, 'learning_rate': 2.6709706724485363e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2153/2224 [3:10:11<05:54,  5.00s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2154/2224 [3:10:17<05:52,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2829, 'grad_norm': 0.8438923035393956, 'learning_rate': 2.5963266356574534e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2154/2224 [3:10:17<05:52,  5.04s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2155/2224 [3:10:22<05:45,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3117, 'grad_norm': 0.8702297863378218, 'learning_rate': 2.522737734685976e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2155/2224 [3:10:22<05:45,  5.01s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2156/2224 [3:10:27<05:40,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2995, 'grad_norm': 0.8588101711489848, 'learning_rate': 2.4502041256372898e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2156/2224 [3:10:27<05:40,  5.01s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2157/2224 [3:10:32<05:36,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2819, 'grad_norm': 0.861891548121014, 'learning_rate': 2.3787259623758718e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2157/2224 [3:10:32<05:36,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2158/2224 [3:10:37<05:31,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2666, 'grad_norm': 0.8579538361451642, 'learning_rate': 2.3083033965275447e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2158/2224 [3:10:37<05:31,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2159/2224 [3:10:42<05:26,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2911, 'grad_norm': 0.8537875888903995, 'learning_rate': 2.238936577478701e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2159/2224 [3:10:42<05:26,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2160/2224 [3:10:47<05:20,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2842, 'grad_norm': 0.887343354652525, 'learning_rate': 2.170625652376246e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2160/2224 [3:10:47<05:20,  5.00s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2161/2224 [3:10:52<05:15,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2617, 'grad_norm': 0.8686035255315964, 'learning_rate': 2.1033707661272108e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2161/2224 [3:10:52<05:15,  5.01s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2162/2224 [3:10:57<05:11,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2655, 'grad_norm': 0.8673813834974342, 'learning_rate': 2.0371720613985846e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2162/2224 [3:10:57<05:11,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2163/2224 [3:11:02<05:06,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2926, 'grad_norm': 0.8576572191163337, 'learning_rate': 1.9720296786167048e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2163/2224 [3:11:02<05:06,  5.03s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2164/2224 [3:11:07<05:00,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2729, 'grad_norm': 0.8413132344286016, 'learning_rate': 1.9079437559673673e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2164/2224 [3:11:07<05:00,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2165/2224 [3:11:12<04:56,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.3014, 'grad_norm': 0.8503978013019572, 'learning_rate': 1.8449144293950504e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2165/2224 [3:11:12<04:56,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2166/2224 [3:11:17<04:49,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2436, 'grad_norm': 0.8496935077162808, 'learning_rate': 1.78294183260308e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2166/2224 [3:11:17<04:49,  4.99s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2167/2224 [3:11:22<04:46,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2725, 'grad_norm': 0.8428787485915016, 'learning_rate': 1.7220260970531312e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2167/2224 [3:11:22<04:46,  5.02s/it]
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2168/2224 [3:11:27<04:41,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2982, 'grad_norm': 0.860188016112826, 'learning_rate': 1.662167351964894e-08, 'epoch': 0.97}
0: 
0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2168/2224 [3:11:27<04:41,  5.03s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2169/2224 [3:11:32<04:34,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2507, 'grad_norm': 0.8257280331071157, 'learning_rate': 1.6033657243160193e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2169/2224 [3:11:32<04:34,  5.00s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2170/2224 [3:11:37<04:30,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3139, 'grad_norm': 0.8934155132809547, 'learning_rate': 1.545621338841563e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2170/2224 [3:11:37<04:30,  5.01s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2171/2224 [3:11:42<04:26,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2671, 'grad_norm': 0.8767340737522106, 'learning_rate': 1.48893431803393e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2171/2224 [3:11:42<04:26,  5.02s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2172/2224 [3:11:47<04:19,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2852, 'grad_norm': 0.8396021607518019, 'learning_rate': 1.4333047821424862e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2172/2224 [3:11:47<04:19,  5.00s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2173/2224 [3:11:52<04:15,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2976, 'grad_norm': 0.8619359639962754, 'learning_rate': 1.3787328491735586e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2173/2224 [3:11:52<04:15,  5.01s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2174/2224 [3:11:57<04:11,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2869, 'grad_norm': 0.8664737794633187, 'learning_rate': 1.3252186348897688e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2174/2224 [3:11:57<04:11,  5.03s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2175/2224 [3:12:02<04:06,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.269, 'grad_norm': 0.8682655620744438, 'learning_rate': 1.2727622528101447e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2175/2224 [3:12:02<04:06,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2176/2224 [3:12:07<04:02,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2853, 'grad_norm': 0.8218792905361455, 'learning_rate': 1.2213638142096751e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2176/2224 [3:12:07<04:02,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2177/2224 [3:12:12<03:56,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2807, 'grad_norm': 0.879550539639064, 'learning_rate': 1.1710234281192555e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2177/2224 [3:12:12<03:56,  5.03s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2178/2224 [3:12:17<03:51,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2825, 'grad_norm': 0.843469509370035, 'learning_rate': 1.1217412013252437e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2178/2224 [3:12:17<03:51,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2179/2224 [3:12:22<03:46,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2863, 'grad_norm': 0.8549914347146637, 'learning_rate': 1.0735172383693481e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2179/2224 [3:12:22<03:46,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2180/2224 [3:12:27<03:41,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.3039, 'grad_norm': 0.8649499692268221, 'learning_rate': 1.026351641548462e-08, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2180/2224 [3:12:27<03:41,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2181/2224 [3:12:32<03:36,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2685, 'grad_norm': 0.873050421138926, 'learning_rate': 9.802445109142745e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2181/2224 [3:12:32<03:36,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2182/2224 [3:12:37<03:31,  5.03s/it]
0:                                                      
0: 
0: {'loss': 0.2746, 'grad_norm': 0.8857429567384857, 'learning_rate': 9.35195944273326e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2182/2224 [3:12:37<03:31,  5.03s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2183/2224 [3:12:42<03:26,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2705, 'grad_norm': 0.8560335414769286, 'learning_rate': 8.912060371865094e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2183/2224 [3:12:42<03:26,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2184/2224 [3:12:47<03:22,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2788, 'grad_norm': 0.8457627250953583, 'learning_rate': 8.482748829690134e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2184/2224 [3:12:47<03:22,  5.05s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2185/2224 [3:12:52<03:17,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2772, 'grad_norm': 0.9055091498054102, 'learning_rate': 8.06402572690157e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2185/2224 [3:12:52<03:17,  5.05s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2186/2224 [3:12:57<03:11,  5.04s/it]
0:                                                      
0: 
0: {'loss': 0.2822, 'grad_norm': 0.8465551528852893, 'learning_rate': 7.655891951730555e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2186/2224 [3:12:57<03:11,  5.04s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2187/2224 [3:13:03<03:07,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2738, 'grad_norm': 0.8283570238648537, 'learning_rate': 7.258348369946211e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2187/2224 [3:13:03<03:07,  5.06s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2188/2224 [3:13:07<03:00,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2584, 'grad_norm': 0.8708025241411255, 'learning_rate': 6.871395824852855e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2188/2224 [3:13:07<03:00,  5.02s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2189/2224 [3:13:12<02:54,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2536, 'grad_norm': 0.8500488135029931, 'learning_rate': 6.49503513728722e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2189/2224 [3:13:12<02:54,  4.99s/it]
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2190/2224 [3:13:17<02:50,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.3178, 'grad_norm': 0.8520872970991265, 'learning_rate': 6.129267105618453e-09, 'epoch': 0.98}
0: 
0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2190/2224 [3:13:17<02:50,  5.01s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2191/2224 [3:13:22<02:43,  4.97s/it]
0:                                                      
0: 
0: {'loss': 0.3002, 'grad_norm': 0.8963944014859917, 'learning_rate': 5.774092505745343e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2191/2224 [3:13:22<02:43,  4.97s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2192/2224 [3:13:27<02:37,  4.93s/it]
0:                                                      
0: 
0: {'loss': 0.2633, 'grad_norm': 0.841941517521239, 'learning_rate': 5.4295120910952126e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2192/2224 [3:13:27<02:37,  4.93s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2193/2224 [3:13:32<02:33,  4.97s/it]
0:                                                      
0: 
0: {'loss': 0.2804, 'grad_norm': 0.8814668449851104, 'learning_rate': 5.0955265926222465e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2193/2224 [3:13:32<02:33,  4.97s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2194/2224 [3:13:37<02:30,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2843, 'grad_norm': 0.859956097475393, 'learning_rate': 4.772136718804721e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2194/2224 [3:13:37<02:30,  5.00s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2195/2224 [3:13:42<02:25,  5.00s/it]
0:                                                      
0: 
0: {'loss': 0.2733, 'grad_norm': 0.8679922149093875, 'learning_rate': 4.459343155645557e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2195/2224 [3:13:42<02:25,  5.00s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2196/2224 [3:13:47<02:20,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2716, 'grad_norm': 0.8773909521864101, 'learning_rate': 4.1571465666701006e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2196/2224 [3:13:47<02:20,  5.01s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2197/2224 [3:13:52<02:15,  5.01s/it]
0:                                                      
0: 
0: {'loss': 0.2459, 'grad_norm': 0.8331569034923787, 'learning_rate': 3.865547592923347e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2197/2224 [3:13:52<02:15,  5.01s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2198/2224 [3:13:57<02:10,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.283, 'grad_norm': 0.8213384353469049, 'learning_rate': 3.584546852970494e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2198/2224 [3:13:57<02:10,  5.02s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2199/2224 [3:14:02<02:04,  4.99s/it]
0:                                                      
0: 
0: {'loss': 0.2662, 'grad_norm': 0.8578181419644602, 'learning_rate': 3.314144942894726e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2199/2224 [3:14:02<02:04,  4.99s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2200/2224 [3:14:07<02:00,  5.02s/it]
0:                                                      
0: 
0: {'loss': 0.2956, 'grad_norm': 0.8470643408346599, 'learning_rate': 3.0543424362960984e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2200/2224 [3:14:07<02:00,  5.02s/it]
0: /usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
0:   return fn(*args, **kwargs)
0: /usr/local/lib/python3.10/dist-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.
0:   warnings.warn(
0: /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:294: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
0:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2201/2224 [3:15:11<08:37, 22.52s/it]
0:                                                      
0: 
0: {'loss': 0.2864, 'grad_norm': 0.8834848354202454, 'learning_rate': 2.8051398842898757e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2201/2224 [3:15:11<08:37, 22.52s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2202/2224 [3:15:16<06:20, 17.28s/it]
0:                                                      
0: 
0: {'loss': 0.2778, 'grad_norm': 0.8588361998187435, 'learning_rate': 2.5665378155070865e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2202/2224 [3:15:16<06:20, 17.28s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2203/2224 [3:15:21<04:45, 13.59s/it]
0:                                                      
0: 
0: {'loss': 0.2751, 'grad_norm': 0.8555934675410771, 'learning_rate': 2.3385367360900802e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2203/2224 [3:15:21<04:45, 13.59s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2204/2224 [3:15:26<03:40, 11.03s/it]
0:                                                      
0: 
0: {'loss': 0.3177, 'grad_norm': 0.8888013468088439, 'learning_rate': 2.1211371296947503e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2204/2224 [3:15:26<03:40, 11.03s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2205/2224 [3:15:31<02:55,  9.23s/it]
0:                                                      
0: 
0: {'loss': 0.2868, 'grad_norm': 0.8789140762895826, 'learning_rate': 1.9143394574883123e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2205/2224 [3:15:31<02:55,  9.23s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2206/2224 [3:15:36<02:23,  7.99s/it]
0:                                                      
0: 
0: {'loss': 0.3036, 'grad_norm': 0.9071685219209783, 'learning_rate': 1.7181441581481938e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2206/2224 [3:15:36<02:23,  7.99s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2207/2224 [3:15:41<01:59,  7.06s/it]
0:                                                      
0: 
0: {'loss': 0.2634, 'grad_norm': 0.8824697080647946, 'learning_rate': 1.5325516478598147e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2207/2224 [3:15:41<01:59,  7.06s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2208/2224 [3:15:46<01:43,  6.46s/it]
0:                                                      
0: 
0: {'loss': 0.3053, 'grad_norm': 0.8523644135739691, 'learning_rate': 1.357562320319361e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2208/2224 [3:15:46<01:43,  6.46s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2209/2224 [3:15:51<01:30,  6.01s/it]
0:                                                      
0: 
0: {'loss': 0.2966, 'grad_norm': 0.8557203699379787, 'learning_rate': 1.1931765467282363e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2209/2224 [3:15:51<01:30,  6.01s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2210/2224 [3:15:56<01:19,  5.69s/it]
0:                                                      
0: 
0: {'loss': 0.2814, 'grad_norm': 0.8304483422713947, 'learning_rate': 1.0393946757969452e-09, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2210/2224 [3:15:56<01:19,  5.69s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2211/2224 [3:16:01<01:12,  5.58s/it]
0:                                                      
0: 
0: {'loss': 0.2871, 'grad_norm': 0.8551937915724575, 'learning_rate': 8.962170337395437e-10, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2211/2224 [3:16:01<01:12,  5.58s/it]
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2212/2224 [3:16:06<01:04,  5.41s/it]
0:                                                      
0: 
0: {'loss': 0.2876, 'grad_norm': 0.8694623493780885, 'learning_rate': 7.636439242780791e-10, 'epoch': 0.99}
0: 
0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2212/2224 [3:16:06<01:04,  5.41s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2213/2224 [3:16:11<00:58,  5.31s/it]
0:                                                      
0: 
0: {'loss': 0.2886, 'grad_norm': 0.836904906451561, 'learning_rate': 6.416756286381498e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2213/2224 [3:16:11<00:58,  5.31s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2214/2224 [3:16:16<00:52,  5.25s/it]
0:                                                      
0: 
0: {'loss': 0.2781, 'grad_norm': 0.8282317663394874, 'learning_rate': 5.3031240554835e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2214/2224 [3:16:16<00:52,  5.25s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2215/2224 [3:16:21<00:46,  5.19s/it]
0:                                                      
0: 
0: {'loss': 0.296, 'grad_norm': 0.8567154807980949, 'learning_rate': 4.295544912430449e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2215/2224 [3:16:21<00:46,  5.19s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2216/2224 [3:16:26<00:41,  5.14s/it]
0:                                                      
0: 
0: {'loss': 0.2547, 'grad_norm': 0.8384924944052801, 'learning_rate': 3.394020994584857e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2216/2224 [3:16:26<00:41,  5.14s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2217/2224 [3:16:31<00:35,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2616, 'grad_norm': 0.824744111263469, 'learning_rate': 2.5985542143336373e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2217/2224 [3:16:31<00:35,  5.07s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2218/2224 [3:16:36<00:30,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2806, 'grad_norm': 0.8623517840087307, 'learning_rate': 1.9091462590881126e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2218/2224 [3:16:36<00:30,  5.06s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2219/2224 [3:16:41<00:25,  5.06s/it]
0:                                                      
0: 
0: {'loss': 0.2929, 'grad_norm': 0.8444598999918295, 'learning_rate': 1.3257985912895622e-10, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2219/2224 [3:16:41<00:25,  5.06s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2220/2224 [3:16:46<00:20,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2704, 'grad_norm': 0.8586842988589626, 'learning_rate': 8.485124483814667e-11, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2220/2224 [3:16:46<00:20,  5.07s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2221/2224 [3:16:52<00:15,  5.08s/it]
0:                                                      
0: 
0: {'loss': 0.2753, 'grad_norm': 0.844156555520163, 'learning_rate': 4.7728884282061126e-11, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2221/2224 [3:16:52<00:15,  5.08s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2222/2224 [3:16:57<00:10,  5.07s/it]
0:                                                      
0: 
0: {'loss': 0.2847, 'grad_norm': 0.8478994377222505, 'learning_rate': 2.1212856208263632e-11, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2222/2224 [3:16:57<00:10,  5.07s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2223/2224 [3:17:02<00:05,  5.05s/it]
0:                                                      
0: 
0: {'loss': 0.2959, 'grad_norm': 0.8601026531539789, 'learning_rate': 5.303216864538385e-12, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2223/2224 [3:17:02<00:05,  5.05s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2224/2224 [3:17:07<00:00,  5.11s/it]
0:                                                      
0: 
0: {'loss': 0.2974, 'grad_norm': 0.8608845558322611, 'learning_rate': 0.0, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2224/2224 [3:17:07<00:00,  5.11s/it]
0:                                                      
0: 
0: {'train_runtime': 11829.1292, 'train_samples_per_second': 6.017, 'train_steps_per_second': 0.188, 'train_loss': 0.33831614204313903, 'epoch': 1.0}
0: 
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2224/2224 [3:17:07<00:00,  5.11s/it]
0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2224/2224 [3:17:07<00:00,  5.32s/it]
0: 
0: Rank 0: 
0:  
0: Only save projectors: False
0: 
0: Rank 0: 
0:  
0: Model saved to /capstor/scratch/cscs/ndeperr/checkpoints/radvlm-sft-cs-long
0: 
0: [1;34mwandb[0m: 
0: [1;34mwandb[0m: ðŸš€ View run [33mradvlm-sft-cs-long[0m at: [34mhttps://wandb.ai/krauthammerlab/huggingface/runs/006og4d0[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250709_225327-006og4d0/logs[0m
